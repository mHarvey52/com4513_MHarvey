[{"tabular": ["    &    &  W2V  &  GloVe ", " Dataset  &  Total  &  found%  &  found% ", " WN  &  40943  &  9.7%  &  51.3% ", " FB15k  &  14951  &  4.0%  &  20.3%  "], "ref_sec": [["<section> <title> 1 Introduction </title>  A surprising result of work on vector-space word embeddings is that word representations that are learned from a large training corpus display semantic regularities in the form of linear vector translations.", "For example, \\newcite mikolov2013linguistic show", "that using their induced word vector representations, $ \\textit{king}-\\textit{man}+\\textit{woman}\\approx\\textit{queen} $ .", "Such a structure is appealing because it provides an interpretation to the distributional vector space through lexical-semantic analogical inferences.", "\\newline Concurrent to that work, \\newcite bordes2013translating proposed translating embeddings (TransE), which takes a pre-existing semantic hierarchy as input and embeds its structure into a vector space.", "In their model, the linear relationship between two entities that are in some semantic relation to each other is an explicit part of the model\u2019s objective function.", "For example, given a relation such as $ \\textit{won}(\\textit{Germany},\\textit{FIFA Worldcup}) $ , the TransE model learns vector representations for won , Germany , and FIFA Worldcup such that $ \\textit{Germany}+\\textit{won}\\approx\\textit{FIFA Worldcup} $ .", "\\newline A natural next step is to attempt to integrate the two approaches in order to develop a representation that is informed by both unstructured text and a structured knowledge base [@bib:faruqui2014retrofitting,xu-etal-2014,fried-duh-2014,yang2014embedding] .", "However, existing work makes a crucial assumption\u2014that reliable distributional vectors are available for all of the entities in the hierarchy being modeled.", "Unfortunately, this assumption does not hold in practice; when moving to a new domain with a new knowledge base, for example, there will likely be many entities or phrases for which there is no distributional information in the training corpus."]], "target": "This important problem is illustrated in Table , where most of the entities from WordNet and Freebase are seen to be missing from the distributional vectors derived using Word2Vec and GloVe trained on the Google News corpus. Even when the entities are found, they may not have occurred enough times in the training corpus for their vector representation to be reliable. What is needed is a method to derive entity representations that works well for both common and rare entities."}, {"tabular": ["  Model  &  Subset  &  $ \\textbf{R}_{2}@1 $  &  $ \\textbf{R}_{10}@1 $  &  $ \\textbf{R}_{10}@2 $  &  $ \\textbf{R}_{10}@5 $ ", " U2R-IMN  &  1 utt.  &  0.936  &  0.733  &  0.863  &  0.974 ", " U2U-IMN  &  1 utt.  &  0.937  &  0.737  &  0.863  &  0.972 ", " U2R-IMN  &  2 utt.  &  0.952  &  0.823  &  0.904  &  0.979 ", " U2U-IMN  &  2 utt.  &  0.956  &  0.831  &  0.911  &  0.984 ", " U2R-IMN  &  3 utt.  &  0.965  &  0.873  &  0.923  &  0.982 ", " U2U-IMN  &  3 utt.  &  0.976  &  0.904  &  0.955  &  0.994  "], "ref_sec": [["<section> <title> I Introduction </title>  Building a chatbot that can converse naturally with humans on open-domain topics is a challenging yet intriguing problem in artificial intelligence.", "Recently, human-computer conversation has attracted increasing attention due to its promising potential and commercial value [@bib:DBLP:journals/sigkdd/ChenLYT17,DBLP:conf/aaai/YoungCCZBH18,DBLP:journals/corr/abs-1812-00686] .", "Existing approaches to building chatbots include generation-based methods [@bib:DBLP:conf/acl/ShangLL15,DBLP:conf/aaai/SerbanSBCP16,DBLP:journals/www/ZhangZWZL19] and retrieval-based methods [@bib:DBLP:conf/sigdial/LowePSP15,DBLP:journals/corr/KadlecSK15,DBLP:journals/dad/LowePSCLP17,DBLP:conf/acl/WuWXZL17,DBLP:conf/acl/WuLCZDYZL18,DBLP:conf/coling/ZhangLZZL18,gu-etal-2019-dually] .", "Response selection, which aims to select the best-matched response from a set of candidates given the context of a conversation, is the key technique for building retrieval-based chatbots.", "\\newline In recent years, neural networks have been adopted to calculate the matching degrees between a context and its response candidates for response selection.", "Existing studies on neural network-based multi-turn response selection follow either context-to-response matching or utterance-to-response matching frameworks.", "The former adopts a coarse granularity for both contexts and responses that concatenates all utterances in a context or in a response into a single word sequence for matching degree calculation [@bib:DBLP:conf/sigdial/LowePSP15,DBLP:journals/corr/KadlecSK15,DBLP:journals/dad/LowePSCLP17] .", "The latter adopts a fine granularity for contexts that separates a context into utterances but still concatenates all utterances in a response [@bib:DBLP:conf/acl/WuWXZL17,DBLP:conf/acl/WuLCZDYZL18,DBLP:conf/coling/ZhangLZZL18] .", "However, both contexts and responses may contain multiple utterances in the response selection task, as illustrated in Table [@ref:LABEL:tab1] .", "Both frameworks mentioned above neglect the relationships among the utterances in a response.", "\\newline Therefore, this paper proposes a neural network model named the utterance-to-utterance interactive matching network (U2U-IMN) for multi-turn response selection in retrieval-based chatbots.", "This model follows a new utterance-to-utterance (U2U) matching framework in order to deal with the situation in which both contexts and responses may contain multiple utterances.", "Different from the context-to-response matching and utterance-to-response matching frameworks, the U2U matching framework treats both contexts and responses as sequences of utterances when calculating the matching degrees between them.", "Therefore, the U2U-IMN model first encodes each utterance separately for a context-response pair.", "A previous study on natural language inference (NLI) [@bib:DBLP:conf/acl/ChenZLWJI17] found that performing interactions between sentence pairs can provide useful matching information.", "Inspired by this, an attention-based interaction between the context and the response is conducted to collect the matching information between them.", "Here, the interaction is global (i.e., crossing utterance boundaries) and bidirectional (i.e., considering both context-to-response and response-to-context directions) in order to enrich the relevance representations of contexts and responses.", "The distances between context and response utterances are employed as a prior component when calculating the attention weights in order to distinguish the semantic contributions of different utterances in a context.", "Finally, sentence-level aggregation and context-response-level aggregation are executed in turn to obtain the feature vector for matching degree prediction.", "\\newline Our proposed methods were evaluated on two English datasets, the Ubuntu Dialogue Corpus V1 [@bib:DBLP:conf/sigdial/LowePSP15] and Ubuntu Dialogue Corpus V2 [@bib:DBLP:journals/dad/LowePSCLP17] , along with two Chinese datasets, the Douban Conversation Corpus [@bib:DBLP:conf/acl/WuWXZL17] and E-commerce Dialogue Corpus [@bib:DBLP:conf/coling/ZhangLZZL18] , which are all public datasets widely used in studies on multi-turn conversation.", "The results showed that our proposed method outperformed baseline methods on all metrics, achieved a new state-of-the-art performance, and demonstrated compatibility across domains for multi-turn response selection.", "\\newline In summary, the main contributions of this paper are twofold.", "First, this paper proposes a neural network model named U2U-IMN to deal with the situation in which both contexts and responses may contain multiple utterances.", "In this model, a matching module with attention-based global and bidirectional interactions is designed to collect the matching information between context and response utterances.", "Second, experimental results demonstrate that our proposed method achieves a new state-of-the-art performance on four public datasets for multi-turn response selection.", "\\newline  </section>"], ["<section> <title> II Related Work </title>  Chatbots aim to engage users in human-computer conversations in the open domain and are currently receiving increasing attention because they can target unstructured dialogue without a priori logical representation of the information exchanged during the conversation.", "Existing work on building chatbots includes generation-based methods [@bib:DBLP:conf/acl/ShangLL15,DBLP:conf/aaai/SerbanSBCP16,DBLP:journals/www/ZhangZWZL19,DBLP:conf/acl/ZhuCZWL19,DBLP:conf/ijcai/SongZCWL19] and retrieval-based methods [@bib:DBLP:conf/sigdial/LowePSP15,DBLP:journals/corr/KadlecSK15,DBLP:journals/dad/LowePSCLP17,DBLP:conf/acl/WuWXZL17,DBLP:conf/acl/WuLCZDYZL18,DBLP:conf/coling/ZhangLZZL18] .", "Generation-based models maximize the probability of generating a response given the previous dialogue.", "This approach enables the incorporation of rich context when mapping between consecutive dialogue turns.", "Retrieval-based chatbots have the advantage of generating informative and fluent responses because they select a proper response for the current conversation from a repository by means of response selection algorithms.", "\\newline Early studies on retrieval-based chatbots focused on single-turn conversation [@bib:DBLP:conf/emnlp/WangLLC13,DBLP:journals/corr/JiLL14] .", "Recently, researchers have extended their attention to multi-turn conversation, which is more practical for real applications.", "A straightforward approach to multi-turn conversation is to match a response with the literal concatenation of context utterances [@bib:DBLP:conf/sigdial/LowePSP15,DBLP:journals/corr/KadlecSK15,DBLP:journals/dad/LowePSCLP17] .", "Then, a multi-view model [@bib:DBLP:conf/emnlp/ZhouDWZYTLY16] , including an utterance view and a word view, was studied.", "Wu et al. [@bib:DBLP:conf/acl/WuWXZL17] proposed the sequential matching network (SMN), which first matched the response with each context utterance and then accumulated the matching information using a recurrent neural network (RNN).", "Zhang et al. [@bib:DBLP:conf/coling/ZhangLZZL18] employed self-matching attention to route the vital information in each utterance based on SMN.", "The method of constructed representations at different granularities with stacked self-attention [@bib:DBLP:conf/acl/WuLCZDYZL18] has also been presented.", "\\newline Our proposed U2U-IMN model has three main differences from the studies mentioned above.", "(1) U2U-IMN adopts a more fine-grained utterance-to-utterance (U2U) matching framework, while previous studies followed the framework of either context-to-response matching or utterance-to-response matching.", "(2) U2U-IMN derives the matching information between contexts and responses through global and bidirectional interactions, while the interactions used in previous studies were usually local and unidirectional [@bib:DBLP:conf/acl/WuWXZL17] .", "(3) U2U-IMN employs the distances between context and response utterances as a prior component for calculating the attention weights in the interactive matching module.", "\\newline  </section>"], ["<section> <title> III Utterance-to-Utterance Interactive Matching Network </title>  <subsection> <title> III-A Model Overview </title> Given a dialogue dataset $ \\mathcal{D} $ , an example of the dataset can be represented as $ (c,r,y) $ .", "Specifically, $ c=\\{u^{c}_{1},u^{c}_{2},...,u^{c}_{n_{c}}\\} $ represents a context with $ \\{u^{c}_{m}\\}_{m=1}^{n_{c}} $ as its utterances and $ n_{c} $ as its utterance number.", "Similarly, $ r=\\{u^{r}_{1},u^{r}_{2},...,u^{r}_{n_{r}}\\} $ represents a response candidate with $ \\{u^{r}_{n}\\}_{n=1}^{n_{r}} $ as its utterances and $ n_{r} $ as its utterance number.", "Here, both the context and the response may be composed of multiple utterances, and the utterances in $ c $ and $ r $ are both chronologically ordered.", "$ y\\in\\{0,1\\} $ denotes a label.", "$ y=1 $ indicates that $ r $ is a proper response for $ c $ ; otherwise, $ y=0 $ .", "Our goal is to learn a matching model $ g(c,r) $ from $ \\mathcal{D} $ .", "For any context-response pair $ (c,r) $ , $ g(c,r) $ measures the matching degree between $ c $ and $ r $ .", "We learn $ g(c,r) $ by minimizing the sigmoid cross-entropy on $ \\mathcal{D} $ .", "Let $ \\Theta $ denote the set of model parameters.", "Then, the objective function $ \\mathcal{L}(\\mathcal{D},\\Theta) $ of learning can be formulated as \\newline <equationgroup> <equation> $ \\mathcal{L}(\\mathcal{D},\\Theta)=-\\sum_{(c,r,y)\\in\\mathcal{D}}[% ylog(g(c,r)) $ $ \\mathcal{L}(\\mathcal{D},\\Theta)=-\\sum_{(c,r,y)\\in\\mathcal{D}}[ $ $  ylog(g(c,r))", "$ </equation> <equation> $ +(1-y)log(1-g(c,r))].", "$ $ +(1-y)log(1-g(c,r))].", "$ </equation> </equationgroup> \\newline The U2U-IMN model is designed to calculate the matching degree $ g(c,r) $ for a context-response pair.", "It is composed of a word representation module, a sentence encoding module, an interactive matching module, an aggregation module and a prediction module, as shown in Fig. [@ref:LABEL:fig1] .", "Details about each module are provided in the following subsections.", "\\newline </subsection> <subsection> <title> III-B Word Representation Module </title> One challenge of word representation for dialogue is the large number of out-of-vocabulary (OOV) words.", "To address this issue, we combine the general pretrained word embeddings with those estimated on a task-specific training set [@bib:DBLP:journals/corr/abs-1802-02614] .", "To further enhance the word embeddings, a convolutional neural network (CNN) is employed to model the morphology information at the character-level [@bib:DBLP:conf/emnlp/LeeHLZ17] .", "\\newline Formally, the word embeddings of the {m} -th utterance in a context and the {n} -th utterance in a response candidate are denoted $ \\textbf{U}_{m}^{c}=\\{\\textbf{u}_{m,i}^{c}\\}_{i=1}^{l_{u_{m}^{c}}} $ and $ \\textbf{U}_{n}^{r}=\\{\\textbf{u}_{n,j}^{r}\\}_{j=1}^{l_{u_{n}^{r}}} $ , respectively, where $ l_{u_{m}^{c}} $ and $ l_{u_{n}^{r}} $ are utterance lengths.", "Each $ \\textbf{u}_{m,i}^{c} $ or $ \\textbf{u}_{n,j}^{r}\\in\\mathbb{R}^{d} $ is an embedding vector of {d} dimensions.", "\\newline </subsection> <subsection> <title> III-C Sentence Encoding Module </title> First, each utterance in a context or in a response candidate is encoded by a bidirectional long short-term memory network (BiLSTM) [@bib:DBLP:journals/neco/HochreiterS97] .", "We denote the calculations as follows: \\newline <equationgroup> <equation> $ \\bar{\\textbf{u}}_{m,i}^{c}=\\textbf{BiLSTM}(\\textbf{U}_{m}^{c},i),% i\\in\\{1,...,l_{u_{m}^{c}}\\}, $ $ \\bar{\\textbf{u}}_{m,i}^{c} $ $ =\\textbf{BiLSTM}(\\textbf{U}_{m}^{c},i),i\\in\\{1,...,l_{u_{m}^{c}}\\}, $ </equation> <equation> $ \\bar{\\textbf{u}}_{n,j}^{r}=\\textbf{BiLSTM}(\\textbf{U}_{n}^{r},j),% j\\in\\{1,...,l_{u_{n}^{r}}\\}. $ $ \\bar{\\textbf{u}}_{n,j}^{r} $ $ =\\textbf{BiLSTM}(\\textbf{U}_{n}^{r},j),j\\in\\{1,...,l_{u_{n}^{r}}\\}. $ </equation> </equationgroup> The parameters in these two BiLSTMs are shared in our implementation.", "\\newline To consider long-term dependency and highlight the semantic influences among adjacent words at the same time, a self-attention layer [@bib:DBLP:conf/iclr/YuDLZ00L18] with a Gaussian prior [@bib:guo2019gaussian] is employed to enhance the performance of BiLSTM-based sentence encoding.", "For a word in a context utterance, its representation after self-attention is calculated as \\newline <equation> $ \\tilde{\\textbf{u}}_{m,i}^{c}=\\sum_{j}\\textbf{Softmax}(-|wd^{2}_{i,j}+b|+\\bar{% \\textbf{u}}_{m,i}^{c\\top}\\cdot\\bar{\\textbf{u}}_{m,j}^{c})\\bar{\\textbf{u}}_{m,j% }^{c}, $ </equation> where $ d_{i,j} $ is the word-level distance between the $ i $ -th word and the $ j $ -th word, and $ w $ and $ b $ are scalar parameters estimated by model training.", "Similarly, for each word in a response utterance, we have \\newline <equation> $ \\tilde{\\textbf{u}}_{n,j}^{r}=\\sum_{i}\\textbf{Softmax}(-|wd^{2}_{i,j}+b|+\\bar{% \\textbf{u}}_{n,i}^{r\\top}\\cdot\\bar{\\textbf{u}}_{n,j}^{r})\\bar{\\textbf{u}}_{n,i% }^{r}. $ </equation> \\newline Finally, the outputs of the sentence encoding module are $ \\widetilde{\\textbf{U}}_{m}^{c}=\\{\\tilde{\\textbf{u}}_{m,i}^{c}\\}_{i=1}^{l_{u_{m% }^{c}}},m\\in\\{1,...,n_{c}\\} $ for context utterances and $ \\widetilde{\\textbf{U}}_{n}^{r}=\\{\\tilde{\\textbf{u}}_{n,j}^{r}\\}_{j=1}^{l_{u_{n% }^{r}}},n\\in\\{1,...,n_{r}\\} $ for response utterances.", "\\newline </subsection> <subsection> <title> III-D Interactive Matching Module </title> Interactions between the context and the response provide useful information for determining the matching degree between them.", "Unlike previous work [@bib:DBLP:conf/acl/WuWXZL17,DBLP:conf/acl/WuLCZDYZL18,DBLP:conf/coling/ZhangLZZL18] , which matched the response to each utterance in the context separately, the U2U-IMN model matches the whole response with the whole context in a global and bidirectional way.", "Both the context and the response are treated as single word sequences, and attention weights are calculated between every word in the context and every word in the response.", "Then, the relevance representations are derived along both the context-to-response and response-to-context directions.", "This global and bidirectional strategy is expected to help neglect the irrelevant utterances and enrich the relevance representations between the context and the response.", "Furthermore, considering that the context utterances adjacent to the response may contribute more in response selection than the distant ones, we propose to introduce an exponential prior based on the distance between context and response utterances when calculating the attention weights.", "\\newline First, the context representation $ \\widetilde{\\textbf{C}}=[\\tilde{\\textbf{c}}_{1},...,\\tilde{\\textbf{c}}_{l_{c}}] $ is formed by concatenating all context utterance representations $ \\{\\widetilde{\\textbf{U}}_{m}^{c}\\}_{m=1}^{n_{c}} $ , where $ l_{c}=\\sum_{m=1}^{n_{c}}l_{u_{m}^{c}} $ is the total number of words in the context.", "Similarly, we obtain $ \\widetilde{\\textbf{R}}=[\\tilde{\\textbf{r}}_{1},...,\\tilde{\\textbf{r}}_{l_{r}}] $ and $ l_{r}=\\sum_{n=1}^{n_{r}}l_{u_{n}^{r}} $ for the response.", "\\newline Then, an attention-based alignment is employed to collect relevance information between these two sequences by computing the attention weight between each pair of {$ \\tilde{\\textbf{c}}_{i},\\tilde{\\textbf{r}}_{j} $ } as \\newline <equation> $ e_{ij}\\propto\\phi(D_{i,j})\\cdot\\exp(\\tilde{\\textbf{c}}_{i}^{\\top}\\cdot\\tilde{% \\textbf{r}}_{j}), $ </equation> where $ D_{i,j} $ is the sentence-level distance between these two words, and $ \\phi(D)=e^{-WD+B} $ is an exponential prior with decay constant $ W $ and initial value $ e^{B} $ .", "Here, $ W $ and $ B $ are model parameters that need to be estimated.", "\\newline Next, the attention weights $ e_{ij} $ computed above are used to bidirectionally obtain the local relevance between a context and a response.", "For a word in the context, its context-to-response relevance representation carried by the response is composed using $ e_{ij} $ as \\newline <equationgroup> <equation> $ \\hat{\\textbf{c}}_{i}=\\sum_{j}e_{ij}\\tilde{\\textbf{r}}_{j} $ $ \\hat{\\textbf{c}}_{i}= $ $ \\sum_{j}e_{ij}\\tilde{\\textbf{r}}_{j} $ </equation> <equation> $ =\\sum_{j}\\textbf{Softmax}(-WD_{i,j}+B+\\tilde{\\textbf{c}}_{i}^{% \\top}\\cdot\\tilde{\\textbf{r}}_{j})\\tilde{\\textbf{r}}_{j}, $ $ = $ $ \\sum_{j}\\textbf{Softmax}(-WD_{i,j}+B+\\tilde{\\textbf{c}}_{i}^{\\top% }\\cdot\\tilde{\\textbf{r}}_{j})\\tilde{\\textbf{r}}_{j}, $ </equation> </equationgroup> where the contents in $ \\{\\tilde{\\textbf{r}}_{j}\\}_{j=1}^{l_{r}} $ relevant to $ \\tilde{\\textbf{c}}_{i} $ are selected to form $ \\hat{\\textbf{c}}_{i} $ .", "The same calculation is also performed for each word in the response to form the response-to-context representation as \\newline <equationgroup> <equation> $ \\hat{\\textbf{r}}_{j}=\\sum_{i}e_{ij}\\tilde{\\textbf{c}}_{i} $ $ \\hat{\\textbf{r}}_{j}= $ $ \\sum_{i}e_{ij}\\tilde{\\textbf{c}}_{i} $ </equation> <equation> $ =\\sum_{i}\\textbf{Softmax}(-WD_{i,j}+B+\\tilde{\\textbf{c}}_{i}^{% \\top}\\cdot\\tilde{\\textbf{r}}_{j})\\tilde{\\textbf{c}}_{i}. $ $ = $ $ \\sum_{i}\\textbf{Softmax}(-WD_{i,j}+B+\\tilde{\\textbf{c}}_{i}^{\\top% }\\cdot\\tilde{\\textbf{r}}_{j})\\tilde{\\textbf{c}}_{i}. $ </equation> </equationgroup> For the whole context and the whole response, we have $ \\widehat{\\textbf{C}}=\\{\\hat{\\textbf{c}}_{i}\\}_{i=1}^{l_{c}} $ and $ \\widehat{\\textbf{R}}=\\{\\hat{\\textbf{r}}_{j}\\}_{j=1}^{l_{r}} $ .", "Following a previous study on interactive matching for NLI [@bib:DBLP:conf/acl/ChenZLWJI17] , we compute the differences and the element-wise products between {$ \\widetilde{\\textbf{C}},\\widehat{\\textbf{C}} $ } and between {$ \\widetilde{\\textbf{R}},\\widehat{\\textbf{R}} $ }.", "The differences and the element-wise products are then concatenated with the original vectors to obtain the enhanced representations as follows: \\newline <equationgroup> <equation> $ \\textbf{C}^{mat}=[\\widetilde{\\textbf{C}},\\widehat{\\textbf{C}},% \\widetilde{\\textbf{C}}-\\widehat{\\textbf{C}},\\widetilde{\\textbf{C}}\\odot% \\widehat{\\textbf{C}}], $ $ \\textbf{C}^{mat} $ $ =[\\widetilde{\\textbf{C}},\\widehat{\\textbf{C}},\\widetilde{\\textbf{% C}}-\\widehat{\\textbf{C}},\\widetilde{\\textbf{C}}\\odot\\widehat{\\textbf{C}}], $ </equation> <equation> $ \\textbf{R}^{mat}=[\\widetilde{\\textbf{R}},\\widehat{\\textbf{R}},% \\widetilde{\\textbf{R}}-\\widehat{\\textbf{R}},\\widetilde{\\textbf{R}}\\odot% \\widehat{\\textbf{R}}].", "$ $ \\textbf{R}^{mat} $ $ =[\\widetilde{\\textbf{R}},\\widehat{\\textbf{R}},\\widetilde{\\textbf{% R}}-\\widehat{\\textbf{R}},\\widetilde{\\textbf{R}}\\odot\\widehat{\\textbf{R}}].", "$ </equation> </equationgroup> \\newline Thus far, the relevant information between the context and the response has been collected, which is further converted back to the matching matrices of separated utterances as \\newline <equationgroup> <equation> $ \\{\\textbf{U}_{m}^{c,mat}\\}_{m=1}^{n_{c}}=\\textbf{Separate}(% \\textbf{C}^{mat}), $ $ \\{\\textbf{U}_{m}^{c,mat}\\}_{m=1}^{n_{c}}=\\textbf{Separate}(% \\textbf{C}^{mat}), $ </equation> <equation> $ \\{\\textbf{U}_{n}^{r,mat}\\}_{n=1}^{n_{r}}=\\textbf{Separate}(% \\textbf{R}^{mat}), $ $ \\{\\textbf{U}_{n}^{r,mat}\\}_{n=1}^{n_{r}}=\\textbf{Separate}(% \\textbf{R}^{mat}), $ </equation> </equationgroup> where the Separate operation is conducted by segmenting the whole sequences of relevant information according to utterance length.", "\\newline </subsection> <subsection> <title> III-E Aggregation Module </title> The aggregation module converts the matching matrices of separated utterances into a final matching vector.", "Previous studies [@bib:DBLP:conf/acl/WuWXZL17,DBLP:conf/acl/WuLCZDYZL18,DBLP:conf/coling/ZhangLZZL18] adopted the utterance-to-response matching framework and only aggregated the matching matrices of utterances in a context.", "In contrast, the U2U-IMN model needs to conduct the aggregation operation for both the context and the response.", "\\newline First, the matching matrix $ \\textbf{U}_{m}^{c,mat} $ or $ \\textbf{U}_{n}^{r,mat} $ for each utterance is processed by a BiLSTM and aggregated by max pooling and last-hidden-state pooling operations.", "For the matching matrix $ \\textbf{U}_{m}^{c,mat} $ of each context utterance, the calculations are as follows: \\newline <equationgroup> <equation> $ \\textbf{u}_{m,i}^{c,utr}=\\textbf{BiLSTM}(\\textbf{U}_{m}^{c,mat},i% ),i\\in\\{1,...,l_{u_{m}^{c}}\\}, $ $ \\textbf{u}_{m,i}^{c,utr} $ $ =\\textbf{BiLSTM}(\\textbf{U}_{m}^{c,mat},i),i\\in\\{1,...,l_{u_{m}^{% c}}\\}, $ </equation> <equation> $ \\textbf{u}_{m}^{c,agr}=[\\textbf{u}_{m,max}^{c,utr};\\textbf{u}_{m,% l_{u_{m}^{c}}}^{c,utr}],m\\in\\{1,...,n_{c}\\}, $ $ \\textbf{u}_{m}^{c,agr} $ $ =[\\textbf{u}_{m,max}^{c,utr};\\textbf{u}_{m,l_{u_{m}^{c}}}^{c,utr}% ],m\\in\\{1,...,n_{c}\\}, $ </equation> </equationgroup> where $ \\textbf{u}_{m,max}^{c,utr} $ and $ \\textbf{u}_{m,l_{u_{m}^{c}}}^{c,utr} $ denote the results of max pooling and last-hidden-state pooling for the sequence of $ \\textbf{u}_{m,i}^{c,utr} $ .", "The same calculations are also performed for the matching matrix $ \\textbf{U}_{n}^{r,mat} $ of each response utterance as follows: \\newline <equationgroup> <equation> $ \\textbf{u}_{n,j}^{r,utr}=\\textbf{BiLSTM}(\\textbf{U}_{n}^{r,mat},j% ),j\\in\\{1,...,l_{u_{n}^{r}}\\}, $ $ \\textbf{u}_{n,j}^{r,utr} $ $ =\\textbf{BiLSTM}(\\textbf{U}_{n}^{r,mat},j),j\\in\\{1,...,l_{u_{n}^{% r}}\\}, $ </equation> <equation> $ \\textbf{u}_{n}^{r,agr}=[\\textbf{u}_{n,max}^{r,utr};\\textbf{u}_{n,% l_{u_{n}^{r}}}^{r,utr}],n\\in\\{1,...,n_{r}\\}. $ $ \\textbf{u}_{n}^{r,agr} $ $ =[\\textbf{u}_{n,max}^{r,utr};\\textbf{u}_{n,l_{u_{n}^{r}}}^{r,utr}% ],n\\in\\{1,...,n_{r}\\}. $ </equation> </equationgroup> The weights for these two BiLSTMs are shared in our implementation.", "Thus far, we have obtained two sets of utterance embeddings $ \\textbf{U}^{c,agr}=\\{\\textbf{u}_{m}^{c,agr}\\}_{m=1}^{n_{c}} $ and $ \\textbf{U}^{r,agr}=\\{\\textbf{u}_{n}^{r,agr}\\}_{n=1}^{n_{r}} $ for the context and the response, respectively.", "The next step is to convert them into aggregated context and response embeddings.", "\\newline The embedding vector of the context is derived in a way similar to the utterance-level aggregation method mentioned above.", "The utterance embeddings in $ \\textbf{U}^{c,agr} $ are sent into another BiLSTM following the chronological order of utterances in the context.", "Combined max pooling and last-hidden-state pooling operations are also performed to obtain the context embedding vector as \\newline <equationgroup> <equation> $ \\textbf{u}_{m}^{c,ctx}=\\textbf{BiLSTM}(\\textbf{U}^{c,agr},m),m\\in% \\{1,...,n_{c}\\}, $ $ \\textbf{u}_{m}^{c,ctx}=\\textbf{BiLSTM} $ $ (\\textbf{U}^{c,agr},m),m\\in\\{1,...,n_{c}\\}, $ </equation> <equation> $ \\textbf{c}^{agr}=[\\textbf{u}_{max}^{c,ctx};\\textbf{u}_{n_{c}}^{c,% ctx}].", "$ $ \\textbf{c}^{agr} $ $ =[\\textbf{u}_{max}^{c,ctx};\\textbf{u}_{n_{c}}^{c,ctx}].", "$ </equation> </equationgroup> \\newline For the response, two aggregation strategies are designed in this paper.", "\\newline <subsubsection> <title> III-E1 RNN Aggregation </title> This is identical to the context aggregation in which the chronological relationships among utterances in the response are modelled.", "The operations can be written as \\newline <equationgroup> <equation> $ \\textbf{u}_{n}^{r,res}=\\textbf{BiLSTM}(\\textbf{U}^{r,agr},n),n\\in% \\{1,...,n_{r}\\}, $ $ \\textbf{u}_{n}^{r,res}=\\textbf{BiLSTM} $ $ (\\textbf{U}^{r,agr},n),n\\in\\{1,...,n_{r}\\}, $ </equation> <equation> $ \\textbf{r}^{agr}=[\\textbf{u}_{max}^{r,res};\\textbf{u}_{n_{r}}^{r,% res}].", "$ $ \\textbf{r}^{agr} $ $ =[\\textbf{u}_{max}^{r,res};\\textbf{u}_{n_{r}}^{r,res}].", "$ </equation> </equationgroup> \\newline </subsubsection> <subsubsection> <title> III-E2 Attention Aggregation </title> Different from contexts that usually contain approximately ten utterances, a response is composed of much fewer utterances (see Fig. [@ref:LABEL:fig3] in the next section for detailed statistics).", "We suppose that chronological relationships in short sequences are not as important as those in long sequences.", "Therefore, attention aggregation is designed to replace the RNN aggregation for deriving the response embedding vector.", "Mathematically, we have \\newline <equationgroup> <equation> $ \\textbf{r}^{agr}=\\sum_{n=1}^{n_{r}}w_{n}^{n_{r}}\\textbf{u}_{n}^{r% ,agr}, $ $ \\textbf{r}^{agr}=\\sum_{n=1}^{n_{r}}w_{n}^{n_{r}}\\textbf{u}_{n}^{r% ,agr}, $ </equation> </equationgroup> where $ w_{n}^{n_{r}} $ denotes softmax-normalized position-dependent utterance weights.", "During model training, the maximum number of utterances in a response $ n_{r}^{max} $ is set manually.", "For each $ n_{r}\\in\\{1,...,n_{r}^{max}\\} $ , a group of weights $ \\{w_{1}^{n_{r}},...,w_{n_{r}}^{n_{r}}\\} $ is estimated with the constraint $ \\sum_{n=1}^{n_{r}}w_{n}^{n_{r}}=1 $ .", "\\newline The final matching feature vector is the concatenation of the context embedding vector and the response embedding vector: \\newline <equation> $ \\textbf{m}=[\\textbf{c}^{agr};\\textbf{r}^{agr}].", "$ </equation> \\newline </subsubsection> </subsection> <subsection> <title> III-F Prediction Module </title> The matching feature vector m is then sent into a multi-layer perceptron (MLP) classifier.", "An MLP is a feedforward neural network estimated in a supervised manner using examples of features together with known labels.", "Here, the MLP is designed to predict whether a context-response pair matches appropriately according to the matching feature vector m .", "Finally, the MLP returns a score to denote the degree of matching.", "\\newline </subsection>  </section>"], ["<section> <title> IV Experiments </title>  <subsection> <title> IV-A Datasets </title> Two English public multi-turn response selection datasets, the Ubuntu Dialogue Corpus V1 [@bib:DBLP:conf/sigdial/LowePSP15] and Ubuntu Dialogue Corpus V2 [@bib:DBLP:journals/dad/LowePSCLP17] , and two Chinese datasets, the Douban Conversation Corpus [@bib:DBLP:conf/acl/WuWXZL17] and E-commerce Dialogue Corpus [@bib:DBLP:conf/coling/ZhangLZZL18] , were adopted to evaluate our proposed methods.", "In our experiments, we followed the splits of training, validation, and test sets provided by the original authors of the four datasets.", "The Ubuntu Dialogue Corpus V1 and V2 contain multi-turn dialogues about Ubuntu system troubleshooting in English.", "Here, we adopted the version of the Ubuntu Dialogue Corpus V1 shared by Xu et al. [@bib:DBLP:journals/corr/XuLWSW16] , in which numbers, paths and URLs were replaced by placeholders.", "Compared with the Ubuntu Dialogue Corpus V1, the training, validation and test dialogues in the V2 dataset were generated in different periods without overlap.", "Moreover, the V2 dataset discriminated between the end of an utterance (_eou_) and the end of a turn (_eot_).", "In both of the Ubuntu corpora, the positive responses are true responses from humans, and the negative responses are randomly sampled.", "The Douban Conversation Corpus was crawled from a Chinese social network on open-domain topics.", "It was constructed in a similar way to the Ubuntu corpus.", "The Douban Conversation Corpus collected responses via a small inverted-index system, and labels were manually annotated.", "The E-commerce Dialogue Corpus collected real-world conversations between customers and customer service staff from the largest e-commerce platform in China.", "Some statistics of these datasets are provided in Table [@ref:LABEL:tab2] .", "\\newline It is worth noting that the Ubuntu Dialogue Corpus V2 was the only dataset in our experiments that explicitly segmented utterances in responses.", "Specifically, approximately 30% of the responses in this dataset consisted of multiple utterances, as shown in Fig. [@ref:", "LABEL:fig3] , which made this dataset a very suitable one for evaluating our proposed U2U matching framework.", "The U2U-IMN model can also be applied to the other three datasets by considering a whole response as a single utterance.", "\\newline </subsection> <subsection> <title> IV-B Evaluation Metrics </title> The evaluation metrics used in previous work [@bib:DBLP:conf/sigdial/LowePSP15,DBLP:journals/dad/LowePSCLP17,DBLP:conf/acl/WuWXZL17,DBLP:conf/coling/ZhangLZZL18] were adopted in our experiments.", "Each model was tasked with selecting the $ k $ best-matched responses from $ n $ available candidates for the given conversation context $ c $ .", "We calculated the recall of the true positive replies among the $ k $ selected responses, denoted $ \\textbf{R}_{n}@k $ , as the main evaluation metric.", "The mean average precision ( MAP ) [@bib:DBLP:books/aw/Baeza-YatesR99] was also adopted for reference since previous work did not list their results in terms of MAP on the Ubuntu V1, Ubuntu V2 and E-commerce datasets.", "In addition to $ \\textbf{R}_{n}@k $ and MAP , we also adopted the mean reciprocal rank ( MRR ) [@bib:DBLP:conf/trec/Voorhees99] and precision-at-one ( $ \\textbf{P}@1 $ ) metrics for the Douban corpus, following the settings of previous work [@bib:DBLP:conf/acl/WuWXZL17] .", "The reason was that the Douban Conversation Corpus was different from the other three datasets in that it included multiple correct candidates for a context in the test set, which may lead to low $ \\textbf{R}_{n}@k $ .", "\\newline </subsection> <subsection> <title> IV-C Training Details </title> The Adam method [@bib:DBLP:journals/corr/KingmaB14] was employed for optimization, with a batch size of 128.", "The initial learning rate was 0.001 and was exponentially decayed by 0.96 every 5000 steps.", "Dropout [@bib:DBLP:journals/jmlr/SrivastavaHKSS14] with a rate of 0.2 was applied to the word embeddings and all hidden layers.", "\\newline The word representations for the English datasets were concatenations of the 300-dimensional GloVe embeddings [@bib:DBLP:conf/emnlp/PenningtonSM14] , the 100-dimensional embeddings estimated on the training set using the Word2Vec algorithm [@bib:DBLP:conf/nips/MikolovSCCD13] and the 150-dimensional character-level embeddings with window sizes of {3, 4, 5}, each consisting of 50 filters.", "The word embeddings for the Chinese datasets were concatenations of the 200-dimensional embeddings from previous work [@bib:DBLP:conf/naacl/SongSLZ18] and the 200-dimensional embeddings estimated on the training set using the Word2Vec algorithm.", "Character-level embeddings were not employed for the two Chinese datasets due to the large number of Chinese characters.", "The word embeddings were not updated during training.", "\\newline All hidden states of LSTMs had 200 dimensions.", "The MLP of the prediction module had a hidden unit size of 256 with ReLU [@bib:DBLP:conf/icml/NairH10] activation.", "The maximum word length, the maximum utterance length, the maximum number of utterances in a context, and the maximum number of utterances in a response were set as 18, 50, 10 and 3 respectively.", "We padded with zeros if the number of utterances in a context was less than 10 and the number of utterances in a response was less than 3.", "Otherwise, the last 10 utterances in the context or the last 3 utterances in the response were kept.", "The development set was used to select the best model for testing.", "\\newline All codes were implemented in the TensorFlow framework [@bib:DBLP:conf/osdi/AbadiBCCDDDGIIK16] and have been published to help replicate our results .", "\\newline </subsection> <subsection> <title> IV-D Experimental Results </title> Table [@ref:LABEL:tab3] and Table [@ref:LABEL:tab4] present the evaluation results of U2U-IMN and previous methods .", "\\newline All the results except ours are copied from the existing literature.", "For each dataset, all results listed in Table [@ref:LABEL:tab3] or Table [@ref:LABEL:tab4] are comparable with each other since they used the same training, validation and test data.", "Here, the U2U-IMN models adopted the attention aggregation strategy introduced in Section [@ref:LABEL:sec5] .", "It can be observed from these two tables that U2U-IMN outperformed the other models on all metrics and datasets, which demonstrates its ability to select the correct response and its compatibility across domains (e.g., the domains of system troubleshooting, social networks and e-commerce covered by these datasets).", "\\newline </subsection>  </section>"], ["<section> <title> V Analysis </title>  <subsection> <title> V-A Effectiveness of U2U matching </title> To further verify the effectiveness of our proposed U2U matching framework, we split the test set of the Ubuntu Dialogue Corpus V2 dataset according to the number of utterances in their correct responses."]], "target": "Then, the performances on these subsets of the U2U-IMN model were compared with those of the model (denoted U2R-IMN) that considered each response as a single utterance, as shown in Table . As demonstrated, the U2U framework can help improve the performance by exploiting the relationships among the utterances in a response. We can see that the advantage of the U2U-IMN model over the U2R-IMN model became larger when the correct responses were composed of more utterances. This was consistent with the motivation of the U2U matching framework. Considering that only 30% of responses in the Ubuntu Dialogue Corpus V2 dataset consisted of multiple utterances, a larger overall improvement may be achieved when applying our proposed U2U models to datasets containing more responses with multiple utterances."}, {"tabular": ["  Algo  &  Ours  &  WEIBO  &  GASPAD  &  DE ", " thd/dB  &  7.40  &  8.87  &  13.05  &  12.39 ", " Pout/dBm  &  23.05  &  23.11  &  24.23  &  24.25 ", " Eff(mean)/%  &  62.64  &  60.29  &  31.63  &  31.54 ", " Eff(median)/%  &  63.16  &  62.23  &  22.58  &  33.35 ", " Eff(best)/%  &  63.65  &  64.02  &  62.02  &  53.69 ", " Eff(worst)/%  &  60.84  &  48.86  &  22.07  &  14.07 ", " Avg. # Sim  &  59  &  82  &  257  &  234 ", " # Success  &  12/12  &  12/12  &  12/12  &  12/12  "], "ref_sec": [["<section> <title> 1 Introduction </title>  As device size shrinks to the nanoscale region, the analog circuit design is facing enormous challenges due to the more and more complicated device models and expensive transistor-level simulations.", "The growing demands for high performance, low power and short time-to-market make manual analog circuit design even more difficult.", "Sophisticated automated analog circuit design is in great demand and has attracted lots of attention in both industry and academic community [@bib:rutenbar2007hierarchical] .", "\\newline Analog circuit simulations are computation intensive.", "Great efforts have been made to reduce the simulation time while searching for the global optimum efficiently.", "Generally, the optimization algorithms can be classified into two categories: the model-based and simulation-based approaches.", "Driven by simplified equations or polynomials, the model-based methods build models to approximate the performance of the analog circuit, so as to fully search the solution space without expensive computational cost.", "One modelling technique that has been studied in depth is the geometric programming based approach [@bib:boyd2007tutorial,del2002design,colleran2003optimization] .", "However, a fundamental limitation of the model-based approach is that the accuracy of the constructed model is not guaranteed, and the model building process itself also requires lots of simulations.", "Simulation-based approaches simply take the objective function as a black-box function and activate the simulation process online.", "In order to better explore the solution space, a variety of well-developed global optimization algorithms have been consecutively proposed, examples include evolutionary algorithm [@bib:liu2009analog] , particle swarm optimization algorithm [@bib:vural2012analog] , multiple starting point optimization algorithm [@bib:yang2018smart,peng2016efficient] , and simulated annealing algorithm [@bib:phelps2000anaconda] .", "The main disadvantages that prevent the simulation-based approach from widespread use are its relatively low convergence rate and the corresponding large simulation costs.", "\\newline To chart a way forward out of the dilemma, the GP-based Bayesian optimization (BO) approach has been proposed recently to combine both the model-based and simulation-based approach [@bib:lyu2018efficient] .", "Generally, Bayesian optimization approach consists of two key elements: the surrogate model and the acquisition function [@bib:shahriari2016taking] .", "The surrogate model is trained to mimic the behavior of the latent function. And the acquisition function helps to select the next query point to balance the exploration and exploitation.", "Compared with the model-based approach which builds the model only once and explores the global optimum offline, BO refines the surrogate model incrementally and searches the optimum online.", "Other successful GP-based approaches have also been proposed [@bib:liu2014gaspad,liu2011global] .", "\\newline In order to further reduce the overall cost of the optimization process, great efforts have been made to exploit the correlations between different fidelity levels of information ( a.k.a. fidelities) [@bib:kennedy2000predicting] .", "The corresponding multi-fidelity Bayesian optimization algorithms have also been proposed [@bib:kandasamy2016gaussian,kandasamy2017multi,poloczek2017multi] .", "However, only linear correlations are considered in these models and Bayesian optimization algorithms.", "Recent work [@bib:perdikaris2017nonlinear] generalized the linear correlations to general nonlinear one, and proposed nonlinear information fusion algorithms for multi-fidelity modeling.", "The nonlinear map between the low-fidelity and high-fidelity models provides more flexility to model the complicated correlations between the low- and high-fidelity black-box functions.", "\\newline Inspired by [@bib:perdikaris2017nonlinear] , we propose an efficient multi-fidelity Bayesian optimization approach for analog circuit synthesis.", "We follow the ideas of [@bib:perdikaris2017nonlinear] to build the multi-fidelity model, but propose the weighted Expected Improvement acquisition function and multi-fidelity optimization strategies for Bayesian optimization.", "GP models are employed to model the low- and high-fidelity black-box functions separately.", "The nonlinear map between the low-fidelity model and high-fidelity model is also modelled as a Gaussian process.", "A fusing GP model which combines the low- and high-fidelity models can thus be built.", "The acquisition function based on the fusing GP model is used to balance the exploitation and exploration.", "By maximizing the acquisition function and carefully selecting the fidelity for evaluation, our optimization algorithm achieves better optimization results within the limited simulation time.", "The ability to combine several levels of information to model the slowest one is extremely useful in analog circuit optimization, since we can always carry out the circuit simulation at different precision levels.", "This would help us to find the global optimums of analog circuits that are impossible to optimize if we only rely on the computationally intensive high-fidelity simulation results.", "The efficiency of our proposed method has been demonstrated with two real-world analog circuits. And the experimental results show that our proposed methodology can reduce up to 65.5% of the simulation time during the optimization process.", "\\newline The remainder of this paper is organized as follow.", "In section (\u00a7 [@ref:LABEL:sec:background] ), we present the problem formulation and briefly review the GP-based Bayesian optimization approach.", "In section (\u00a7 [@ref:LABEL:sec:proposal] ), the multi-fidelity Bayesian optimization algorithm is proposed.", "The implementation details are presented in section (\u00a7 [@ref:LABEL:sec:details] ). And the experimental results is presented in section (\u00a7 [@ref:LABEL:sec:experiment] ).", "We conclude the paper in section (\u00a7 [@ref:LABEL:sec:conclusion] ).", "\\newline  </section>"], ["<section> <title> 2 Background </title>  In this section, we first present the problem formulation of analog circuit optimization (\u00a7 [@ref:LABEL:sec:problem_formulation] ).", "Then, we give a brief overview of Bayesian optimization (\u00a7 [@ref:LABEL:sec:BO] ), and the Gaussian process regression (\u00a7 [@ref:LABEL:sec:GP] ) and the acquisition functions (\u00a7 [@ref:LABEL:sec:acquisition_function] ).", "\\newline <subsection> <title> 2.1 Problem Formulation </title> A typical analog circuit optimization problem can be formulated as a constrained optimization problem, which can be expressed as follows: \\newline <equationgroup> <equation> $  f(\\bm{x}) $ $  f(\\bm{x}) $ </equation> <equation> $  c_{i}(\\bm{x})<0 $ $  c_{i}(\\bm{x})<0 $ </equation> <equation> $ \\forall i\\in{1\\dots N_{c}}, $ $ \\forall i\\in{1\\dots N_{c}}, $ </equation> </equationgroup> where $ \\bm{x} $ is a $ d $ -dimensional vector denoting the $ d $ design variables for the analog circuit optimization problem.", "The $ f(\\bm{x}) $ and $ c_{i}(\\bm{x}) $ in equation ( [@ref:LABEL:eq:Formulation] ) represent the objective function and the i-th constraint function respectively.", "The goal is to minimize $ f(x) $ while satisfying the constraints.", "\\newline </subsection> <subsection> <title> 2.2 Overview of Bayesian Optimization </title> Bayesian optimization (BO) is a sequential model-based framework that helps to find the global optimum of noisy and expensive black-box functions.", "Generally, it contains two key ingredients: the surrogate model and the acquisition function [@bib:shahriari2016taking] .", "The probabilistic surrogate model captures our beliefs about the unknown objective function, and provides posterior distribution with both predictions and uncertainties.", "The acquisition function utilizes the posterior distribution to select the query points sequentially, and it is carefully designed to trade off between exploitation and exploration.", "The exploitation means the algorithm tends to find the optimum point of the current search space with high confidence.", "The exploration means the algorithm tries to search the promising unknown area with high predictive uncertainty.", "\\newline Starting with a randomly sampled training set, Bayesian optimization is able to construct a probabilistic surrogate model to provide the posterior distribution.", "At each iteration, the training set is incremented by a new point with maximum acquisition function value to refine the surrogate model.", "After a limited number of iterations, the global optimum is possibly to be found.", "\\newline </subsection> <subsection> <title> 2.3 Gaussian Process Regression </title> For Bayesian optimization process, one commonly used surrogate model is the Gaussian process regression (GPR) model [@bib:shahriari2016taking] .", "The GPR model captures our prior beliefs about the objective function and provides the posterior predictive means and well-calibrated uncertainty estimations [@bib:rasmussen2004gaussian] .", "\\newline We consider to approximate a scalar-valued black-box function $ f(\\bm{x}) $ with observation noise $ \\epsilon\\sim N(0,\\sigma^{2}_{n}) $ , where $ N(\\cdot,\\cdot) $ denotes a Gaussian distribution.", "The training set can be expressed as $ D_{T}=\\{X,\\bm{y}\\} $ , where $ X=\\{\\bm{x}_{1},\\bm{x}_{2},\\dots,\\bm{x}_{N}\\} $ denote the inputs, $ \\bm{y}=\\{y_{1},y_{2},\\dots,y_{N}\\} $ denotes the observations of the black-box function at $ \\{\\bm{x}_{1},\\bm{x}_{2},\\dots,\\bm{x}_{N}\\} $ .", "We can fully characterize our prior beliefs about the latent function with a Gaussian process whose mean function is $ m(\\bm{x}) $ and kernel function is $ k(\\cdot,\\cdot) $ .", "$ k(\\cdot,\\cdot) $ should be carefully designed to make sure the covariance matrix a symmetric positive definite (SPD) matrix.", "In this paper, we fix $ m(\\bm{x})=0 $ and select the kernel function as the squared exponential (SE) function, which can be expressed as below \\newline <equation> $ k_{SE}(\\bm{x}_{i},\\bm{x}_{j})=\\sigma^{2}_{f}exp(-\\frac{1}{2}(\\bm{x}_{i}-\\bm{x}% _{j})^{T}\\Lambda^{-1}(\\bm{x}_{i}-\\bm{x}_{j})).", "$ </equation> In equation ( [@ref:LABEL:eq:SE] ), $ \\Lambda=\\text{diag}(l_{1},l_{2},\\dots,l_{d}) $ is a diagonal matrix and $ l_{i} $ denotes the i-th length scale of the i-th dimension.", "The hyperparameters can be assembled into a single vector $ \\bm{\\theta}=(\\sigma_{n},\\sigma_{f},l_{1},l_{2},\\dots,l_{d}) $ to help optimize the GPR model.", "By maximize the likelihood, the GPR model can be optimized to approximate the behavior of the expensive latent function $ f(\\bm{x}) $ .", "In other words, we can train the model by minimizing the negative log marginal likelihood (NLML): \\newline <equation> $ NLML=\\frac{1}{2}(\\bm{y}^{T}K^{-1}_{\\bm{\\theta}}\\bm{y}+\\log\\lvert K_{\\bm{\\theta% }}\\rvert+N\\log(2\\pi)), $ </equation> where $ K_{\\bm{\\theta}}=K(X,X)+\\sigma^{2}_{n}I $ is a $ N\\times N $ covariance matrix.", "\\newline Given a new input vector $ \\bm{x}_{\\ast} $ , the trained GPR model is capable of providing the corresponding posterior prediction $ \\mu(\\bm{x}_{\\ast}) $ and uncertainty measurement $ \\sigma^{2}(\\bm{x}_{\\ast}) $ .", "The formulations are as follows: \\newline <equation> $ \\begin{cases}&\\mu(\\bm{x}_{\\ast})=k(\\bm{x}_{\\ast},X)(K+\\sigma^{2}_{n}I)^{-1}\\bm% {y}\\\\ &\\sigma^{2}(\\bm{x}_{\\ast})=\\sigma^{2}_{n}+k(\\bm{x}_{\\ast},\\bm{x}_{\\ast})-k(\\bm% {x}_{\\ast},X)(K+\\sigma^{2}_{n}I)^{-1}k(X,\\bm{x}_{\\ast}),\\end{cases} $ </equation> where $ k(\\bm{x}_{\\ast},X)=(k(\\bm{x}_{\\ast},\\bm{x}_{1}),k(\\bm{x}_{\\ast},\\bm{x}_{2}),{% }\\dots,k(\\bm{x}_{\\ast},\\bm{x}_{N})) $ and $ k(X,\\bm{x}_{\\ast})=k(\\bm{x}_{\\ast},X)^{T} $ .", "\\newline </subsection> <subsection> <title> 2.4 Acquisition Function </title> For simplicity, let us first ignore the constraints and consider the minimization of the objective function.", "We use weighted expected improvement as acquisition function.", "Given the current best objective value $ \\tau $ , the improvement can be calculated with $ I(y)=max(0,\\tau-y) $ , and the posterior distribution of $ y $ can be provided by the surrogate model $ y\\sim N(\\mu(\\bm{x}),\\sigma^{2}(\\bm{x})) $ .", "Therefore, the expected improvement (EI) [@bib:jones1998efficient] over $ \\tau $ can be expressed as follows: \\newline <equation> $ EI(\\bm{x})=\\sigma(\\bm{x})(\\lambda\\Phi(\\lambda)+\\phi(\\lambda)), $ </equation> where $ \\lambda=(\\tau-\\mu(\\bm{x}))/\\sigma(\\bm{x}) $ .", "In equation ( [@ref:LABEL:eq:EI] ), $ \\Phi(\\cdot) $ and $ \\phi(\\cdot) $ represent the CDF and PDF of the standard normal distribution respectively.", "The EI function favors the unknown regions with large uncertainty estimation and the high confidence regions with minimum predictive means.", "\\newline For constrained optimization problem, there are certain regions that are invalid.", "The weighted expected improvement (wEI) [@bib:schonlau1998global,gardner2014bayesian] algorithm has been proposed to address this problem.", "By multiplying the EI function with the probability of feasibility (PF), the wEI function favors the feasible regions and reduces the points that are more likely to violate constraints.", "The corresponding formulation is as below: \\newline <equation> $ wEI(\\bm{x})=EI(\\bm{x})\\prod^{N_{c}}_{i=1}PF_{i}(\\bm{x}), $ </equation> where $ PF_{i}(\\bm{x})=\\Phi(-\\mu_{i}(\\bm{x})/\\sigma_{i}(\\bm{x}))", "$ gives a potential measurement of the i-th constraint being satisfied. And $ \\mu_{i}(\\bm{x}) $ and $ \\sigma_{i}(\\bm{x}) $ represent posterior prediction and uncertainty estimation of the i-th constraint function respectively.", "There are other well-designed acquisition functions, such as upper confidence bounds (UCB) [@bib:auer2003using] , predictive entropy search [@bib:hernandez2014predictive] , Thompson sampling [@bib:chapelle2011empirical] , and knowledge gradient [@bib:scott2011correlated] .", "\\newline </subsection>  </section>"], ["<section> <title> 3 Multi-fidelity Bayesian Optimization </title>  In this section, we will present our proposed multi-fidelity Bayesian optimization approach for analog circuit synthesis.", "We will first present the multi-fidelity model and the corresponding training techniques in detail (\u00a7 [@ref:LABEL:sec:multi-fidelity_model] ).", "Then, we give the posterior prediction and uncertainty estimation of multi-fidelity model in (\u00a7 [@ref:LABEL:sec:multi-fidelity_prediction] ).", "Next, the Bayesian optimization based on the multi-fidelity model is presented in (\u00a7 [@ref:LABEL:sec:summary] ).", "In (\u00a7 [@ref:LABEL:sec:Fidelity_Selection] ), we give a carefully designed fidelity selection criterion.", "\\newline For simplicity, we only consider modelling technique that involves two levels of fidelities in this paper.", "We refer the potentially inaccurate but cheap to evaluate model as the low-fidelity model, and the accurate but computation intensive model as the high-fidelity model.", "We use $ f_{l}(\\cdot) $ and $ f_{h}(\\cdot) $ denote the GPR model that are trained with the low- and high-fidelity dataset respectively.", "Given an input vector $ \\bm{x} $ , we use $ \\mu_{l}(\\bm{x}) $ , $ \\sigma^{2}_{l}(\\bm{x}) $ , $ \\mu_{h}(\\bm{x}) $ and $ \\sigma^{2}_{h}(\\bm{x}) $ to represent the corresponding posterior prediction and uncertainty estimation provided by the low- and high-fidelity models respectively.", "The current best results of coarse and fine data are denoted as $ \\tau_{l} $ and $ \\tau_{h} $ respectively.", "\\newline <subsection> <title> 3.1 Multi-fidelity Model </title> In [@bib:kennedy2000predicting] , a multi-fidelity modelling approach which exploits the linear correlations between low-fidelity and high-fidelity models has been proposed as follows: \\newline <equation> $ f_{h}(\\bm{x})=\\rho*f_{l}(\\bm{x})+\\delta(\\bm{x}), $ </equation> where $ \\rho $ denotes the regression parameter and $ \\delta(\\bm{x}) $ represents the independent noise term which follows a Gaussian distribution.", "This method exhibits great improvement compared with the traditional single-fidelity method.", "However, it is not suitable for situations when the coarse and fine models exhibit more complex correlations.", "\\newline Based on the fact that non-functional interplay exists between different fidelity level of information, a nonlinear information fusion algorithm was proposed in [@bib:perdikaris2017nonlinear] to further exploit the corresponding nonlinear cross-correlations between the low- and high-fidelity models.", "\\newline The first level of the multi-fidelity model is based on the traditional GPR model presented in (\u00a7 [@ref:LABEL:sec:GP] ).", "The mean function of the low-fidelity model is fixed as $ m(\\bm{x})=0 $ and the corresponding kernel function is the SE function in equation ( [@ref:LABEL:eq:SE] ).", "By minimizing the equation ( [@ref:LABEL:eq:NLML] ), the low-fidelity model trained with the coarse data $ D_{l}=\\{X_{l},\\bm{y}_{l}\\} $ can be optimized to approximate the behavior of the low-fidelity latent function.", "\\newline In order to enhance the low-fidelity model, the nonlinear correlations between the low- and high-fidelity data should be fully explored.", "The correlations between low- and high-fidelity models can be generalized as \\newline <equation> $ f_{h}(\\bm{x})=z(f_{l}(\\bm{x}))+\\delta(\\bm{x}), $ </equation> where $ z(\\cdot) $ represents the unknown space-dependent correlations between the low- and high-fidelity model, and $ \\delta(\\cdot) $ is a Gaussian process model with respect to the high-fidelity data, which is independent of the low-fidelity model.", "The nonlinear map $ z(\\cdot) $ can also be modelled by a Gaussian process model.", "Since the low-fidelity model $ f_{l}(\\bm{x}) $ is also a Gaussian process model, $ z(f_{l}(\\bm{x})) $ is a deep Gaussian process model, and the posterior distribution of $ f_{h}(\\bm{x}) $ would thus not be Gaussian.", "In [@bib:perdikaris2017nonlinear] , the posterior prediction $ \\mu_{l}(\\bm{x}) $ provided by the low-fidelity model $ f_{l}(\\bm{x}) $ is mapped to the high-fidelity output.", "With the assumption that the Gaussian processes $ \\delta(\\cdot) $ and $ z(\\cdot) $ are independent, $ f_{h}(\\bm{x}) $ is approximated by a Gaussian process with mean function $ m_{h}(\\bm{x})=0 $ and the kernel function defined as follows [@bib:perdikaris2017nonlinear] : \\newline <equationgroup> <equation> $  k_{h}(\\bm{x}_{1},\\bm{x}_{2};\\theta_{h})=\\quad k_{h,1}(f_{l}(\\bm{% x}_{1}),f_{l}(\\bm{x}_{2});\\theta_{h,1}) $ $  k_{h}(\\bm{x}_{1},\\bm{x}_{2};\\theta_{h}) $ $ =\\quad k_{h,1}(f_{l}(\\bm{x}_{1}),f_{l}(\\bm{x}_{2});\\theta_{h,1}) $ </equation> <equation> $ *\\quad k_{h,2}(\\bm{x}_{1},\\bm{x}_{2};\\theta_{h,2})+k_{h,3}(\\bm{x}% _{1},\\bm{x}_{2};\\theta_{h,3}), $ $ *\\quad k_{h,2}(\\bm{x}_{1},\\bm{x}_{2};\\theta_{h,2})+k_{h,3}(\\bm{x}% _{1},\\bm{x}_{2};\\theta_{h,3}), $ </equation> </equationgroup> where $ k_{h,1}(\\cdot,\\cdot) $ , $ k_{h,2}(\\cdot,\\cdot) $ and $ k_{h,3}(\\cdot,\\cdot) $ are valid covariance functions, and the squared exponential function in ( [@ref:LABEL:eq:SE] ) is used in our paper.", "$ \\theta_{h,1},\\theta_{h,2},\\theta_{h,3} $ are the corresponding hyperparameters for the kernel functions $ k_{h,1}(\\cdot,\\cdot) $ , $ k_{h,2}(\\cdot,\\cdot) $ and $ k_{h,3}(\\cdot,\\cdot) $ .", "We use $ \\theta_{h} $ to denote the combined hyperparameters of $ \\theta_{h,1},\\theta_{h,2},\\theta_{h,3} $ .", "With the $ m_{h}(\\bm{x})=0 $ and the kernel function $ k_{h}(\\bm{x}_{1},\\bm{x}_{2};\\theta_{h}) $ defined in ( [@ref:LABEL:eq:high-fidelity_kernel] ), the hyperparameters $ \\theta_{h} $ can be learnt from the high-fidelity dataset via the minimizing the NLML as shown in ( [@ref:LABEL:eq:NLML] ).", "The corresponding posterior distribution is denoted as $ f_{h}(\\bm{x})\\sim N(\\mu_{h}(\\bm{x}),\\sigma^{2}_{h}(\\bm{x})) $ .", "\\newline </subsection> <subsection> <title> 3.2 Prediction and Uncertainty Estimation </title> The basic idea of the multi-fidelity model is to use the value of low-fidelity function as additional input parameters, however, if the low fidelity and high fidelity data do not share same input, then $ f_{l}(\\bm{x}) $ could be unknown, in which case, we model the low fidelity function as Gaussian process and then integrate $ f_{l}(\\bm{x}) $ out [@bib:perdikaris2017nonlinear] .", "\\newline <equation> $ p(y_{\\ast,h}|D,\\bm{x}_{\\ast})=\\int p(y_{\\ast,h}|D_{h},y_{\\ast,l},\\bm{x}_{\\ast}% )p(y_{\\ast,l}|D_{l},\\bm{x}_{\\ast})d\\bm{x}_{\\ast}, $ </equation> \\newline The posterior distribution of low-fidelity model is Gaussian.", "The corresponding high-fidelity counterpart is not Gaussian because it depends on the posterior prediction of the low-fidelity model.", "Given an input vector $ \\bm{x}_{\\ast} $ , the posterior uncertainty estimation of the low-fidelity model will propagate to the higher-fidelity model.", "With the posterior uncertainty measurement of the low-fidelity model, the overall posterior distribution of the multi-fidelity model can be expressed as follows [@bib:perdikaris2017nonlinear] : where $ D_{l}=\\{X_{l},\\bm{y}_{l}\\} $ and $ D_{h}=\\{X_{h},\\bm{y}_{h}\\} $ correspond to the low- and high-fidelity training data set respectively.", "$ D=\\{D_{l},D_{h}\\} $ is the combination of the the low- and high-fidelity training data set.", "$ y_{\\ast,l} $ and $ y_{\\ast,h} $ denote the posterior distribution provided by the low- and high-fidelity model.", "\\newline However, computing the posterior distribution with equation ( [@ref:LABEL:eq:multi-fidelity_prediction] ) faces great challenge.", "The fundamental problem is that analytically generating the exact posterior predictions and uncertainty estimations is intractable.", "Therefore, the prediction and uncertainty estimation of the high-fidelity model are obtained through Monte-Carlo integration of ( [@ref:LABEL:eq:multi-fidelity_prediction] ).", "We randomly sample $ N $ points $ X_{\\ast}=(\\bm{x}_{\\ast,1},\\bm{x}_{\\ast,2},\\dots,\\bm{x}_{\\ast,N}) $ that follow the posterior distribution $ X_{\\ast}\\sim N(\\mu_{l}(\\bm{x}),\\sigma^{2}_{l}(\\bm{x})) $ , and calculate the prediction and posterior uncertainty with Monte-Carlo process.", "It will be used in the subsequent Bayesian optimization process.", "Figure [@ref:LABEL:fig:multi-single] shows that the posterior prediction fits the latent function better and the uncertainty estimation is much lower compared with the single-fidelity GPR model.", "The corresponding latent functions come from the pedagogical example of [@bib:perdikaris2017nonlinear] .", "\\newline </subsection> <subsection> <title> 3.3 Bayesian Optimization Based on the Multi-fidelity Model </title> The proposed multi-fidelity Bayesian optimization approach effectively enhances the low-fidelity model by learning the nonlinear correlations between coarse and fine data automatically.", "With carefully designed multiple starting point [@bib:peng2016efficient,yang2018smart] strategy, the exploitation is greatly improved during the optimization procedure.", "Fueled with explicitly designed optimization algorithms and the fidelity selection criterion, our proposed method can efficiently reduce the overall optimization cost.", "\\newline At each iteration, we first construct the low-fidelity GP model and optimize the low-fidelity wEI acquisition function to find an optimal point $ \\bm{x}^{*}_{l} $ , then, the high-fidelity GP model is built, and the high fidelity wEI function is optimized based on $ \\bm{x}^{*}_{l} $ .", "The summary of the proposed multi-fidelity Bayesian optimization is given in Algorithm [@ref:LABEL:algo:summary] .", "\\newline <float> Multi-fidelity Bayesian optimization approach \\ Initialize a training set $ D_{T}=\\{X_{l},\\bm{y}_{l},X_{h},\\bm{y}_{h}\\} $ \\ \\ for t = 1 to N do \\ \\ Build the multi-fidelity model \\ \\ Randomly scatter a set of data \\ \\ Maximize the acquisition function of the low-fidelity model, and get $ \\bm{x}^{*}_{l} $ \\ \\ Find $ \\bm{x}_{t} $ that maximizes the acquisition function based on $ \\bm{x}^{*}_{h} $ \\ \\ Select the fidelity level for evaluation (\u00a7 [@ref:LABEL:sec:Fidelity_Selection] ) \\ \\ Update the training set with the newly sampled data $ (\\bm{x}_{t},y_{t}) $ \\ \\ end for \\ </float> \\newline </subsection> <subsection> <title> 3.4 Fidelity Selection Criterion </title> During the optimization process, the surrogate models are incrementally refined with new observed data.", "By maximizing the acquisition function of the multi-fidelity model, the selected query point can trade off between the exploration and exploitation and reduce the overall optimization cost.", "Apart from choosing the next query point, we also need select the evaluation fidelity for the multi-fidelity Bayesian optimization process.", "Based on the fact that the computational cost of sampling the high-fidelity model is much higher than that of the low-fidelity counterpart, we should only sample the high-fidelity data if it is necessary.", "If the uncertainty of the query point based on the low-fidelity model is low, which means that this data point cannot further improve the accuracy of the low-fidelity model, we will evaluate this query point in high fidelity level to improve the accuracy of multi-fidelity model.", "Therefore, we select $ \\bm{x} $ to be sampled in the high-fidelity level if the following criterion is satisfied: \\newline <equation> $ \\sigma^{2}_{l}(\\bm{x})<\\gamma, $ </equation> where $ \\sigma^{2}_{l}(\\bm{x}) $ is the uncertainty estimation with the low-fidelity model, and $ \\gamma $ is empirically set to be 0.01 in this paper.", "For constrained optimization problem, the corresponding criterion is expressed as below: \\newline <equation> $ \\text{max}(\\sigma^{2}_{l,0}(\\bm{x}),\\sigma^{2}_{l,1}(\\bm{x}),\\dots,% \\sigma^{2}_{l,N_{c}}(\\bm{x}))<(1+N_{c})*\\gamma, $ </equation> where $ \\sigma^{2}_{l,0}(\\bm{x}) $ and $ \\sigma^{2}_{l,i}(\\bm{x}) $ are the posterior uncertainty estimation of the objective function and the i-th constraint function provided by the corresponding low-fidelity models.", "With the proposed fidelity selection criterion, our proposed multi-fidelity Bayesian optimization process would only sample high-fidelity data if it is necessary.", "\\newline </subsection>  </section>"], ["<section> <title> 4 Implementation Details </title>  We will present several implementation details in this section.", "\\newline <subsection> <title> 4.1 Multiple Starting Point strategy </title> We use multiple starting point [@bib:peng2016efficient,yang2018smart] (MSP) strategy to optimize the acquisition function.", "The region-hit property of the MSP strategy makes it more suitable for global optimization.", "The stochastic characteristic of MSP algorithm prevents it from getting stuck in the local optimum.", "By randomly sampling a set of starting point in the solution space, we can expect to cover almost all valleys of the objective function and thus get all local optimum to achieve the global optimum.", "\\newline For the optimization of the acquisition function of the multi-fidelity model, we propose a more sophisticated strategy based on the current search space of the low- and high-fidelity models.", "During the optimization of the acquisition function of the multi-fidelity model, instead of scattering the starting point randomly, we sample a small fraction of the starting points around $ \\tau_{h} $ .", "Considering the fact that the optimum points of the low- and high-fidelity latent function are within a small range in most cases, we also sample a fraction of points around $ \\tau_{l} $ .", "By scattering a fraction of points around the best results of the current search space, we can better explore the region that has low posterior predictive mean with high confidence.", "The reason is that the uncertainty estimation of the current best result is extremely small. And even if there is only a small deviation from the current best result, the gradient of the expected improvement function will be zero as shown in Figure [@ref:LABEL:fig:EI] .", "The latent functions in Figure [@ref:LABEL:fig:EI] come from the pedagogical example of [@bib:perdikaris2017nonlinear] .", "Therefore, it is not possible to fully explore the current best region with randomly scattered starting points.", "For constrained optimization problem, where the input dimension is over 20, the problem will be more serious.", "The ratio at which we sample around $ \\tau_{l} $ and $ \\tau_{h} $ in this paper is 10 percent and 40 percent respectively.", "\\newline </subsection> <subsection> <title> 4.2 Finding the First Feasible Solution </title> For the constrained optimization problem, there is a possibility that the initial dataset has no feasible points.", "Considering the narrow bell shape of both EI and PF function, we minimize the sum of the posterior means which violate the constraints to force the starting point into a feasible region.", "The corresponding formulation is as follow: \\newline <equation> $ \\text{minimize}\\quad\\sum^{n_{c}}_{i=1}\\text{max}(0,\\mu_{h,i}(\\bm{x})).", "$ </equation> This operation helps to accelerate the process of finding the first feasible point. And it further reduces the computational resources spent on searching the infeasible region during the optimizaion process.", "\\newline </subsection>  </section>"], ["<section> <title> 5 Experimental Results </title>  In this section, we demonstrate the efficiency of our proposed optimization approach with two real-world analog circuits: power amplifier (\u00a7 [@ref:LABEL:sec:PA] ) and charge pump (\u00a7 [@ref:LABEL:sec:charge_pump] ).", "We quantitatively compare our approach with three state-of-the-art algorithms: WEIBO [@bib:lyu2018efficient] , GASPAD [@bib:liu2014gaspad] and DE [@bib:liu2009analog] .", "The WEIBO method is a traditional single-fidelity GP-based BO approach with wEI function works as the acquisition function.", "The GASPAD approach is a GP-based approach with evolutionary algorithm works as the optimization engine and lower confidence bound [@bib:dennis1997managing,emmerich2006single] works as the acquisition function.", "The DE methodology is an optimization approach based on the evolutionary algorithm.", "All our experiments are conducted on a Linux workstation with two Intel Xeon CPUs and 128G memory.", "\\newline <subsection> <title> 5.1 Power Amplifier </title> The power amplifier circuit is implemented in a TSMC 65nm process and works at a frequency of 2.4GHz.", "The array-based power amplifier contains 2048 duplicated cells, and they have 4 transistors each.", "We aim to maximize the output efficiency (Eff) of this circuit while trying to meet two constrains, which are the output power (Pout) and the total harmonic distortion (thd).", "The corresponding design specifications are as follows: \\newline <equationgroup> <equation> $ \\text{maximize}\\mathit{Eff} $ $ \\mathit{Eff} $ </equation> <equation> $ \\text{s.t.}\\mathit{Pout} $ $ \\mathit{Pout} $ $ >23dBm $ $ > $ $  23dBm $ </equation> <equation> $ \\mathit{thd} $ $ \\mathit{thd} $ $ <13.65dB. $ $ < $ $  13.65dB. $ </equation> </equationgroup> \\newline There are 5 design variables in the power amplifier circuit.", "Figure [@ref:LABEL:fig:PA] fixes four design variables $ C_{s},C_{p},W,V_{dd} $ and presents the low- and high-fidelity simulation results.", "It shows that nonlinear cross-correlations exist between the low- and high-fidelity dataset.", "In this experiment, we set the simulation time of each transistor as 10ns and 200ns for low- and high-fidelity model respectively.", "As the evaluation time of high-fidelity model is much higher than that of the low-fidelity one, we limit the high-fidelity data budget.", "For our proposed method, the budget is set to be 150 and initialized with 10 low-fidelity data and 5 high-fidelity data.", "WEIBO is initialized with 40 high-fidelity data points and limited with 150 simulations.", "For both GASPAD and DE, the simulation budget is set to be 300.", "We run our proposed method 12 times to average out the random fluctuations."]], "target": "The experimental results are shown in Table . On average, our proposed method requires 252 coarse data and 46 fine data to reach the corresponding results, which is equivalent to the simulation time of 59 high-fidelity data. Although the best optimization result of WEIBO is slightly better than ours by 0.6%, our proposed method reduces about 28.0% of the simulation time and exhibits more stable performance. Compared with GASPAD and DE which also achieve feasible design, our proposed method achieves much better results with less simulation time. The experimental results also show that our proposed method is still competitive even if the evaluation cost has already been reduced to a relatively low level."}, {"tabular": ["  FRR(%)  &  SNIPS (FAH=0.5)  &  Mobvoi (FAH=1.5) ", " w/o data augmentation  &  0.6  &  5.6 ", " w/ data augmentation  &  0  &  0.4  "], "ref_sec": [["<section> <title> 1 Introduction </title>  Wake word detection is the task of detecting a predefined keyword from a continuous stream of audio.", "It has become an important component in today\u2019s voice-controlled digital assistants and smart phones.", "Voice-controlled devices, with wake word detection system running in the background, require a low power solution.", "When people wish to to interact with such devices by voice, they {wake up} the device by saying a predefined word like \u201cAlexa\u201d for Amazon Echo or \u201cOkay Google\u201d for Google Home.", "If the word is identified and accepted, the device turns on, i.e. goes into a state with higher power consumption to recognize and understand more complex spoken instructions [@bib:wang2019end] .", "\\newline HMM-based keyword-filler models are used to represent both the keyword and filler (background) models [@bib:rohlicek1989continuous,rose1990hidden,szoke2005phoneme] .", "The keyword model consists of all valid phone sequences from the keyword, and the filler model includes all other speech and non-speech.", "During the decoding phase, usually the ratio of the scores with keyword graph and to the filler graph is computed for determining the presence of the wake word.", "With recent advances in deep learning, HMM-DNN hybrid wake word systems replace GMM-based acoustic models with a neural network to classify individual frames [@bib:panchapagesan2016multi,sun2016compressed,wu2018monophone] .", "While the filler model for background speech is specified as an ergodic topology between speech and non-speech in [@bib:panchapagesan2016multi,sun2016compressed] , it is represented as an all-phones loop in [@bib:wu2018monophone] , increasing both the neural network model size and decoding graph size due to the increased number of modeling units.", "Finally, some methods add automatic speech recognition (ASR) as an auxiliary task during training.", "\\newline Pure neural models abandon HMMs and completely rely on neural networks for acoustic modeling, where the subwords or even the whole word of the wake word phrase (wake phrase, for short) is directly used as modeling units.", "The first successful wake word detection systems of this type are proposed in [@bib:chen2014small,sainath2015convolutional] .", "They use individual words in the wake phrase as modeling units to reduce the network size.", "However, they still need a forced alignment obtained from an existing HMM-based ASR system, to obtain training labels, which limits their applications if an ASR system is unavailable.", "For decoding, they adopt a fast posterior handling approach where the posterior of words is smoothed within a sliding window over the audio frames.", "[@bib:myer2018efficient,coucke2019efficient] use the whole wake phrase as the training target, but it still needs phone-level alignments to pretrain a small network being part of a larger one.", "There are also several proposals that do not require frame-level alignment for training, including max-pooling [@bib:sun2016max,hou2020mining] , the attention mechanism [@bib:shan2018attention,wang2019adversarial] , and global mean-pooling [@bib:bai2019time] .", "\\newline It has been shown that a {sequence-level training criteria} perform better than frame-level criteria for ASR.", "The output in a wake word detection task, by contrast, is relatively simple.", "However, if the modeling units are subwords (e.g., phonemes or HMM states), wake word detection may still be considered as a sequence prediction task.", "Sequence-level discriminative training such as CTC loss [@bib:graves2006connectionist] has been explored for the wake word detection task with graphemes or phonemes as subword units [@bib:fernandez2007application,lengerich2016end,wang2017small,zhuang2016unrestricted] .", "Lattice-free maximum mutual information (LF-MMI) is an HMM-based sequence-level loss first proposed in [@bib:povey2016purely] for ASR.", "In the context of wake word detection, it is recently investigated in [@bib:chen2018sequence] , where it still requires alignments from a prior model like an HMM-GMM system to generate numerator graphs.", "\\newline In this paper we propose a wake word detection system with alignment-free LF-MMI as training criterion, while not requiring any forced alignments for training.", "Alignment-free LF-MMI was initially proposed for ASR [@bib:hadian2018end] .", "In order to make it work for our task, we made several necessary adaptations/changes to the lexicon, HMM topology, data preprocessing for both efficiency and performance reasons.", "A fast online decoder is also proposed for our task.", "The experiments on three real wake word data sets all show its superior performance compared to other systems recently reported on the same data sets.", "\\newline  </section>"], ["<section> <title> 2 The Proposed System </title>  <subsection> <title> 2.1 HMM Topology </title> Different from other traditional HMM-based keyword-filler models (e.g., those proposed in [@bib:panchapagesan2016multi,sun2016compressed,wu2018monophone,chen2018sequence] where each phoneme of the whole wake phrase corresponds to an HMM or HMM state, we propose to model the whole wake phrase (in positive recordings) with a single HMM (referred to as {word} HMM), and the number of distinct states within that HMM is a predefined value which is not necessarily proportional to the number of phonemes in its pronunciation.", "We argue that using a fixed number of HMM states, which is usually less than the number of the actual phonemes, has enough modeling power for the wake word task.", "Similarly, we use another HMM of the same topology (referred to as {freetext} HMM) to model all non-silence speech (in negative recordings).", "From our preliminary experiments we also found that having an additional HMM dedicated to non-speech sounds, denoted {SIL} and called the {\u201csilence\u201d phone} , is crucial for good performance.", "The {SIL} phone is added as {optional} silence [@bib:chen2015pronunciation] to the beginning and end of each positive/negative recording so that it can learn the actual silence properly.", "The resulting topologies are shown in Fig. [@ref:LABEL:fig:hmm_topo] .", "\\newline </subsection> <subsection> <title> 2.2 Alignment-Free Lattice-Free MMI </title> Lattice-free MMI (LF-MMI) loss [@bib:povey2016purely] is a sequence-level criterion and can be formulated as: \\newline <equation> $ \\mathcal{F}_{\\textrm{LF-MMI}}=\\sum_{n=1}^{N}\\log P(L_{n}|\\mathbf{O}_{n})=\\sum_% {n=1}^{N}\\log\\frac{P(\\mathbf{O}_{n}|L_{n})P(L_{n})}{\\sum_{L}{P(\\mathbf{O}_{n}|% L)}P(L)} $ </equation> where $ L_{n} $ and $ L $ are the subword truth sequence and a competing hypothesis sequence respectively, and $ \\mathbf{O}_{n} $ is the input audio.", "In the regular LF-MMI the numerator graph used to compute the truth sequence is an acyclic graph generated from an existing GMM model.", "In alignment-free LF-MMI [@bib:hadian2018end] , the numerator graph is an unexpanded FST directly generated from training transcripts, giving more freedom to learn the alignments during the forward-backward pass in training.", "\\newline For ASR, the competing hypotheses in the denominator graph are constructed from a phone LM trained from the training transcripts.", "On the contrary, for our wake word detection task, we manually specify the topology of the phone LM FST as in Fig. [@ref:LABEL:fig:phonelm] .", "One path containing the {word} HMM corresponds to positive recordings , and the other two correspond to negative recordings (other speech/non-speech and silence).", "We assign final weights in a way such that they reflect the ratio of the number of positive/negative examples in the training set.", "\\newline </subsection> <subsection> <title> 2.3 Acoustic Modeling </title> Owing to efficiency and latency concerns specific to our task, the family of recurrent [@bib:hochreiter1997long,cho2014learning] or self-attention-based [@bib:vaswani2017attention] neural networks is not within our consideration.", "Instead, we use factorized TDNN (TDNN-F) with skip connections [@bib:povey2018semi] for acoustic modeling.", "In a TDNN-F layer, the number of parameters is reduced by factorizing the weight matrix in TDNN layers [@bib:peddinti2015time] into the product of two low-rank matrices, the first of which is constrained to be semi-orthogonal.", "\\newline As in other architectures like ResNet [@bib:he2016deep] , we incorporate skip connections: each TDNN-F layer receives its immediate prior layer\u2019s output as the skip connection, which is added to the input of the current layer after being scaled down by 0.66.", "\\newline We use a narrow (hidden dimension is 80) but deep (20 layers) network with each output frame covering a receptive field of size 80.", "The output is evaluated every 3 frames for LF-MMI loss to reduce the the computation cost both in training and test time.", "We also find a cross-entropy regularization together with the main LF-MMI loss helpful.", "As a result, the total number of parameters is about 150k, with the number of targets being only 18 using the HMM topologies described in Sec. [@ref:LABEL:sec:hmm_topo] .", "\\newline </subsection> <subsection> <title> 2.4 Data Preprocessing and Augmentation </title> Compared with the positive recordings, the negative recordings usually have a longer duration and have more variability as they can include all possible speech except the wake phrase.", "However we only use a single {freetext} HMM for it, making it difficult for the model to learn smoothly if batching them with positives directly (see Sec. [@ref:LABEL:sec:exp_subseg] ).", "To tackle this problem, before training we segment all negative recordings into several chunks so that the chunk lengths are draw from the empirical distribution of the positive recordings.", "As any segments from the original negative recordings can still be considered as negative, we just assign a negative label to each of them.", "\\newline Although all our training data is recorded in real environments with background noise, we still found that data augmentation is helpful.", "Therefore we apply the same type of data augmentation techniques as used in [@bib:wang2019jhu] , making use of noise, music, background speech from the MUSAN corpus [@bib:snyder2015musan] , simulated reverberation [@bib:ko2017study] and speed perturbation [@bib:ko2015audio] .", "This effectively increases the amount of training data 7 times.", "\\newline </subsection> <subsection> <title> 2.5 Decoding </title> We propose online decoding without lattice generation for wake word detection in the following way.", "\\newline First we construct our decoding graph with a word-level FST specifying the prior probabilities of all possible word paths, in a similar way as we specify the phone language model FST in Fig. [@ref:LABEL:fig:phonelm] , except that the start state and final states are merged to form a loop.", "The loop allows decoding with an audio interleaving with wake words and other possible speech.", "\\newline <float> Update the Immortal Token for Backtracking Input : activeTokList $ \\triangleright $ represents all current hypotheses Output : immortalTok $ \\triangleright $ is global, storing the latest one \\newline \\ procedure UpdateImmortalToken (activeTokList) \\ \\ emitting $ \\leftarrow $ $ \\emptyset $ \\ \\ for tok in activeTokList do \\ \\ while isNonEmittingToken(tok) do tok $ \\leftarrow $ tok.prev \\ \\ if tok $ \\neq $ NULL then emitting.insert(tok) \\ \\ tokenOne $ \\leftarrow $ NULL \\ \\ while True do \\ \\ if $ | $ emitting $ | $ = 1 then \\ \\ tokenOne $ \\leftarrow $ emitting[0]; break \\ \\ if emitting $ \\neq\\emptyset $ then break \\ \\ prevEmitting $ \\leftarrow $ $ \\emptyset $ \\ \\ for tok in emitting do \\ \\ prevTok $ \\leftarrow $ tok \\ \\ while isNonEmittingToken(tok) do \\ \\ prevTok $ \\leftarrow $ tok.prev \\ \\ if prevTok = NULL then continue \\ \\ prevEmitting.insert(prevTok) \\ \\ emitting $ \\leftarrow $ prevEmitting \\ \\ if tokenOne $ \\neq $ NULL then \\ \\ immortalTok $ \\leftarrow $ tokenOne \\ </float> \\newline During online decoding, every time after processing a fixed-length chunk from a recording, we backtrack along the frames delimited by two most recent \u201cimmortal tokens\u201d by calling the routine UpdateImmortalToken in Algorithm [@ref:LABEL:code:update_immortal_token] , checking if there is a wake word detected from this partial backtracking.", "If a wake word is found, we just stop decoding and trigger the system; otherwise continue the decoding process.", "The \u201cimmortal token\u201d is the common ancestor of all active tokens.", "Line [@ref:LABEL:code:update_immortal_token:init_begin] - [@ref:LABEL:code:update_immortal_token:init_end] obtain the last emitting token from each active token.", "Line [@ref:LABEL:code:update_immortal_token:track_begin] - [@ref:LABEL:code:update_immortal_token:track_end] are trying to find the common ancestor of all active tokens.", "Line [@ref:LABEL:code:update_immortal_token:update_begin] - [@ref:LABEL:code:update_immortal_token:update_end] update the immortal token if a newer one is found; otherwise keep the old one from the previous decoding step.", "\\newline The intuition is that, if all currently active partial hypotheses are from the same token at a previous time-step, all hypotheses before that token had already collapsed to one hypothesis (due to beam search and pruning), from which we would check whether it contains the wake word in a chunk-by-chunk fashion.", "\\newline </subsection>  </section>"], ["<section> <title> 3 Experiments </title>  <subsection> <title> 3.1 Data Sets </title> There are three real wake word data sets to be evaluated: SNIPS data set [@bib:coucke2019efficient] with the wake word \u2018\u2018Hey Snips\u2019\u2019, Mobvoi single wake word data set [@bib:wang2019adversarial] with the wake word \u2018\u2018Hi Xiaowen\u2019\u2019, and Mobvoi (SLR87) data set [@bib:hou2019region] with two wake words \u201cHi Xiaowen\u201d and \u201cNihao Wenwen\u201d.", "The statistics for each data set are summarized in Table [@ref:LABEL:tab:datasets] .", "We will use the first two data sets to demonstrate the effects of several design choices in our system, and give the final results on all these three data sets when comparing our system with others.", "If not otherwise specified, we show our experimental results in an incremental way, meaning that later experiments would be conducted on top of the one that is better from the previous experiment.", "The operating points in DET curves are obtained by varying the cost corresponding to the positive path in the decoding graph while keeping the cost corresponding to negative path at 0.", "40-dimensional MFCC features are extracted in all the experiments.", "\\newline </subsection> <subsection> <title> 3.2 Effect of Negative Recordings Sub-segmentation </title> We first show the effect of sub-segmenting negative recordings on training.", "We start from the training data with only speed-perturbation applied.", "To keep consistent with the numbers reported in others\u2019 work on the same data sets, false rejection rate (FRR) is reported in Table [@ref:LABEL:tab:subsegmentation] at 0.5 false alarms per hour (FAH) on the SNIPS data set, and at 1.5 FAH for Mobvoi.", "Apparently without sub-segmentation the performance is far from satisfactory, indicating the alignments learned with the LF-MMI system is poor.", "We also inspected the training/validation loss in both cases (not shown here), and found that there is severe overfitting when training without sub-segmentation.", "\\newline </subsection> <subsection> <title> 3.3 Effect of Data Augmentation </title> We augment the training data using the MUSAN corpus in the following way: we randomly apply additive noise from the \u201cbabble\u201d, \u201cmusic\u201d and \u201cnoise\u201d data sets separately on each copy of the original training data once.", "For each training example, \u201cbabble\u201d is added as background noises 3 to 7 times with SNRs ranging from 13 to 20; \u201cmusic\u201d is added as background noises once with SNRs ranging from 5 to 15; \u201cnoise\u201d is added as foreground noises at the interval of 1 second with SNRs ranging from 0 to 15.", "Then reverberation is also separately applied to the training data using the simulated RIRs with room sizes uniformly sampled from 1 meter to 30 meters.", "The above procedure increase the training data by a factor of 4.", "We then apply 3 way speed-perturbation on top of the original training set.", "The augmentation strategy together results in about 7 $ \\times $ more training data."]], "target": "The results before and after augmentation are shown in Table . It can be seen the augmentation strategy is highly effective, where FRR with SNIPS is even 0 at FAH=0.5."}, {"tabular": ["  Layer  &  Structure  &  Output  &  Parameters ", " Conv1+bn  &  32x11x11 (9)  &  72x72  &  11,712 ", " MaxPool  &  (2,2)  &  36x36  &  \u2013 ", " Conv2+bn  &  64x9x9 (7)  &  42x42  &  166,080 ", " MaxPool  &  (2,2)  &  21x21  &  \u2013 ", " Conv3+bn  &  128x7x7 (5)  &  25x25  &  401,792 ", " MaxPool  &  (2,2)  &    &  \u2013 ", " Conv4+bn  &  256x5x5 (3)  &  14x14  &  819,968 ", " MaxPool  &  (2,2)  &    &  \u2013 ", " Conv5+bn  &  512x3x3 (1)  &  7x7  &  1,181,184 ", " MaxPool  &  (2,2)  &    &  \u2013 ", " Linear  &  \u2013  &  1024  &  4,719,616 ", " BatchNorm1d  &  \u2013  &  1024  &  2,048 ", " Linear  &  \u2013  &  256  &  262,400  "], "ref_sec": [["<section> <title> 1 Introduction </title>  In the fight against the spread of disinformation [@bib:cdc,conv] through manipulated media content [@bib:wsj] , understanding the story and intent behind the manipulated object is critical.", "Whether the manipulations are benign or malicious, a step-by-step analysis of how the current version of the manipulated image or video was generated helps us in answering more holistic and contextual questions than just whether a given image or video is real or fake.", "Unlike the early days of the web, a media object today does not exist in isolation.", "Most often there are multiple versions of a single image online at any given time (Fig. [@ref:LABEL:fig:teaserreal] ).", "Tracing the uploads, downloads and re-uploads of original image content with modifications can help assess the reach of that content.", "If an image is a composite, analyzing its different versions and other images that donated content to its creation can provide explanations for the types of manipulations and their complexity.", "Beyond images on social media, we can also consider other image-based applications such as autonomous driving, where it is important to know if the data being used to train a visual recognition system is the original version and distinguish between benign and malicious data attacks.", "\\newline Image Provenance Analysis [@bib:moreira2018image] aims to understand the evolution of a media object in question by establishing pairwise associations among images related in content to generate its provenance graph.", "An example of provenance analysis for a small set of images is shown in Figure [@ref:LABEL:fig:teaser] .", "Provenance analysis usually involves two stages: (1) a specialized image retrieval stage to obtain images related to a query called provenance filtering , and (2) a graph construction stage to model the relationships between the retrieved images.", "While both steps employ vision-based techniques (they rely on establishing image correspondences), there has been limited research on the latter problem.", "\\newline In the literature, provenance graph construction algorithms are based on image phylogeny algorithms [@bib:dias2012image,dias2013toward,oliveira2014multiple] .", "Provenance graph construction uses generalized definitions of image relationships and circumvents the phylogeny constraints such as specific image formats and a two-donor limit for composite images [@bib:bharati2017uphy] .", "These algorithms use points of interest to establish image correspondence and compute pairwise image dissimilarity by describing matched local regions.", "Upon computing a dissimilarity score for all possible pairs, a greedy algorithm is employed to create a minimally connected Directed Acyclic Graph (DAG) [@bib:moreira2018image] of images.", "Depending on the type of manipulations performed on an image, the graph can be very complicated with multiple donor images (images that share partial content) and long chains formed by near-duplicate images (images derived from a single image through a series of transformations).", "This work focuses on improving the fidelity of reconstructing the chains in the provenance graphs.", "\\newline Greedy graph algorithms treat dissimilarity values as adjacency weights and rely heavily on invariant image matching.", "Image matching is a common task in many computer vision problems such as object recognition, 3D reconstruction, scene understanding and image retrieval.", "A desired property for representations used to solve these problems is invariance to view changes, compression and other image transformations.", "Optimizing this property while learning the mapping between the data $ \\mathbb{X} $ and label $ \\mathbb{Y} $ can easily miss understanding the fine differences between image transformations.", "Thus invariance becomes something of a misnomer in the context of forensics.", "\\newline We propose to learn representations that are aware of different versions of images in the transformation space, and depending on the number of transformations, can encode appropriate distance among near-duplicate images.", "This approach can be useful for improving dissimilarity computation between different versions of an image.", "Better understanding of the subtle differences in the variants can lead to improved rank-based output in deducing a sequence of transformations for image forensics [@bib:farid2009image] and cultural analytics [@bib:manovich2009cultural] .", "It can also be used to define acceptable standards of edited data for learning algorithms.", "To our knowledge, this is the first work that focuses on solving the image ordering aspect of provenance analysis.", "This work also highlights the importance of awareness of transformation-based differences among near-duplicate images while learning image representations.", "\\newline  </section>"], ["<section> <title> 2 Related Work </title>  Most of the recent research related to image provenance analysis and other media forensics problems [@bib:moreira2018image,huh2018fighting,zhou2018learning,d2018patchmatch,wu2018busternet,rossler2018faceforensics] is related to the DARPA Media Forensics program [@bib:darpa] through its challenges and datasets.", "The first end-to-end algorithm [@bib:moreira2018image] proposed for provenance analysis coming out of that program followed a two-step strategy to obtain a graph-based relationship representation of images that are near-duplicates of or share partial content with the query image.", "Firstly, a specialized image retrieval algorithm that employed Speeded Up Robust Features (SURF) [@bib:bay2006surf] for description, Optimized Product Quantization (OPQ) [@bib:ge2013optimized,johnson2017billion] for efficient indexing, and iterative filtering is used to obtain a list of images related to the query.", "In the second step, a keypoint-based pairwise image comparison is performed to obtain a dissimilarity matrix.", "The most feasible provenance graph is created using this matrix through a hierarchical clustering-based graph expansion method.", "The latter step can also be interpreted as ordering pair similarities between multiple image pairs, and is therefore a natural extension to pairwise image comparison.", "For output, the ordered pairings are modeled as a graph, where each edge denotes a transformation-based correspondence, and the images in the pair are the vertices [@bib:bharati2017uphy] .", "\\newline Image ordering can be defined as a specific task that outputs an ordered list of provided input images along with producing a matching score between two images.", "This can correspond to ranking images with respect to distance from a query image in retrieval algorithms or identification scenarios in recognition.", "Approaches in the existing literature which are a source of inspiration for the proposed work are discussed below.", "We categorize them broadly to explain how advances in techniques for these general tasks become useful in designing a pipeline to learn transformation-aware embeddings for ordering images in provenance analysis.", "\\newline <subsection> <title> 2.1 Image Matching </title> Ordering can be considered as an additional step to image matching as it involves a measure of how well images match.", "Image matching algorithms use points of interest to learn whether two images have the same content with respect to structure [@bib:bay2006surf,yi2016lift,leng2018local] .", "Such algorithms are integral parts of correspondence tasks where one cares about the exact locations two image contents match.", "Due to the nature of the applications that require image correspondence (dense or sparse), these matches [@bib:zagoruyko_2015,revaud2016deepmatching] are intended to encode invariance across a range of transformations, i.e. , they attempt to compute a feature space where different versions of an image region map to the same vicinity.", "In the case of handcrafted feature matching, the design of keypoint detectors and patch descriptors such as SIFT [@bib:lowe1999object] and SURF [@bib:bay2006surf] mostly imparts invariance to linear affine transformations such as scale, transformation, rotation and others.", "\\newline For learning-based correspondence, invariance to more transformations can be achieved through hierarchical convolutional architectures and by introducing transformed or augmented versions of data points during training [@bib:xuzhang2017learningdisc,fischer2014descriptor] .", "Encoding invariance in one or the other way helps to improve the robustness of matching and make it stable under extreme transformations or view-point changes.", "For applications such as provenance analysis, we require an understanding of the measure of invariance ( i.e. , how much invariance is present) in addition to mapping features of near-duplicate or similar images closer to each other than those of dissimilar images.", "\\newline </subsection> <subsection> <title> 2.2 Distance Metric Learning </title> Learning an embedding space and similarity score with respect to a specific semantic label becomes useful in creating generalizable frameworks that can be used for classification as well as clustering tasks.", "They are more suited for open set scenarios as the scores are learned on a match vs. non-match basis and do not compute per-class probability forcing the score to be maximized for one of $ n $ classes.", "Classification-based losses for metric learning have shown promising results in applications such as face verification, but pairwise losses are more prevalent for image retrieval scenarios [@bib:zhai2019classification,movshovitz2017no] .", "Intuitively, networks trained with pairwise losses are intended to generalize better.", "Among the large pool of such loss functions including, but not limited to, contrastive [@bib:simo2015discriminative] , triplet [@bib:kumar2016learning] and N-pair [@bib:sohn2016improved] , triplet loss is the most popular and is effective in numerous image comparison tasks.", "Most of these approaches learn similarity in a presumably metric space [@bib:scheirer2014good] but mostly for classification.", "Methods that use augmented triplet loss [@bib:xuzhang2017learningdisc,xuzhang2017learningspr] for detection and description in a transformation-aware space have inspired this work.", "The method proposed in this paper uses quadruplets that contain an extra weak positive sample for ordering distance between positive samples.", "A different quadruplet approach was employed by Chen et al. [@bib:chen2017beyond] for person re-identification and Zhang et al. [@bib:zhang2017learning] for image correspondence.", "Chen et al. claim better generalization for their test sets through the use of two references, one each for the positive and negative samples in the quadruplets (set of two triplets), than triplet loss.", "\\newline </subsection> <subsection> <title> 2.3 Learning to Rank </title> Image ordering involves comparing all possible image pairs and then ranking them based on the distance from the reference or query.", "Hence, this operation is similar to algorithms from the information retrieval domain such as RankNet [@bib:burges2005learning] and ListNet [@bib:cao2007learning] that aim towards learning a correct ranking among a set of objects.", "RankNet minimizes the number of inversions in the rank as its objective whereas ListNet employs listwise loss functions.", "Chen et al. [@bib:chen2009ranking] explain the relationship between losses used for learning to rank and the evaluation metrics.", "They do so using essential loss to model ranking as a sequence of classification tasks and establish that the minimization of ranking losses lead to the maximization of evaluation measure.", "Our quadruplets are lists of four images where three are related through image transformations.", "\\newline So far, in the image domain, learning to rank methods have focused more on semantically similar images versus dissimilar images rather than near-duplicates [@bib:hu2008multiple,Cakir_2019_CVPR] .", "Semantically similar images are images that share content but have not been derived from the same image and come from different imaging pipelines.", "It makes sense for applications such as content-based image retrieval to focus more on such images as they care about variety and relevance in the retrieved results.", "Retrieving near-duplicates would not be very useful in such scenarios as a user would want to see different images (in terms of camera, view and context) of the same content.", "Whereas", "in order to perform provenance analysis on any image, it becomes imperative to attempt to understand the near-duplicate variants at a finer granularity in terms of the image space.", "\\newline </subsection>  </section>"], ["<section> <title> 3 Transformation-Aware Distance Learning </title>  A fundamental task in provenance analysis, like other visual recognition problems, is computing the dissimilarity between two images.", "Techniques currently used in the literature, such as keypoint matching and color-wise content comparison with mutual information, present the drawback of not taking into account modifications other than affine and basic color transformations.", "They also do not consider complex transformations and their ordering.", "Aiming to increase the reliability of dissimilarity computation between two images that share content in a way where one gives origin to the other, after a number of transformations, we propose the use of deep distance learning.", "\\newline A key contribution of the proposed framework is a ranking-based network training approach that learns how to express the dissimilarity between such images as a function of the number of transformations.", "We describe the approach in detail in the following sections.", "\\newline <subsection> <title> 3.1 Training with Quadruplets </title> Similar to triplet-loss training, we want to encode images in a space where, given an image of reference ( i.e. , the anchor), positively related images lie closer to it than unrelated images.", "For our purpose, we have the additional requirement of ordering the positively related images, which limits the use of conventional triplet sampling regime.", "Hence, we propose learning embeddings using quadruplets to better facilitate ordering among the related images.", "\\newline When training the network, we provide sets of four patches, namely (i) the {anchor} patch, which represents the original content, (ii) the {positive} patch, which stores the anchor after $ M $ image processing transformations, (iii) the {weak positive} patch, which stores the positive patch after $ N $ transformations, and (iv) the {negative} patch, a patch that is unrelated to the others (see Figure [@ref:LABEL:fig:patches] ).", "The idea is to train the embedding network to provide a distance score to a given pair of patches, where the output score between the anchor and the positive patch is smaller than the one between the anchor and the weak positive, which, in turn, is smaller than the score between the anchor and the negative patch.", "\\newline To obtain the quadruplets of patches for training, we employ a specific set of image transformations that are of interest to provenance analysis.", "They are: (i) projective changes ( e.g. , content scaling, rotation, flipping, shear, and projection), (ii) color-space changes ( e.g. , changes in brightness, in contrast, gamma correction, and grayscaling), (iii) frequency-space changes ( e.g. , blurring and sharpening), and (iv) data-lossy compression.", "For each anchor patch, random transformations from this pool are sequentially applied, one on top of the result of the other, allowing us to generate positive and weak positive patches from the anchor, after $ M $ and $ M+N $ transformations, respectively.", "Figure [@ref:LABEL:fig:patches] depicts a few examples of these quadruplets.", "\\newline </subsection> <subsection> <title> 3.2 Network, Loss Function and Optimization </title> The network used to learn representations is a four-way Siamese structure with all four branches of the network sharing weights (see the training block of Figure [@ref:LABEL:fig:pipeline] ).", "Each embedding module is a five-layer convolutional neural network (CNN) with one batch normalization layer and two fully connected layers, very similar to the one used in [@bib:chen2017image] to identify image processing operations.", "In contrast to them, we do not preprocess the images using high-pass filters and let the network learn useful noise patterns depending on the transformation.", "Our convolutional layers use Rectified Linear Unit (ReLU) as activation function for faster convergence.", "Max-pooling is applied to feature maps from convolutional layers.", "We use relatively large kernels in our convolutional layers to have a larger receptive field for our learned feature maps."]], "target": "Details of the network architecture are provided in Table . The features extracted from the embedding units are used by the pairwise distance ranking unit to compute the loss."}, {"tabular": ["  Literature  &  Pr(Defect) <ln> Observed  &  Pr(Defect) <ln> Computed(Model 1)  &  Fit errors <ln> model 1  &  Pr(Defect) <ln> Computed(model 2)  &  Fit errors <ln> model 2  &  Pr(Defect) <ln> Computed(Proposed model)  &  Fie errors <ln> Proposed model ", " Li and Taplin,2002 1  &  0.8667  &  0.6334  &  0.2692  &  0.8113  &  0.0639  &  0.8623  &  0.0051 ", " Li and Taplin,2002 2  &  0.7000  &  0.5333  &  0.2381  &  0.7006  &  0.0009  &  0.6691  &  0.0441 ", " Li and Taplin,2002 3  &  0.7667  &  0.5500  &  0.2826  &  0.7159  &  0.0663  &  0.7005  &  0.0863 ", " Busemeyer et al.,2006a  &  0.6600  &  0.6250  &  0.0531  &  0.7995  &  0.2113  &  0.6069  &  0.0805 ", " Hristova and Grinberg, 2008  &  0.8800  &  0.7000  &  0.2045  &  0.8968  &  0.0191  &  0.9045  &  0.0279 ", " Average fit error  &  -  &  -  &  0.2095  &  -  &  0.0723  &  -  &  0.04878  "], "ref_sec": [["<section> <title> 1 Introduction </title>  Prisoner\u2019s dilemma game is a famous metaphor for the problem of cooperation, which is a critical issue in evolutionary game theory [@bib:Wang2017Onymity,Deng2017Sensors922] .", "If two players all defect, the payoff will be lower than if they all cooperates, as shown in Table [@ref:LABEL:payoff] .", "The paradoxical findings are shown in Table [@ref:LABEL:experimentresultsinliterature] , where the unknown part is not equal to the last column.", "The violation of $ The\\ Sure\\ thing\\ Principle $ [@bib:Zachow1978Positive] shows humans break the law of classic probability when making decision under risk [@bib:Ellsberg1963Risk] .", "Many analytical mythologies have been made to the explanation of this phenomenon but the underlying mechanisms are still enigmatic.", "Nevertheless, the quantum theory seems to be a practical method to uncover the mystery lying behind this incredible phenomenon [@bib:Pawela2013Quantum,Bruza2009Is] .", "\\newline The quantum theory has been applied in many filed including information theory [@bib:song2015finite] , decision making system [@bib:yufirst2016,yu2015enhancing] , social and information networks [@bib:wang2016statistical,yu2016system] .", "Busemeyer $ et\\ al. $ [@bib:Busemeyer2009Empirical,Pothos2009A] proposed a Quantum Dynamical model based on a quantum version of a classical dynamical Markov model, which takes the process of making decisions into account of time evolution.", "The quantum-like approach developed by Khrennikov [@bib:Khrennikov2003Quantum] is based on contextual probabilities which can be applied to many domains like cognitive science economics, game theory, etc [@bib:Khrennikov1997Non,Khrennikov2012On,Khrennikov2009Quantum] .", "Masanari $ et\\ al. $ [@bib:Asano2011Quantum,Asano2012Quantum] proposed a quantum-like model to simulate the brain function.", "Li $ et\\ al. $ [@bib:Li2011Quantum] proposed a quantum strategies into evolutionary games.", "\\newline Though there are many models based on quantum probability theory, few of them are predictable.", "Inspired by the work [@bib:Moreira2016Quantum,Iqbal2010Constructing,Shah2008Heuristics] , we propose a novel Bayesian Networks model based on quantum probability.", "This paper does not consider the noise effect in the quantum information systems [@bib:Situ2016Relativistic,Situ2016Noise] .", "In this model, the violation of rational decision making in many experiments like Prisoner\u2019s dilemma game and the Two Stage Gambling game is characterized as interference effect between competing states.", "This paper regards man\u2019s mental beliefs as wave functions.", "Before the final decision is made, all potential decisions coexists in man\u2019s mind.", "Such uncertainty is like superposition state of wave functions [@bib:Moreira2014Interference] .", "The interference effect is actually influenced by the partiality of the man towards to the decisions.", "Once the interference effect is determined, the man\u2019s behavior can be predicted and described by quantum probability theory.", "This paper proposes Belief Distance to measure the uncertainty with the assistance of belief entropy, named as Deng entropy.", "Uncertainty processing in decision making was firstly developed by Mich le and Jean Yves [@bib:Cohen1985Decision] and the uncertainty can be measured based on distance [@bib:AnImprovedAPIN2017,mo2016generalized] .", "The knowledge to the uncertainty in decision making can help psychologists predict the behavior of humans with few fit errors.", "With the ability to compute the uncertainty of decision, the proposed model is predictable and simple for calculating.", "\\newline  </section>"], ["<section> <title> 2 Organization of this paper </title>  This paper is organized in the following manner.", "In section [@ref:LABEL:Preliminaries] , basic mathematical preliminaries will be introduced.", "In this section, a kind of belief entropy, called Deng entropy, will be introduced, which plays an important role in the model.", "After that the Bayesian model based on quantum probability will be presented in section [@ref:LABEL:proposedmodel] .", "Numerical examples will be illustrated in section to show how this model works.", "In the end, the proposed model will be compared with two models proposed in other literature to show its effectiveness.", "\\newline  </section>"], ["<section> <title> 3 Preliminaries </title>  <subsection> <title> 3.1 Belief Entropy </title> Many contributions [@bib:Vourdas2015Mobius,Vourdas2014Lower,Vourdas2014Quantum] have been made to interpret quantum probability into Dempster-Shafer probability, in which basic belief assignment is used to describe the probability of an event [@bib:Zhang2016ANP,Tang2017A] .", "A new belief entropy, named as Deng entropy [@bib:dengentropy] is a measure of uncertainty of basic belief assignment [@bib:deng2015Generalized,Jiang2017mGCR] .", "Basic belief assignment(BBA) is widely used in the field of information fusion [@bib:Jiang2017Ordered,Jiang2016CAIE,Wangjw2016evidence] which has been applied in many fields like Failure Mode and Effect Analysis [@bib:jiang2017FMEA,jiang2017Ranking] , Fault Diagnose [@bib:Yuan2016Modeling,Jiang2016sensor] and so on.", "\\newline <theorem> Definition 3.1 Let $ \\theta=\\left\\{{{H_{1}},{H_{2}},...,{H_{N}}}\\right\\} $ be a finite nonempty set of N elements which is mutually exclusive and exhaustive.", "Denote $ P(\\theta) $ as the power set composed of $ 2^{N} $ elements of $ \\theta $ .", "The basic belief assignments(BBAs) function is defined as a mapping of the power set $ P(\\theta) $ to the value between $ 0 $ and $ 1 $ .", "$ m:P(\\theta)\\to[0,1] $ , which satisfies the following conditions: \\newline <equationgroup> <equation> $  m(\\emptyset)=0 $ $  m(\\emptyset)=0 $ </equation> <equation> $ \\sum\\nolimits_{A\\subseteq P(\\theta)}{m(A)=1} $ $ \\sum\\nolimits_{A\\subseteq P(\\theta)}{m(A)=1} $ </equation> </equationgroup> where the mass m(A) represents the support degree of evidence to event $ A $ .", "\\newline </theorem> Shannon entropy, also named as information entropy, is the expected value of the information contained in each message which can be modeled by any flow of information.", "\\newline <theorem> Definition 3.2 The Shannon entropy is defined as follows: \\newline <equation> $ H=-\\sum\\nolimits_{i}{{P_{i}}}{\\log_{b}}{P_{i}} $ </equation> where $ P_{i} $ satisfies $ \\sum\\nolimits_{i}{P_{i}}=1 $ , $ b $ is base of logarithm.", "When $ b=2 $ , the unit of Shannon entropy is bit.", "\\newline </theorem> The Belief entropy, named as Deng entropy, is introduced here to measure the uncertainty degree of BBAs, which is defined by: \\newline <theorem> Definition 3.3 <equation> $ {E_{d}}=-\\sum\\nolimits_{i}{m(A)}\\log\\frac{{m(A)}}{{{2^{\\left|A\\right|}}-1}} $ </equation> Where $ m $ is the BBAs function, and $ A $ is the element of $ P(\\theta) $ , $ \\left|A\\right| $ is the cardinality of $ A $ .", "When $ \\left|A\\right| $ is equal to $ 1 $ , the belief entropy will degenerate into Shannon entropy.", "The term $ 2^{\\left|A\\right|}-1 $ represents the potential states in A. \\newline </theorem> Example 1 Assume there is a BBAs function m(a)=1.", "The Shannon entropy and Deng entropy are computed as follows: \\newline $ H=-1\\times log_{2}1=0 $ \\newline $ E_{d}=-1\\times log_{2}\\frac{1}{2^{1}-1}=1 $ \\newline This example shows if $ \\left|A\\right| $ is equal to $ 1 $ , the belief entropy is similar with the classic Shannon entropy.", "\\newline Example 2 Given a set $ \\Theta=\\{a,b,c\\} $ with $ m(\\{a\\})=\\frac{1}{2} $ and $ m(\\{b,c\\})=\\frac{1}{2} $ .", "The Deng entropy will be: \\newline $ E_{d}=-\\frac{1}{2}\\times log_{2}\\frac{\\frac{1}{2}}{2^{2}-1} $ \\newline The above examples show how Deng entropy works and overcomes the insufficiency of Shannon entropy when measuring the uncertainty in problems like Example 2.", "\\newline </subsection> <subsection> <title> 3.2 The Classic Bayesian Network and the Quantum-like Bayesian Model </title> <subsubsection> <title> 3.2.1 Classic Bayesian Network </title> A classic Bayesian Network is a kind of probabilistic directed acyclic graphical model, which has been successfully applied in the field of decision making [@bib:Myung2005A] .", "In this model, a set of random variables and their conditional dependencies are represented via a directed acyclic graph.", "Each node that represents a variable is associated with a conditional probability table, as shown in Fig. [@ref:LABEL:BayesianNetwork] .", "\\newline <theorem> Definition 3.4 The full joint distribution of a Bayesian Network is defined by: \\newline <equation> $ \\Pr({X_{1}},{X_{2}}...,{X_{n}})=\\prod\\limits_{i=1}^{n}{\\Pr({X_{i}}|Parents({X_% {i}}))} $ </equation> \\newline </theorem> where $ X $ is the list of variables, $ Parents(X_{i}) $ means nodes pointing to $ X_{i} $ .", "The model can answer any query with response of $ yes $ or $ no $ by using conditional probability formula and summing over all nuisance variables.", "For some query $ X $ , the inference is given by Eq.( [@ref:LABEL:123] ) \\newline <equation> $ \\begin{split}\\Pr(X|e)=\\alpha[\\sum\\limits_{y\\in Y}{\\Pr(X,e,y)}]\\\\  where\\quad\\alpha=\\frac{1}{{\\sum\\nolimits_{x\\in X}{{{\\Pr}_{c}}(X=% x,e)}}}\\end{split} $ </equation> where $ e $ is the list of observed variables (nodes) and $ y $ is the remaining unobserved variables(nodes) in the network, the $ \\alpha $ is the normalization factor for the distribution $ \\Pr(X|e) $ [@bib:Russel2002Artificial] .", "\\newline Example: Fig. [@ref:LABEL:BayesianNetwork] shows an example of Bayesian Network.", "Assume there are two servers $ S1 $ and $ S2 $ transmitting data packets to $ User $ .", "Apparently, the parent nodes of $ User $ are $ S1 $ and $ S2 $ and the parent node of $ S2 $ is $ S1 $ .", "Each node has a conditional probability table which represents if a packet is transmitted successfully.", "If there is a query, for example, what is the probability when $ user $ successively receives one data packet.", "The inference is computed by Eq.( [@ref:LABEL:123] ) as follows: \\newline <equationgroup> <equation> $ \\Pr(One\\ Packet)=\\alpha\\{Pr(S2=T|S1=F)*Pr(S1=F) $ $ \\Pr(One\\ Packet)=\\alpha\\{Pr(S2=T|S1=F)*Pr(S1=F) $ </equation> <equation> $ +Pr(S2=F|S1=T)*Pr(S1=T)\\}=\\alpha(0.3*0.1+0.3*0.9)=0.3\\alpha $ $ +Pr(S2=F|S1=T)*Pr(S1=T)\\}=\\alpha(0.3*0.1+0.3*0.9)=0.3\\alpha $ </equation> <equation> $ \\Pr(two\\ or\\ zero\\ Packets)=\\alpha\\{Pr(S2=T|S1=T)*Pr(S1=T) $ $ \\Pr(two\\ or\\ zero\\ Packets)=\\alpha\\{Pr(S2=T|S1=T)*Pr(S1=T) $ </equation> <equation> $ +Pr(S2=F|S1=F)*Pr(S1=F)\\}=\\alpha(0.7*0.9+0.7*0.1)=0.7\\alpha $ $ +Pr(S2=F|S1=F)*Pr(S1=F)\\}=\\alpha(0.7*0.9+0.7*0.1)=0.7\\alpha $ </equation> <equation> $ \\alpha=\\frac{1}{\\Pr(One\\ Packet)+\\Pr(two\\ or\\ zero\\ Packets)}=1 $ $ \\alpha=\\frac{1}{\\Pr(One\\ Packet)+\\Pr(two\\ or\\ zero\\ Packets)}=1 $ </equation> </equationgroup> The above example shows the basic idea of Bayesian network and procedure of deriving inferences according to some queries.", "\\newline </subsubsection> <subsubsection> <title> 3.2.2 Quantum-like Bayesian Model </title> Bayesian networks can split complex problem into small modules that can be combined to perform inferences [@bib:Khrennikov2016Quantum,Barros2009Quantum] .", "The quantum-like Bayesian Model [@bib:Moreira2016Quantum] replaces the real probability numbers in the classic probability Bayesian Network model with quantum probability amplitudes [@bib:ROBERT1997QUANTUM,Leifer2008Quantum] .", "\\newline The corresponding part of quantum-like Bayesian Network model to the application of Born\u2019s rule to Eq.( [@ref:LABEL:classicfulljoint] ) is: \\newline <equation> $ \\Pr({X_{1}},...,{X_{n}})=|\\prod\\limits_{i=1}^{n}{\\psi({X_{i}}|Parents({X_{i}})% )}{|^{2}} $ </equation> \\newline The quantum application of Born\u2019s rule to the classic marginal probability distribution Eq.( [@ref:LABEL:123] ) is defined by the equation below: \\newline <equationgroup> <equation> $ \\Pr(X|e)=\\partial{\\rm{|}}\\sum\\limits_{Y}{\\prod\\limits_{x}^{N}{% \\psi({X_{x}}|Parents({X_{x}}),e,y)}}{{\\rm{|}}^{2}} $ $ \\Pr(X|e)=\\partial{\\rm{|}}\\sum\\limits_{Y}{\\prod\\limits_{x}^{N}{% \\psi({X_{x}}|Parents({X_{x}}),e,y)}}{{\\rm{|}}^{2}} $ </equation> </equationgroup> <equation> $ Where\\quad\\partial=\\frac{1}{{\\sum\\nolimits_{x\\in X}{{{\\Pr}_{c}}(X=x,e)}}}=1 $ </equation> \\newline A quantum marginalization formula with interference effects [@bib:Moreira2014Interference] emerges when the Eq.( [@ref:LABEL:margin] ) expands, as shown in below, \\newline <equationgroup> <equation> $ \\Pr(X|e)=\\partial\\sum\\limits_{i=1}^{|Y|}{|\\prod\\limits_{x}^{N}{% \\psi({X_{x}}|Parents({X_{x}}),e,y=i)}{|^{2}}+2\\cdot Interference} $ $ \\Pr(X|e)=\\partial\\sum\\limits_{i=1}^{|Y|}{|\\prod\\limits_{x}^{N}{% \\psi({X_{x}}|Parents({X_{x}}),e,y=i)}{|^{2}}+2\\cdot Interference} $ </equation> <equation> $  Interference=\\sum\\limits_{i=1}^{|Y|-1}{\\sum\\limits_{j=i+1}^{|Y|}% {|\\prod\\limits_{x}^{N}{\\psi({X_{x}}|Parents({X_{x}}),e,y=i)}|\\cdot}} $ $  Interference=\\sum\\limits_{i=1}^{|Y|-1}{\\sum\\limits_{j=i+1}^{|Y|}% {|\\prod\\limits_{x}^{N}{\\psi({X_{x}}|Parents({X_{x}}),e,y=i)}|\\cdot}} $ </equation> <equation> $ \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad{{|\\prod\\limits_{x}^{N}{% \\psi({X_{x}}|Parents({X_{x}}),e,y=j)}|}\\cdot\\cos({\\theta_{i}}-{\\theta_{j}})} $ $ \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad{{|\\prod\\limits_{x}^{N}{% \\psi({X_{x}}|Parents({X_{x}}),e,y=j)}|}\\cdot\\cos({\\theta_{i}}-{\\theta_{j}})} $ </equation> </equationgroup> \\newline Example: Fig. [@ref:LABEL:model1] shows an instance of Quantum-like Bayesian Network.", "\\newline This network can only answer queries with $ yes $ or $ no $ answer, which are regarded as base vectors $ |0> $ and $ |1> $ .", "Fig. [@ref:LABEL:model2] shows any actions the node will take can be seen as wave functions characterized by base vectors $ |0> $ and $ |1> $ , as defined by: \\newline <equationgroup> <equation> $ |T>\\ =\\ cos\\theta_{T}|1>\\ +\\ sin\\theta_{T}|0>\\ =\\ e^{j\\theta_{T}} $ $ |T>\\ =\\ cos\\theta_{T}|1>\\ +\\ sin\\theta_{T}|0>\\ =\\ e^{j\\theta_{T}} $ </equation> <equation> $ |F>\\ =\\ cos\\theta_{F}|1>\\ +\\ sin\\theta_{F}|0>\\ =\\ e^{j\\theta_{F}} $ $ |F>\\ =\\ cos\\theta_{F}|1>\\ +\\ sin\\theta_{F}|0>\\ =\\ e^{j\\theta_{F}} $ </equation> </equationgroup> \\newline Thus, the decision vector for node $ A $ is defined by: \\newline <equation> $ |{\\phi_{A}}>\\ =\\ {\\psi_{A=T}}{\\rm{|}}{{\\rm{T}}_{A}}>{\\rm{\\ +\\ }}{\\psi_{A=F}}|{% F_{A}}>\\ =\\ \\psi_{A=T}\\cdot e^{j\\theta_{T_{A}}}\\ +\\ \\psi_{A=F}\\cdot e^{j\\theta% _{F_{A}}}\\\\ $ </equation> where the action states $ |{F_{A}}> $ and $ |{T_{A}}> $ means the actions the node can take.", "The index $ A $ in $ |{F_{A}}> $ and $ |{T_{A}}> $ represents this decision is made by node $ A $ .", "\\newline In the same way, Decision vector for node $ B $ is \\newline <equation> $ |{\\phi_{B}}>\\ =\\ {\\psi_{B=T}}{\\rm{|}}{{\\rm{T}}_{B}}>{\\rm{\\ +\\ }}{\\psi_{B=F}}|{% F_{B}}>\\ =\\ \\psi_{B=T}\\cdot e^{j\\theta_{T_{B}}}\\ +\\ \\psi_{B=F}\\cdot e^{j\\theta% _{F_{B}}}\\\\ $ </equation> \\newline For a query \u201dwhat is the probability for B to adopt action T ?\u201d, the inference is computed by Eq.( [@ref:LABEL:margin] ): \\newline <equationgroup> <equation> $ \\Pr(T|A)\\ =\\ \\partial|\\psi_{B=T}\\cdot e^{j\\theta_{T_{B}}}\\cdot% \\psi_{A=T}\\cdot e^{j\\theta_{T_{A}}}\\ +\\ \\psi_{B=T}\\cdot e^{j\\theta_{T_{B}}}% \\cdot\\psi_{A=F}\\cdot e^{j\\theta_{F_{A}}}|^{2} $ $ \\Pr(T|A) $ $ \\ =\\ \\partial|\\psi_{B=T}\\cdot e^{j\\theta_{T_{B}}}\\cdot\\psi_{A=T}% \\cdot e^{j\\theta_{T_{A}}}\\ +\\ \\psi_{B=T}\\cdot e^{j\\theta_{T_{B}}}\\cdot\\psi_{A=% F}\\cdot e^{j\\theta_{F_{A}}}|^{2} $ </equation> <equation> $ \\ =\\ \\partial|\\psi_{B=T}\\cdot\\psi_{A=T}\\cdot e^{j\\theta_{1}}\\ +\\ % \\psi_{B=T}\\cdot\\psi_{A=F}\\cdot e^{j\\theta_{2}}|^{2} $ $ \\ =\\ \\partial|\\psi_{B=T}\\cdot\\psi_{A=T}\\cdot e^{j\\theta_{1}}\\ +\\ % \\psi_{B=T}\\cdot\\psi_{A=F}\\cdot e^{j\\theta_{2}}|^{2} $ </equation> <equation> $ \\ =\\ \\partial|\\psi_{B=T}\\cdot\\psi_{A=T}\\cdot e^{j\\theta_{1}}\\ +\\ % \\psi_{B=T}\\cdot\\psi_{A=F}\\cdot e^{j\\theta_{2}}| $ $ \\ =\\ \\partial|\\psi_{B=T}\\cdot\\psi_{A=T}\\cdot e^{j\\theta_{1}}\\ +\\ % \\psi_{B=T}\\cdot\\psi_{A=F}\\cdot e^{j\\theta_{2}}| $ </equation> <equation> $ \\ \\ \\ \\ \\ \\cdot|\\psi_{B=T}\\cdot\\psi_{A=T}\\cdot e^{j\\theta_{1}}\\ +% \\ \\psi_{B=T}\\cdot\\psi_{A=F}\\cdot e^{j\\theta_{2}}|^{*} $ $ \\ \\ \\ \\ \\ \\cdot|\\psi_{B=T}\\cdot\\psi_{A=T}\\cdot e^{j\\theta_{1}}\\ +% \\ \\psi_{B=T}\\cdot\\psi_{A=F}\\cdot e^{j\\theta_{2}}|^{*} $ </equation> <equation> $ \\ =\\ \\partial|\\psi_{B=T}\\cdot\\psi_{A=T}|^{2}\\ +\\ |\\psi_{B=T}\\cdot% \\psi_{A=F}|^{2} $ $ \\ =\\ \\partial|\\psi_{B=T}\\cdot\\psi_{A=T}|^{2}\\ +\\ |\\psi_{B=T}\\cdot% \\psi_{A=F}|^{2} $ </equation> <equation> $ \\ \\ \\ \\ +\\ \\psi_{B=T}\\cdot\\psi_{A=T}\\cdot e^{j\\theta_{1}}\\cdot% \\psi_{B=T}\\cdot\\psi_{A=F}\\cdot e^{-j\\theta_{2}} $ $ \\ \\ \\ \\ +\\ \\psi_{B=T}\\cdot\\psi_{A=T}\\cdot e^{j\\theta_{1}}\\cdot% \\psi_{B=T}\\cdot\\psi_{A=F}\\cdot e^{-j\\theta_{2}} $ </equation> <equation> $ \\ \\ \\ \\ +\\ \\psi_{B=T}\\cdot\\psi_{A=F}\\cdot e^{j\\theta_{2}}\\cdot% \\psi_{B=T}\\cdot\\psi_{A=T}\\cdot e^{-j\\theta_{1}} $ $ \\ \\ \\ \\ +\\ \\psi_{B=T}\\cdot\\psi_{A=F}\\cdot e^{j\\theta_{2}}\\cdot% \\psi_{B=T}\\cdot\\psi_{A=T}\\cdot e^{-j\\theta_{1}} $ </equation> <equation> $ \\ =\\ \\partial|\\psi_{B=T}\\cdot\\psi_{A=T}|^{2}\\ +\\ |\\psi_{B=T}\\cdot% \\psi_{A=F}|^{2}\\ + $ $ \\ =\\ \\partial|\\psi_{B=T}\\cdot\\psi_{A=T}|^{2}\\ +\\ |\\psi_{B=T}\\cdot% \\psi_{A=F}|^{2}\\ + $ </equation> <equation> $ \\ \\ \\ \\ 2\\cdot|\\psi_{B=T}\\cdot\\psi_{A=T}\\cdot\\psi_{B=T}\\cdot\\psi_% {A=F}|\\cdot cos(\\theta_{1}-\\theta_{2}) $ $ \\ \\ \\ \\ 2\\cdot|\\psi_{B=T}\\cdot\\psi_{A=T}\\cdot\\psi_{B=T}\\cdot\\psi_% {A=F}|\\cdot cos(\\theta_{1}-\\theta_{2}) $ </equation> </equationgroup> <equationgroup> <equation> $ \\Pr(F|A)\\ =\\ \\partial|\\psi_{B=F}\\cdot\\psi_{A=T}|^{2}\\ +\\ |\\psi_{B% =F}\\cdot\\psi_{A=F}|^{2}\\ + $ $ \\Pr(F|A) $ $ \\ =\\ \\partial|\\psi_{B=F}\\cdot\\psi_{A=T}|^{2}\\ +\\ |\\psi_{B=F}\\cdot% \\psi_{A=F}|^{2}\\ + $ </equation> <equation> $ \\ \\ \\ \\ 2\\cdot|\\psi_{B=F}\\cdot\\psi_{A=T}\\cdot\\psi_{B=F}\\cdot\\psi_% {A=F}|\\cdot cos(\\theta_{3}-\\theta_{4}) $ $ \\ \\ \\ \\ 2\\cdot|\\psi_{B=F}\\cdot\\psi_{A=T}\\cdot\\psi_{B=F}\\cdot\\psi_% {A=F}|\\cdot cos(\\theta_{3}-\\theta_{4}) $ </equation> </equationgroup> where $ \\theta_{1}=\\theta_{T_{B}}+\\theta_{T_{A}} $ , $ \\theta_{2}=\\theta_{T_{B}}+\\theta_{F_{A}} $ , $ \\theta_{3}=\\theta_{F_{B}}+\\theta_{T_{A}} $ , $ \\theta_{4}=\\theta_{F_{B}}+\\theta_{F_{A}} $ .", "\\newline Therefore, the answer to query is $ \\Pr(T|A) $ once the $ \\partial $ is determined by Eq.( [@ref:LABEL:partial] ).", "This example illustrates the definition of Quantum-like Bayesian Network and the detail derivation of Eq.( [@ref:LABEL:Interfernce] ).", "\\newline </subsubsection> </subsection>  </section>"], ["<section> <title> 4 The proposed model </title>  Unlike the method in the literature [@bib:Moreira2016Quantum] , this paper proposes a new way to calculate the interference value in the quantum-like Bayesian Network model.", "The biggest difference is this paper replaces the term $ \\cos(\\theta_{i}-\\theta_{j}) $ in Eq.( [@ref:LABEL:Interfernce] ) with the uncertainty degree value $ E_{d} $ calculated by Deng entropy.", "Deng entropy, also named as Belief entropy, is a powerful tool to measure the belief degree.", "The term $ \\cos(\\theta_{i}-\\theta_{j}) $ in Eq.( [@ref:LABEL:Interfernce] ) is a degree of belief uncertainty in the quantum interference term.", "When prisoner has no information about the other prisoner, his/her decision is influenced by his/her belief about the rival\u2019s decision.", "That\u2019s why the classic probability framework can not describe the game properly because to some extant the prisoner is not totally \u201dignorant\u201d about the other prisoner but has his/her own belief about the other part.", "This uncertain belief causes the interference term in the Bayesian Network model, which seems to be variable because the human\u2019s mind is changeable and is hard to measure.", "However, many experiments in literatures have revealed that the human\u2019s belief was inclined to certain degree, which means the interference term has a tendency value.", "Once the degree of belief uncertainty could be measured, the model can be established to describe the behavior that violates the Sure Thing Principle.", "Here the Deng entropy is introduced to measure the belief degree and the results turns out to be fit for the model to describe the game.", "\\newline <subsection> <title> 4.1 Acquisition of Belief Degree </title> This paper presents such a concept that the existing of interference term is because the prisoner\u2019s belief to the other prisoner.", "According to the classic probability theory, as analysed in the above section, the probability of a prisoner to defect the other under unknown condition should be equal to $ \\frac{1}{2}(Pr(P2=Defect|P1=Cooperate)+Pr(P2=Defect|P1=Defect)) $ . But the experiment results in literature denied this, which means the interference term truly affects.", "That\u2019s because actually the prisoner is not totally \u201dignorant\u201d about the other, for he/she will predict the other prisoner\u2019s decision from his/her own perspective and then make self\u2019s decision.", "For every individual, every one has his/her own characteristics.", "When predicting other\u2019s decision from self\u2019s perspective, the result seems to be diverse. But it is known that there are something that is common for everyone called human nature which results in most people that they tend to have a same predication tendency.", "\\newline <theorem> Definition 4.1 Belief Degree is defined by: \\newline <equationgroup> <equation> $  D_{b}=cos(\\theta_{i}-\\theta_{j}) $ $  D_{b}=cos(\\theta_{i}-\\theta_{j}) $ </equation> </equationgroup> where $ \\theta_{i} $ and $ \\theta_{j} $ are angles in interference term in Eq.( [@ref:LABEL:Interfernce] ).", "Belief degree represents people\u2019s predication toward their opponents and their belief tendency to certain actions in prisoners\u2019 dilemma game.", "\\newline </theorem> This predication tendency or Belief Degree determines the value of interference term.", "According to the previous experiments shown in Table [@ref:LABEL:experimentresultsinliterature] , the value of interference term is inclined to a certain value, which means there indeed exists predication tendency or Belief Degree.", "Hence the Belief Degree can be determined as shown in following.", "\\newline The quantum marginalization formula comprises two parts, the classic probability term and interference term, as Eq.( [@ref:LABEL:Interfernce] ) shows.", "It is the interference term that equips the model with ability to accommodate the violation of Sure Thing Principle.", "In this section, Deng entropy will be introduced to calculate the belief uncertainty to obtain the interference value.", "Notice that the Eq.( [@ref:LABEL:basevecotrs] ) has two basis states, as shown in Fig. [@ref:LABEL:model2] .", "\\newline There always be two vectors representation of Eq.( [@ref:LABEL:Interfernce] ), for the variable $ X $ has two alternative value, $ T $ and $ F $ .", "\\newline <equation> $ \\left[{\\begin{array}[]{*{20}{c}}{{\\alpha_{T}}}\\\\ {{\\beta_{T}}}\\end{array}}\\right]=\\left[{\\begin{array}[]{*{20}{c}}{{\\psi_{{P_{N% }}=T}}\\cdot{\\psi_{{P_{Parents}}=T}}}\\\\ {{\\psi_{{P_{N}}=T}}\\cdot{\\psi_{{P_{Parents}}=F}}}\\end{array}}\\right]{\\rm}% \\left[{\\begin{array}[]{*{20}{c}}{{\\alpha_{F}}}\\\\ {{\\beta_{F}}}\\end{array}}\\right]=\\left[{\\begin{array}[]{*{20}{c}}{{\\psi_{{P_{N% }}=F}}\\cdot{\\psi_{{P_{Parents}}=T}}}\\\\ {{\\psi_{{P_{N}}=F}}\\cdot{\\psi_{{P_{Parents}}=F}}}\\end{array}}\\right] $ </equation> Before applying Deng entropy to obtain the uncertain term $ cos(\\theta_{i}-\\theta_{j}) $ in the interference term, we should process the data in Eq.( [@ref:LABEL:vectors] ) firstly.", "The vector representation of Eq.( [@ref:LABEL:vectors] ) is shown in Fig. [@ref:LABEL:model6] .", "As can be inferred from Eq.( [@ref:LABEL:infer1] ) and Eq.( [@ref:LABEL:infer2] ), two $ \\theta $ in the Fig. [@ref:LABEL:model6] have the same value.", "Though we have known the value of two pairs of $ \\alpha $ and $ \\beta $ , the value of $ \\theta $ can hardly be determined through existing methods.", "One possible solution is just to regard $ cos(\\theta_{i}-\\theta_{j}) $ as an uncertain variable, which can be replaced by belief degree $ D_{b} $ .", "Hence once the belief degree is determined, the interference term is settled.", "The belief degree can be determined through belief entropy, which can calculate the uncertainty from Belief Distance.", "\\newline <theorem> Definition 4.2 The Belief Distance is defined by: \\newline <equationgroup> <equation> $  B_{d_{X}}=|{\\alpha_{X}}{\\rm{+}}\\frac{\\alpha_{X}-\\beta_{X}}{|% \\alpha_{X}+\\beta_{X}-1|}| $ $  B_{d_{X}}=|{\\alpha_{X}}{\\rm{+}}\\frac{\\alpha_{X}-\\beta_{X}}{|% \\alpha_{X}+\\beta_{X}-1|}| $ </equation> <equation> $  where\\quad|{\\alpha_{X}}-0.5|<|{\\beta_{X}}-0.5| $ $  where\\quad|{\\alpha_{X}}-0.5|<|{\\beta_{X}}-0.5| $ </equation> </equationgroup> \\newline </theorem> If $ |{\\alpha_{X}}-0.5|\\geq|{\\beta_{X}}-0.5| $ , the position of $ \\alpha_{X} $ and $ \\beta_{X} $ should be switched.", "\\newline The Belief Distance measuring the deviation from $ 0.5 $ .", "If no information is provided, the value of $ \\alpha $ and $ \\beta $ would be $ 0.5 $ because node A has two actions with each amplitude $ \\sqrt{5} $ and so does node B. $ \\frac{|\\alpha_{X}-\\beta_{X}|}{|\\alpha_{X}+\\beta_{X}-1|} $ is actually a derivation of $ \\frac{|\\alpha_{X}-0.5|-|\\beta_{X}-0.5|}{|\\alpha_{X}-0.5|+|\\beta_{X}-0.5|} $ .", "\\newline <theorem> Lemma 4.1 With the relative deviation information provided, Belief degree can be computed by Eq.( [@ref:LABEL:heuristicdistance] ) and Eq.( [@ref:LABEL:DengEntropy] ): \\newline <equation> $ D_{b}=-{E_{d}}=\\sum\\nolimits_{x}{{{\\rm{B}}_{{d_{x}}}}}\\log\\frac{{{{\\rm{B}}_{{d% _{x}}}}}}{{{2^{\\left|{{A_{i}}}\\right|}}-1}} $ </equation> $ {{\\left|{{A_{i}}}\\right|}} $ means the number of unobserved variables.", "\\newline </theorem> In quantum mechanics, the $ cos(\\theta_{1}-\\theta_{2}) $ is given by the inner product between two wave functions [@bib:Busemeyer2012Hierarchical] , which describes the subtraction of phases of the two wave function.", "Because it is difficult to compute $ cos(\\theta_{i}-\\theta_{j}) $ from geometric perspective, this paper just regards it as a variable which can be computed through belief entropy.", "\\newline </subsection>  </section>"], ["<section> <title> 5 Numerical example </title>  In this section, the proposed method will be applied in the Bayesian Network model to analyze the average results presented in Table [@ref:LABEL:experimentresultsinliterature] .", "The process could be summarized as below.", "\\newline Step 1: Create the model for the problem: If nothing is told, the first participant in the Prisoner\u2019s dilemma game will choose $ Defect $ or $ Cooperate $ with probability of $ 0.5 $ .", "The reason we assume the probability equals to $ 0.5 $ is that the first participant in the model do not have $ parents $ and nothing is told to him/her.", "However, in the real situation, the participants will wonder the other participant\u2019s action and make decisions based on the judgement.", "Therefore the assumption that the probability $ 0.5 $ is uncertain.", "In the Eq.( [@ref:LABEL:Heuristicwithdengentropy] ), $ {{\\left|{{A_{i}}}\\right|}} $ means the number of variables whose decision are not sure.", "Under this situation, the first participant\u2019s decision assumed by us is not exactly certain, so the term $ {{\\left|{{A_{i}}}\\right|}} $ will equal to $ 1 $ .", "With the data from Table [@ref:LABEL:experimentresultsinliterature] , we can establish a model as shown in Fig. [@ref:LABEL:model3] ; \\newline Step 2: Compute the Belief distance: According to Fig. [@ref:LABEL:model3] , the Eq.( [@ref:LABEL:vectors] ) can be paraphrased as below: \\newline <equation> $ \\begin{array}[]{l}\\left[{\\begin{array}[]{*{20}{c}}{{\\alpha_{T}}}\\\\ {{\\beta_{T}}}\\end{array}}\\right]=\\left[{\\begin{array}[]{*{20}{c}}{{\\psi_{{P_{N% }}=T}}\\cdot{\\psi_{{P_{Parents}}=T}}}\\\\ {{\\psi_{{P_{N}}=T}}\\cdot{\\psi_{{P_{Parents}}=F}}}\\end{array}}\\right]{\\rm{=}}% \\left[{\\begin{array}[]{*{20}{c}}{\\sqrt{0.5}\\cdot\\sqrt{0.26}}\\\\ {\\sqrt{0.5}\\cdot\\sqrt{0.13}}\\end{array}}\\right]=\\left[{\\begin{array}[]{*{20}{c% }}{0.3606}\\\\ {0.2550}\\end{array}}\\right]\\\\ \\left[{\\begin{array}[]{*{20}{c}}{{\\alpha_{F}}}\\\\ {{\\beta_{F}}}\\end{array}}\\right]=\\left[{\\begin{array}[]{*{20}{c}}{{\\psi_{{P_{N% }}=F}}\\cdot{\\psi_{{P_{Parents}}=T}}}\\\\ {{\\psi_{{P_{N}}=F}}\\cdot{\\psi_{{P_{Parents}}=F}}}\\end{array}}\\right]=\\left[{% \\begin{array}[]{*{20}{c}}{\\sqrt{0.5}\\cdot\\sqrt{0.74}}\\\\ {\\sqrt{0.5}\\cdot\\sqrt{0.87}}\\end{array}}\\right]=\\left[{\\begin{array}[]{*{20}{c% }}{0.6083}\\\\ {0.6595}\\end{array}}\\right]\\end{array} $ </equation> In this way, one can calculate the belief distance with Eq.( [@ref:LABEL:heuristicdistance] ).", "Here we take the calculation process of $ {\\alpha_{F}} $ and $ {\\beta_{F}} $ for example: notice that $ |{\\beta_{T}}-0.5|>|{\\alpha_{T}}-0.5| $ .", "The Belief Distance for $ {\\alpha_{F}} $ and $ {\\beta_{F}} $ is: \\newline <equation> $ {B_{{d_{T}}}}=|0.6083{\\rm{+}}\\frac{0.6083-0.6595}{|0.6083+0.6595-1|}|=0.41711 $ </equation> And the Belief distance for $ {\\alpha_{T}} $ and $ {\\beta_{T}} $ can be computed in the same way: \\newline <equation> $ {B_{{d_{F}}}}=|0.3606{\\rm{+}}\\frac{0.3606-0.2550}{|0.3606+0.2550-1|}|=0.63531 $ </equation> \\newline Step 3: Calculate the belief degree using Deng entropy: In the Step 2 we obtain the Belief Distance $ B_{{d_{T}}} $ and $ B_{{d_{F}}} $ .", "The Belief Distance represents the inner connection between two actions decided by the participants, as a reflection of a prisoner\u2019s belief to the other.", "The Deng entropy is an efficient tool to reveal this connection.", "In Step 1, we have analyzed that the term $ |A_{i}| $ in the Eq.( [@ref:LABEL:Heuristicwithdengentropy] ) equals to 1.", "Hence the results of Eq.( [@ref:LABEL:Heuristicwithdengentropy] ) is: \\newline <equation> $ D_{b}=-{E_{d}}=0.41711\\cdot\\log\\frac{{0.41711}}{{{2^{1}}-1}}+{\\rm{0}}{\\rm{.635% 31}}\\cdot\\log\\frac{{{\\rm{0}}{\\rm{.63531}}}}{{{2^{1}}-1}}=-0.9420 $ </equation> The $ D_{b} $ will replace the term $ cos(\\theta_{i}-\\theta_{j}) $ in the Eq.( [@ref:LABEL:Interfernce] ) to perform the probabilistic interference.", "\\newline <equationgroup> <equation> $ \\Pr(P2=Defect)=\\partial[|{\\psi_{P2=D|P1=D}}{|^{2}}+|{\\psi_{P2=D|P% 1=C}}{|^{2}} $ $ \\Pr(P2=Defect)=\\partial[|{\\psi_{P2=D|P1=D}}{|^{2}}+|{\\psi_{P2=D|P% 1=C}}{|^{2}} $ </equation> <equation> $ +2\\cdot|{\\psi_{P2=D|P1=D}}|\\cdot|{\\psi_{P2=D|P1=C}}|\\cdot\\cos(% \\theta_{1}-\\theta_{2})] $ $ +2\\cdot|{\\psi_{P2=D|P1=D}}|\\cdot|{\\psi_{P2=D|P1=C}}|\\cdot\\cos(% \\theta_{1}-\\theta_{2})] $ </equation> <equation> $ =\\partial[0.5\\times 0.87+0.5\\times 0.74+2\\cdot\\sqrt{0.5\\times 0.8% 7}\\cdot\\sqrt{0.5\\times 0.74}\\cdot-0.9420] $ $ =\\partial[0.5\\times 0.87+0.5\\times 0.74+2\\cdot\\sqrt{0.5\\times 0.8% 7}\\cdot\\sqrt{0.5\\times 0.74}\\cdot-0.9420] $ </equation> </equationgroup> $ \\Pr(P2=Cooperate) $ can be obtained i the same way: \\newline <equationgroup> <equation> $ \\Pr(P2=Defect)=\\partial 0.04917 $ $ \\Pr(P2=Defect)=\\partial 0.04917 $ </equation> <equation> $ \\Pr(P2=Cooperate)=\\partial 0.02182 $ $ \\Pr(P2=Cooperate)=\\partial 0.02182 $ </equation> </equationgroup> \\newline And the final result is: \\newline <equationgroup> <equation> $ \\Pr(P2=Defect)=\\frac{\\partial 0.04917}{\\partial 0.04917+\\partial 0% .02182}=0.6926 $ $ \\Pr(P2=Defect)=\\frac{\\partial 0.04917}{\\partial 0.04917+\\partial 0% .02182}=0.6926 $ </equation> <equation> $ \\Pr(P2=Defect)=\\frac{\\partial 0.02182}{\\partial 0.04917+\\partial 0% .02182}=0.3074 $ $ \\Pr(P2=Defect)=\\frac{\\partial 0.02182}{\\partial 0.04917+\\partial 0% .02182}=0.3074 $ </equation> </equationgroup> Compare the result with probability in Table [@ref:LABEL:experimentresultsinliterature] , the model this paper proposes produces a result with fit error percentage of $ {8.2{\\%}} $ .", "\\newline Fig. [@ref:LABEL:model4] shows the comparison of results from literature and prediction of model, from which we can see that the model prediction is coincident to the probability observed with little fit errors.", "\\newline  </section>"], ["<section> <title> 6 Conclusion </title>  Quantum Bayesian Network inherits inference ability from classic Bayesian network and has the ability to explain the violation of $ Sure\\ Thing\\ Principle $ .", "The model proposed by this paper successfully described the paradoxical phenomenon in Prisoners\u2019 dilemma game.", "Unlike other existing methods, the proposed model regards the violation as an effect of interference and utilizes the concept of \u201dBelief Degree\u201d to make prediction though belief entropy.", "the model is compared with two other Quantum models.", "The first model (model 1) is the Quantum Prospect Decision Theory(QPDT) model developed by Yukalov and Sornette [@bib:Yukalov2015Quantum,Yukalov2011Decision,Yukalov2010Entanglement,VYACHESLAV2010MATHEMATICAL] .", "In (QPDT) model, a static heuristic is used to predict the results.", "The second model is the Quantum-Like Bayesian Network proposed by Moreira [@bib:Moreira2016Quantum] , in which a dynamic heuristic is used to predict the results.", "\\newline From Table [@ref:LABEL:comparisonmodel] we can clearly notice that the average fit errors of proposed model is smaller than other two models, which shows the new method for Quantum Bayesian Network proposed by this paper is effective and reliable.", "\\newline Fig. [@ref:LABEL:model5] visualizes the results from Table [@ref:LABEL:experimentresultsinliterature] , from which we can see that the result predicted by the proposed method is occupying the least area of the bar.", "\\newline The dilemma situation considered in this paper is prisoner\u2019s dilemma game with two strategies [@bib:Tanimoto2007Relationship] .", "The dilemma strength [@bib:Wang2015Universal] of the game discussed in this paper is $ Dg^{\\prime}=Dr^{\\prime}=1 $ .", "There are also cases with dilemma strength different from $ Dg^{\\prime}=Dr^{\\prime}=1 $ , depending on the payoff table to the players.", "Admittedly, the method proposed in this paper is designed to accommodate the paradoxical findings in dilemma strength $ Dg^{\\prime}=Dr^{\\prime}=1 $ ."]], "target": "Nevertheless, the method can well predict behaviours the player will take, as shown in Table . Comparing with other similar methods, the proposed Bayesian Network works with the least fit errors. In the prisoner\u2019s dilemma game, the prisoner will predict the other\u2019s action if he/she knows little about the rival. Therefore the probability of the prisoner to choose $ Defect $ under unknown case will be smaller than the value computed from the classic way. The $ Sure\\ Thing\\ Principle $ is violated because the belief in the prisoner\u2019s mind affects. On the other hand, the belief degree is not totally irregular. Lots of evidence have examined the value is closed within a small range. The advantages of Quantum-like Bayesian Network is it regards two strategies in people\u2019s mind as two wave functions, which will produce the interference effect. Hence, this paper proposes Belief Degree to represent the interference effect and utilizes Belief Distance to calculate the deviation from totally uncertainty. The belief entropy will produce a corresponding Belief Degree according to Belief Distance. We analyze the Prisoners\u2019 dilemma game with the model that applied our method and the prediction results are close to the observed probability with little fit error. In the end, we compare the model with two models which use a parameter called heuristic to predict the probability. The comparison results shows the effectiveness and reliability of our method."}, {"tabular": ["    &    &  $ r_{socp}=\\sqrt{0.1m} $  &  $ r_{socp}=\\sqrt{0.5m} $  &  $ r_{socp}=\\sqrt{m} $ ", " $ \\alpha $  &  $ \\beta_{w}/\\alpha $  &  $ -\\frac{Ef_{obj}}{\\sqrt{n}} $  &  $ E\\|{\\bf w}_{socp}\\|_{2} $  &  $ -\\frac{Ef_{obj}}{\\sqrt{n}} $  &  $ E\\|{\\bf w}_{socp}\\|_{2} $  &  $ -\\frac{Ef_{obj}}{\\sqrt{n}} $  &  $ E\\|{\\bf w}_{socp}\\|_{2} $ ", " $ 0.3 $  &  $ 0.249 $  &  $ 0.0011 $ / $ \\bf{0} $  &  $ 3.1956 $ / $ \\bf{3} $  &  $ 0.1613 $ / $ \\bf{0.1639} $  &  $ 3.2050 $ / $ \\bf{3.1710} $  &  $ 0.2763 $ / $ \\bf{0.2792} $  &  $ 3.5261 $ / $ \\bf{3.5053} $ ", " $ 0.5 $  &  $ 0.325 $  &  $ 0.0004 $ / $ \\bf{0} $  &  $ 3.0154 $ / $ \\bf{3} $  &  $ 0.2757 $ / $ \\bf{0.2722} $  &  $ 3.4015 $ / $ \\bf{3.2840} $  &  $ 0.4623 $ / $ \\bf{0.4576} $  &  $ 3.9177 $ / $ \\bf{3.7774} $ ", " $ 0.7 $  &  $ 0.41 $  &  $ 0.0002 $ / $ \\bf{0} $  &  $ 3.0147 $ / $ \\bf{3} $  &  $ 0.4143 $ / $ \\bf{0.4145} $  &  $ 3.4878 $ / $ \\bf{3.4563} $  &  $ 0.5530 $ / $ \\bf{0.6857} $  &  $ 4.3548 $ / $ \\bf{4.1603} $  "], "ref_sec": [["<section> <title> 1 Introduction </title>  In this paper we focus on studying mathematical properties of under-determined systems of linear equations with sparse solutions (studying these systems from both, theoretical and practical point of view attracted enormous attention in recent years, see, e.g. [@bib:ECicm,DDTLSKB,CT,JRimaging,BCDH08,CRchannel,VPH,PVMHjournal,WM08,Olgica,RFPrank,MBPSZ08,RS08] and references therein).", "In its simplest form solving an under-determined system of linear equations amounts to finding a, say, $ k $ -sparse $ {\\bf x} $ such that \\newline <equation> $ A{\\bf x}={\\bf y} $ </equation> where $ A $ is an $ m\\times n $ ( $ m<n $ ) matrix and $ {\\bf y} $ is an $ m\\times 1 $ vector (see Figure [@ref:LABEL:fig:model] ; here and in the rest of the paper, under $ k $ -sparse vector we assume a vector that has at most $ k $ nonzero components).", "Of course, the assumption will be that such an $ {\\bf x} $ exists.", "To make writing in the rest of the paper easier, we will assume the so-called {linear} regime, i.e. we will assume that $ k=\\beta n $ and that the number of equations is $ m=\\alpha n $ where $ \\alpha $ and $ \\beta $ are constants independent of $ n $ (more on the non-linear regime, i.e. on the regime when $ m $ is larger than linearly proportional to $ k $ can be found in e.g. [@bib:CoMu05,GiStTrVe06,GiStTrVe07] ).", "\\newline If one has freedom to design matrix $ A $ then the results from [@bib:FHicassp,Tarokh,MaVe05] demonstrated that the techniques from coding theory (based on the coding/decoding of Reed-Solomon codes) can be employed to determine {any} $ k $ -sparse $ {\\bf x} $ in ( [@ref:LABEL:eq:system] ) for any $ 0<\\alpha\\leq 1 $ and any $ \\beta\\leq\\frac{\\alpha}{2} $ in polynomial time.", "It is relatively easy to show that under the unique recoverability assumption $ \\beta $ can not be greater than $ \\frac{\\alpha}{2} $ .", "Therefore, as long as one is concerned with the unique recovery of $ k $ -sparse $ {\\bf x} $ in ( [@ref:LABEL:eq:system] ) in polynomial time the results from [@bib:FHicassp,Tarokh,MaVe05] are optimal.", "The complexity of algorithms from [@bib:FHicassp,Tarokh,MaVe05] is roughly $ O(n^{3}) $ .", "In a similar fashion one can, instead of using coding/decoding techniques associated with Reed/Solomon codes, design the matrix and the corresponding recovery algorithm based on the techniques related to the coding/decoding of Expander codes (see e.g. [@bib:XHexpander,JXHC08,InRu08] and references therein).", "In that case recovering $ {\\bf x} $ in ( [@ref:LABEL:eq:system] ) is significantly faster for large dimensions $ n $ .", "Namely, the complexity of the techniques from e.g. [@bib:XHexpander,JXHC08,InRu08] (or their slight modifications) is usually $ O(n) $ which is clearly for large $ n $ significantly smaller than $ O(n^{3}) $ .", "However, the techniques based on coding/decoding of Expander codes usually do not allow for $ \\beta $ to be as large as $ \\frac{\\alpha}{2} $ .", "\\newline On the other hand, if one has no freedom in choice of $ A $ designing the algorithms to find $ k $ -sparse $ {\\bf x} $ in ( [@ref:LABEL:eq:system] ) is substantially harder.", "In fact, when there is no choice in $ A $ the recovery problem ( [@ref:LABEL:eq:system] ) becomes NP-hard.", "Two algorithms 1) {Orthogonal matching pursuit - OMP} and 2) {Basis pursuit - $ \\ell_{1} $ -optimization} (and their different variations) have been often viewed as solid heuristics for solving ( [@ref:LABEL:eq:system] ) (in recent years belief propagation type of algorithms are emerging as strong alternatives as well).", "Roughly speaking, OMP algorithms are faster but can recover smaller sparsity whereas the BP ones are slower but recover higher sparsity.", "In a more precise way, under certain probabilistic assumptions on the elements of $ A $ it can be shown (see e.g. [@bib:JATGomp,JAT,NeVe07] ) that if $ m=O(k\\log(n)) $ OMP (or a slightly modified OMP) can recover $ {\\bf x} $ in ( [@ref:LABEL:eq:system] ) with complexity of recovery $ O(n^{2}) $ .", "On the other hand a stage-wise OMP from [@bib:DTDSomp] recovers $ {\\bf x} $ in ( [@ref:LABEL:eq:system] ) with complexity of recovery $ O(n\\log n) $ .", "Somewhere in between OMP and BP are recent improvements CoSAMP (see e.g. [@bib:NT08] ) and Subspace pursuit (see e.g. [@bib:DaiMil08] ), which guarantee (assuming the linear regime) that the $ k $ -sparse $ {\\bf x} $ in ( [@ref:LABEL:eq:system] ) can be recovered in polynomial time with $ m=O(k) $ equations.", "This is the same performance guarantee established in [@bib:CanRomTao06,DonohoPol] for the BP.", "\\newline We now introduce the BP concept (or, as we will refer to it, the $ \\ell_{1} $ -optimization concept; a slight modification/adaptation of it will actually be the main topic of this paper).", "Variations of the standard $ \\ell_{1} $ -optimization from e.g. [@bib:CWBreweighted,SChretien08,SaZh08] as well as those from [@bib:SCY08,FL08,GN03,GN04,GN07,DG08] related to $ \\ell_{q} $ -optimization, $ 0<q<1 $ are possible as well; moreover they can all be incorporated in what we will present below.", "The $ \\ell_{1} $ -optimization concept suggests that one can maybe find the $ k $ -sparse $ {\\bf x} $ in ( [@ref:LABEL:eq:system] ) by solving the following $ \\ell_{1} $ -norm minimization problem \\newline <equationgroup> <equation> $ \\mbox{min}\\|{\\bf x}\\|_{1} $ $ \\|{\\bf x}\\|_{1} $ </equation> <equation> $ \\mbox{subject to}A{\\bf x}={\\bf y}. $ $  A{\\bf x}={\\bf y}. $ </equation> </equationgroup> As is then shown in [@bib:CanRomTao06] if $ \\alpha $ and $ n $ are given, $ A $ is given and satisfies the restricted isometry property (RIP) (more on this property the interested reader can find in e.g. [@bib:Crip,CRT,CanRomTao06,Bar,Ver,ALPTJ09] ), then any unknown vector $ {\\bf x} $ with no more than $ k=\\beta n $ (where $ \\beta $ is a constant dependent on $ \\alpha $ and explicitly calculated in [@bib:CanRomTao06] ) non-zero elements can indeed be recovered by solving ( [@ref:LABEL:eq:l1] ).", "In a statistical and large dimensional context in [@bib:DonohoPol] and later in [@bib:StojnicCSetam09] for any given value of $ \\beta $ the exact value of the maximum possible $ \\alpha $ was determined.", "\\newline As we mentioned earlier the above scenario is in a sense idealistic.", "Namely, it assumes that $ {\\bf y} $ in ( [@ref:LABEL:eq:l1] ) was obtained through ( [@ref:LABEL:eq:system] ).", "On other hand in many applications only a {noisy} version of $ A{\\bf x} $ may be available for $ {\\bf y} $ (this is especially so in measuring type of applications) see, e.g. [@bib:CanRomTao06,HN,W] .", "When that happens one has the following equivalent to ( [@ref:LABEL:eq:system] ) (see, Figure [@ref:LABEL:fig:modelnoise] ) \\newline <equation> $ {\\bf y}=A{\\bf x}+{\\bf v}, $ </equation> where $ {\\bf v} $ is an $ m\\times 1 $ so-called noise vector (the so-called ideal case presented above is of course a special case of the noisy one given in ( [@ref:LABEL:eq:systemnoise] )).", "\\newline Finding the $ k $ -sparse $ {\\bf x} $ in ( [@ref:LABEL:eq:systemnoise] ) is now incredibly hard, in fact it is pretty much impossible.", "Basically, one is looking for a $ k $ -sparse $ {\\bf x} $ such that ( [@ref:LABEL:eq:systemnoise] ) holds and on top of that $ {\\bf v} $ is unknown.", "Although the problem is hard there are various heuristics throughout the literature that one can use to solve it approximately.", "Majority of these heuristics are based on appropriate generalizations of the corresponding algorithms one would use in the noiseless case.", "Thinking along the same lines as in the noiseless case one can distinguish two scenarios depending on the availability of the freedom to choose/design $ A $ .", "If one has the freedom to design $ A $ then one can adapt the corresponding noiseless algorithms to the noisy scenario as well (more on this can be found in e.g. [@bib:BGIKS] ).", "However, in this paper we mostly focus on the scenario where one has no control over $ A $ .", "In such a scenario one can again make a parallel to the noiseless case and distinguish two groups of algorithms that were historically viewed as good heuristics for finding approximate solutions to noisy under-determined systems: 1) {Generalizations of OMP} and 2) {Generalizations of BP} .", "Among various generalizations of OMP we briefly focus only on the following three that we think had a significant impact on the field in recent years.", "Namely, an improvement of standard OMP called ROMP introduced in [@bib:NeVe07] can be proven to work well in the noisy case as well.", "The same is true for CoSAMP from [@bib:NT08] or Subspace pursuit from [@bib:DaiMil09] .", "Essentially, in a statistical context, the latter two (the one from [@bib:NeVe07] has a slightly worse performance guarantee) can provably recover a linear sparsity while maintaining the approximation error proportional to the norm-2 of the noise vector.", "These algorithms are very successful in quick recovery of linear sparsity of certain level.", "In the noiseless case, all of them can be thought of as perfected versions of OMP.", "Given their robustness with respect to the noise one can think of them as perfected noisy versions of OMP as well.", "\\newline In this paper we will focus on the second group of algorithms, i.e. we will focus on generalizations of BP that can handle the noisy case.", "To introduce a bit or tractability in finding the $ k $ -sparse $ {\\bf x} $ in ( [@ref:LABEL:eq:systemnoise] ) one usually assumes certain amount of knowledge about either $ {\\bf x} $ or $ {\\bf v} $ .", "As far as tractability assumptions on $ {\\bf v} $ are concerned one typically (and possibly fairly reasonably in applications of interest)", "assumes that $ \\|{\\bf v}\\|_{2} $ is bounded (or highly likely to be bounded) from above by a certain known quantity.", "The following second-order cone programming (SOCP) analogue to (or say noisy generalization of) ( [@ref:LABEL:eq:l1] ) is one of the approaches that utilizes such an assumption (more on this approach and its variations can be found in e.g. [@bib:CanRomTao06] ) \\newline <equationgroup> <equation> $ \\min_{{\\bf x}}\\|{\\bf x}\\|_{1} $ $ \\min_{{\\bf x}} $ $ \\|{\\bf x}\\|_{1} $ </equation> <equation> $ \\mbox{subject to}\\|{\\bf y}-A{\\bf x}\\|_{2}\\leq r $ $ \\|{\\bf y}-A{\\bf x}\\|_{2}\\leq r $ </equation> </equationgroup> where, $ r $ is a quantity such that $ \\|{\\bf v}\\|_{2}\\leq r $ (or $ r $ is a quantity such that $ \\|{\\bf v}\\|_{2}\\leq r $ is say highly likely).", "For example, in [@bib:CanRomTao06] a statistical context is assumed and based on the statistics of $ {\\bf v} $ , $ r $ was chosen such that $ \\|{\\bf v}\\|_{2}\\leq r $ happens with overwhelming probability (as usual, under overwhelming probability we in this paper assume a probability that is no more than a number exponentially decaying in $ n $ away from $ 1 $ ).", "Given that ( [@ref:LABEL:eq:socp] ) is now among few almost standard choices when it comes to finding an approximation to the $ k $ -sparse $ {\\bf x} $ in ( [@ref:LABEL:eq:systemnoise] ), the literature on its properties when applied in various contexts is vast (see, e.g. [@bib:CanRomTao06,DonElaTem06,Tropp06] and references therein).", "We here briefly mention only what we consider to be the most influential work on this topic in recent years.", "Namely, in [@bib:CanRomTao06] the authors analyzed the performance of ( [@ref:LABEL:eq:socp] ) and showed a result similar in flavor to the one that holds in the ideal - noiseless - case.", "In a nutshell the following was shown in [@bib:CanRomTao06] : let $ {\\bf x} $ be a $ \\beta n $ -sparse vector such that ( [@ref:LABEL:eq:systemnoise] ) holds and let $ {\\bf x}_{socp} $ be the solution of ( [@ref:LABEL:eq:socp] ).", "Then \\newline <equation> $ \\|{\\bf x}_{socp}-{\\bf x}\\|_{2}\\leq Cr $ </equation> where $ \\beta $ is a constant independent of $ n $ and $ C $ is a constant independent of $ n $ and of course dependent on $ \\alpha $ and $ \\beta $ .", "This result in a sense establishes a noisy equivalent to the fact that a linear sparsity can be recovered from an under-determined system of linear equations.", "In an informal language, it states that a linear sparsity can be {approximately} recovered in polynomial time from a noisy under-determined system with the norm of the recovery error guaranteed to be within a constant multiple of the noise norm (as mentioned above, the same was also established later in [@bib:NT08] for CoSAMP and in [@bib:DaiMil09] for Subspace pursuit).", "Establishing such a result is, of course, a feat in its own class, not only because of its technical contribution but even more so because of the amount of interest that it generated in the field.", "\\newline In this paper we will also consider an approximate recovery of the $ k $ -sparse $ {\\bf x} $ in ( [@ref:LABEL:eq:systemnoise] ).", "Moreover, we will also focus on the SOCP algorithms defined in ( [@ref:LABEL:eq:socp] ).", "We will develop a novel framework for performance characterization of these algorithms.", "Among other things, in a statistical context, the framework will enable us to precisely characterize their approximation error.", "\\newline We should also mention that SOCP algorithms are by no means the only possible generalizations (adaptations) of $ \\ell_{1} $ optimization to the noisy case.", "For example, LASSO algorithms (more on these algorithms can be found in e.g. [@bib:Tibsh96,CheDon95,CheDonSau98,BunTsyWeg07,vandeGeer08,MeinYu09] as well as in recent developments [@bib:DonMalMon10,BayMon10lasso,StojnicGenLasso10] ) are a very successful alternative.", "In our recent work [@bib:StojnicGenLasso10] we established a nice connection between some of the algorithms from the LASSO group and certain SOCP algorithms.", "Towards the end of the present paper we will revisit that connection and provide a few additional insights.", "Another interesting alternative to the SOCP or the LASSO algorithms is the so-called Dantzig selector introduced in [@bib:CanTao07] (more on the Dantzig selector as well as on its relation to the LASSO algorithms can be found in e.g. [@bib:MeiRocYu07,BicRitTsy09,FriSau07,EfrHatTib07,AsiRom10,JamRadLv09,Koltch09] ).", "In the nutshell, LASSO and SOCP algorithms are likely to provide a better recovery performance than the Dantzig selector in a variety of scenarios and with respect to a variety of performance measures whereas the Dantzig selector as a linear program promises to be faster.", "Of course a fair comparison would go way beyond this short observation; especially so with a plenty of room for improvement in numerical implementations specifically tailored for linear programs such as the Dantzig selector or with the recent development of fast belief propagation type of LASSO-like implementations (see, e.g. [@bib:DonMalMon10,BayMon10lasso] ).", "\\newline Before we proceed further we briefly summarize the organization of the rest of the paper.", "In Section [@ref:LABEL:sec:unsigned] , we present a statistical framework for the performance analysis of the SOCP algorithms.", "To demonstrate its power we towards the end of Section [@ref:LABEL:sec:unsigned] , for any given $ \\alpha $ and $ \\beta $ , compute the worst case approximation error that ( [@ref:LABEL:eq:socp] ) makes when used for approximate recovery of general sparse vectors $ {\\bf x} $ from ( [@ref:LABEL:eq:systemnoise] ).", "In Section [@ref:LABEL:sec:signed] we then specialize results from Section [@ref:LABEL:sec:unsigned] to the so-called signed vectors $ {\\bf x} $ .", "In Section [@ref:LABEL:sec:connectlasso] we will revisit a connection between the SOCP algorithms and the LASSO alternatives.", "Finally, in Section [@ref:LABEL:sec:discuss] we discuss obtained results.", "\\newline  </section>"], ["<section> <title> 2 SOCP\u2019s performance analysis framework \u2013 general x  </title>  In this section we create a statistical SOCP\u2019s performance analysis framework.", "Before proceeding further we will now explicitly state the major assumptions that we will make (the remaining ones will be made appropriately throughout the analysis).", "Namely, in the rest of the paper we will assume that the elements of $ A $ are i.i.d.", "standard normal random variables.", "We will also assume that the elements of $ {\\bf v} $ are i.i.d.", "Gaussian random variables with zero mean and variance $ \\sigma $ .", "We will assume that $ \\tilde{{\\bf x}} $ is the original $ {\\bf x} $ in ( [@ref:LABEL:eq:systemnoise] ) that we are trying to recover and that it is {any} $ k $ -sparse vector with a given fixed location of its nonzero elements and a given fixed combination of their signs.", "Since the analysis (and the performance of ( [@ref:LABEL:eq:socp] )) will clearly be irrelevant with respect to what particular location and what particular combination of signs of nonzero elements are chosen, we can for the simplicity of the exposition and without loss of generality assume that the components $ {\\bf x}_{1},{\\bf x}_{2},\\dots,{\\bf x}_{n-k} $ of $ {\\bf x} $ are equal to zero and the components $ {\\bf x}_{n-k+1},{\\bf x}_{n-k+2},\\dots,{\\bf x}_{n} $ of $ {\\bf x} $ are greater than or equal to zero.", "Moreover, throughout the paper we will call such an $ {\\bf x} $ $ k $ -sparse and positive.", "In a more formal way we will set \\newline <equationgroup> <equation> $ \\tilde{{\\bf x}}_{1}=\\tilde{{\\bf x}}_{2}=\\dots=\\tilde{{\\bf x}}_{n-% k}=0 $ $ \\tilde{{\\bf x}}_{1}=\\tilde{{\\bf x}}_{2}=\\dots=\\tilde{{\\bf x}}_{n-% k}=0 $ </equation> <equation> $ \\tilde{{\\bf x}}_{n-k+1}\\geq 0,\\tilde{{\\bf x}}_{n-k+1}\\geq 0,\\dots% ,\\tilde{{\\bf x}}_{n}\\geq 0.", "$ $ \\tilde{{\\bf x}}_{n-k+1}\\geq 0,\\tilde{{\\bf x}}_{n-k+1}\\geq 0,\\dots% ,\\tilde{{\\bf x}}_{n}\\geq 0.", "$ </equation> </equationgroup> We also now take the opportunity to point out a rather obvious detail.", "Namely, the fact that $ \\tilde{{\\bf x}} $ is positive is assumed for the purpose of the analysis.", "However, this fact is not known {a priori} and is not available to the solving algorithm (this will of course change in Section [@ref:LABEL:sec:signed] ).", "\\newline Once we establish the framework it will be clear that it can be used to characterize many of the SOCP features.", "We will defer these details to a collection of forthcoming papers.", "However in this paper we will demonstrate a small application that relates to a classical question of determining the approximation error that ( [@ref:LABEL:eq:socp] ) makes when used to recover {any} $ k $ -sparse $ {\\bf x} $ that satisfies ( [@ref:LABEL:eq:systemnoise] ) and is from a set of $ {\\bf x} $ \u2019s with a given fixed location of nonzero elements and a given fixed combination of their signs.", "The approximation error that we will focus on will be the norm-2 of the error vector.", "(one can of course characterize the approximation error in many other ways; for example one such a way that attracted a lot of attention in recent years is the so called error in the support recovery; more in this direction can be found in e.g. [@bib:W] or in e.g. [@bib:BunTsyWeg07,Koltch09] when one is not necessarily concerned with the SOCP type of algorithms).", "\\newline Before proceeding further we will introduce a few definitions that will be useful in formalizing the above mentioned application as well as in conducting the entire analysis.", "As it is natural we start with the solution of ( [@ref:LABEL:eq:socp] ).", "As earlier, let $ {\\bf x}_{socp} $ be the solution of ( [@ref:LABEL:eq:socp] ) and further let $ {\\bf w}_{socp}\\in R^{n} $ be such that \\newline <equation> $ {\\bf x}_{socp}=\\tilde{{\\bf x}}+{\\bf w}_{socp}. $ </equation> As mentioned above, as an application of our framework we will compute the largest possible value of $ \\|{\\bf x}_{socp}-\\tilde{{\\bf x}}\\|_{2}=\\|{\\bf w}_{socp}\\|_{2} $ for any combination $ (\\alpha,\\beta) $ . Or more rigorously, for any combination $ (\\alpha,\\beta) $ , we will find a $ d_{socp} $ such that \\newline <equation> $ \\lim_{n\\rightarrow\\infty}P(d_{socp}-\\epsilon\\leq\\max_{\\tilde{{\\bf x}}}\\|{\\bf w% }_{socp}\\|_{2}\\leq d_{socp}+\\epsilon)=1 $ </equation> for an arbitrarily small constant $ \\epsilon $ .", "However, before doing so in the following three subsections we will present the general framework.", "Towards the end of the third subsection and in the fourth one we will then demonstrate how it can be used to determine the $ d_{socp} $ .", "\\newline The framework that we will present below will center around the optimal value of the objective function in ( [@ref:LABEL:eq:socp] ) (of course in a probabilistic context).", "We will divide presentation in several subsections.", "In the first one we will compute a \u201chigh-probability\u201d upper bound on the value of that objective.", "In the second one we will then show how one can design a mechanism to obtain a \u201chigh-probability\u201d lower bound on the optimal value of ( [@ref:LABEL:eq:socp] ).", "In later subsections we will show that the two bounds can match each other.", "Now, before we start the technical details we will rewrite ( [@ref:LABEL:eq:socp] ) in the following way \\newline <equationgroup> <equation> $ \\min_{{\\bf x}}\\|{\\bf x}\\|_{1}-\\|\\tilde{{\\bf x}}\\|_{1} $ $ \\min_{{\\bf x}} $ $ \\|{\\bf x}\\|_{1}-\\|\\tilde{{\\bf x}}\\|_{1} $ </equation> <equation> $ \\mbox{subject to}\\|{\\bf y}-A{\\bf x}\\|_{2}\\leq r_{socp}. $ $ \\|{\\bf y}-A{\\bf x}\\|_{2}\\leq r_{socp}. $ </equation> </equationgroup> One should note that this modification of ( [@ref:LABEL:eq:socp] ) is for the analysis purposes only, i.e. ( [@ref:LABEL:eq:socp1] ) is not the algorithm one would be running in the search of an approximation to $ \\tilde{{\\bf x}} $ (( [@ref:LABEL:eq:socp1] ) can not be run anyway, since it requires knowledge of $ \\|\\tilde{{\\bf x}}\\|_{1} $ which is of course unavailable).", "The SOCP algorithm one would actually use to find an approximation to $ \\tilde{{\\bf x}} $ is the one in ( [@ref:LABEL:eq:socp] ).", "It is just for the easiness of exposition that we will look at the modification ( [@ref:LABEL:eq:socp1] ) and not at the original problem ( [@ref:LABEL:eq:socp] ).", "Also, one should note that $ r $ in ( [@ref:LABEL:eq:socp] ) or $ r_{socp} $ in ( [@ref:LABEL:eq:socp1] ) is a parameter that critically impacts the outcome of any SOCP type of algorithm (in fact for different $ r $ \u2019s one will have different SOCP\u2019s).", "The analysis that we will present assumes a general $ r $ that we will call $ r_{socp} $ .", "We will of course later in the paper (basically when the analysis is done) comment in more detail on the effect that choice of $ r_{socp} $ has on the analysis or more importantly on the performance of the optimization algorithm from ( [@ref:LABEL:eq:socp] ).", "\\newline Given that we will be dealing with ( [@ref:LABEL:eq:socp1] ) let us define the optimal value of its objective in the following way \\newline <equationgroup> <equation> $  f_{obj}(\\sigma,\\tilde{{\\bf x}},A,{\\bf v},r_{socp})=\\min_{{\\bf x}% }\\|{\\bf x}\\|_{1}-\\|\\tilde{{\\bf x}}\\|_{1} $ $  f_{obj}(\\sigma,\\tilde{{\\bf x}},A,{\\bf v},r_{socp})=\\min_{{\\bf x}} $ $ \\|{\\bf x}\\|_{1}-\\|\\tilde{{\\bf x}}\\|_{1} $ </equation> <equation> $ \\mbox{subject to}\\|{\\bf y}-A{\\bf x}\\|_{2}\\leq r_{socp}. $ $ \\|{\\bf y}-A{\\bf x}\\|_{2}\\leq r_{socp}. $ </equation> </equationgroup> To make writing easier we will instead of $ f_{obj}(\\sigma,\\tilde{{\\bf x}},A,{\\bf v},r_{socp}) $ write just $ f_{obj} $ .", "A similar convention will be applied to few other functions throughout the paper.", "On many occasions, though, (especially where we deem it as substantial to the derivation) we will also keep all (of a majority of) arguments of the corresponding functions.", "\\newline <subsection> <title> 2.1 Upper-bounding f \u2062 o b j </title> In this section we present a general framework for finding a \u201chigh-probability\u201d upper bound on $ f_{obj} $ .", "We start by noting that if one knows that $ {\\bf y}=A\\tilde{{\\bf x}}+{\\bf v} $ holds then ( [@ref:LABEL:eq:objlassol1] ) can be rewritten as \\newline <equationgroup> <equation> $ \\min_{{\\bf x}}\\|{\\bf x}\\|_{1}-\\|\\tilde{{\\bf x}}\\|_{1} $ $ \\min_{{\\bf x}} $ $ \\|{\\bf x}\\|_{1}-\\|\\tilde{{\\bf x}}\\|_{1} $ </equation> <equation> $ \\mbox{subject to}\\|{\\bf v}+A\\tilde{{\\bf x}}-A{\\bf x}\\|_{2}\\leq r_% {socp}. $ $ \\|{\\bf v}+A\\tilde{{\\bf x}}-A{\\bf x}\\|_{2}\\leq r_{socp}. $ </equation> </equationgroup> After a small change of variables, $ {\\bf x}=\\tilde{{\\bf x}}+{\\bf w} $ , ( [@ref:LABEL:eq:ubobjlassol11] ) becomes \\newline <equationgroup> <equation> $ \\min_{{\\bf w}}\\|\\tilde{{\\bf x}}+{\\bf w}\\|_{1}-\\|\\tilde{{\\bf x}}\\|% _{1} $ $ \\min_{{\\bf w}} $ $ \\|\\tilde{{\\bf x}}+{\\bf w}\\|_{1}-\\|\\tilde{{\\bf x}}\\|_{1} $ </equation> <equation> $ \\mbox{subject to}\\|{\\bf v}-A{\\bf w}\\|_{2}\\leq r_{socp}, $ $ \\|{\\bf v}-A{\\bf w}\\|_{2}\\leq r_{socp}, $ </equation> </equationgroup> or in a more compact form \\newline <equationgroup> <equation> $ \\min_{{\\bf w}}\\|\\tilde{{\\bf x}}+{\\bf w}\\|_{1}-\\|\\tilde{{\\bf x}}\\|% _{1} $ $ \\min_{{\\bf w}} $ $ \\|\\tilde{{\\bf x}}+{\\bf w}\\|_{1}-\\|\\tilde{{\\bf x}}\\|_{1} $ </equation> <equation> $ \\mbox{subject to}\\|A_{{\\bf v}}\\begin{bmatrix}\\hidden@noalign% \\hfil\\textstyle{\\bf w}\\\\ \\hidden@noalign\\hfil\\textstyle\\sigma\\end{bmatrix}\\|_{2}\\leq r_{socp}, $ $ \\|A_{{\\bf v}}\\begin{bmatrix}\\hidden@noalign\\hfil\\textstyle{\\bf w% }\\\\ \\hidden@noalign\\hfil\\textstyle\\sigma\\end{bmatrix}\\|_{2}\\leq r_{socp}, $ </equation> </equationgroup> where $ A_{{\\bf v}}=\\begin{bmatrix}-A&{\\bf v}\\end{bmatrix} $ is now an $ m\\times(n+1) $ random matrix with i.i.d. standard normal components.", "Now, let $ C_{{\\bf w}_{up}} $ be a positive scalar.", "Then the optimal value of the objective of the following optimization problem is an upper bound on $ f_{obj} $ \\newline <equationgroup> <equation> $ \\min_{{\\bf w}}\\|\\tilde{{\\bf x}}+{\\bf w}\\|_{1}-\\|\\tilde{{\\bf x}}\\|% _{1}\\|A_{\\bf v}\\begin{bmatrix}\\hidden@noalign\\hfil\\textstyle{\\bf w}\\\\ \\hidden@noalign\\hfil\\textstyle\\sigma\\end{bmatrix}\\|_{2}\\leq r_{socp}\\|{\\bf w% }\\|_{2}^{2}\\leq C_{{\\bf w}_{up}}^{2}, $ $ \\min_{{\\bf w}} $ $ \\|\\tilde{{\\bf x}}+{\\bf w}\\|_{1}-\\|\\tilde{{\\bf x}}\\|_{1} $ $ \\|A_{\\bf v}\\begin{bmatrix}\\hidden@noalign\\hfil\\textstyle{\\bf w}% \\\\ \\hidden@noalign\\hfil\\textstyle\\sigma\\end{bmatrix}\\|_{2}\\leq r_{socp} $ $ \\|{\\bf w}\\|_{2}^{2}\\leq C_{{\\bf w}_{up}}^{2}, $ </equation> </equationgroup> One can then proceed by solving the above optimization problem through the Lagrange duality.", "However, instead of doing that we recognize that ( [@ref:LABEL:eq:upperobjlassol11] ) is the same as the first equation in Section 3.2 in [@bib:StojnicGenLasso10] .", "One can then repeat all the steps from Section 3.2 in [@bib:StojnicGenLasso10] until the last equation before Lemma 6 to obtain \\newline <equationgroup> <equation> $ -f_{obj}^{(up)}=-\\min_{\\lambda^{(2)},\\nu^{(1)}}\\max_{\\|{\\bf a}\\|_% {2}=C_{{\\bf w}_{up}}}(({\\bf z}^{(1)}-2\\lambda^{(2)})^{T}-\\nu^{(1)}A){\\bf a}-% \\nu^{(1)}{\\bf v}\\sigma+\\|\\nu^{(1)}\\|_{2}r_{socp}+2\\sum_{i=n-k+1}^{n}\\lambda_{i% }^{(2)}\\tilde{{\\bf x}}_{i} $ $ -f_{obj}^{(up)}=-\\min_{\\lambda^{(2)},\\nu^{(1)}}\\max_{\\|{\\bf a}\\|_% {2}=C_{{\\bf w}_{up}}} $ $ (({\\bf z}^{(1)}-2\\lambda^{(2)})^{T}-\\nu^{(1)}A){\\bf a}-\\nu^{(1)}{% \\bf v}\\sigma+\\|\\nu^{(1)}\\|_{2}r_{socp}+2\\sum_{i=n-k+1}^{n}\\lambda_{i}^{(2)}% \\tilde{{\\bf x}}_{i} $ </equation> <equation> $ \\mbox{subject to}0\\leq\\lambda_{i}^{(2)}\\leq 1,1\\leq i\\leq n, $ $  0\\leq\\lambda_{i}^{(2)}\\leq 1,1\\leq i\\leq n, $ </equation> </equationgroup> where $ {\\bf z}^{(1)} $ is an $ n $ dimensional vector of all ones, $ \\lambda^{(2)} $ and $ \\nu^{(1)} $ are $ n $ and $ m $ dimensional vectors of Lagrange variables, respectively, and $ -f_{obj}^{(up)} $ is the optimal value of ( [@ref:LABEL:eq:upperobjlassol11] ).", "If we can establish a \u201chigh-probability\u201d lower bound on $ f_{obj}^{(up)} $ we will have a \u201chigh-probability\u201d upper bound on the objective value of ( [@ref:LABEL:eq:upperobjlassol11] ).", "To do so, we recall on Lemma 6 from [@bib:StojnicGenLasso10] (Lemma 6 from [@bib:StojnicGenLasso10] is a slightly modified Lemma 3.1 from [@bib:Gordon88] which is the backbone of the escape through a mesh theorem utilized in [@bib:StojnicCSetam09] ).", "\\newline <theorem> Lemma 1 . Let $ A $ be an $ m\\times n $ matrix with i.i.d. standard normal components.", "Let $ {\\bf g} $ and $ {\\bf h} $ be $ m\\times 1 $ and $ (n+1)\\times 1 $ vectors, respectively, with i.i.d.", "standard normal components.", "Also, let $ g $ be a standard normal random variable and let $ \\Lambda $ be a set such that $ \\Lambda=(\\lambda^{(2)}|0\\leq\\lambda_{i}^{(2)}\\leq 1,1\\leq i\\leq n) $ .", "Then \\newline <equation> $ P(\\min_{\\lambda^{(2)}\\in\\Lambda,\\nu^{(1)}\\in R^{m}\\setminus 0}\\max_{\\|{\\bf a}% \\|_{2}=C_{{\\bf w}_{up}}}(-\\nu^{(1)}\\begin{bmatrix}A&{\\bf v}\\end{bmatrix}\\begin% {bmatrix}{\\bf a}\\\\ \\sigma\\end{bmatrix}+\\|\\nu^{(1)}\\|_{2}g-\\psi_{{\\bf a},\\lambda^{(2)},\\nu^{(1)}})% \\geq 0)\\\\ \\geq P(\\min_{\\lambda^{(2)}\\in\\Lambda,\\nu^{(1)}\\in R^{m}\\setminus 0}\\max_{\\|{% \\bf a}\\|_{2}=C_{{\\bf w}_{up}}}(\\|\\nu^{(1)}\\|_{2}(\\sum_{i=1}^{n}{\\bf h}_{i}{\\bf a% }_{i}+{\\bf h}_{n+1}\\sigma)+\\sqrt{C_{{\\bf w}_{up}}^{2}+\\sigma^{2}}\\sum_{i=1}^{m% }{\\bf g}_{i}\\nu_{i}^{(1)}-\\psi_{{\\bf a},\\lambda^{(2)},\\nu^{(1)}})\\geq 0).", "$ </equation> \\newline </theorem> Let \\newline <equation> $ \\psi_{{\\bf a},\\lambda^{(2)},\\nu^{(1)}}=\\epsilon_{3}^{(g)}\\sqrt{n}\\|\\nu^{(1)}\\|% _{2}-{\\bf a}^{T}({\\bf z}^{(1)}-2\\lambda^{(2)})-\\|\\nu^{(1)}\\|_{2}r_{socp}-2\\sum% _{i=n-k+1}^{n}\\lambda_{i}^{(2)}\\tilde{{\\bf x}}_{i}+\\widehat{f_{obj}^{(up)}}, $ </equation> with $ \\epsilon_{3}^{(g)}>0 $ being an arbitrarily small constant independent of $ n $ and $ \\widehat{f_{obj}^{(up)}} $ being a constant to be specified later.", "The left-hand side of the inequality in ( [@ref:LABEL:eq:upperproblemma] ) is then the following probability of interest \\newline <equation> $ p_{u}=P(\\min_{\\lambda^{(2)}\\in\\Lambda,\\nu^{(1)}\\in R^{m}\\setminus 0}\\max_{\\|{% \\bf a}\\|_{2}=C_{{\\bf w}_{up}}}(\\|\\nu^{(1)}\\|_{2}(\\sum_{i=1}^{n}{\\bf h}_{i}{\\bf a% }_{i}+{\\bf h}_{n+1}\\sigma)+\\sqrt{C_{{\\bf w}_{up}}^{2}+\\sigma^{2}}\\sum_{i=1}^{m% }{\\bf g}_{i}\\nu_{i}^{(1)}\\\\ -\\epsilon_{3}^{(g)}\\sqrt{n}\\|\\nu^{(1)}\\|_{2}+{\\bf a}^{T}({\\bf z}^{(1)}-2% \\lambda^{(2)})+\\|\\nu^{(1)}\\|_{2}r_{socp}+2\\sum_{i=n-k+1}^{n}\\lambda_{i}^{(2)}% \\tilde{{\\bf x}}_{i})\\geq\\widehat{f_{obj}^{(up)}}).", "$ </equation> After solving the inner maximization over $ {\\bf a} $ one has \\newline <equation> $ p_{u}=P(\\min_{\\lambda^{(2)}\\in\\Lambda,\\nu\\in R^{m}\\setminus 0}(C_{{\\bf w}_{up}% }\\|\\|\\nu^{(1)}\\|_{2}{\\bf h}+({\\bf z}^{(1)}-2\\lambda^{(2)})\\|_{2}+({\\bf h}_{n+1% }\\sigma-\\epsilon_{3}^{(g)}\\sqrt{n})\\|\\nu^{(1)}\\|_{2}\\\\ -\\sqrt{C_{{\\bf w}_{up}}^{2}+\\sigma^{2}}\\sum_{i=1}^{m}{\\bf g}_{i}\\nu_{i}^{(1)}+% r_{socp}\\|\\nu^{(1)}\\|_{2}+2\\sum_{i=n-k+1}^{n}\\lambda_{i}^{(2)}\\tilde{{\\bf x}}_% {i})\\geq\\widehat{f_{obj}^{(up)}}).", "$ </equation> After minimization of the third term over norm $ \\|\\nu^{(1)}\\|_{2} $ vector $ \\nu^{(1)} $ we further have \\newline <equation> $ p_{u}=P(\\min_{\\lambda^{(2)}\\in\\Lambda,\\nu\\in R^{m}\\setminus 0}(C_{{\\bf w}_{up}% }\\|\\|\\nu^{(1)}\\|_{2}{\\bf h}+({\\bf z}^{(1)}-2\\lambda^{(2)})\\|_{2}+({\\bf h}_{n+1% }\\sigma-\\epsilon_{3}^{(g)}\\sqrt{n})\\|\\nu^{(1)}\\|_{2}\\\\ -\\sqrt{C_{{\\bf w}_{up}}^{2}+\\sigma^{2}}\\|{\\bf g}\\|_{2}\\|\\nu^{(1)}\\|_{2}+r_{% socp}\\|\\nu^{(1)}\\|_{2}+2\\sum_{i=n-k+1}^{n}\\lambda_{i}^{(2)}\\tilde{{\\bf x}}_{i}% )\\geq\\widehat{f_{obj}^{(up)}}).", "$ </equation> Now we change variables so that $ \\nu=\\|\\nu^{(1)}\\|_{2} $ and assume that there is an arbitrarily large constant $ C_{\\nu} $ such that $ \\hat{\\nu}\\leq C_{\\nu} $ where $ \\hat{\\nu} $ is the solution of the optimization inside probability (using this assumption here will not affect substantially the value of the above probability if it eventually turns out that this assumption is valid with overwhelming probability; of course, this will turn out to be the case in all scenarios of interest in our analysis; strictly speaking from this point on all our overwhelming probabilities should be multiplied by a probability that $ \\hat{\\nu}\\leq C_{\\nu} $ ; to make writing less tedious we omit this probability and use strict inequalities).", "Returning back to ( [@ref:LABEL:eq:upperprobanal1] ) gives us \\newline <equation> $ p_{u}>P(\\min_{\\lambda^{(2)}\\in\\Lambda,\\nu\\in(0,C_{\\nu})}(C_{{\\bf w}_{up}}\\|\\nu% {\\bf h}+({\\bf z}^{(1)}-2\\lambda^{(2)})\\|_{2}+({\\bf h}_{n+1}\\sigma-\\epsilon_{3}% ^{(g)}\\sqrt{n})\\nu\\\\ -\\sqrt{C_{{\\bf w}_{up}}^{2}+\\sigma^{2}}\\|{\\bf g}\\|_{2}\\nu+r_{socp}\\nu+2\\sum_{i% =n-k+1}^{n}\\lambda_{i}^{(2)}\\tilde{{\\bf x}}_{i})\\geq\\widehat{f_{obj}^{(up)}}).", "$ </equation> Since $ {\\bf h}_{n+1} $ is a standard normal one has $ P({\\bf h}_{n+1}\\sigma\\geq-\\epsilon_{1}^{({\\bf h})}\\sqrt{n})\\geq 1-e^{-\\epsilon% _{2}^{({\\bf h})}n} $ where $ \\epsilon_{1}^{({\\bf h})}>0 $ is an arbitrarily small constant and $ \\epsilon_{2}^{({\\bf h})} $ is a constant dependent on $ \\epsilon_{1}^{({\\bf h})} $ and $ \\sigma $ but independent on $ n $ .", "Then from ( [@ref:LABEL:eq:upperprobanal2] ) we obtain \\newline <equation> $ p_{u}>P(\\min_{\\lambda^{(2)}\\in\\Lambda,\\nu\\in(0,C_{\\nu})}(C_{{\\bf w}_{up}}\\|\\nu% {\\bf h}+({\\bf z}^{(1)}-2\\lambda^{(2)})\\|_{2}-(\\epsilon_{1}^{({\\bf h})}+% \\epsilon_{3}^{(g)})\\sqrt{n}\\nu\\\\ -\\sqrt{C_{{\\bf w}_{up}}^{2}+\\sigma^{2}}\\|{\\bf g}\\|_{2}\\nu+r_{socp}\\nu+2\\sum_{i% =n-k+1}^{n}\\lambda_{i}^{(2)}\\tilde{{\\bf x}}_{i})\\geq\\widehat{f_{obj}^{(up)}})(% 1-e^{-\\epsilon_{2}^{({\\bf h})}n}).", "$ </equation> Set $ \\Lambda^{(2)}=\\{\\lambda^{(2)}|0\\leq\\lambda_{i}^{(2)}\\leq 2,1\\leq i\\leq n\\} $ and \\newline <equation> $ \\xi_{up}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp},C_{{\\bf w}_{up}})=% \\min_{\\lambda^{(2)}\\in\\Lambda^{(2)},\\nu\\in(0,C_{\\nu})}(C_{{\\bf w}_{up}}\\|\\nu{% \\bf h}+({\\bf z}^{(1)}-\\lambda^{(2)})\\|_{2}-(\\epsilon_{1}^{({\\bf h})}+\\epsilon_% {3}^{(g)})\\sqrt{n}\\nu\\\\ -\\sqrt{C_{{\\bf w}_{up}}^{2}+\\sigma^{2}}\\|{\\bf g}\\|_{2}\\nu+r_{socp}\\nu+\\sum_{i=% n-k+1}^{n}\\lambda_{i}^{(2)}\\tilde{{\\bf x}}_{i}).", "$ </equation> Now, before proceeding further we first recall on the following incredible result from [@bib:CIS76] related to the concentrations of Lipschitz functions of Gaussian random variables.", "\\newline <theorem> Lemma 2 . Let $ f_{lip}(\\cdot):R^{n}\\longrightarrow R $ be a Lipschitz function such that $ |f_{lip}({\\bf a})-f_{lip}({\\bf b})|\\leq c_{lip}\\|{\\bf a}-{\\bf b}\\|_{2} $ . Let $ {\\bf a} $ be a vector comprised of i.i.d.", "zero-mean, unit variance Gaussian random variables and let $ \\epsilon_{lip}>0 $ .", "Then \\newline <equation> $ P(|f_{lip}({\\bf a})-Ef_{lip}({\\bf a})|\\geq\\epsilon_{lip}|Ef_{lip}({\\bf a})|)% \\leq\\exp\\left\\{-\\frac{(\\epsilon_{lip}Ef_{lip}({\\bf", "a}))^{2}}{2c_{lip}^{2}}% \\right\\}. $ </equation> \\newline </theorem> In the following lemma we will show that $ \\xi_{up}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp},C_{{\\bf w}_{up}}) $ is a Lipschitz function.", "As such it will then concentrate according to the above lemma.", "\\newline <theorem> Lemma 3 . Let $ {\\bf g} $ and $ {\\bf h} $ be $ m $ and $ n $ dimensional vectors, respectively, with i.i.d. standard normal variables as their components.", "Let $ \\sigma>0 $ be an arbitrary scalar.", "Let $ \\xi_{up}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp},C_{{\\bf w}_{up}}) $ be as in ( [@ref:LABEL:eq:upperdefxi] ).", "Further let $ \\epsilon_{lip}>0 $ be any constant.", "Then \\newline <equation> $ P(|\\xi_{up}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp},C_{{\\bf w}_{up}})-% E\\xi_{up}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp},C_{{\\bf w}_{up}})|% \\geq\\epsilon_{lip}|E\\xi_{up}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp},C% _{{\\bf w}_{up}})|)\\\\ \\leq\\exp\\left\\{-\\frac{(\\epsilon_{lip}E\\xi_{up}(\\sigma,{\\bf g},{\\bf h},\\tilde{{% \\bf x}},r_{socp},C_{{\\bf w}_{up}}))^{2}}{2(2C_{{\\bf w}_{up}}^{2}+\\sigma^{2})}% \\right\\}. $ </equation> \\newline </theorem> <proof> Proof.", "The proof will parallel the corresponding one from [@bib:StojnicGenLasso10] .", "We start by setting \\newline <equation> $ f_{lip}({\\bf g}^{(1)},{\\bf h}^{(1)})=\\xi_{up}(\\sigma,{\\bf g}^{(1)},{\\bf h}^{(1% )},\\tilde{{\\bf x}},r_{socp},C_{{\\bf w}_{up}}).", "$ </equation> Further, let $ \\nu^{(lip_{1})} $ and $ \\lambda^{(lip_{1})} $ be the solutions of the minimization on the right-hand side of ( [@ref:LABEL:eq:upperlipproof1] ).", "In an analogous fashion set \\newline <equation> $ f_{lip}({\\bf g}^{(2)},{\\bf h}^{(2)})=\\xi_{up}(\\sigma,{\\bf g}^{(2)},{\\bf h}^{(2% )},\\tilde{{\\bf x}},,r_{socp},C_{{\\bf w}_{up}}), $ </equation> and let $ \\nu^{(lip_{2})} $ and $ \\lambda^{(lip_{2})} $ be the solutions of the minimization on the right-hand side of ( [@ref:LABEL:eq:upperlipproof3] ).", "Now assume that $ f_{lip}({\\bf g}^{(1)},{\\bf h}^{(1)})\\neq f_{lip}({\\bf g}^{(2)},{\\bf h}^{(2)}) $ (if they are equal we are trivially done).", "Further let $ f_{lip}({\\bf g}^{(1)},{\\bf h}^{(1)})<f_{lip}({\\bf g}^{(2)},{\\bf h}^{(2)}) $ (the rest of the argument of course can trivially be flipped if $ f_{lip}({\\bf g}^{(1)},{\\bf h}^{(1)})>f_{lip}({\\bf g}^{(2)},{\\bf h}^{(2)}) $ ).", "We then have \\newline <equation> $ |f_{lip}({\\bf g}^{(2)},{\\bf h}^{(2)})-f_{lip}({\\bf g}^{(1)},{\\bf h}^{(1)})|=f_% {lip}({\\bf g}^{(2)},{\\bf h}^{(2)})-f_{lip}({\\bf g}^{(1)},{\\bf h}^{(1)})\\\\ =(r_{socp}-(\\epsilon_{3}^{({\\bf h})}+\\epsilon_{3}^{(g)})\\sqrt{n})\\nu^{(lip_{2}% )}+(\\sqrt{C_{{\\bf w}_{up}}^{2}+\\sigma^{2}}\\|{\\bf g}^{(2)}\\|_{2}\\nu^{(lip_{2})}% -C_{{\\bf w}_{up}}\\|\\nu^{(lip_{2})}{\\bf h}^{(2)}+{\\bf z}^{(1)}-\\lambda^{(lip_{2% })}\\|_{2}-\\sum_{i=n-k+1}^{n}\\lambda_{i}^{(lip_{2})}\\tilde{{\\bf x}}_{i})\\\\ -((r_{socp}-(\\epsilon_{3}^{({\\bf h})}+\\epsilon_{3}^{(g)})\\sqrt{n})\\nu^{(lip_{1% })}+\\sqrt{C_{{\\bf w}_{up}}^{2}+\\sigma^{2}}\\|{\\bf g}^{(1)}\\|_{2}\\nu^{(lip_{1})}% -C_{{\\bf w}_{up}}\\|\\nu^{(lip_{1})}{\\bf h}^{(1)}+{\\bf z}^{(1)}-\\lambda^{(lip_{1% })}\\|_{2}-\\sum_{i=n-k+1}^{n}\\lambda_{i}^{(lip_{1})}\\tilde{{\\bf x}}_{i})\\\\ \\leq((r_{socp}-(\\epsilon_{3}^{({\\bf h})}+\\epsilon_{3}^{(g)})\\sqrt{n})\\nu^{(lip% _{1})}+\\sqrt{C_{{\\bf w}_{up}}^{2}+\\sigma^{2}}\\|{\\bf g}^{(2)}\\|_{2}\\nu^{(lip_{1% })}-C_{{\\bf w}_{up}}\\|\\nu^{(lip_{1})}{\\bf h}^{(2)}+{\\bf z}^{(1)}-\\lambda^{(lip% _{1})}\\|_{2}-\\sum_{i=n-k+1}^{n}\\lambda_{i}^{(lip_{1})}\\tilde{{\\bf x}}_{i})\\\\ -((r_{socp}-(\\epsilon_{3}^{({\\bf h})}+\\epsilon_{3}^{(g)})\\sqrt{n})\\nu^{(lip_{1% })}+\\sqrt{C_{{\\bf w}_{up}}^{2}+\\sigma^{2}}\\|{\\bf g}^{(1)}\\|_{2}\\nu^{(lip_{1})}% -C_{{\\bf w}_{up}}\\|\\nu^{(lip_{1})}{\\bf h}^{(1)}+{\\bf z}^{(1)}-\\lambda^{(lip_{1% })}\\|_{2}-\\sum_{i=n-k+1}^{n}\\lambda_{i}^{(lip_{1})}\\tilde{{\\bf x}}_{i})\\\\ =\\sqrt{C_{{\\bf w}_{up}}^{2}+\\sigma^{2}}(\\|{\\bf g}^{(2)}\\|_{2}-\\|{\\bf g}^{(1)}% \\|_{2})\\nu^{(lip_{1})}-C_{{\\bf w}_{up}}(\\|\\nu^{(lip_{1})}{\\bf h}^{(2)}+{\\bf z}% ^{(1)}-\\lambda^{(lip_{1})}\\|_{2}-\\|\\nu^{(lip_{1})}{\\bf h}^{(2)}+{\\bf z}^{(1)}-% \\lambda^{(lip_{1})}\\|_{2})\\\\ \\leq C_{\\nu}(\\sqrt{C_{{\\bf w}_{up}}^{2}+\\sigma^{2}}\\|{\\bf g}^{(2)}-{\\bf g}^{(1% )}\\|_{2}+C_{{\\bf w}_{up}}\\|{\\bf h}^{(2)}-{\\bf h}^{(1)}\\|_{2})\\\\ \\leq", "C_{\\nu}\\sqrt{2C_{{\\bf w}_{up}}^{2}+\\sigma^{2}}\\sqrt{\\|{\\bf g}^{(2)}-{\\bf g% }^{(1)}\\|_{2}^{2}+(\\|{\\bf h}^{(2)}-{\\bf h}^{(1)}\\|_{2}^{2})}, $ </equation> where the first inequality follows by sub-optimality of $ \\nu^{(lip_{1})} $ and $ \\lambda^{(lip_{1})} $ in ( [@ref:LABEL:eq:upperlipproof3] ).", "Connecting beginning and end in ( [@ref:LABEL:eq:upperlipproof5] ) and combining it with ( [@ref:LABEL:eq:upperlipproof1] ) one then has that $ \\xi_{up}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp},C_{{\\bf w}_{up}}) $ is Lipschitz with $ c_{lip}=C_{\\nu}\\sqrt{2C_{\\bf w}^{2}+\\sigma^{2}} $ . ( [@ref:LABEL:eq:upperlipsch1] ) then easily follows by Lemma [@ref:LABEL:thm:lipsch] .", "\u220e \\newline </proof> Let $ \\widehat{\\nu_{up}} $ and $ \\widehat{\\lambda_{up}^{(2)}} $ be the solutions of the optimization in ( [@ref:LABEL:eq:upperdefxi] ).", "One then has that $ \\|\\widehat{\\nu_{up}}{\\bf h}+{\\bf z}^{(1)}-\\widehat{\\lambda_{up}^{(2)}}\\|_{2} $ , $ \\widehat{\\nu_{up}} $ concentrate as well.", "More formally, one then has analogues to ( [@ref:LABEL:eq:upperlipsch1] ) \\newline <equationgroup> <equation> $  P(|\\|\\widehat{\\nu_{up}}{\\bf h}+{\\bf z}^{(1)}-\\widehat{\\lambda_{% up}^{(2)}}\\|_{2}-E\\|\\widehat{\\nu_{up}}{\\bf h}+{\\bf z}^{(1)}-\\widehat{\\lambda_{% up}^{(2)}}\\|_{2}|\\geq\\epsilon_{1}^{(normup)}E\\|\\widehat{\\nu_{up}}{\\bf h}+{\\bf z% }^{(1)}-\\widehat{\\lambda_{up}^{(2)}}\\|_{2})\\leq e^{-\\epsilon_{2}^{(normup)}n} $ $  P(|\\|\\widehat{\\nu_{up}}{\\bf h}+{\\bf z}^{(1)}-\\widehat{\\lambda_{% up}^{(2)}}\\|_{2}-E\\|\\widehat{\\nu_{up}}{\\bf h}+{\\bf z}^{(1)}-\\widehat{\\lambda_{%", "up}^{(2)}}\\|_{2}|\\geq\\epsilon_{1}^{(normup)}E\\|\\widehat{\\nu_{up}}{\\bf h}+{\\bf z% }^{(1)}-\\widehat{\\lambda_{up}^{(2)}}\\|_{2}) $ $ \\leq $ $  e^{-\\epsilon_{2}^{(normup)}n} $ </equation> <equation> $  P(|\\widehat{\\nu_{up}}-E\\widehat{\\nu_{up}}|\\geq\\epsilon_{1}^{(\\nu% _{up})}E\\widehat{\\nu_{up}})\\leq e^{-\\epsilon_{2}^{(\\nu_{up})}n}, $ $  P(|\\widehat{\\nu_{up}}-E\\widehat{\\nu_{up}}|\\geq\\epsilon_{1}^{(\\nu% _{up})}E\\widehat{\\nu_{up}}) $ $ \\leq $ $  e^{-\\epsilon_{2}^{(\\nu_{up})}n}, $ </equation> </equationgroup> where as usual $ \\epsilon_{1}^{(normup)}>0 $ and $ \\epsilon_{1}^{(\\nu_{up})}>0 $ are arbitrarily small constants and $ \\epsilon_{2}^{(normup)} $ and $ \\epsilon_{2}^{(\\nu_{up})} $ are constants dependent on $ \\epsilon_{1}^{(normup)}>0 $ and $ \\epsilon_{1}^{(\\nu_{up})}>0 $ , respectively, but independent of $ n $ .", "\\newline Set \\newline <equation> $ \\widehat{f_{obj}^{(up)}}=E\\xi_{up}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{% socp},C_{{\\bf w}_{up}})-\\epsilon_{lip}|E\\xi_{up}(\\sigma,{\\bf g},{\\bf h},\\tilde% {{\\bf x}},r_{socp},C_{{\\bf w}_{up}})|, $ </equation> where $ \\epsilon_{lip}>0 $ is an arbitrarily small constant.", "From ( [@ref:LABEL:eq:upperprobanal2] ) one then has \\newline <equation> $ p_{u}\\geq P(\\xi_{up}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp},C_{{\\bf w% }_{up}})\\geq\\widehat{f_{obj}^{(up)}})(1-e^{-\\epsilon_{2}^{({\\bf h})}n})\\geq% \\left(1-\\exp\\left\\{-\\frac{(\\epsilon_{lip}E\\xi_{up}(\\sigma,{\\bf g},{\\bf h},% \\tilde{{\\bf x}},r_{socp},C_{{\\bf w}_{up}}))^{2}}{2C_{\\nu}^{2}(2C_{{\\bf w}_{up}% }^{2}+\\sigma^{2})}\\right\\}\\right)(1-e^{-\\epsilon_{2}^{({\\bf h})}n}).", "$ </equation> ( [@ref:LABEL:eq:upperprobanalcont2] ) is conceptually enough to establish a \u201chigh probability\u201d upper bound on $ f_{obj} $ .", "What is left is to connect it with ( [@ref:LABEL:eq:upperLagran14] ).", "Combining ( [@ref:LABEL:eq:upperprobanalcont2] ), ( [@ref:LABEL:eq:upperproblemma] ), and ( [@ref:LABEL:eq:upperLagran14] ) we then obtain \\newline <equation> $ P(f_{obj}^{(up)}\\geq\\widehat{f_{obj}^{(up)}})\\geq\\left(1-\\exp\\left\\{-\\frac{(% \\epsilon_{lip}E\\xi_{up}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp},C_{{% \\bf w}_{up}}))^{2}}{2(2C_{{\\bf w}_{up}}^{2}+\\sigma^{2})}\\right\\}\\right)(1-e^{-% \\epsilon_{2}^{({\\bf h})}n})(1-e^{-\\epsilon_{4}^{(g)}n}), $ </equation> where we used the fact that $ g $ is the standard normal and therefore $ P(g-\\epsilon_{3}^{(g)}\\sqrt{n}\\leq 0)\\geq(1-e^{-\\epsilon_{4}^{(g)}n}) $ for an arbitrarily small $ \\epsilon_{3}^{(g)}>0 $ and a constant $ \\epsilon_{4}^{(g)} $ dependent on $ \\epsilon_{3}^{(g)} $ but independent of $ n $ .", "Let $ \\epsilon_{upper} $ be a constant such that \\newline <equation> $ 1-e^{-\\epsilon_{upper}n}<\\left(1-\\exp\\left\\{-\\frac{(\\epsilon_{lip}E\\xi_{up}(% \\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp},C_{{\\bf w}_{up}}))^{2}}{2C_{% \\nu}(2C_{{\\bf w}_{up}}^{2}+\\sigma^{2})}\\right\\}\\right)(1-e^{-\\epsilon_{2}^{({% \\bf h})}n})(1-e^{-\\epsilon_{4}^{g)}n}).", "$ </equation> \\newline We now summarize results from this subsection in the following lemma.", "\\newline <theorem> Lemma 4 . Let $ {\\bf v} $ be an $ n\\times 1 $ vector of i.i.d.", "zero-mean variance $ \\sigma^{2} $ Gaussian random variables and let $ A $ be an $ m\\times n $ matrix of i.i.d. standard normal random variables.", "Consider an $ \\tilde{{\\bf x}} $ defined in ( [@ref:LABEL:eq:xtildedef] ) and a $ {\\bf y} $ defined in ( [@ref:LABEL:eq:systemnoise] ) for $ {\\bf x}=\\tilde{{\\bf x}} $ .", "Let then $ f_{obj} $ be as defined in ( [@ref:LABEL:eq:objlassol1] ) and let $ {\\bf w} $ be the solution of ( [@ref:LABEL:eq:upperobjlassol11] ).", "There is a constant $ \\epsilon_{upper}>0 $ defined in ( [@ref:LABEL:eq:defepsupper] ) such that \\newline <equation> $ P(f_{obj}\\leq f_{obj}^{(upper)})\\geq 1-e^{-\\epsilon_{upper}n}, $ </equation> where \\newline <equation> $ f_{obj}^{(upper)}=-E\\xi_{up}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp},C% _{{\\bf", "w}_{up}})+\\epsilon_{lip}|E\\xi_{up}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x% }},r_{socp},C_{{\\bf w}_{up}})|+\\epsilon_{1}^{({\\bf h})}\\sqrt{n}+\\epsilon_{3}^{% (g)}\\sqrt{n}, $ </equation> $ \\xi_{up}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp},C_{{\\bf w}_{up}}) $ is as defined in ( [@ref:LABEL:eq:upperdefxi] ), $ \\epsilon_{lip},\\epsilon_{1}^{({\\bf h})},\\epsilon_{3}^{(g)} $ are all positive arbitrarily small constants, and $ C_{{\\bf w}_{up}} $ is a constant such that $ \\|{\\bf w}\\|_{2}\\leq C_{{\\bf w}_{up}} $ .", "\\newline </theorem> <proof> Proof.", "Follows from the discussion above.", "\u220e \\newline </proof> </subsection> <subsection> <title> 2.2 Lower-bounding f \u2062 o b j </title> In this section we present the part of the framework that relates to finding a \u201chigh-probability\u201d lower bound on $ f_{obj} $ .", "To make arguments that will follow less tedious we will already here make an assumption that is significantly weaker than what we will eventually prove.", "Namely, we will assume that there is a (if necessary arbitrarily large) constant $ C_{\\bf w} $ such that \\newline <equation> $ P(\\|{\\bf w}\\|_{2}\\leq C_{\\bf w})\\geq 1-e^{-\\epsilon_{C_{\\bf w}}n}, $ </equation> for an arbitrarily large constant $ C_{\\bf w} $ and a constant $ \\epsilon_{C_{\\bf w}}>0 $ dependent on $ C_{\\bf w} $ but independent of $ n $ .", "The flow of our presentation would probably be more natural if one provides a direct proof of this statement right here.", "However, given the difficulty of the task ahead we refrain from doing that and assume that the statement is correct.", "Roughly speaking, what we actually assume is that $ \\|{\\bf w}_{socp}\\|_{2} $ is bounded by an arbitrarily large constant (of course, as mentioned above, we hope to create a machinery that can prove much \u201cbigger\u201d things than ( [@ref:LABEL:eq:assumplasso] )).", "\\newline Now we will look at the following optimization problem \\newline <equationgroup> <equation> $ \\min_{{\\bf x}}\\|{\\bf y}-A{\\bf x}\\|_{2} $ $ \\min_{{\\bf x}} $ $ \\|{\\bf y}-A{\\bf x}\\|_{2} $ </equation> <equation> $ \\mbox{subject to}\\|{\\bf x}\\|_{1}-\\|\\tilde{{\\bf x}}\\|_{1}\\leq f_{% obj}^{(lower)}. $ $ \\|{\\bf x}\\|_{1}-\\|\\tilde{{\\bf x}}\\|_{1}\\leq f_{obj}^{(lower)}. $ </equation> </equationgroup> If we can show that for certain $ f_{obj}^{(lower)} $ the objective of ( [@ref:LABEL:eq:lbobjlassol1] ) is with overwhelming probability larger then $ r_{socp} $ , then $ f_{obj}^{(lower)} $ will be a \u201chigh-probability\u201d lower bound on the optimal value of the objective of ( [@ref:LABEL:eq:objlassol1] ), i.e. on $ f_{obj} $ .", "Hence, the strategy will be to show that for certain $ f_{obj}^{(lower)} $ the optimal value of the objective in ( [@ref:LABEL:eq:lbobjlassol1] )", "is with overwhelming probability lower bounded by a quantity larger than $ r_{socp} $ .", "We again start by noting that if one knows that $ {\\bf y}=A\\tilde{{\\bf x}}+{\\bf v} $ holds then ( [@ref:LABEL:eq:lbobjlassol1] ) can be rewritten as \\newline <equationgroup> <equation> $ \\min_{{\\bf x}}\\|{\\bf v}+A\\tilde{{\\bf x}}-A{\\bf x}\\|_{2} $ $ \\min_{{\\bf x}} $ $ \\|{\\bf v}+A\\tilde{{\\bf x}}-A{\\bf x}\\|_{2} $ </equation> <equation> $ \\mbox{subject to}\\|{\\bf x}\\|_{1}-\\|\\tilde{{\\bf x}}\\|_{1}\\leq f_{% obj}^{(lower)}. $ $ \\|{\\bf x}\\|_{1}-\\|\\tilde{{\\bf x}}\\|_{1}\\leq f_{obj}^{(lower)}. $ </equation> </equationgroup> After a small change of variables, $ {\\bf x}=\\tilde{{\\bf x}}+{\\bf w} $ , ( [@ref:LABEL:eq:lbobjlassol11] ) becomes \\newline <equationgroup> <equation> $ \\min_{{\\bf w}}\\|{\\bf v}-A{\\bf w}\\|_{2} $ $ \\min_{{\\bf w}} $ $ \\|{\\bf v}-A{\\bf w}\\|_{2} $ </equation> <equation> $ \\mbox{subject to}\\|\\tilde{{\\bf x}}+{\\bf w}\\|_{1}-\\|\\tilde{{\\bf x}% }\\|_{1}\\leq f_{obj}^{(lower)}, $ $ \\|\\tilde{{\\bf x}}+{\\bf w}\\|_{1}-\\|\\tilde{{\\bf x}}\\|_{1}\\leq f_{% obj}^{(lower)}, $ </equation> </equationgroup> or in a more compact form \\newline <equationgroup> <equation> $ \\min_{{\\bf w}}\\|A_{{\\bf v}}\\begin{bmatrix}\\hidden@noalign\\hfil% \\textstyle{\\bf w}\\\\ \\hidden@noalign\\hfil\\textstyle\\sigma\\end{bmatrix}\\|_{2} $ $ \\min_{{\\bf w}} $ $ \\|A_{{\\bf v}}\\begin{bmatrix}\\hidden@noalign\\hfil\\textstyle{\\bf w% }\\\\ \\hidden@noalign\\hfil\\textstyle\\sigma\\end{bmatrix}\\|_{2} $ </equation> <equation> $ \\mbox{subject to}\\|\\tilde{{\\bf x}}+{\\bf w}\\|_{1}-\\|\\tilde{{\\bf x}% }\\|_{1}\\leq f_{obj}^{(lower)}, $ $ \\|\\tilde{{\\bf x}}+{\\bf w}\\|_{1}-\\|\\tilde{{\\bf x}}\\|_{1}\\leq f_{% obj}^{(lower)}, $ </equation> </equationgroup> where as in the previous section $ A_{{\\bf v}}=\\begin{bmatrix}-A&{\\bf v}\\end{bmatrix} $ is now an $ m\\times(n+1) $ random matrix with i.i.d. standard normal components.", "Set \\newline <equationgroup> <equation> $ \\zeta_{obj}=\\min_{{\\bf w}}\\|A_{{\\bf v}}\\begin{bmatrix}% \\hidden@noalign\\hfil\\textstyle{\\bf w}\\\\ \\hidden@noalign\\hfil\\textstyle\\sigma\\end{bmatrix}\\|_{2} $ $ \\zeta_{obj}=\\min_{{\\bf w}} $ $ \\|A_{{\\bf v}}\\begin{bmatrix}\\hidden@noalign\\hfil\\textstyle{\\bf w% }\\\\ \\hidden@noalign\\hfil\\textstyle\\sigma\\end{bmatrix}\\|_{2} $ </equation> <equation> $ \\mbox{subject to}\\|\\tilde{{\\bf x}}+{\\bf w}\\|_{1}-\\|\\tilde{{\\bf x}% }\\|_{1}\\leq f_{obj}^{(lower)}. $ $ \\|\\tilde{{\\bf x}}+{\\bf w}\\|_{1}-\\|\\tilde{{\\bf x}}\\|_{1}\\leq f_{% obj}^{(lower)}. $ </equation> </equationgroup> Let \\newline <equation> $ S_{{\\bf w}}(\\sigma,\\tilde{{\\bf x}},C_{\\bf w},f_{obj}^{(lower)})=\\{\\begin{% bmatrix}{\\bf w}\\\\ \\sigma\\end{bmatrix}\\in R^{n+1}|\\quad\\|{\\bf w}\\|_{2}\\leq C_{\\bf w}\\quad\\mbox{% and}\\quad\\|\\tilde{{\\bf x}}+{\\bf w}\\|_{1}-\\|\\tilde{{\\bf x}}\\|_{1}\\leq f_{obj}^{% (lower)}\\}. $ </equation> Set \\newline <equation> $ \\zeta_{obj}^{(help)}=\\min_{[{\\bf w}^{T}\\sigma]^{T}\\in S_{{\\bf w}}(\\sigma,% \\tilde{{\\bf x}},C_{\\bf w},f_{obj}^{(lower)})}\\|A_{{\\bf v}}\\begin{bmatrix}{\\bf w% }\\\\ \\sigma\\end{bmatrix}\\|_{2}=\\min_{[{\\bf w}^{T}\\sigma]^{T}\\in S_{{\\bf w}}(\\sigma,% \\tilde{{\\bf x}},C_{\\bf w},f_{obj}^{(lower)})}\\max_{\\|{\\bf a}\\|_{2}=1}{\\bf a}^{% T}A_{{\\bf v}}\\begin{bmatrix}{\\bf w}\\\\ \\sigma\\end{bmatrix}. $ </equation> Now, after applying Lemma 3.1 from [@bib:Gordon88] one has \\newline <equation> $ P\\left(\\min_{[{\\bf w}^{T}\\sigma]^{T}\\in S_{{\\bf w}}(\\sigma,\\tilde{{\\bf x}},C_{% \\bf w},f_{obj}^{(lower)})}\\max_{\\|{\\bf a}\\|_{2}=1}\\left({\\bf a}^{T}A_{{\\bf v}}% \\begin{bmatrix}{\\bf w}\\\\ \\sigma\\end{bmatrix}+\\sqrt{\\|{\\bf w}\\|_{2}^{2}+\\sigma^{2}}g\\right)\\geq\\zeta_{% obj}^{(l)}\\right)\\\\ \\geq P\\left(\\min_{[{\\bf w}^{T}\\sigma]^{T}\\in S_{{\\bf w}}(\\sigma,\\tilde{{\\bf x}% },C_{\\bf w},f_{obj}^{(lower)})}\\max_{\\|{\\bf a}\\|_{2}=1}\\left(\\sqrt{\\|{\\bf w}\\|% _{2}^{2}+\\sigma^{2}}\\sum_{i=1}^{m}{\\bf g}_{i}{\\bf a}_{i}+\\sum_{i=1}^{n}{\\bf h}% _{i}{\\bf w}_{i}+{\\bf h}_{n+1}\\sigma\\right)\\geq\\zeta_{obj}^{(l)}\\right).", "$ </equation> In what follows we will analyze the following probability \\newline <equation> $ p_{l}=P\\left(\\min_{[{\\bf w}^{T}\\sigma]^{T}\\in S_{{\\bf w}}(\\sigma,\\tilde{{\\bf x% }},C_{\\bf w},f_{obj}^{(lower)})}\\max_{\\|{\\bf a}\\|_{2}=1}\\left(\\sqrt{\\|{\\bf w}% \\|_{2}^{2}+\\sigma^{2}}\\sum_{i=1}^{m}{\\bf g}_{i}{\\bf a}_{i}+\\sum_{i=1}^{n}{\\bf h% }_{i}{\\bf w}_{i}+{\\bf h}_{n+1}\\sigma\\right)\\geq\\zeta_{obj}^{(l)}\\right), $ </equation> which is of course nothing but the probability on the left-hand side of the inequality in ( [@ref:LABEL:eq:objlassol15] ).", "We will essentially show that for certain $ \\zeta_{obj}^{(l)} $ this probability is close to $ 1 $ .", "That will rather obviously imply that we have a \u201chigh probability\u201d lower bound on $ \\zeta_{obj} $ .", "Moreover, if such a lower bound is larger than $ r_{socp} $ we will be done in terms of establishing a \u201chigh probability\u201d lower bound on $ f_{obj} $ .", "To that end, we first note that the maximization over $ {\\bf a} $ is trivial and one obtains \\newline <equation> $ p_{l}=P\\left(\\min_{[{\\bf w}^{T}\\sigma]^{T}\\in S_{{\\bf w}}(\\sigma,\\tilde{{\\bf x% }},C_{\\bf w},f_{obj}^{(lower)})}\\left(\\sqrt{\\|{\\bf w}\\|_{2}^{2}+\\sigma^{2}}\\|{% \\bf g}\\|_{2}+\\sum_{i=1}^{n}{\\bf h}_{i}{\\bf w}_{i}\\right)+{\\bf h}_{n+1}\\sigma% \\geq\\zeta_{obj}^{(l)}\\right).", "$ </equation> To facilitate the exposition that will follow let \\newline <equation> $ \\xi(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},f_{obj}^{(lower)})=\\min_{[{\\bf w}^{% T}\\sigma]^{T}\\in S_{{\\bf w}}(\\sigma,\\tilde{{\\bf x}},C_{\\bf w},f_{obj}^{(lower)% })}\\left(\\sqrt{\\|{\\bf w}\\|_{2}^{2}+\\sigma^{2}}\\|{\\bf g}\\|_{2}+\\sum_{i=1}^{n}{% \\bf h}_{i}{\\bf w}_{i}\\right).", "$ </equation> Since $ C_{{\\bf w}} $ is not a substantially important parameter in our derivation we omit it from the list of arguments of $ \\xi $ ; this a practice that we will adopt many occasions below, fairly often, without explicitly mentioning it.", "Also, one should note here that, although present in the definition of $ S_{{\\bf w}} $ , $ \\sigma $ clearly does not have an impact through $ S_{{\\bf w}} $ on the result of the above optimization.", "Now we split the analysis into two parts.", "The first one will be a deterministic analysis of $ \\xi(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},f_{obj}^{(lower)}) $ and will be presented in Subsection [@ref:LABEL:sec:unsigneddet] .", "In the second part (that will be presented in Subsection [@ref:LABEL:sec:unsignedconc] ) we will use the results of that analysis and continue the above probabilistic arguments applying various concentration results.", "\\newline <subsubsection> <title> 2.2.1 Optimizing \u2062 \u03be ( \u03c3 , g , h ,  x , f \u2062 o b j ( \u2062 l o w e r ) ) </title> In this section we compute $ \\xi(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},f_{obj}^{(lower)}) $ .", "We first rewrite the optimization problem from ( [@ref:LABEL:eq:defxi] ) in the following form \\newline <equationgroup> <equation> $ \\xi(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},f_{obj}^{(lower)})=% \\min_{{\\bf w}}\\sqrt{\\|{\\bf w}\\|_{2}^{2}+\\sigma^{2}}\\|{\\bf g}\\|_{2}+\\sum_{i=1}^% {n}{\\bf h}_{i}{\\bf w}_{i} $ $ \\xi(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},f_{obj}^{(lower)})=% \\min_{{\\bf w}} $ $ \\sqrt{\\|{\\bf w}\\|_{2}^{2}+\\sigma^{2}}\\|{\\bf g}\\|_{2}+\\sum_{i=1}^{% n}{\\bf h}_{i}{\\bf w}_{i} $ </equation> <equation> $ \\mbox{subject to}\\|\\tilde{{\\bf x}}+{\\bf w}\\|_{1}-\\|\\tilde{{\\bf x}% }\\|_{1}\\leq f_{obj}^{(lower)}\\sqrt{\\|{\\bf w}\\|_{2}^{2}+\\sigma^{2}}\\leq\\sqrt{C_% {\\bf w}^{2}+\\sigma^{2}}. $ $ \\|\\tilde{{\\bf x}}+{\\bf w}\\|_{1}-\\|\\tilde{{\\bf x}}\\|_{1}\\leq f_{% obj}^{(lower)} $ $ \\sqrt{\\|{\\bf w}\\|_{2}^{2}+\\sigma^{2}}\\leq\\sqrt{C_{\\bf w}^{2}+% \\sigma^{2}}. $ </equation> </equationgroup> From this point one can proceed with solving the above problem through Lagrangian duality.", "However, instead one can recognize that the above optimization problem is fairly similar to $ (23) $ in [@bib:StojnicGenLasso10] .", "The difference is only in the constant term in the first constraint.", "After carefully repeating all the steps between $ (23) $ and $ (39) $ in [@bib:StojnicGenLasso10] one then arrives at the following analogue to $ (39) $ from [@bib:StojnicGenLasso10] \\newline <equationgroup> <equation> $ \\xi(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},f_{obj}^{(lower)})=% \\max_{\\nu,\\lambda^{(2)},\\gamma}\\sigma\\sqrt{(\\|{\\bf g}\\|_{2}+\\gamma)^{2}-\\|{\\bf h% }+\\nu{\\bf z}^{(1)}-\\lambda^{(2)}\\|_{2}^{2}}-\\sum_{i=n-k+1}^{n}\\lambda_{i}^{(2)% }\\tilde{{\\bf x}}_{i}-\\gamma\\sqrt{C_{\\bf w}^{2}+\\sigma^{2}}-\\nu f_{obj}^{(lower)} $ $ \\xi(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},f_{obj}^{(lower)})=% \\max_{\\nu,\\lambda^{(2)},\\gamma} $ $ \\sigma\\sqrt{(\\|{\\bf g}\\|_{2}+\\gamma)^{2}-\\|{\\bf h}+\\nu{\\bf z}^{(1% )}-\\lambda^{(2)}\\|_{2}^{2}}-\\sum_{i=n-k+1}^{n}\\lambda_{i}^{(2)}\\tilde{{\\bf x}}% _{i}-\\gamma\\sqrt{C_{\\bf w}^{2}+\\sigma^{2}}-\\nu f_{obj}^{(lower)} $ </equation> <equation> $ \\mbox{subject to}\\nu\\geq 00\\leq\\lambda_{i}^{(2)}\\leq 2\\nu,1\\leq i% \\leq n\\|{\\bf g}\\|_{2}+\\gamma-\\|{\\bf h}+\\nu{\\bf z}^{(1)}-\\lambda^{(2)}\\|_{2}% \\geq 0\\gamma\\geq 0.", "$ $ \\nu\\geq 0 $ $  0\\leq\\lambda_{i}^{(2)}\\leq 2\\nu,1\\leq i\\leq n $ $ \\|{\\bf g}\\|_{2}+\\gamma-\\|{\\bf h}+\\nu{\\bf z}^{(1)}-\\lambda^{(2)}\\|% _{2}\\geq 0 $ $ \\gamma\\geq 0.", "$ </equation> </equationgroup> Now, the maximization over $ \\gamma $ can be done.", "After setting the derivative to zero one finds \\newline <equation> $ \\frac{\\|{\\bf g}\\|_{2}+\\gamma}{\\sqrt{(\\|{\\bf g}\\|_{2}+\\gamma)^{2}-\\|{\\bf h}+\\nu% {\\bf z}^{(1)}-\\lambda^{(2)}\\|_{2}^{2}}}-\\sqrt{C_{\\bf w}^{2}+\\sigma^{2}}=0 $ </equation> and after some algebra \\newline <equation> $ \\gamma_{opt}=\\sqrt{1+\\frac{\\sigma^{2}}{C_{\\bf w}^{2}}}\\|{\\bf h}+\\nu{\\bf z}^{(1% )}-\\lambda^{(2)}\\|_{2}-\\|{\\bf g}\\|_{2}, $ </equation> where of course $ \\gamma_{opt} $ would be the solution of ( [@ref:LABEL:eq:Lagran10] ) only if larger than or equal to zero.", "Alternatively of course $ \\gamma_{opt}=0 $ .", "Now, based on these two scenarios we distinguish two different optimization problems: \\newline <list> \\ {The \u201coverwhelming\u201d optimization} \\newline <equationgroup> <equation> $ \\xi_{ov}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},f_{obj}^{(lower)}% )=\\max_{\\nu,\\lambda^{(2)}}\\sigma\\sqrt{\\|{\\bf g}\\|_{2}^{2}-\\|{\\bf h}+\\nu{\\bf z}% ^{(1)}-\\lambda^{(2)}\\|_{2}^{2}}-\\sum_{i=n-k+1}^{n}\\lambda_{i}^{(2)}\\tilde{{\\bf x% }}_{i}-\\nu f_{obj}^{(lower)} $ $ \\xi_{ov}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},f_{obj}^{(lower)}% )=\\max_{\\nu,\\lambda^{(2)}} $ $ \\sigma\\sqrt{\\|{\\bf g}\\|_{2}^{2}-\\|{\\bf h}+\\nu{\\bf z}^{(1)}-% \\lambda^{(2)}\\|_{2}^{2}}-\\sum_{i=n-k+1}^{n}\\lambda_{i}^{(2)}\\tilde{{\\bf x}}_{i% }-\\nu f_{obj}^{(lower)} $ </equation> <equation> $ \\mbox{subject to}\\nu\\geq 00\\leq\\lambda_{i}^{(2)}\\leq 2\\nu,1\\leq i% \\leq n. $ $ \\nu\\geq 0 $ $  0\\leq\\lambda_{i}^{(2)}\\leq 2\\nu,1\\leq i\\leq n. $ </equation> </equationgroup> \\newline \\ \\ {The \u201cnon-overwhelming\u201d optimization} \\newline <equationgroup> <equation> $ \\xi_{nov}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},f_{obj}^{(lower)% })=\\max_{\\nu,\\lambda^{(2)}}\\sqrt{C_{\\bf w}^{2}+\\sigma^{2}}\\|{\\bf g}\\|_{2}-C_{% \\bf w}\\|{\\bf h}+\\nu{\\bf z}^{(1)}-\\lambda^{(2)}\\|_{2}-\\sum_{i=n-k+1}^{n}\\lambda% _{i}^{(2)}\\tilde{{\\bf x}}_{i}-\\nu f_{obj}^{(lower)} $ $ \\xi_{nov}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},f_{obj}^{(lower)% })=\\max_{\\nu,\\lambda^{(2)}} $ $ \\sqrt{C_{\\bf w}^{2}+\\sigma^{2}}\\|{\\bf g}\\|_{2}-C_{\\bf w}\\|{\\bf h}% +\\nu{\\bf z}^{(1)}-\\lambda^{(2)}\\|_{2}-\\sum_{i=n-k+1}^{n}\\lambda_{i}^{(2)}% \\tilde{{\\bf x}}_{i}-\\nu f_{obj}^{(lower)} $ </equation> <equation> $ \\mbox{subject to}\\nu\\geq 00\\leq\\lambda_{i}^{(2)}\\leq 2\\nu,1\\leq i% \\leq n. $ $ \\nu\\geq 0 $ $  0\\leq\\lambda_{i}^{(2)}\\leq 2\\nu,1\\leq i\\leq n. $ </equation> </equationgroup> \\newline \\ </list> The \u201coverwhelming\u201d optimization is the equivalent to ( [@ref:LABEL:eq:Lagran10] ) if for its optimal values $ \\hat{\\nu} $ and $ \\widehat{\\lambda^{(2)}} $ one has \\newline <equation> $ \\sqrt{1+\\frac{\\sigma^{2}}{C_{\\bf w}^{2}}}\\|{\\bf h}+\\hat{\\nu}{\\bf z}^{(1)}-% \\widehat{\\lambda^{(2)}}\\|_{2}\\leq\\|{\\bf g}\\|_{2}, $ </equation> We now summarize in the following lemma the results of this subsection.", "\\newline <theorem> Lemma 5 . Let $ \\hat{\\nu} $ and $ \\widehat{\\lambda^{(2)}} $ be the solutions of ( [@ref:LABEL:eq:Lagran12] ) and analogously let $ \\tilde{\\nu} $ and $ \\widetilde{\\lambda^{(2)}} $ be the solutions of ( [@ref:LABEL:eq:Lagran13] ).", "Let $ \\xi(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},f_{obj}^{(lower)}) $ be, as defined in ( [@ref:LABEL:eq:defxi] ), the optimal value of the objective function in ( [@ref:LABEL:eq:defxi] ).", "Then \\newline <equation> $ \\xi(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},f_{obj}^{(lower)})=\\begin{cases}% \\sigma\\sqrt{\\|{\\bf g}\\|_{2}^{2}-\\|{\\bf h}+\\hat{\\nu}{\\bf z}^{(1)}-\\widehat{% \\lambda^{(2)}}\\|_{2}^{2}}-\\sum_{i=n-k+1}^{n}\\widehat{\\lambda_{i}^{(2)}}\\tilde{% {\\bf x}}_{i}-\\nu f_{obj}^{(lower)},&\\mbox{if}\\quad\\frac{\\sqrt{1+\\frac{\\sigma^{% 2}}{C_{\\bf w}^{2}}}\\|{\\bf h}+\\hat{\\nu}{\\bf z}^{(1)}-\\widehat{\\lambda^{(2)}}\\|_% {2}}{\\|{\\bf g}\\|_{2}^{-1}}\\leq 1\\\\ \\sqrt{C_{\\bf w}^{2}+\\sigma^{2}}\\|{\\bf g}\\|_{2}-C_{\\bf w}\\|{\\bf h}+\\tilde{\\nu}{% \\bf z}^{(1)}-\\widetilde{\\lambda^{(2)}}\\|_{2}-\\sum_{i=n-k+1}^{n}\\widetilde{% \\lambda_{i}^{(2)}}\\tilde{{\\bf x}}_{i}-\\nu f_{obj}^{(lower)},&\\mbox{otherwise}% \\end{cases}. $ </equation> Moreover, let $ \\hat{{\\bf w}} $ be the solution of ( [@ref:LABEL:eq:defxi] ).", "Then \\newline <equation> $ \\hat{{\\bf w}}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf", "x}},f_{obj}^{(lower)})=\\begin% {cases}\\frac{\\sigma({\\bf h}+\\hat{\\nu}{\\bf z}^{(1)}-\\widehat{\\lambda^{(2)}})}{% \\sqrt{\\|{\\bf g}\\|_{2}^{2}-\\|{\\bf h}+\\hat{\\nu}{\\bf z}^{(1)}-\\widehat{\\lambda^{(% 2)}}\\|_{2}^{2}}},&\\mbox{if}\\quad\\sqrt{1+\\frac{\\sigma^{2}}{C_{\\bf w}^{2}}}\\|{% \\bf h}+\\hat{\\nu}{\\bf z}^{(1)}-\\widehat{\\lambda^{(2)}}\\|_{2}\\leq\\|{\\bf g}\\|_{2}% \\\\ \\frac{C_{\\bf w}({\\bf h}+\\tilde{\\nu}{\\bf z}^{(1)}-\\widetilde{\\lambda^{(2)}})}{% \\|{\\bf h}+\\tilde{\\nu}{\\bf z}^{(1)}-\\widetilde{\\lambda^{(2)}}\\|_{2}},&\\mbox{% otherwise}\\end{cases}, $ </equation> and \\newline <equation> $ \\|\\hat{{\\bf w}}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},f_{obj}^{(lower)})\\|_{2% }=\\begin{cases}\\frac{\\sigma\\|{\\bf h}+\\hat{\\nu}{\\bf z}^{(1)}-\\widehat{\\lambda^{% (2)}})\\|_{2}}{\\sqrt{\\|{\\bf g}\\|_{2}^{2}-\\|{\\bf h}+\\hat{\\nu}{\\bf z}^{(1)}-% \\widehat{\\lambda^{(2)}}\\|_{2}^{2}}},&\\mbox{if}\\quad\\sqrt{1+\\frac{\\sigma^{2}}{C% _{\\bf w}^{2}}}\\|{\\bf h}+\\hat{\\nu}{\\bf z}^{(1)}-\\widehat{\\lambda^{(2)}}\\|_{2}% \\leq\\|{\\bf g}\\|_{2}\\\\ C_{\\bf w},&\\mbox{otherwise}\\end{cases}. $ </equation> \\newline </theorem> <proof> Proof.", "The first part follows trivially.", "The second one follows the same way it does in Lemma 2 in [@bib:StojnicGenLasso10] .", "\u220e \\newline </proof> </subsubsection> <subsubsection> <title> 2.2.2 Concentration of \u2062 \u03be ( \u03c3 , g , h ,  x , f \u2062 o b j ( \u2062 l o w e r ) ) </title> In this section we establish that $ \\xi(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},f_{obj}^{(lower)}) $ concentrates with high probability around its mean.", "\\newline <theorem> Lemma 6 . Let $ {\\bf g} $ and $ {\\bf h} $ be $ m $ and $ n $ dimensional vectors, respectively, with i.i.d. standard normal variables as their components.", "Let $ \\sigma>0 $ be an arbitrary scalar.", "Let $ \\xi(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},f_{obj}^{(lower)}) $ be as in ( [@ref:LABEL:eq:defxi] ).", "Further let $ \\epsilon_{lip}>0 $ be any constant.", "Then \\newline <equation> $ P(|\\xi(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},f_{obj}^{(lower)})-E\\xi(\\sigma,{% \\bf g},{\\bf h},\\tilde{{\\bf x}},f_{obj}^{(lower)})|\\geq\\epsilon_{lip}E\\xi(% \\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},f_{obj}^{(lower)}))\\leq\\exp\\left\\{-% \\frac{(\\epsilon_{lip}E\\xi(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},f_{obj}^{(% lower)}))^{2}}{2(2C_{\\bf w}^{2}+\\sigma^{2})}\\right\\}. $ </equation> \\newline </theorem> <proof> Proof.", "The proof is the same as the proof of Lemma 4 in [@bib:StojnicGenLasso10] .", "The only difference is the structure of set $ S_{{\\bf w}} $ which does not impact substantially any of the arguments in the proof presented in [@bib:StojnicGenLasso10] .", "\u220e \\newline </proof> One then has that $ \\|{\\bf h}+\\hat{\\nu}{\\bf z}^{(1)}-\\widehat{\\lambda^{(2)}}\\|_{2} $ , $ \\|{\\bf h}+\\tilde{\\nu}{\\bf z}^{(1)}-\\widetilde{\\lambda^{(2)}}\\|_{2} $ , $ \\hat{\\nu} $ , and $ \\tilde{\\nu} $ concentrate as well which automatically implies that $ \\hat{{\\bf w}} $ also concentrates.", "More formally, one then has analogues to ( [@ref:LABEL:eq:lipsch1] ) \\newline <equationgroup> <equation> $  P(|\\|{\\bf h}+\\hat{\\nu}{\\bf z}^{(1)}-\\widehat{\\lambda^{(2)}}\\|_{2% }-E\\|{\\bf h}+\\hat{\\nu}{\\bf z}^{(1)}-\\widehat{\\lambda^{(2)}}\\|_{2}|\\geq\\epsilon% _{1}^{(norm)}E\\|{\\bf h}+\\hat{\\nu}{\\bf z}^{(1)}-\\widehat{\\lambda^{(2)}}\\|_{2})% \\leq e^{-\\epsilon_{2}^{(norm)}n} $ $  P(|\\|{\\bf h}+\\hat{\\nu}{\\bf z}^{(1)}-\\widehat{\\lambda^{(2)}}\\|_{2% }-E\\|{\\bf h}+\\hat{\\nu}{\\bf z}^{(1)}-\\widehat{\\lambda^{(2)}}\\|_{2}|\\geq\\epsilon% _{1}^{(norm)}E\\|{\\bf h}+\\hat{\\nu}{\\bf z}^{(1)}-\\widehat{\\lambda^{(2)}}\\|_{2}) $ $ \\leq $ $  e^{-\\epsilon_{2}^{(norm)}n} $ </equation> <equation> $  P(|\\|{\\bf h}+\\tilde{\\nu}{\\bf z}^{(1)}-\\widetilde{\\lambda^{(2)}}% \\|_{2}-E\\|{\\bf h}+\\tilde{\\nu}{\\bf z}^{(1)}-\\widetilde{\\lambda^{(2)}}\\|_{2}|% \\geq\\epsilon_{3}^{(norm)}E\\|{\\bf h}+\\tilde{\\nu}{\\bf z}^{(1)}-\\widetilde{% \\lambda^{(2)}}\\|_{2})\\leq e^{-\\epsilon_{4}^{(norm)}n} $ $  P(|\\|{\\bf h}+\\tilde{\\nu}{\\bf z}^{(1)}-\\widetilde{\\lambda^{(2)}}% \\|_{2}-E\\|{\\bf h}+\\tilde{\\nu}{\\bf z}^{(1)}-\\widetilde{\\lambda^{(2)}}\\|_{2}|% \\geq\\epsilon_{3}^{(norm)}E\\|{\\bf h}+\\tilde{\\nu}{\\bf z}^{(1)}-\\widetilde{% \\lambda^{(2)}}\\|_{2}) $ $ \\leq $ $  e^{-\\epsilon_{4}^{(norm)}n} $ </equation> <equation> $  P(|\\hat{\\nu}-E\\hat{\\nu}|\\geq\\epsilon_{1}^{(\\nu)}E\\hat{\\nu})\\leq e% ^{-\\epsilon_{2}^{(\\nu)}n} $ $  P(|\\hat{\\nu}-E\\hat{\\nu}|\\geq\\epsilon_{1}^{(\\nu)}E\\hat{\\nu}) $ $ \\leq $ $  e^{-\\epsilon_{2}^{(\\nu)}n} $ </equation> <equation> $  P(|\\tilde{\\nu}-E\\tilde{\\nu}|\\geq\\epsilon_{3}^{(\\nu)}E\\tilde{\\nu}% )\\leq e^{-\\epsilon_{4}^{(\\nu)}n} $ $  P(|\\tilde{\\nu}-E\\tilde{\\nu}|\\geq\\epsilon_{3}^{(\\nu)}E\\tilde{\\nu}) $ $ \\leq $ $  e^{-\\epsilon_{4}^{(\\nu)}n} $ </equation> <equation> $  P(|\\|\\hat{{\\bf w}}\\|_{2}-E\\|\\hat{{\\bf w}}\\|_{2}|\\geq\\epsilon_{1}% ^{({\\bf w})}E\\|\\hat{{\\bf w}}\\|_{2})\\leq e^{-\\epsilon_{2}^{({\\bf w})}n}, $ $  P(|\\|\\hat{{\\bf w}}\\|_{2}-E\\|\\hat{{\\bf w}}\\|_{2}|\\geq\\epsilon_{1}% ^{({\\bf w})}E\\|\\hat{{\\bf w}}\\|_{2}) $ $ \\leq $ $  e^{-\\epsilon_{2}^{({\\bf w})}n}, $ </equation> </equationgroup> where as usual $ \\epsilon_{1}^{(norm)}>0 $ , $ \\epsilon_{3}^{(norm)}>0 $ , $ \\epsilon_{1}^{(\\nu)}>0 $ , $ \\epsilon_{3}^{(\\nu)}>0 $ , and $ \\epsilon_{1}^{({\\bf w})}>0 $ are arbitrarily small constants and $ \\epsilon_{2}^{(norm)} $ , $ \\epsilon_{4}^{(norm)} $ , $ \\epsilon_{2}^{(\\nu)} $ , $ \\epsilon_{4}^{(\\nu)} $ , and $ \\epsilon_{2}^{({\\bf w})} $ are constant dependent on $ \\epsilon_{1}^{(norm)}>0 $ , $ \\epsilon_{3}^{(norm)}>0 $ , $ \\epsilon_{1}^{(\\nu)}>0 $ , $ \\epsilon_{3}^{(\\nu)}>0 $ , and $ \\epsilon_{1}^{({\\bf w})}>0 $ , respectively, but independent of $ n $ .", "\\newline Now, we return to the probabilistic analysis of ( [@ref:LABEL:eq:probint1] ).", "Combining ( [@ref:LABEL:eq:probint1] ), ( [@ref:LABEL:eq:defxi] ), and ( [@ref:LABEL:eq:lipsch1] ) we have \\newline <equationgroup> <equation> $  p_{l}=P\\left(\\min_{[{\\bf w}^{T}\\sigma]^{T}\\in S_{{\\bf w}}(\\sigma% ,\\tilde{{\\bf x}},C_{\\bf w},f_{obj}^{(lower)})}\\left(\\sqrt{\\|{\\bf w}\\|_{2}^{2}+% \\sigma^{2}}\\|{\\bf g}\\|_{2}+\\sum_{i=1}^{n}{\\bf h}_{i}{\\bf w}_{i}\\right)+{\\bf h}% _{n+1}\\sigma\\geq\\zeta_{obj}^{(l)}\\right)=P\\left(\\xi(\\sigma,{\\bf g},{\\bf h},% \\tilde{{\\bf x}},f_{obj}^{(lower)})+{\\bf h}_{n+1}\\sigma\\geq\\zeta_{obj}^{(l)}% \\right)\\geq\\left(1-\\exp\\left\\{-\\frac{(\\epsilon_{lip}E\\xi(\\sigma,{\\bf g},{\\bf h% },\\tilde{{\\bf x}},f_{obj}^{(lower)}))^{2}}{2(2C_{\\bf w}^{2}+\\sigma^{2})}\\right% \\}\\right)P\\left((1-\\epsilon_{lip})E\\xi(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},% f_{obj}^{(lower)})+{\\bf h}_{n+1}\\sigma\\geq\\zeta_{obj}^{(l)}\\right), $ $  p_{l} $ $ = $ $  P\\left(\\min_{[{\\bf w}^{T}\\sigma]^{T}\\in S_{{\\bf w}}(\\sigma,% \\tilde{{\\bf x}},C_{\\bf w},f_{obj}^{(lower)})}\\left(\\sqrt{\\|{\\bf w}\\|_{2}^{2}+% \\sigma^{2}}\\|{\\bf g}\\|_{2}+\\sum_{i=1}^{n}{\\bf h}_{i}{\\bf w}_{i}\\right)+{\\bf h}% _{n+1}\\sigma\\geq\\zeta_{obj}^{(l)}\\right) $ $ = $ $  P\\left(\\xi(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},f_{obj}^{(% lower)})+{\\bf h}_{n+1}\\sigma\\geq\\zeta_{obj}^{(l)}\\right) $ $ \\geq $ $ \\left(1-\\exp\\left\\{-\\frac{(\\epsilon_{lip}E\\xi(\\sigma,{\\bf g},{\\bf h% },\\tilde{{\\bf x}},f_{obj}^{(lower)}))^{2}}{2(2C_{\\bf w}^{2}+\\sigma^{2})}\\right% \\}\\right)P\\left((1-\\epsilon_{lip})E\\xi(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},% f_{obj}^{(lower)})+{\\bf h}_{n+1}\\sigma\\geq\\zeta_{obj}^{(l)}\\right), $ </equation> </equationgroup> where we consider only the interesting case $ E\\xi(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},f_{obj}^{(lower)}))\\geq 0 $ .", "Since $ {\\bf h}_{n+1} $ is a standard normal one easily has $ P({\\bf h}_{n+1}\\sigma\\geq-\\epsilon_{1}^{({\\bf h})}\\sqrt{n})\\geq 1-e^{-\\epsilon% _{2}^{({\\bf h})}n} $ where $ \\epsilon_{1}^{({\\bf h})}>0 $ is an arbitrarily small constant and $ \\epsilon_{2}^{({\\bf h})} $ is a constant dependent on $ \\epsilon_{1}^{({\\bf h})} $ and $ \\sigma $ but independent on $ n $ .", "By choosing \\newline <equation> $ \\zeta_{obj}^{(l)}=(1-\\epsilon_{lip})E\\xi(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}% },f_{obj}^{(lower)})-\\epsilon_{1}^{({\\bf h})}\\sqrt{n}, $ </equation> one then from ( [@ref:LABEL:eq:probanalcont1] ) has \\newline <equation> $ p_{l}=P\\left(\\min_{[{\\bf w}^{T}\\sigma]^{T}\\in S_{{\\bf w}}(\\sigma,\\tilde{{\\bf x% }},C_{\\bf w},f_{obj}^{(lower)})}\\left(\\sqrt{\\|{\\bf w}\\|_{2}^{2}+\\sigma^{2}}\\|{% \\bf g}\\|_{2}+\\sum_{i=1}^{n}{\\bf h}_{i}{\\bf w}_{i}\\right)+{\\bf h}_{n+1}\\sigma% \\geq(1-\\epsilon_{lip})E\\xi(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},f_{obj}^{(% lower)})-\\epsilon_{1}^{({\\bf h})}\\sqrt{n}\\right)\\\\ \\geq\\left(1-\\exp\\left\\{-\\frac{(\\epsilon_{lip}E\\xi(\\sigma,{\\bf g},{\\bf h},% \\tilde{{\\bf x}},f_{obj}^{(lower)}))^{2}}{2(2C_{\\bf w}^{2}+\\sigma^{2})}\\right\\}% \\right)(1-e^{-\\epsilon_{2}^{({\\bf h})}n}).", "$ </equation> ( [@ref:LABEL:eq:probanalcont2] ) is conceptually enough to establish a \u201chigh probability\u201d lower bound on $ \\zeta_{obj} $ .", "Mimicking the steps between $ (58) $ and $ (64) $ in [@bib:StojnicGenLasso10] one obtains the following analogue to $ (64) $ in [@bib:StojnicGenLasso10] \\newline <equation> $ P(\\zeta_{obj}\\geq\\zeta_{obj}^{(lower)})\\geq P(\\zeta_{obj}^{(help)}\\geq\\zeta_{% obj}^{(lower)})(1-e^{-\\epsilon_{C_{w}}n})\\\\ =P(\\min_{[{\\bf w}^{T}\\sigma]^{T}\\in S_{{\\bf w}}(\\sigma,\\tilde{{\\bf x}},C_{\\bf w% },f_{obj}^{(lower)})}(\\|A_{{\\bf v}}\\begin{bmatrix}{\\bf w}\\\\ \\sigma\\end{bmatrix}\\|_{2})\\geq\\zeta_{obj}^{(lower)})(1-e^{-\\epsilon_{C_{w}}n})% \\geq(1-e^{-\\epsilon_{lower}n})(1-e^{-\\epsilon_{C_{w}}n}).", "$ </equation> where \\newline <equation> $ \\zeta_{obj}^{(lower)}=(1-\\epsilon_{lip})E\\xi(\\sigma,{\\bf g},{\\bf h},\\tilde{{% \\bf x}},f_{obj}^{(lower)})-\\epsilon_{1}^{({\\bf h})}\\sqrt{n}-\\epsilon_{1}^{(g)}% \\sqrt{n}, $ </equation> and \\newline <equation> $ 1-e^{-\\epsilon_{lower}n}<\\left(1-\\exp\\left\\{-\\frac{(\\epsilon_{lip}E\\xi(\\sigma,% {\\bf g},{\\bf h},\\tilde{{\\bf x}},f_{obj}^{(lower)}))^{2}}{2(2C_{\\bf w}^{2}+% \\sigma^{2})}\\right\\}\\right)(1-e^{-\\epsilon_{2}^{({\\bf h})}n})(1-e^{-\\epsilon_{% 1}^{(g)}n}).", "$ </equation> We summarize the results from this subsection in the following lemma.", "\\newline <theorem> Lemma 7 . Let $ {\\bf v} $ be an $ n\\times 1 $ vector of i.i.d.", "zero-mean variance $ \\sigma^{2} $ Gaussian random variables and let $ A $ be an $ m\\times n $ matrix of i.i.d. standard normal random variables.", "Consider an $ \\tilde{{\\bf x}} $ defined in ( [@ref:LABEL:eq:xtildedef] ) and a $ {\\bf y} $ defined in ( [@ref:LABEL:eq:systemnoise] ) for $ {\\bf x}=\\tilde{{\\bf x}} $ .", "Let then $ \\zeta_{obj} $ be as defined in ( [@ref:LABEL:eq:objlassol14] ) and let $ {\\bf w} $ be the solution of ( [@ref:LABEL:eq:objlassol14] ).", "Assume $ P(\\|{\\bf w}\\|_{2}\\leq C_{\\bf w})\\geq 1-e^{-\\epsilon_{C_{\\bf w}}n} $ for an arbitrarily large constant $ C_{\\bf w} $ and a constant $ \\epsilon_{C_{\\bf w}}>0 $ dependent on $ C_{\\bf w} $ but independent of $ n $ .", "Then there is a constant $ \\epsilon_{lower}>0 $ \\newline <equation> $ P(\\zeta_{obj}\\geq\\zeta_{obj}^{(lower)})\\geq(1-e^{-\\epsilon_{lower}n})(1-e^{-% \\epsilon_{C_{w}}n}), $ </equation> where \\newline <equation> $ \\zeta_{obj}^{(lower)}=(1-\\epsilon_{lip})E\\xi(\\sigma,{\\bf g},{\\bf h},\\tilde{{% \\bf x}},f_{obj}^{(lower)})-\\epsilon_{1}^{({\\bf h})}\\sqrt{n}-\\epsilon_{1}^{(g)}% \\sqrt{n}, $ </equation> $ \\xi(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},f_{obj}^{(lower)}) $ is as defined in ( [@ref:LABEL:eq:defxi] ) (and can be computed through ( [@ref:LABEL:eq:Lagran12] ) and ( [@ref:LABEL:eq:Lagran13] )), and $ \\epsilon_{lip},\\epsilon_{1}^{({\\bf h})},\\epsilon_{1}^{(g)} $ are all positive arbitrarily small constants.", "\\newline </theorem> <proof> Proof.", "Follows from the discussion above.", "\u220e \\newline </proof> The above Lemma achieves one of the goals established at the beginning of this section.", "Namely, for a $ f_{obj}^{(lower)} $ it establishes a high probability lower bound $ \\zeta_{obj}^{(lower)} $ on $ \\zeta_{obj} $ .", "As we stated earlier, if we can find $ f_{obj}^{(lower)} $ such that $ \\zeta_{obj}^{(lower)}>r_{socp} $ then $ f_{obj}^{(lower)} $ is a high probability lower bound on $ f_{obj} $ .", "Moreover, we hope that $ f_{obj}^{(upper)}\\approx f_{obj}^{(lower)} $ and that $ C_{{\\bf w}_{up}} $ for which this would happen is such that $ C_{{\\bf w}_{up}}\\approx\\|{\\bf w}_{socp}\\|_{2} $ .", "All of this is established in the following section.", "\\newline </subsubsection> </subsection> <subsection> <title> 2.3 Matching upper and lower bounds </title> In this section we specialize the general bounds $ f_{obj}^{(upper)} $ and $ f_{obj}^{(lower)} $ introduced above and show how they can match each other.", "We will divide presentation in several subsections.", "In the first of the subsections we will make a connection to the noiseless case and show how one can then remove the constraint from ( [@ref:LABEL:eq:defhatxi] ), ( [@ref:LABEL:eq:defhatw] ), and ( [@ref:LABEL:eq:defhatwnorm] ).", "In the second and third subsection we will specialize the upper and lower bounds on $ f_{obj} $ computed in Sections [@ref:LABEL:sec:unsignedlbzetaobj] and [@ref:LABEL:sec:unsignedubzetaobj] and show that they can match each other.", "In the fourth subsection we will quantify how much the lower bound on $ \\zeta_{obj} $ that can be computed through the framework presented in Section [@ref:LABEL:sec:unsignedlbzetaobj] for a \u201csuboptimal\u201d $ {\\bf w} $ deviates from the \u201coptimal\u201d one obtained for $ \\hat{{\\bf w}} $ .", "In the last subsection we will connect all the pieces and draw conclusions regarding the consequences that their a combination leaves on several SOCP parameters.", "\\newline <subsubsection> <title> 2.3.1 Connection to the \u2113 1 optimization </title> In this subsection we establish a connection between the constraint in ( [@ref:LABEL:eq:defhatxi] ), ( [@ref:LABEL:eq:defhatw] ), and ( [@ref:LABEL:eq:defhatwnorm] ) and the fundamental performance characterization of $ \\ell_{1} $ optimization derived in [@bib:StojnicUpper10] (and of course earlier in the context of neighborly polytopes in [@bib:DonohoPol] ).", "What we present here is exactly the same as what was presented in the corresponding section in [@bib:StojnicGenLasso10] .", "However, given its importance/relevance to the current analysis we include it here again.", "We first recall on the condition from Lemma [@ref:LABEL:thm:optsollower] .", "The condition states \\newline <equation> $ \\sqrt{1+\\frac{\\sigma^{2}}{C_{\\bf w}^{2}}}\\|{\\bf h}+\\hat{\\nu}{\\bf z}^{(1)}-% \\widehat{\\lambda^{(2)}}\\|_{2}\\leq\\|{\\bf g}\\|_{2}, $ </equation> where $ C_{\\bf w} $ is an arbitrarily large constant and $ \\hat{\\nu} $ and $ \\widehat{\\lambda^{(2)}} $ are the solution of \\newline <equationgroup> <equation> $ \\max\\sigma\\sqrt{\\|{\\bf g}\\|_{2}^{2}-\\|{\\bf h}+\\nu{\\bf z}^{(1)}-% \\lambda^{(2)}\\|_{2}^{2}}-\\sum_{i=n-k+1}^{n}\\lambda_{i}^{(2)}\\tilde{{\\bf x}}_{i} $ $ \\max $ $ \\sigma\\sqrt{\\|{\\bf g}\\|_{2}^{2}-\\|{\\bf h}+\\nu{\\bf z}^{(1)}-% \\lambda^{(2)}\\|_{2}^{2}}-\\sum_{i=n-k+1}^{n}\\lambda_{i}^{(2)}\\tilde{{\\bf x}}_{i} $ </equation> <equation> $ \\mbox{subject to}0\\leq\\lambda_{i}^{(2)}\\leq 2\\nu,1\\leq i\\leq n\\nu% \\geq 0.", "$ $  0\\leq\\lambda_{i}^{(2)}\\leq 2\\nu,1\\leq i\\leq n $ $ \\nu\\geq 0.", "$ </equation> </equationgroup> Now we note the following equivalent to ( [@ref:LABEL:eq:matchopt] ) in the case when nonzero components of $ \\tilde{{\\bf x}} $ are infinite \\newline <equationgroup> <equation> $ \\max\\sigma\\sqrt{\\|{\\bf g}\\|_{2}^{2}-\\|{\\bf h}+\\nu{\\bf z}^{(1)}-% \\lambda^{(2)}\\|_{2}^{2}} $ $ \\max $ $ \\sigma\\sqrt{\\|{\\bf g}\\|_{2}^{2}-\\|{\\bf h}+\\nu{\\bf z}^{(1)}-% \\lambda^{(2)}\\|_{2}^{2}} $ </equation> <equation> $ \\mbox{subject to}0\\leq\\lambda_{i}^{(2)}\\leq 2\\nu,1\\leq i\\leq n-k% \\lambda_{i}^{(2)}=0,n-k+1\\leq i\\leq n\\nu\\geq 0.", "$ $  0\\leq\\lambda_{i}^{(2)}\\leq 2\\nu,1\\leq i\\leq n-k $ $ \\lambda_{i}^{(2)}=0,n-k+1\\leq i\\leq n $ $ \\nu\\geq 0.", "$ </equation> </equationgroup> To make the new observations easily comparable to the corresponding ones from [@bib:StojnicCSetam09,StojnicEquiv10] we set \\newline <equation> $ \\bar{{\\bf h}}=[|{\\bf h}|_{(1)}^{(1)},|{\\bf h}|_{(2)}^{(2)},\\dots,|{\\bf h}|_{(n% -k)}^{(n-k)},{\\bf h}_{n-k+1},{\\bf h}_{n-k+2},\\dots,{\\bf h}_{n}]^{T}, $ </equation> where $ [|{\\bf h}|_{(1)}^{(1)},|{\\bf h}|_{(2)}^{(2)},\\dots,|{\\bf h}|_{(n-k)}^{(n-k)}] $ are the magnitudes of $ [{\\bf h}_{1},{\\bf h}_{2},\\dots,{\\bf h}_{n-k}] $ sorted in increasing order (possible ties in the sorting process are of course broken arbitrarily).", "Also we let $ {\\bf z}^{(2)} $ be such that $ {\\bf z}_{i}^{(2)}=-{\\bf z}_{i}^{(1)},n-k+1\\leq i\\leq n $ and $ {\\bf z}_{i}^{(2)}={\\bf z}_{i}^{(1)},1\\leq i\\leq n-k $ .", "It is then relatively easy to see that the above optimization problem is equivalent to \\newline <equationgroup> <equation> $ \\max\\sigma\\sqrt{\\|{\\bf g}\\|_{2}^{2}-\\|\\bar{{\\bf h}}-\\nu{\\bf z}^{(% 2)}+\\lambda^{(2)}\\|_{2}^{2}} $ $ \\max $ $ \\sigma\\sqrt{\\|{\\bf g}\\|_{2}^{2}-\\|\\bar{{\\bf h}}-\\nu{\\bf z}^{(2)}+% \\lambda^{(2)}\\|_{2}^{2}} $ </equation> <equation> $ \\mbox{subject to}0\\leq\\lambda_{i}^{(2)}\\leq\\nu,1\\leq i\\leq n-k% \\lambda_{i}^{(2)}=0,n-k+1\\leq i\\leq n\\nu\\geq 0.", "$ $  0\\leq\\lambda_{i}^{(2)}\\leq\\nu,1\\leq i\\leq n-k $ $ \\lambda_{i}^{(2)}=0,n-k+1\\leq i\\leq n $ $ \\nu\\geq 0.", "$ </equation> </equationgroup> Let $ \\nu_{\\ell_{1}} $ and $ \\lambda^{(\\ell_{1})} $ be the solution of the above maximization.", "Then, as we showed in [@bib:StojnicCSetam09] and [@bib:StojnicUpper10] , the inequality \\newline <equation> $ E\\|{\\bf g}\\|_{2}>E\\|\\bar{{\\bf h}}-\\nu_{\\ell_{1}}{\\bf z}^{(2)}+\\lambda^{(\\ell_{% 1})}\\|_{2} $ </equation> establishes the following fundamental performance characterization of the $ \\ell_{1} $ optimization algorithm from ( [@ref:LABEL:eq:l1] ) that could be used instead of SOCP to recover $ {\\bf x} $ in ( [@ref:LABEL:eq:system] ) (which is a noiseless version of ( [@ref:LABEL:eq:systemnoise] )) \\newline <equation> $ (1-\\beta_{w})\\frac{\\sqrt{\\frac{2}{\\pi}}e^{-(\\mbox{erfinv}(\\frac{1-\\alpha_{w}}{% 1-\\beta_{w}}))^{2}}}{\\alpha_{w}}-\\sqrt{2}\\mbox{erfinv}(\\frac{1-\\alpha_{w}}{1-% \\beta_{w}})=0.", "$ </equation> Clearly, in ( [@ref:LABEL:eq:fundl1] ) one has $ \\alpha_{w}=\\frac{m}{n} $ and $ \\beta_{w}=\\frac{k}{n} $ .", "As it is also shown in [@bib:StojnicCSetam09] and [@bib:StojnicUpper10] both of the quantities under the expected values in ( [@ref:LABEL:eq:fundl1exp] ) nicely concentrate.", "Then with overwhelming probability one has that for any pair $ (\\alpha,\\beta) $ that satisfies (or lies below) the above fundamental performance characterization of $ \\ell_{1} $ optimization \\newline <equation> $ \\|{\\bf g}\\|_{2}>\\|\\bar{{\\bf h}}-\\nu_{\\ell_{1}}{\\bf z}^{(2)}+\\lambda^{(\\ell_{1}% )}\\|_{2}. $ </equation>", "Moreover, since $ \\lambda_{i}^{(2)}\\geq 0,n-k+1\\leq i\\leq n $ , in ( [@ref:LABEL:eq:matchopt] ) one actually has that ( [@ref:LABEL:eq:fundl1noexp] ) implies that with overwhelming probability \\newline <equation> $ \\|{\\bf g}\\|_{2}>\\|{\\bf h}+\\hat{\\nu}{\\bf z}^{(1)}-\\widehat{\\lambda^{(2)}}\\|_{2}, $ </equation> which for sufficiently large $ C_{\\bf w} $ is the same as ( [@ref:LABEL:eq:condoptsollower] ).", "We then in what follows assume that pair $ (\\alpha,\\beta) $ is such that it satisfies the fundamental $ \\ell_{1} $ optimization performance characterization (or is in the region below it) and therefore proceed by ignoring the condition ( [@ref:LABEL:eq:condoptsollower] ).", "(Strictly speaking, all our overwhelming probabilities below should be multiplied with an overwhelming probability that ( [@ref:LABEL:eq:fundl1] ) holds; to maintain writing easier we will skip this detail.) \\newline </subsubsection> <subsubsection> <title> 2.3.2 Optimizing f \u2062 o b j \u2019s upper bound </title> In this section we will lower the value of the upper bound created in Section [@ref:LABEL:sec:unsignedubzetaobj] as much as we can by a particular choice of $ C_{{\\bf w}_{up}} $ . Let $ \\xi_{dual}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp}) $ be \\newline <equationgroup> <equation> $ \\xi_{dual}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp})=\\min_% {d\\geq 0}\\max_{\\nu,\\lambda^{(2)}}\\sqrt{d^{2}+\\sigma^{2}}\\|{\\bf g}\\|_{2}\\nu-d\\|% \\nu{\\bf h}+{\\bf z}^{(1)}-\\lambda^{(2)}\\|_{2}-\\sum_{i=n-k+1}^{n}\\lambda_{i}^{(2% )}\\tilde{{\\bf x}}_{i}-\\nu r_{socp} $ $ \\xi_{dual}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp})=\\min_% {d\\geq 0}\\max_{\\nu,\\lambda^{(2)}} $ $ \\sqrt{d^{2}+\\sigma^{2}}\\|{\\bf g}\\|_{2}\\nu-d\\|\\nu{\\bf h}+{\\bf z}^{% (1)}-\\lambda^{(2)}\\|_{2}-\\sum_{i=n-k+1}^{n}\\lambda_{i}^{(2)}\\tilde{{\\bf x}}_{i% }-\\nu r_{socp} $ </equation> <equation> $ \\mbox{subject to}\\nu\\geq 00\\leq\\lambda_{i}^{(2)}\\leq 2,1\\leq i% \\leq n. $ $ \\nu\\geq 0 $ $  0\\leq\\lambda_{i}^{(2)}\\leq 2,1\\leq i\\leq n. $ </equation> </equationgroup> Rewriting ( [@ref:LABEL:eq:devubLagran11] ) with a simple sign flipping turns out to be useful in what follows \\newline <equationgroup> <equation> $ -\\xi_{dual}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp})=\\max% _{d\\geq 0}\\min_{\\nu,\\lambda^{(2)}}-\\sqrt{d^{2}+\\sigma^{2}}\\|{\\bf g}\\|_{2}\\nu+d% \\|\\nu{\\bf h}+{\\bf z}^{(1)}-\\lambda^{(2)}\\|_{2}+\\sum_{i=n-k+1}^{n}\\lambda_{i}^{% (2)}\\tilde{{\\bf x}}_{i}+\\nu r_{socp} $ $ -\\xi_{dual}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp})=\\max% _{d\\geq 0}\\min_{\\nu,\\lambda^{(2)}} $ $ -\\sqrt{d^{2}+\\sigma^{2}}\\|{\\bf g}\\|_{2}\\nu+d\\|\\nu{\\bf h}+{\\bf z}^% {(1)}-\\lambda^{(2)}\\|_{2}+\\sum_{i=n-k+1}^{n}\\lambda_{i}^{(2)}\\tilde{{\\bf x}}_{% i}+\\nu r_{socp} $ </equation> <equation> $ \\mbox{subject to}\\nu\\geq 00\\leq\\lambda_{i}^{(2)}\\leq 2,1\\leq i% \\leq n. $ $ \\nu\\geq 0 $ $  0\\leq\\lambda_{i}^{(2)}\\leq 2,1\\leq i\\leq n. $ </equation> </equationgroup> The following lemma provides a powerful tool to deal with ( [@ref:LABEL:eq:devubLagran12] ).", "\\newline <theorem> Lemma 8 . Let $ \\xi_{dual}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp}) $ be as defined in ( [@ref:LABEL:eq:devubLagran12] ).", "Further, let \\newline <equationgroup> <equation> $ -\\xi_{prim}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp})=\\min% _{\\nu,\\lambda^{(2)}}\\max_{d\\geq 0}-\\sqrt{d^{2}+\\sigma^{2}}\\|{\\bf g}\\|_{2}\\nu+d% \\|\\nu{\\bf h}+{\\bf z}^{(1)}-\\lambda^{(2)}\\|_{2}+\\sum_{i=n-k+1}^{n}\\lambda_{i}^{% (2)}\\tilde{{\\bf x}}_{i}+\\nu r_{socp} $ $ -\\xi_{prim}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp})=\\min% _{\\nu,\\lambda^{(2)}}\\max_{d\\geq 0} $ $ -\\sqrt{d^{2}+\\sigma^{2}}\\|{\\bf g}\\|_{2}\\nu+d\\|\\nu{\\bf h}+{\\bf z}^% {(1)}-\\lambda^{(2)}\\|_{2}+\\sum_{i=n-k+1}^{n}\\lambda_{i}^{(2)}\\tilde{{\\bf x}}_{% i}+\\nu r_{socp} $ </equation> <equation> $ \\mbox{subject to}\\nu\\geq 00\\leq\\lambda_{i}^{(2)}\\leq 2,1\\leq i% \\leq n. $ $ \\nu\\geq 0 $ $  0\\leq\\lambda_{i}^{(2)}\\leq 2,1\\leq i\\leq n. $ </equation> </equationgroup> Then \\newline <equation> $ \\xi_{dual}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp})=\\xi_{prim}(\\sigma,% {\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp}).", "$ </equation> \\newline </theorem> <proof> Proof.", "After solving the inner maximization over $ d $ in ( [@ref:LABEL:eq:devublemmadet1] ) one has \\newline <equation> $ d_{opt}=\\sigma\\frac{\\|\\nu{\\bf h}+{\\bf z}^{(1)}-\\lambda^{(2)}\\|_{2}}{\\sqrt{\\|{% \\bf g}\\|_{2}^{2}\\nu^{2}-\\|\\nu{\\bf h}+{\\bf z}^{(1)}-\\lambda^{(2)}\\|_{2}^{2}}}. $ </equation> Such a $ d $ then establishes that the right-hand side of ( [@ref:LABEL:eq:devublemmadet1] ) is \\newline <equationgroup> <equation> $ -\\xi_{prim}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp})=\\min% _{\\nu,\\lambda^{(2)}}-\\sigma\\sqrt{\\|{\\bf g}\\|_{2}^{2}\\nu-\\|\\nu{\\bf h}+{\\bf z}^{% (1)}-\\lambda^{(2)}\\|_{2}^{2}}+\\sum_{i=n-k+1}^{n}\\lambda_{i}^{(2)}\\tilde{{\\bf x% }}_{i}+\\nu r_{socp} $ $ -\\xi_{prim}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp})=\\min% _{\\nu,\\lambda^{(2)}} $ $ -\\sigma\\sqrt{\\|{\\bf g}\\|_{2}^{2}\\nu-\\|\\nu{\\bf h}+{\\bf z}^{(1)}-% \\lambda^{(2)}\\|_{2}^{2}}+\\sum_{i=n-k+1}^{n}\\lambda_{i}^{(2)}\\tilde{{\\bf x}}_{i% }+\\nu r_{socp} $ </equation> <equation> $ \\mbox{subject to}\\nu\\geq 00\\leq\\lambda_{i}^{(2)}\\leq 2,1\\leq i% \\leq n. $ $ \\nu\\geq 0 $ $  0\\leq\\lambda_{i}^{(2)}\\leq 2,1\\leq i\\leq n. $ </equation> </equationgroup> Now we digress for a moment and consider the following optimization problem \\newline <equationgroup> <equation> $ \\min_{\\nu,\\lambda^{(2)},{\\bf q}_{1},{\\bf q}_{2}}-\\sigma{\\bf q}_{1% }+\\sum_{i=n-k+1}^{n}\\lambda_{i}^{(2)}\\tilde{{\\bf x}}_{i}+\\nu r_{socp} $ $ \\min_{\\nu,\\lambda^{(2)},{\\bf q}_{1},{\\bf q}_{2}} $ $ -\\sigma{\\bf q}_{1}+\\sum_{i=n-k+1}^{n}\\lambda_{i}^{(2)}\\tilde{{\\bf x% }}_{i}+\\nu r_{socp} $ </equation> <equation> $ \\mbox{subject to}\\|\\nu{\\bf h}+{\\bf z}^{(1)}-\\lambda^{(2)}\\|_{2}% \\leq{\\bf q}_{2}\\sqrt{{\\bf q}_{1}^{2}+{\\bf q}_{2}^{2}}\\leq\\|{\\bf g}\\|_{2}\\nu\\nu% \\geq 00\\leq\\lambda_{i}^{(2)}\\leq 2,1\\leq i\\leq n. $ $ \\|\\nu{\\bf h}+{\\bf z}^{(1)}-\\lambda^{(2)}\\|_{2}\\leq{\\bf q}_{2} $ $ \\sqrt{{\\bf q}_{1}^{2}+{\\bf q}_{2}^{2}}\\leq\\|{\\bf g}\\|_{2}\\nu $ $ \\nu\\geq 0 $ $  0\\leq\\lambda_{i}^{(2)}\\leq 2,1\\leq i\\leq n. $ </equation> </equationgroup> Let $ -\\xi_{prim}^{(1)}(\\sigma,{\\bf g},{\\bf", "h},\\tilde{{\\bf x}},r_{socp}) $ be the optimal value of its objective function.", "Let quadruplet $ \\hat{\\nu},\\widehat{\\lambda^{(2)}},\\hat{{\\bf q}_{1}},\\hat{{\\bf q}_{2}} $ be the solution of the above optimization problem.", "Then it must be \\newline <equation> $ \\|\\hat{\\nu}{\\bf h}+{\\bf z}^{(1)}-\\widehat{\\lambda^{(2)}}\\|_{2}=\\hat{{\\bf q}_{2}} $ </equation> and consequently \\newline <equationgroup> <equation> $ \\hat{{\\bf q}_{1}}=\\sqrt{\\|{\\bf g}\\|_{2}^{2}\\hat{\\nu}^{2}-\\|\\hat{% \\nu}{\\bf h}+{\\bf z}^{(1)}-\\widehat{\\lambda^{(2)}}\\|_{2}^{2}} $ $ \\hat{{\\bf q}_{1}} $ $ = $ $ \\sqrt{\\|{\\bf g}\\|_{2}^{2}\\hat{\\nu}^{2}-\\|\\hat{\\nu}{\\bf h}+{\\bf z}% ^{(1)}-\\widehat{\\lambda^{(2)}}\\|_{2}^{2}} $ </equation> <equation> $ -\\xi_{prim}^{(1)}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp}% )=-\\sigma\\sqrt{\\|{\\bf g}\\|_{2}^{2}\\hat{\\nu}^{2}-\\|\\hat{\\nu}{\\bf h}+{\\bf z}^{(1% )}-\\widehat{\\lambda^{(2)}}\\|_{2}^{2}}+\\sum_{i=n-k+1}^{n}\\widehat{\\lambda_{i}^{% (2)}}\\tilde{{\\bf x}}_{i}+\\hat{\\nu}r_{socp}. $ $ -\\xi_{prim}^{(1)}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp}) $ $ = $ $ -\\sigma\\sqrt{\\|{\\bf g}\\|_{2}^{2}\\hat{\\nu}^{2}-\\|\\hat{\\nu}{\\bf h}+% {\\bf z}^{(1)}-\\widehat{\\lambda^{(2)}}\\|_{2}^{2}}+\\sum_{i=n-k+1}^{n}\\widehat{% \\lambda_{i}^{(2)}}\\tilde{{\\bf x}}_{i}+\\hat{\\nu}r_{socp}. $ </equation> </equationgroup> The above claim is rather obvious but for the completeness we sketch the argument that supports it. Assume that $ \\|\\hat{\\nu}{\\bf h}+{\\bf z}^{(1)}-\\widehat{\\lambda^{(2)}}\\|_{2}<\\hat{{\\bf q}_{2}} $ .", "Then $ \\hat{{\\bf q}_{1}}<\\sqrt{\\|{\\bf g}\\|_{2}^{2}\\hat{\\nu}^{2}-\\|\\hat{\\nu}{\\bf h}+{% \\bf z}^{(1)}-\\widehat{\\lambda^{(2)}}\\|_{2}^{2}} $ , and $ -\\xi_{prim}^{(1)}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp}) $ would be larger then the expression on the right-hand side of ( [@ref:LABEL:eq:devuboptprimaldet] ).", "Now, since ( [@ref:LABEL:eq:devuboptq2det] ) and ( [@ref:LABEL:eq:devuboptprimaldet] ) hold one has that $ -\\xi_{prim}^{(1)}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp}) $ can be determined through the following equivalent to ( [@ref:LABEL:eq:devubprimaldet] ) \\newline <equationgroup> <equation> $ -\\xi_{prim}^{(1)}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp}% )=\\min_{\\nu,\\lambda^{(2)}}-\\sigma\\sqrt{\\|{\\bf g}\\|_{2}^{2}\\nu^{2}-\\|\\nu{\\bf h}% +{\\bf z}^{(1)}-\\lambda^{(2)}\\|_{2}^{2}}+\\sum_{i=n-k+1}^{n}\\lambda_{i}^{(2)}% \\tilde{{\\bf x}}_{i}+\\nu r_{socp} $ $ -\\xi_{prim}^{(1)}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp}% )=\\min_{\\nu,\\lambda^{(2)}} $ $ -\\sigma\\sqrt{\\|{\\bf g}\\|_{2}^{2}\\nu^{2}-\\|\\nu{\\bf h}+{\\bf z}^{(1)% }-\\lambda^{(2)}\\|_{2}^{2}}+\\sum_{i=n-k+1}^{n}\\lambda_{i}^{(2)}\\tilde{{\\bf x}}_% {i}+\\nu r_{socp} $ </equation> <equation> $ \\mbox{subject to}\\nu\\geq 00\\leq\\lambda_{i}^{(2)}\\leq 2,1\\leq i\\leq n $ $ \\nu\\geq 0 $ $  0\\leq\\lambda_{i}^{(2)}\\leq 2,1\\leq i\\leq n $ </equation> </equationgroup> After comparing ( [@ref:LABEL:eq:devublemmadet3] ) and ( [@ref:LABEL:eq:devubprimaldet1] ) we have \\newline <equation> $ -\\xi_{prim}^{(1)}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp})=-\\xi_{prim}% (\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp}).", "$ </equation> Now, let us write the Lagrange dual of the optimization problem in ( [@ref:LABEL:eq:devubprimaldet] ).", "Let $ d $ and $ \\gamma_{1} $ be Lagrangian variables such that \\newline <equationgroup> <equation> $ \\max_{d\\geq 0,\\gamma_{1}\\geq 0}\\min_{\\nu,\\lambda^{(2)},{\\bf q}_{1% },{\\bf q}_{2}}-\\sigma{\\bf q}_{1}+\\sum_{i=n-k+1}^{n}\\lambda_{i}^{(2)}\\tilde{{% \\bf x}}_{i}+d\\|\\nu{\\bf h}+{\\bf z}^{(1)}-\\lambda^{(2)}\\|_{2}-d{\\bf q}_{2}+% \\gamma_{1}\\sqrt{{\\bf q}_{1}^{2}+{\\bf q}_{2}^{2}}-\\gamma_{1}\\|{\\bf g}\\|_{2}\\nu $ $ \\max_{d\\geq", "0,\\gamma_{1}\\geq 0}\\min_{\\nu,\\lambda^{(2)},{\\bf q}_{1% },{\\bf q}_{2}} $ $ -\\sigma{\\bf q}_{1}+\\sum_{i=n-k+1}^{n}\\lambda_{i}^{(2)}\\tilde{{\\bf x% }}_{i}+d\\|\\nu{\\bf h}+{\\bf z}^{(1)}-\\lambda^{(2)}\\|_{2}-d{\\bf q}_{2}+\\gamma_{1}% \\sqrt{{\\bf q}_{1}^{2}+{\\bf q}_{2}^{2}}-\\gamma_{1}\\|{\\bf g}\\|_{2}\\nu $ </equation> <equation> $ \\mbox{subject to}\\nu\\geq 00\\leq\\lambda_{i}^{(2)}\\leq 2,1\\leq i% \\leq n. $ $ \\nu\\geq 0 $ $  0\\leq\\lambda_{i}^{(2)}\\leq 2,1\\leq i\\leq n. $ </equation> </equationgroup> After solving the inner minimization over $ {\\bf q}_{1},{\\bf q}_{2} $ and maximization over $ \\gamma_{1} $ one finally has \\newline <equationgroup> <equation> $ \\max_{d\\geq 0}\\min_{\\nu,\\lambda^{(2)}}-\\sqrt{\\sigma^{2}+d^{2}}\\|{% \\bf g}\\|_{2}\\nu+\\sum_{i=n-k+1}^{n}\\lambda_{i}^{(2)}\\tilde{{\\bf x}}_{i}+d\\|\\nu{% \\bf h}+{\\bf z}^{(1)}-\\lambda^{(2)}\\|_{2}+\\nu{\\bf r}_{socp} $ $ \\max_{d\\geq 0}\\min_{\\nu,\\lambda^{(2)}} $ $ -\\sqrt{\\sigma^{2}+d^{2}}\\|{\\bf g}\\|_{2}\\nu+\\sum_{i=n-k+1}^{n}% \\lambda_{i}^{(2)}\\tilde{{\\bf x}}_{i}+d\\|\\nu{\\bf h}+{\\bf z}^{(1)}-\\lambda^{(2)}% \\|_{2}+\\nu{\\bf r}_{socp} $ </equation> <equation> $ \\mbox{subject to}\\nu\\geq 00\\leq\\lambda_{i}^{(2)}\\leq 2,1\\leq i% \\leq n. $ $ \\nu\\geq 0 $ $  0\\leq\\lambda_{i}^{(2)}\\leq 2,1\\leq i\\leq n. $ </equation> </equationgroup> Let $ -\\xi_{prim}^{(2)}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}}) $ be the optimal value of the objective function in ( [@ref:LABEL:eq:devubprimaldet4] ).", "Since ( [@ref:LABEL:eq:devubprimaldet4] ) is the dual of ( [@ref:LABEL:eq:devubprimaldet] ) and since the strict duality obviously holds (the optimization problem in ( [@ref:LABEL:eq:devubprimaldet] ) is clearly convex) one has \\newline <equation> $ -\\xi_{prim}^{(2)}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp})=-\\xi_{prim}% ^{(1)}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp}).", "$ </equation> On the other hand the optimization problem in ( [@ref:LABEL:eq:devubprimaldet4] ) is the same as the one in ( [@ref:LABEL:eq:devubLagran12] ) and therefore \\newline <equation> $ -\\xi_{prim}^{(2)}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp})=-\\xi_{dual}% (\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp}).", "$ </equation> Connecting ( [@ref:LABEL:eq:devubdeteq1] ), ( [@ref:LABEL:eq:devubdualprimal41] ), and ( [@ref:LABEL:eq:devubdualprimal42] ) one finally has \\newline <equation> $ -\\xi_{dual}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp})=-\\xi_{prim}(% \\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp}) $ </equation> which is what is stated in ( [@ref:LABEL:eq:devublemmadet2] ).", "This concludes the proof.", "\u220e \\newline </proof> Let $ \\hat{d},\\widehat{\\nu_{up}},\\widehat{\\lambda_{up}^{(2)}} $ be the solution of ( [@ref:LABEL:eq:devubLagran11] ) (or alternatively let $ \\widehat{\\nu_{up}},\\widehat{\\lambda_{up}^{(2)}} $ be the solution of ( [@ref:LABEL:eq:devublemmadet1] ) or ( [@ref:LABEL:eq:devublemmadet3] )).", "Clearly, \\newline <equation> $ \\hat{d}=\\sigma\\frac{\\|\\widehat{\\nu_{up}}{\\bf h}+{\\bf z}^{(1)}-\\widehat{\\lambda% _{up}^{(2)}}\\|_{2}}{\\sqrt{\\|{\\bf g}\\|_{2}^{2}\\widehat{\\nu_{up}}^{2}-\\|\\widehat% {\\nu_{up}}{\\bf h}+{\\bf z}^{(1)}-\\widehat{\\lambda^{(2)}}\\|_{2}^{2}}}. $ </equation> As shown in Section [@ref:LABEL:sec:unsignedubzetaobj] all quantities of interest concentrate and one has \\newline <equation> $ E\\hat{d}\\doteq\\sigma\\frac{E\\|\\widehat{\\nu_{up}}{\\bf h}+{\\bf z}^{(1)}-\\widehat{% \\lambda_{up}^{(2)}}\\|_{2}}{\\sqrt{E\\|{\\bf g}\\|_{2}^{2}E\\widehat{\\nu_{up}}^{2}-E% \\|\\widehat{\\nu_{up}}{\\bf h}+{\\bf z}^{(1)}-\\widehat{\\lambda^{(2)}}\\|_{2}^{2}}}, $ </equation> where $ \\doteq $ indicates that the equality is not exact but can be made through the concentrations as close to it as needed.", "Now, set $ C_{{\\bf w}_{up}}=E\\hat{d} $ in ( [@ref:LABEL:eq:upperdefxi] ).", "Then a combination of ( [@ref:LABEL:eq:upperdefxi] ), ( [@ref:LABEL:eq:devubLagran11] ), and Lemma [@ref:LABEL:thm:devublemmadet] gives \\newline <equation> $ E\\xi_{up}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp},E\\hat{d})\\doteq E% \\max_{\\lambda^{(2)}\\in\\Lambda^{(2)},\\nu\\geq 0}(\\sqrt{(E\\hat{d})^{2}+\\sigma^{2}% }\\|{\\bf g}\\|_{2}\\nu-E\\hat{d}\\|\\nu{\\bf h}+{\\bf z}^{(1)}-\\lambda^{(2)})\\|_{2}-% \\sum_{i=n-k+1}^{n}\\lambda_{i}^{(2)}\\tilde{{\\bf x}}_{i}-\\nu r_{socp})\\\\ \\doteq E\\min_{d\\geq 0}\\max_{\\lambda^{(2)}\\in\\Lambda^{(2)},\\nu\\geq 0}(\\sqrt{d^{% 2}+\\sigma^{2}}\\|{\\bf g}\\|_{2}\\nu-d\\|\\nu{\\bf h}+{\\bf z}^{(1)}-\\lambda^{(2)})\\|_% {2}-\\sum_{i=n-k+1}^{n}\\lambda_{i}^{(2)}\\tilde{{\\bf x}}_{i}-\\nu r_{socp})=E\\xi_% {prim}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp}).", "$ </equation>", "Moreover, one then from ( [@ref:LABEL:eq:devublemmadet3] ) has \\newline <equation> $ -E\\xi_{prim}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp})\\doteq-\\sigma% \\sqrt{E\\|{\\bf g}\\|_{2}^{2}E\\widehat{\\nu_{up}}^{2}-E\\|\\widehat{\\nu_{up}}{\\bf h}% +{\\bf z}^{(1)}-\\widehat{\\lambda_{up}^{(2)}}\\|_{2}^{2}}+E(\\sum_{i=n-k+1}^{n}(% \\widehat{\\lambda_{up}^{(2)}})_{i}\\tilde{{\\bf x}}_{i})+E\\widehat{\\nu_{up}}r_{% socp}, $ </equation> where $ (\\widehat{\\lambda_{up}^{(2)}})_{i} $ is the $ i $ -th component of $ \\widehat{\\lambda_{up}^{(2)}} $ .", "\\newline Let $ \\widehat{{\\bf w}_{up}} $ be the solution of ( [@ref:LABEL:eq:upperobjlassol11] ).", "Then $ E\\|\\widehat{{\\bf w}_{up}}\\|_{2}=C_{{\\bf w}_{up}}=E\\hat{d} $ and with overwhelming probability $ f_{obj}\\leq f_{obj}^{(upper)}<E\\xi_{prim}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x% }},r_{socp})+\\epsilon_{lip}|E\\xi_{prim}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}}% ,r_{socp})| $ for an arbitrarily small positive constant $ \\epsilon_{lip} $ ( $ E\\hat{d} $ is of course as defined in ( [@ref:LABEL:eq:upperdefoptd1] )).", "In the following section we will show that with overwhelming probability $ f_{obj}\\geq f_{obj}^{(lower)}>E\\xi_{prim}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x% }},r_{socp})-\\epsilon_{lip}|E\\xi_{prim}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}}% ,r_{socp})| $ which will establish $ E\\xi_{prim}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp}) $ as the concentrating point of $ f_{obj} $ .", "Moreover, we will show that if $ {\\bf w}_{socp} $ is such that $ E\\|{\\bf w}_{socp}\\|_{2} $ substantially deviates from $ E\\|\\widehat{{\\bf w}_{up}}\\|_{2} $ then $ f_{obj} $ would substantially deviate from $ E\\xi_{prim}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp}) $ which will establish $ E\\|\\widehat{{\\bf w}_{up}}\\|_{2}=C_{{\\bf w}_{up}}=E\\hat{d} $ as the concentrating point of $ \\|{\\bf w}_{socp}\\|_{2} $ .", "\\newline </subsubsection> <subsubsection> <title> 2.3.3 Specializing f \u2062 o b j \u2019s lower-bound </title> In this section we finally determine the concentrating point of $ f_{obj} $ .", "To that end let us assume \\newline <equation> $ f_{obj}^{(lower)}\\leq\\sigma\\sqrt{E\\|{\\bf g}\\|_{2}^{2}E\\widehat{\\nu_{up}}^{2}-E% \\|\\widehat{\\nu_{up}}{\\bf h}+{\\bf z}^{(1)}-\\widehat{\\lambda_{up}^{(2)}}\\|_{2}^{% 2}}-E(\\sum_{i=n-k+1}^{n}(\\widehat{\\lambda_{up}^{(2)}})_{i}\\tilde{{\\bf x}}_{i})% -E\\widehat{\\nu_{up}}(1+\\epsilon_{r_{socp}})r_{socp}, $ </equation> where $ \\epsilon_{r_{socp}}>0 $ is an arbitrarily small but fixed constant.", "From ( [@ref:LABEL:eq:Lagran12] ) one then has \\newline <equationgroup> <equation> $ \\xi_{ov}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp})=\\max_{% \\nu,\\lambda^{(2)}}\\sigma\\sqrt{\\|{\\bf g}\\|_{2}^{2}-\\|{\\bf h}+\\nu{\\bf z}^{(1)}-% \\lambda^{(2)}\\|_{2}^{2}}-\\sum_{i=n-k+1}^{n}\\lambda_{i}^{(2)}\\tilde{{\\bf x}}_{i% }-\\nu f_{obj}^{(lower)} $ $ \\xi_{ov}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp})=\\max_{% \\nu,\\lambda^{(2)}} $ $ \\sigma\\sqrt{\\|{\\bf g}\\|_{2}^{2}-\\|{\\bf h}+\\nu{\\bf z}^{(1)}-% \\lambda^{(2)}\\|_{2}^{2}}-\\sum_{i=n-k+1}^{n}\\lambda_{i}^{(2)}\\tilde{{\\bf x}}_{i% }-\\nu f_{obj}^{(lower)} $ </equation> <equation> $ \\mbox{subject to}\\nu\\geq 00\\leq\\lambda_{i}^{(2)}\\leq 2\\nu,1\\leq i% \\leq n. $ $ \\nu\\geq 0 $ $  0\\leq\\lambda_{i}^{(2)}\\leq 2\\nu,1\\leq i\\leq n. $ </equation> </equationgroup> Let us choose $ \\nu=\\frac{1}{\\widehat{\\nu_{up}}} $ and $ \\lambda^{(2)}=\\frac{\\widehat{\\lambda_{up}^{(2)}}}{\\widehat{\\nu_{up}}} $ in the above optimization.", "Since this choice is suboptimal and since all the quantities concentrate ( [@ref:LABEL:eq:specsetfobjlow] ) would imply \\newline <equation> $ E\\xi_{ov}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp})\\geq(1+\\epsilon_{r_{% socp}})r_{socp}. $ </equation> On the other hand based on a combination of the arguments from Section [@ref:LABEL:sec:connectl1] and ( [@ref:LABEL:eq:specanal1] ) one would also have \\newline <equation> $ E\\xi(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},f_{obj}^{(lower)})\\doteq E\\xi_{ov}% (\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}})\\geq(1+\\epsilon_{r_{socp}})r_{socp}. $ </equation> Finally a combination of ( [@ref:LABEL:eq:specanal2] ) and Lemma [@ref:LABEL:thm:lowerbound] would give \\newline <equation> $ P(\\zeta_{obj}\\geq(1+\\epsilon_{r_{socp}})(1-\\epsilon_{lip})r_{socp}-\\epsilon_{1% }^{({\\bf h})}\\sqrt{n}-\\epsilon_{1}^{(g)}\\sqrt{n})\\geq(1-e^{-\\epsilon_{lower}n}% )(1-e^{-\\epsilon_{C_{{\\bf w}}}n}), $ </equation> where for any arbitrarily small but fixed $ \\epsilon_{r_{socp}} $ one can choose much smaller $ \\epsilon_{lip},\\epsilon_{1}^{(h)},\\epsilon_{1}^{(g)} $ and make their presence in the above inequality negligible.", "On the other hand, in a statistical sense, ( [@ref:LABEL:eq:specanal3] ) would contradict the setup of ( [@ref:LABEL:eq:socp1] ).", "Therefore our assumption that $ f_{obj}^{(lower)} $ satisfies ( [@ref:LABEL:eq:specsetfobjlow] ) is with overwhelming probability unsustainable.", "A combination of ( [@ref:LABEL:eq:specanal3] ), ( [@ref:LABEL:eq:devubfinal] ), ( [@ref:LABEL:eq:devubxiprimopt] ), results from Lemma [@ref:LABEL:thm:upperbound] , and the discussion right after Lemma [@ref:LABEL:thm:lowerbound] imply that $ f_{obj} $ concentrates around $ E\\xi_{prim}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp}) $ .", "\\newline </subsubsection> <subsubsection> <title> 2.3.4 \u2225 w \u2062 s o c p \u2225 2 \u2019s deviation from \u2225 ^ w \u2062 u p \u2225 2 </title> In this subsection we will show that $ \\|{\\bf w}_{socp}\\|_{2} $ can not deviate substantially from $ \\|\\widehat{{\\bf w}_{up}}\\|_{2} $ without substantially affecting the value of the lower bound on the objective in ( [@ref:LABEL:eq:socp1] ) that is derived in Section [@ref:LABEL:sec:unsignedlbzetaobj] .", "To that end let us assume that there is a $ {\\bf w}_{off} $ that is the solution of the SOCP from ( [@ref:LABEL:eq:socp1] ) (or to be slightly more precise that is such that $ {\\bf x}_{socp}=\\tilde{{\\bf x}}+{\\bf w}_{off} $ , where obviously $ {\\bf x}_{socp} $ is the solution of ( [@ref:LABEL:eq:socp1] ) or ( [@ref:LABEL:eq:socp] )).", "Further, let $ |\\|{\\bf w}_{off}\\|_{2}-\\|\\widehat{{\\bf w}_{up}}\\|_{2}|\\geq\\epsilon_{{\\bf w}_{% up}}\\|\\widehat{{\\bf w}_{up}}\\|_{2} $ , where $ \\epsilon_{{\\bf w}_{up}} $ is an arbitrarily small constant.", "\\newline One can then proceed by repeating the same line of thought as in Section [@ref:LABEL:sec:unsignedlbzetaobj] .", "The only difference will be that now $ C_{\\bf w}=\\|{\\bf w}_{off}\\|_{2} $ and consequently in the definition of $ S_{\\bf w}(\\sigma,\\tilde{{\\bf x}},C_{\\bf w},f_{obj}^{(lower)}) $ , $ \\|{\\bf w}\\|_{2}\\leq C_{\\bf w} $ changes to $ \\|{\\bf w}\\|_{2}=C_{\\bf w}=\\|{\\bf w}_{off}\\|_{2} $ .", "This difference will not of course affect the concept presented in Section [@ref:LABEL:sec:unsignedlbzetaobj] .", "The only real consequence will be the change of ( [@ref:LABEL:eq:defxi2] ).", "Adapted to the new scenario ( [@ref:LABEL:eq:defxi2] ) becomes \\newline <equationgroup> <equation> $ \\xi_{off}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp},\\|{\\bf w% }_{off}\\|_{2})=\\min_{{\\bf w}}\\sqrt{\\|{\\bf w}_{off}\\|_{2}^{2}+\\sigma^{2}}\\|{\\bf g% }\\|_{2}+\\sum_{i=1}^{n}{\\bf h}_{i}{\\bf w}_{i} $ $ \\xi_{off}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp},\\|{\\bf w% }_{off}\\|_{2})=\\min_{{\\bf w}} $ $ \\sqrt{\\|{\\bf w}_{off}\\|_{2}^{2}+\\sigma^{2}}\\|{\\bf g}\\|_{2}+\\sum_{% i=1}^{n}{\\bf h}_{i}{\\bf w}_{i} $ </equation> <equation> $ \\mbox{subject to}\\|\\tilde{{\\bf x}}+{\\bf w}\\|_{2}-\\|\\tilde{{\\bf x}% }\\|_{1}\\leq E\\xi_{prim}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp})\\sqrt{% \\|{\\bf w}\\|_{2}^{2}+\\sigma^{2}}\\leq\\sqrt{\\|{\\bf w}_{off}\\|_{2}^{2}+\\sigma^{2}}. $ $ \\|\\tilde{{\\bf x}}+{\\bf w}\\|_{2}-\\|\\tilde{{\\bf x}}\\|_{1}\\leq E\\xi_% {prim}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp}) $ $ \\sqrt{\\|{\\bf w}\\|_{2}^{2}+\\sigma^{2}}\\leq\\sqrt{\\|{\\bf w}_{off}\\|_% {2}^{2}+\\sigma^{2}}. $ </equation> </equationgroup> One can then proceed further with solving the Lagrangian to obtain (this is pretty much analogous to what was done in Section 3.3.2 in [@bib:StojnicGenLasso10] ; the only difference is a subtle change in the first constraint) \\newline <equation> $ \\xi_{off}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp},\\|{\\bf w}_{off}\\|_{2% })=\\max_{\\lambda^{(2)}\\in\\Lambda_{2\\nu}^{(2)},\\nu\\geq 0}(\\sqrt{\\|{\\bf w}_{off}% \\|_{2}^{2}+\\sigma^{2}}\\|{\\bf g}\\|_{2}-\\|{\\bf w}_{off}\\|_{2}\\|{\\bf h}+\\nu{\\bf z% }^{(1)}-\\lambda^{(2)})\\|_{2}\\\\ -\\sum_{i=n-k+1}^{n}\\lambda_{i}^{(2)}\\tilde{{\\bf x}}_{i}-\\nu E\\xi_{prim}(\\sigma% ,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp})), $ </equation> where $ \\Lambda_{2\\nu}^{(2)}=\\{\\lambda^{(2)}|0\\leq\\lambda_{i}^{(2)}\\leq 2\\nu,1\\leq i% \\leq n\\} $ .", "Using the probabilistic arguments from Section [@ref:LABEL:sec:unsignedlbzetaobj] one then from Lemma [@ref:LABEL:thm:lowerbound] has that if $ {\\bf w}_{off} $ is the solution of ( [@ref:LABEL:eq:socp1] ) then the objective value of ( [@ref:LABEL:eq:lbobjlassol13] ) (or the objective value of ( [@ref:LABEL:eq:lbobjlassol1] )) is with overwhelming probability lower bounded by $ (1-\\epsilon_{lip})E\\xi_{off}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp},% \\|{\\bf w}_{off}\\|_{2}) $ ( $ \\xi_{off}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp},\\|{\\bf w}_{off}\\|_{2}) $ is structurally the same as $ \\xi_{up}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp},C_{{\\bf w}_{up}}) $ from ( [@ref:LABEL:eq:upperdefxi] ) and therefore easily concentrates based on Lemma [@ref:LABEL:thm:upperlipsch1] ).", "We will now consider in parallel the following lower bound on the objective value of ( [@ref:LABEL:eq:lbobjlassol13] ) that is presented in ( [@ref:LABEL:eq:Lagran12] ).", "\\newline <equation> $ \\xi_{ov}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp})=\\max_{\\nu\\geq 0,% \\lambda^{(2)}\\in\\Lambda_{2\\nu}^{(2)}}\\sigma\\sqrt{\\|{\\bf g}\\|_{2}^{2}-\\|{\\bf h}% +\\nu{\\bf z}^{(1)}-\\lambda^{(2)}\\|_{2}^{2}}-\\sum_{i=n-k+1}^{n}\\lambda_{i}^{(2)}% \\tilde{{\\bf x}}_{i}-\\nu E\\xi_{prim}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{% socp})).", "$ </equation> Let $ \\hat{\\nu} $ and $ \\widehat{\\lambda^{(2)}} $ be the solution of ( [@ref:LABEL:eq:matchoptlower] ) and let \\newline <equation> $ \\xi_{help}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp},\\|{\\bf w}_{off}\\|_{% 2})=\\sqrt{\\|{\\bf w}_{off}\\|_{2}^{2}+\\sigma^{2}}\\|{\\bf g}\\|_{2}-\\|{\\bf w}_{off}% \\|_{2}\\|{\\bf h}+\\hat{\\nu}{\\bf z}^{(1)}-\\widehat{\\lambda^{(2)}}\\|_{2}\\\\ -\\sum_{i=n-k+1}^{n}\\widehat{\\lambda_{i}^{(2)}}\\tilde{{\\bf x}}_{i}-\\nu E\\xi_{% prim}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp})).", "$ </equation> Repeating the arguments presented between $ (115) $ and $ (122) $ in [@bib:StojnicGenLasso10] one obtains the following analogue to $ (122) $ from [@bib:StojnicGenLasso10] \\newline <equation> $ E\\xi_{off}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp},\\|{\\bf w}_{off}\\|_{% 2})-E\\xi_{ov}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp})\\geq\\frac{% \\epsilon_{{\\bf w}_{up}}^{2}}{2(1+\\epsilon_{{\\bf w}_{up}})}E\\xi_{E}, $ </equation> where $ \\xi_{E}=\\sigma\\sqrt{(E\\|{\\bf g}\\|_{2})^{2}-(E\\|{\\bf h}+\\hat{\\nu}{\\bf z}^{(1)}-% \\widehat{\\lambda^{(2)}}\\|_{2})^{2}} $ .", "As shown in Section [@ref:LABEL:sec:devlb] if one has that $ f_{obj}^{(lower)}=E\\xi_{prim}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp}) $ (which is the case in ( [@ref:LABEL:eq:matchdefxi] )) then $ E\\xi_{ov}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp})\\geq r_{socp} $ .", "Knowing that, ( [@ref:LABEL:eq:matchdiff6] ) basically shows that if $ \\|{\\bf w}_{socp}\\|_{2} $ were to deviate from $ \\|\\widehat{{\\bf w}_{up}}\\|_{2} $ the optimal value of the objective in ( [@ref:LABEL:eq:lbobjlassol13] ) would concentrate around point that is non-trivially higher than $ r_{socp} $ (note that $ E\\xi_{E}\\sim\\sqrt{n} $ ).", "This again contradicts the setup of ( [@ref:LABEL:eq:socp1] ) and makes our deviating assumption unsustainable with overwhelming probability.", "Hence $ {\\bf w}_{socp} $ is such that $ \\|{\\bf w}_{socp}\\|_{2} $ concentrates around $ E\\|\\widehat{{\\bf w}_{up}}\\|_{2} $ with overwhelming probability.", "\\newline </subsubsection> </subsection> <subsection> <title> 2.4 Connecting all pieces </title> In this section we connect all of the above.", "We will summarize the results obtained so far in the following theorem.", "\\newline <theorem> Theorem 1 . Let $ {\\bf v} $ be an $ n\\times 1 $ vector of i.i.d.", "zero-mean variance $ \\sigma^{2} $ Gaussian random variables and let $ A $ be an $ m\\times n $ matrix of i.i.d. standard normal random variables.", "Further, let $ {\\bf g} $ and $ {\\bf h} $ be $ m\\times 1 $ and $ n\\times 1 $ vectors of i.i.d. standard normals, respectively.", "Consider a $ k $ -sparse $ \\tilde{{\\bf x}} $ defined in ( [@ref:LABEL:eq:xtildedef] ) and a $ {\\bf y} $ defined in ( [@ref:LABEL:eq:systemnoise] ) for $ {\\bf x}=\\tilde{{\\bf x}} $ . Let the solution of ( [@ref:LABEL:eq:socp] ) be $ {\\bf x}_{socp} $ and let the so-called error vector of the SOCP from ( [@ref:LABEL:eq:socp] ) be $ {\\bf w}_{socp}={\\bf x}_{socp}-\\tilde{{\\bf x}} $ . Let $ r_{socp} $ in ( [@ref:LABEL:eq:socp] ) be a positive scalar.", "Let $ n $ be large and let constants $ \\alpha=\\frac{m}{n} $ and $ \\beta_{w}=\\frac{k}{n} $ be below the fundamental characterization ( [@ref:LABEL:eq:fundl1] ).", "Consider the following optimization problem: \\newline <equationgroup> <equation> $ \\xi_{prim}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp})=\\max_% {\\nu,\\lambda^{(2)}}\\sigma\\sqrt{\\|{\\bf g}\\|_{2}^{2}\\nu^{2}-\\|\\nu{\\bf h}+{\\bf z}% ^{(1)}-\\lambda^{(2)}\\|_{2}^{2}}-\\sum_{i=n-k+1}^{n}\\lambda_{i}^{(2)}\\tilde{{\\bf x% }}_{i}-\\nu r_{socp} $ $ \\xi_{prim}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp})=\\max_% {\\nu,\\lambda^{(2)}} $ $ \\sigma\\sqrt{\\|{\\bf g}\\|_{2}^{2}\\nu^{2}-\\|\\nu{\\bf h}+{\\bf z}^{(1)}% -\\lambda^{(2)}\\|_{2}^{2}}-\\sum_{i=n-k+1}^{n}\\lambda_{i}^{(2)}\\tilde{{\\bf x}}_{% i}-\\nu r_{socp} $ </equation> <equation> $ \\mbox{subject to}\\nu\\geq 00\\leq\\lambda_{i}^{(2)}\\leq 2,1\\leq i% \\leq n. $ $ \\nu\\geq 0 $ $  0\\leq\\lambda_{i}^{(2)}\\leq 2,1\\leq i\\leq n. $ </equation> </equationgroup> Let $ \\widehat{\\nu_{up}} $ and $ \\widehat{\\lambda_{up}^{(2)}} $ be the solution of ( [@ref:LABEL:eq:mainlasso1] ).", "Set \\newline <equation> $ \\|\\widehat{{\\bf w}_{up}}\\|_{2}=\\sigma\\frac{\\|\\widehat{\\nu_{up}}{\\bf h}+{\\bf z}% ^{(1)}-\\widehat{\\lambda_{up}^{(2)}}\\|_{2}}{\\sqrt{\\|{\\bf g}\\|_{2}^{2}\\widehat{% \\nu_{up}}^{2}-\\|\\widehat{\\nu_{up}}{\\bf h}+{\\bf z}^{(1)}-\\widehat{\\lambda_{up}^% {(2)}}\\|_{2}^{2}}}. $ </equation> Then: \\newline <equation> $ P(\\|\\tilde{{\\bf x}}+{\\bf w}_{socp}\\|_{1}-\\|\\tilde{{\\bf x}}\\|_{1}\\in(E\\xi_{prim% }(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp}))-\\epsilon_{1}^{(socp)}|E\\xi% _{prim}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp}))|,\\\\ E\\xi_{prim}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp}))+\\epsilon_{1}^{(% socp)}|E\\xi_{prim}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{socp}))|)=1-e^{-% \\epsilon_{2}^{(socp)}n} $ </equation> and \\newline <equation> $ P((1-\\epsilon_{1}^{(socp)})E\\|\\widehat{{\\bf w}_{up}}\\|_{2}\\leq\\|{\\bf w}_{socp}% \\|_{2}\\leq(1+\\epsilon_{1}^{(socp)})E\\|\\widehat{{\\bf w}_{up}}\\|_{2})=1-e^{-% \\epsilon_{2}^{(socp)}n}, $ </equation> where $ \\epsilon_{1}^{(socp)}>0 $ is an arbitrarily small constant and $ \\epsilon_{2}^{(socp)} $ is a constant dependent on $ \\epsilon_{1}^{(socp)} $ and $ \\sigma $ but independent of $ n $ .", "\\newline </theorem> <proof> Proof.", "Follows from the above discussion and a combination of ( [@ref:LABEL:eq:Lagran12] ), discussions in Section [@ref:LABEL:sec:connectl1] and those after ( [@ref:LABEL:eq:specanal3] ) and ( [@ref:LABEL:eq:matchdiff6] ), and Lemmas [@ref:LABEL:thm:upperbound] and [@ref:LABEL:thm:lowerbound] .", "\u220e \\newline </proof> The above result is fairly powerful.", "In a sense it is for the SOCP algorithms what Theorem 2 from [@bib:StojnicGenLasso10] is for the LASSO algorithms.", "It enables one to compute many quantities that could be of interest in characterizing performance of SOCP algorithms.", "For example, one can precisely estimate the norm of the error vector for the SOCP and can do so for any given $ k $ -sparse vector $ \\tilde{{\\bf x}} $ .", "Furthermore, all of it is done through a transformation of the original SOCP from ( [@ref:LABEL:eq:socp] ) to a much simpler optimization program ( [@ref:LABEL:eq:mainlasso1] ).", "While many quantities of interest in SOCP recovery can be computed through the mechanism presented above, below we focus only on a couple of quantities that relate to what we will call SOCP\u2019s {generic} performance scenario.", "Computation of all other quantities that we consider are of interest in generic or other type of performance scenarios will be presented in a series of forthcoming papers.", "\\newline <subsubsection> <title> 2.4.1 SOCP\u2019s generic performance </title> The results presented in the above theorem are rather general and can be used to analyze pretty much any possible scenario where SOCP algorithms can be applied.", "Here we will focus on the so-called \u201cworst-case\u201d scenario or as we will refer to it \u201cgeneric performance\u201d scenario.", "We will consider a simplification of ( [@ref:LABEL:eq:mainlasso1] ) which, among other things, enables one to find a particular \u201cgeneric\u201d choice of $ r_{socp} $ for which $ E\\|\\widehat{{\\bf w}_{up}}\\|_{2} $ from Theorem [@ref:LABEL:thm:mainlasso] can be upper-bounded over set of all $ \\tilde{{\\bf x}} $ \u2019s. Let us now assume that all nonzero components of $ \\tilde{{\\bf x}} $ in ( [@ref:LABEL:eq:systemnoise] ) are infinite.", "Then the simplification that we will consider will be ( [@ref:LABEL:eq:mainlasso1] ) with such an $ \\tilde{{\\bf x}} $ .", "In such a scenario the optimization problem from ( [@ref:LABEL:eq:mainlasso1] ) clearly becomes \\newline <equationgroup> <equation> $ \\xi_{prim}^{(gen)}(\\sigma,{\\bf g},{\\bf h},r_{socp})=\\max_{\\nu,% \\lambda^{(2)}}\\sigma\\sqrt{\\|{\\bf g}\\|_{2}^{2}\\nu^{2}-\\|\\nu{\\bf h}+{\\bf z}^{(1)% }-\\lambda^{(2)}\\|_{2}^{2}}-\\nu r_{socp} $ $ \\xi_{prim}^{(gen)}(\\sigma,{\\bf g},{\\bf h},r_{socp})=\\max_{\\nu,% \\lambda^{(2)}} $ $ \\sigma\\sqrt{\\|{\\bf g}\\|_{2}^{2}\\nu^{2}-\\|\\nu{\\bf h}+{\\bf z}^{(1)}% -\\lambda^{(2)}\\|_{2}^{2}}-\\nu r_{socp} $ </equation> <equation> $ \\mbox{subject to}\\nu\\geq 00\\leq\\lambda_{i}^{(2)}=0,n-k+1\\leq i% \\leq n0\\leq\\lambda_{i}^{(2)}\\leq 2,1\\leq i\\leq n-k.", "$ $ \\nu\\geq 0 $ $  0\\leq\\lambda_{i}^{(2)}=0,n-k+1\\leq i\\leq n $ $  0\\leq\\lambda_{i}^{(2)}\\leq 2,1\\leq i\\leq n-k.", "$ </equation> </equationgroup> Obviously, $ \\xi_{prim}^{(gen)}(\\sigma,{\\bf g},{\\bf h},r_{socp})\\leq\\xi_{prim}(\\sigma,{\\bf g% },{\\bf h},\\tilde{{\\bf x}},r_{socp}) $ .", "Then the following {generic} equivalent to Theorem [@ref:LABEL:thm:mainlasso] can be established.", "\\newline <theorem> Theorem 2 .", "Assume the setup of Theorem [@ref:LABEL:thm:mainlasso] .", "Consider the following optimization problem: \\newline <equationgroup> <equation> $ \\xi_{prim}^{(gen)}(\\sigma,{\\bf g},{\\bf h},r_{socp})=\\max_{\\nu,% \\lambda^{(2)}}\\sigma\\sqrt{\\|{\\bf g}\\|_{2}^{2}\\nu^{2}-\\|\\nu{\\bf h}+{\\bf z}^{(1)% }-\\lambda^{(2)}\\|_{2}^{2}}-\\nu r_{socp} $ $ \\xi_{prim}^{(gen)}(\\sigma,{\\bf g},{\\bf h},r_{socp})=\\max_{\\nu,% \\lambda^{(2)}} $ $ \\sigma\\sqrt{\\|{\\bf g}\\|_{2}^{2}\\nu^{2}-\\|\\nu{\\bf h}+{\\bf z}^{(1)}% -\\lambda^{(2)}\\|_{2}^{2}}-\\nu r_{socp} $ </equation> <equation> $ \\mbox{subject to}\\nu\\geq 0\\lambda_{i}^{(2)}=0,n-k+1\\leq i\\leq n0% \\leq\\lambda_{i}^{(2)}\\leq 2,1\\leq i\\leq n-k.", "$ $ \\nu\\geq 0 $ $ \\lambda_{i}^{(2)}=0,n-k+1\\leq i\\leq n $ $  0\\leq\\lambda_{i}^{(2)}\\leq 2,1\\leq i\\leq n-k.", "$ </equation> </equationgroup> Let $ \\nu_{gen} $ and $ \\lambda^{(gen)} $ be the solution of ( [@ref:LABEL:eq:genlasso4] ).", "Set \\newline <equation> $ \\|{\\bf w}_{gen}\\|_{2}=\\sigma\\frac{\\|\\nu_{gen}{\\bf h}+{\\bf z}^{(1)}-\\lambda^{(% gen)}\\|_{2}}{\\sqrt{\\|{\\bf g}\\|_{2}^{2}\\nu_{gen}^{2}-\\|\\nu_{gen}{\\bf h}+{\\bf z}% ^{(1)}-\\lambda^{(gen)}\\|_{2}^{2}}}. $ </equation> Then: \\newline <equation> $ P(\\min_{\\tilde{{\\bf x}}}(\\xi_{prim}(\\sigma,{\\bf g},{\\bf h},\\tilde{{\\bf x}},r_{% socp}))\\in(E\\xi_{prim}^{(gen)}(\\sigma,{\\bf g},{\\bf h},r_{socp}))-\\epsilon_{1}^% {(socp)}|E\\xi_{prim}^{(gen)}(\\sigma,{\\bf g},{\\bf h},r_{socp}))|,\\\\ E\\xi_{prim}^{(gen)}(\\sigma,{\\bf g},{\\bf h},r_{socp}))+\\epsilon_{1}^{(socp)}|E% \\xi_{prim}^{(gen)}(\\sigma,{\\bf g},{\\bf h},r_{socp}))|))=1-e^{-\\epsilon_{2}^{(% socp)}n} $ </equation> <equation> $ P(\\exists{\\bf w}_{socp}|\\|{\\bf w}_{socp}\\|_{2}\\in((1-\\epsilon_{1}^{(socp)})E\\|% {\\bf w}_{gen}\\|_{2},(1+\\epsilon_{1}^{(socp)})E\\|{\\bf w}_{gen}\\|_{2}))\\geq 1-e^% {-\\epsilon_{2}^{(socp)}n}, $ </equation> where $ \\epsilon_{1}^{(socp)}>0 $ is an arbitrarily small constant and $ \\epsilon_{2}^{(socp)} $ is a constant dependent on $ \\epsilon_{1}^{(socp)} $ and $ \\sigma $ but independent of $ n $ .", "\\newline </theorem> <proof> Proof.", "Follows from the above discussion and Theorem [@ref:LABEL:thm:mainlasso] .", "\u220e \\newline </proof> </subsubsection> <subsubsection> <title> 2.4.2 Optimal r \u2062 s o c p </title> In this section we design a particular choice of $ r_{socp} $ that enables favorable performance of ( [@ref:LABEL:eq:socp] ) as far as the norm-2 of the error vector is concerned (of course, the norm-2 of the error vector is not the only possible measure of performance of ( [@ref:LABEL:eq:socp] )).", "To that end let us slightly change the objective of ( [@ref:LABEL:eq:genlasso4] ) in the following way \\newline <equationgroup> <equation> $ \\xi_{prim}^{(gen)}(\\sigma,{\\bf g},{\\bf h},r_{socp})=\\max_{\\nu,% \\lambda^{(2)}}\\frac{1}{\\nu}(\\sigma\\sqrt{\\|{\\bf g}\\|_{2}^{2}-\\|{\\bf h}+\\nu{\\bf z% }^{(1)}-\\lambda^{(2)}\\|_{2}^{2}}-r_{socp}) $ $ \\xi_{prim}^{(gen)}(\\sigma,{\\bf g},{\\bf h},r_{socp})=\\max_{\\nu,% \\lambda^{(2)}} $ $ \\frac{1}{\\nu}(\\sigma\\sqrt{\\|{\\bf g}\\|_{2}^{2}-\\|{\\bf h}+\\nu{\\bf z% }^{(1)}-\\lambda^{(2)}\\|_{2}^{2}}-r_{socp}) $ </equation> <equation> $ \\mbox{subject to}\\nu\\geq 0\\lambda_{i}^{(2)}=0,n-k+1\\leq i\\leq n0% \\leq\\lambda_{i}^{(2)}\\leq 2\\nu,1\\leq i\\leq n-k.", "$ $ \\nu\\geq 0 $ $ \\lambda_{i}^{(2)}=0,n-k+1\\leq i\\leq n $ $  0\\leq\\lambda_{i}^{(2)}\\leq 2\\nu,1\\leq i\\leq n-k.", "$ </equation> </equationgroup> Repeating the arguments between ( [@ref:LABEL:eq:matchl1] ) and ( [@ref:LABEL:eq:matchl11] ) one has that the following is equivalent to ( [@ref:LABEL:eq:genlasso4opt] ) \\newline <equationgroup> <equation> $ \\xi_{prim}^{(gen)}(\\sigma,{\\bf g},{\\bf h},r_{socp})=\\max_{\\nu,% \\lambda^{(2)}}\\frac{1}{\\nu}(\\sigma\\sqrt{\\|{\\bf g}\\|_{2}^{2}-\\|\\bar{{\\bf h}}-% \\nu{\\bf z}^{(2)}+\\lambda^{(2)}\\|_{2}^{2}}-r_{socp}) $ $ \\xi_{prim}^{(gen)}(\\sigma,{\\bf g},{\\bf h},r_{socp})=\\max_{\\nu,% \\lambda^{(2)}} $ $ \\frac{1}{\\nu}(\\sigma\\sqrt{\\|{\\bf g}\\|_{2}^{2}-\\|\\bar{{\\bf h}}-\\nu% {\\bf z}^{(2)}+\\lambda^{(2)}\\|_{2}^{2}}-r_{socp}) $ </equation> <equation> $ \\mbox{subject to}\\nu\\geq 0\\lambda_{i}^{(2)}=0,n-k+1\\leq i\\leq n0% \\leq\\lambda_{i}^{(2)}\\leq\\nu,1\\leq i\\leq n-k.", "$ $ \\nu\\geq 0 $ $ \\lambda_{i}^{(2)}=0,n-k+1\\leq i\\leq n $ $  0\\leq\\lambda_{i}^{(2)}\\leq\\nu,1\\leq i\\leq n-k.", "$ </equation> </equationgroup> Set \\newline <equation> $ r_{socp}^{(opt)}=\\sigma\\sqrt{(E\\|{\\bf g}\\|_{2})^{2}-E(\\|\\bar{{\\bf h}}-\\nu_{% \\ell_{1}}{\\bf z}^{(2)}+\\lambda^{(\\ell_{1})}\\|_{2})^{2}}, $ </equation> where $ \\nu_{\\ell_{1}} $ and $ \\lambda^{(\\ell_{1})} $ are as defined in Section [@ref:LABEL:sec:connectl1] .", "Clearly, \\newline <equation> $ (\\nu_{\\ell_{1}},\\lambda^{(\\ell_{1})})=\\mbox{arg}\\max_{\\nu\\geq 0,\\lambda^{(2)}% \\in\\Lambda_{\\nu}^{(2,gen)}}\\sqrt{\\|{\\bf g}\\|_{2}^{2}-\\|\\bar{{\\bf h}}-\\nu{\\bf z% }^{(2)}+\\lambda^{(2)}\\|_{2}^{2}}, $ </equation> where $ \\Lambda_{\\nu}^{(2,gen)}=\\{\\lambda^{(2)}|0\\leq\\lambda_{i}^{(2)}\\leq\\nu,1\\leq i% \\leq n-k,\\lambda_{i}^{(2)}=0,n-k+1\\leq i\\leq n\\} $ .", "Using further the arguments from Section [@ref:LABEL:sec:connectl1] we have \\newline <equation> $ r_{socp}^{(opt)}=\\sigma\\sqrt{(\\alpha-\\alpha_{w})n}, $ </equation> where $ \\alpha_{w} $ is as defined in the fundamental characterization ( [@ref:LABEL:eq:fundl1] ).", "Let $ {\\bf w}_{gen}^{(opt)} $ be $ {\\bf w}_{gen} $ in Theorem [@ref:LABEL:thm:genlasso] obtained for $ r_{socp}=r_{socp}^{(opt)} $ .", "Then \\newline <equation> $ E\\|{\\bf w}_{gen}^{(opt)}\\|_{2}=\\sigma\\frac{E\\|\\bar{{\\bf h}}-\\nu_{\\ell_{1}}{\\bf z% }^{(2)}+\\lambda^{(\\ell_{1})}\\|_{2}}{\\sqrt{(E\\|{\\bf g}\\|_{2})^{2}-(E\\|\\bar{{\\bf h% }}-\\nu_{\\ell_{1}}{\\bf z}^{(2)}+\\lambda^{(\\ell_{1})}\\|_{2})^{2}}}=\\sigma\\sqrt{% \\frac{\\alpha_{w}}{\\alpha-\\alpha_{w}}}. $ </equation> Now, let us consider $ \\nu_{gen} $ and $ \\lambda^{(gen)} $ that are the solution of ( [@ref:LABEL:eq:genlasso4] ) obtained for $ r_{socp}\\neq r_{socp}^{(opt)} $ .", "Since $ \\nu_{\\ell_{1}} $ and $ \\lambda^{(\\ell_{1})} $ are optimal in the optimization in ( [@ref:LABEL:eq:optrsocp2] ) we have \\newline <equation> $ \\sqrt{\\|{\\bf g}\\|_{2}^{2}-\\|\\bar{{\\bf h}}-\\nu_{\\ell_{1}}{\\bf z}^{(2)}+\\lambda^% {(\\ell_{1})}\\|_{2}^{2}}=\\max_{\\nu\\geq 0,\\lambda^{(2)}\\in\\Lambda_{\\nu}^{(2,gen)% }}\\sqrt{\\|{\\bf g}\\|_{2}^{2}-\\|\\bar{{\\bf h}}-\\nu{\\bf z}^{(2)}+\\lambda^{(2)}\\|_{% 2}^{2}}\\\\ =\\max_{\\nu\\geq 0,\\lambda^{(2)}\\in\\Lambda_{2\\nu}^{(2,gen)}}\\sqrt{\\|{\\bf g}\\|_{2% }^{2}-\\|{\\bf h}+\\nu{\\bf z}^{(1)}-\\lambda^{(2)}\\|_{2}^{2}}\\geq\\sqrt{\\|{\\bf g}\\|% _{2}^{2}-\\|{\\bf h}+\\frac{1}{\\nu_{gen}}{\\bf z}^{(1)}-\\frac{\\lambda^{(gen)}}{\\nu% _{gen}}\\|_{2}^{2}}, $ </equation> where $ \\Lambda_{2\\nu}^{(2,gen)}=\\{\\lambda^{(2)}|0\\leq\\lambda_{i}^{(2)}\\leq 2\\nu,1\\leq i% \\leq n-k,\\lambda_{i}^{(2)}=0,n-k+1\\leq i\\leq n\\} $ .", "Finally we obtain \\newline <equation> $ E\\|{\\bf w}_{gen}^{(opt)}\\|_{2}=\\sigma\\frac{E\\|\\bar{{\\bf h}}-\\nu_{\\ell_{1}}{\\bf z% }^{(2)}+\\lambda^{(\\ell_{1})}\\|_{2}}{\\sqrt{(E\\|{\\bf g}\\|_{2})^{2}-(E\\|\\bar{{\\bf h% }}-\\nu_{\\ell_{1}}{\\bf z}^{(2)}-\\lambda^{(\\ell_{1})}\\|_{2})^{2}}}\\leq\\sigma% \\frac{E\\|{\\bf h}+\\frac{1}{\\nu_{gen}}{\\bf z}^{(1)}-\\frac{\\lambda^{(gen)}}{\\nu_{% gen}}\\|_{2}}{\\sqrt{(E\\|{\\bf g}\\|_{2})^{2}-E\\|{\\bf h}+\\frac{1}{\\nu_{gen}}{\\bf z% }^{(1)}-\\frac{\\lambda^{(gen)}}{\\nu_{gen}}\\|_{2}^{2}}}=E\\|{\\bf w}_{gen}\\|_{2}. $ </equation> Since both $ \\|{\\bf w}_{gen}^{(opt)}\\|_{2} $ and $ \\|{\\bf w}_{gen}\\|_{2} $ concentrate one also has \\newline <equation> $ P(\\|{\\bf w}_{gen}^{(opt)}\\|_{2}\\leq\\|{\\bf w}_{gen}\\|_{2})\\geq 1-e^{-\\epsilon_{% {\\bf w}_{gen}}n}, $ </equation> where $ \\epsilon_{{\\bf w}_{gen}}>0 $ is a constant independent of $ n $ .", "Roughly speaking ( [@ref:LABEL:eq:optrsocp5] ) shows that if $ r_{socp}\\neq r_{socp}^{opt} $ then with overwhelming probability there will be a solution to the SOCP from ( [@ref:LABEL:eq:socp] ), $ {\\bf w}_{socp} $ , such that $ \\|{\\bf w}_{socp}\\|_{2}\\geq\\|{\\bf w}_{gen}^{(opt)}\\|_{2} $ .", "\\newline Now let us look at general $ \\tilde{{\\bf x}} $ and the corresponding optimization problem ( [@ref:LABEL:eq:mainlasso1] ).", "Let $ r_{socp}=r_{socp}^{(opt)} $ in ( [@ref:LABEL:eq:mainlasso1] ).", "Further, let $ \\widehat{\\nu_{up}} $ and $ \\widehat{\\lambda_{up}^{(2)}} $ be the solution of ( [@ref:LABEL:eq:mainlasso1] ) obtained for $ r_{socp}=r_{socp}^{(opt)} $ .", "Then clearly, \\newline <equation> $ \\sigma\\sqrt{(E\\|{\\bf g}\\|_{2})^{2}-(E\\|{\\bf h}+\\frac{1}{\\widehat{\\nu_{up}}}{% \\bf z}^{(1)}-\\frac{\\widehat{\\lambda_{up}^{(2)}}}{\\widehat{\\nu_{up}}}\\|_{2})^{2% }}-\\sum_{i=n-k+1}^{n}\\frac{(\\widehat{\\lambda_{up}^{(2)}})_{i}}{\\widehat{\\nu_{% up}}}\\tilde{{\\bf x}}_{i}\\geq r_{socp}^{(opt)}=\\sigma\\sqrt{(\\alpha-\\alpha_{w})n}. $ </equation> The nonnegativity of $ \\widehat{\\nu_{up}} $ and the components of $ \\widehat{\\lambda_{up}^{(2)}} $ and $ \\tilde{{\\bf x}} $ implies \\newline <equation> $ \\sigma\\sqrt{(E\\|{\\bf g}\\|_{2})^{2}-(E\\|{\\bf h}+\\frac{1}{\\widehat{\\nu_{up}}}{% \\bf z}^{(1)}-\\frac{\\widehat{\\lambda_{up}^{(2)}}}{\\widehat{\\nu_{up}}}\\|_{2})^{2% }}\\geq r_{socp}^{(opt)}=\\sigma\\sqrt{(\\alpha-\\alpha_{w})n}. $ </equation> Finally one has \\newline <equation> $ E\\|\\widehat{{\\bf w}_{up}}\\|_{2}=\\sigma\\frac{E\\|{\\bf h}+\\frac{1}{\\widehat{\\nu_{% up}}}{\\bf z}^{(1)}-\\frac{\\lambda^{(2)}}{\\widehat{\\nu_{up}}}\\|_{2}}{(E\\sqrt{\\|{% \\bf g}\\|_{2})^{2}-(E\\|{\\bf h}+\\frac{1}{\\widehat{\\nu_{up}}}{\\bf z}^{(1)}-\\frac{% \\lambda^{(2)}}{\\widehat{\\nu_{up}}}\\|_{2})^{2}}}\\leq\\sigma\\sqrt{\\frac{\\alpha_{w% }}{\\alpha-\\alpha_{w}}}=E\\|{\\bf w}_{gen}^{(opt)}\\|_{2}. $ </equation> Since all random quantities of interest concentrate we have the following lemma.", "\\newline <theorem> Theorem 3 .", "Assume the setup of Theorem [@ref:LABEL:thm:mainlasso] .", "Let $ r_{socp} $ in ( [@ref:LABEL:eq:socp] ) be \\newline <equation> $ r_{socp}=r_{socp}^{(opt)}=\\sigma\\sqrt{(\\alpha-\\alpha_{w})n}. $ </equation> Then \\newline <equation> $ P(\\|{\\bf w}_{socp}\\|_{2}\\leq\\sigma\\sqrt{\\frac{\\alpha_{w}}{\\alpha-\\alpha_{w}}})% \\geq 1-e^{-\\epsilon_{1}^{({\\bf w}_{socp})}n}, $ </equation> where $ \\epsilon_{1}^{({\\bf w}_{socp})}>0 $ is a constant independent of $ n $ and $ \\alpha_{w} $ is as defined in fundamental characterization ( [@ref:LABEL:eq:fundl1] ).", "Moreover, if $ r_{socp} $ in ( [@ref:LABEL:eq:socp] ) is such that \\newline <equation> $ r_{socp}>r_{socp}^{(opt)}=\\sigma\\sqrt{(\\alpha-\\alpha_{w})n}, $ </equation> then \\newline <equation> $ P(\\exists{\\bf w}_{socp}|\\|{\\bf w}_{socp}\\|_{2}>\\sigma\\sqrt{\\frac{\\alpha_{w}}{% \\alpha-\\alpha_{w}}}))\\geq 1-e^{-\\epsilon_{2}^{({\\bf w}_{socp})}n}. $ </equation> where $ \\epsilon_{2}^{({\\bf w}_{socp})}>0 $ is a constant independent of $ n $ .", "\\newline </theorem> <proof> Proof.", "Follows from the discussion presented above and Theorem [@ref:LABEL:thm:mainlasso] .", "\u220e \\newline </proof> </subsubsection> <subsubsection> <title> 2.4.3 Computing \u2062 E \u2225 w \u2062 g e n \u2225 2 and \u2062 E \u03be \u2062 p r i m ( \u2062 g e n ) ( \u03c3 , g , h , r \u2062 s o c p ) </title> In this section we present a framework to compute $ \\|{\\bf w}_{gen}\\|_{2} $ and $ \\xi_{prim}^{(gen)}(\\sigma,{\\bf g},{\\bf h},r_{socp}) $ or more precisely their concentrating points $ E\\|{\\bf w}_{gen}\\|_{2} $ and $ E\\xi_{prim}^{(gen)}(\\sigma,{\\bf g},{\\bf h},r_{socp}) $ .", "All other parameters such as $ \\nu_{gen} $ , $ \\lambda_{gen}^{(2)} $ can (and some of them will) be computed through the framework as well.", "We do however mention right here that what we present below assumes a fair share of familiarity with the techniques introduced in our earlier papers [@bib:StojnicCSetam09,StojnicGenLasso10] .", "To shorten the exposition we will skip many details presented in those papers and present only the key differences.", "\\newline We start by looking at the following optimization problem from ( [@ref:LABEL:eq:genlasso1] ) \\newline <equationgroup> <equation> $ \\xi_{prim}^{(gen)}(\\sigma,{\\bf g},{\\bf h},r_{socp})=\\max_{\\nu,% \\lambda^{(2)}}\\sigma\\sqrt{\\|{\\bf g}\\|_{2}^{2}\\nu^{2}-\\|\\nu{\\bf h}+{\\bf z}^{(1)% }-\\lambda^{(2)}\\|_{2}^{2}}-\\nu r_{socp} $ $ \\xi_{prim}^{(gen)}(\\sigma,{\\bf g},{\\bf h},r_{socp})=\\max_{\\nu,% \\lambda^{(2)}} $ $ \\sigma\\sqrt{\\|{\\bf g}\\|_{2}^{2}\\nu^{2}-\\|\\nu{\\bf h}+{\\bf z}^{(1)}% -\\lambda^{(2)}\\|_{2}^{2}}-\\nu r_{socp} $ </equation> <equation> $ \\mbox{subject to}\\nu\\geq 0\\lambda_{i}^{(2)}=0,n-k+1\\leq i\\leq n0% \\leq\\lambda_{i}^{(2)}\\leq 2,1\\leq i\\leq n-k.", "$ $ \\nu\\geq 0 $ $ \\lambda_{i}^{(2)}=0,n-k+1\\leq i\\leq n $ $  0\\leq\\lambda_{i}^{(2)}\\leq 2,1\\leq i\\leq n-k.", "$ </equation> </equationgroup> Using the definitions of $ \\bar{{\\bf h}} $ and $ {\\bf z}^{(2)} $ from Section [@ref:LABEL:sec:connectl1] we modify the above problem in the following way.", "\\newline <equationgroup> <equation> $ \\xi_{prim}^{(gen)}(\\sigma,{\\bf g},{\\bf h},r_{socp})=\\max_{\\nu,% \\lambda^{(2)}}\\sigma\\sqrt{\\|{\\bf g}\\|_{2}^{2}\\nu^{2}-\\|\\nu\\bar{{\\bf h}}-{\\bf z% }^{(2)}+\\lambda^{(2)})\\|_{2}^{2}}-\\nu r_{socp} $ $ \\xi_{prim}^{(gen)}(\\sigma,{\\bf g},{\\bf h},r_{socp})=\\max_{\\nu,% \\lambda^{(2)}} $ $ \\sigma\\sqrt{\\|{\\bf g}\\|_{2}^{2}\\nu^{2}-\\|\\nu\\bar{{\\bf h}}-{\\bf z}% ^{(2)}+\\lambda^{(2)})\\|_{2}^{2}}-\\nu r_{socp} $ </equation> <equation> $ \\mbox{subject to}\\nu\\geq 0\\lambda_{i}^{(2)}=0,n-k+1\\leq i\\leq n0% \\leq\\lambda_{i}^{(2)}\\leq 1,1\\leq i\\leq n-k.", "$ $ \\nu\\geq 0 $ $ \\lambda_{i}^{(2)}=0,n-k+1\\leq i\\leq n $ $  0\\leq\\lambda_{i}^{(2)}\\leq 1,1\\leq i\\leq n-k.", "$ </equation> </equationgroup> Now, let $ \\lambda^{(gen)} $ be the solution of the above optimization (this is a slight abuse of notation since due to the above restructuring of $ {\\bf h} $ this $ \\lambda^{(gen)} $ is different from the one in the above Theorem).", "Following what was presented in [@bib:StojnicCSetam09] there will be a parameter $ c_{gen} $ such that $ \\lambda^{(gen)}=[\\lambda_{1}^{(gen)},\\lambda_{2}^{(gen)},\\dots,\\lambda_{c_{gen% }}^{(gen)},0,0,\\dots,0] $ and obviously $ c_{gen}\\leq n-k $ .", "At this point let us assume that this parameter is known and fixed.", "Then following [@bib:StojnicCSetam09] the above optimization becomes \\newline <equationgroup> <equation> $ \\max_{\\nu}\\sigma\\sqrt{\\|{\\bf g}\\|_{2}^{2}\\nu^{2}-\\|\\nu\\bar{{\\bf h% }}_{c_{gen}+1:n}-{\\bf z}_{c_{gen}+1:n}^{(2)})\\|_{2}^{2}}-\\nu r_{socp} $ $ \\max_{\\nu} $ $ \\sigma\\sqrt{\\|{\\bf g}\\|_{2}^{2}\\nu^{2}-\\|\\nu\\bar{{\\bf h}}_{c_{gen% }+1:n}-{\\bf z}_{c_{gen}+1:n}^{(2)})\\|_{2}^{2}}-\\nu r_{socp} $ </equation> <equation> $ \\mbox{subject to}\\nu\\geq 0.", "$ $ \\nu\\geq 0.", "$ </equation> </equationgroup> We then proceed by solving the above optimization over $ \\nu $ .", "To do so we first look at the derivative with respect to $ \\nu $ of the objective in ( [@ref:LABEL:eq:compwgen3] ).", "Computing the derivative and equalling it to zero gives \\newline <equationgroup> <equation> $ \\frac{d\\sigma\\sqrt{\\|{\\bf g}\\|_{2}^{2}\\nu^{2}-\\|\\nu\\bar{{\\bf h}}_% {c_{gen}+1:n}-{\\bf z}_{c_{gen}+1:n}^{(2)})\\|_{2}^{2}}-\\nu r_{socp}}{d\\nu}=0 $ $ \\frac{d\\sigma\\sqrt{\\|{\\bf g}\\|_{2}^{2}\\nu^{2}-\\|\\nu\\bar{{\\bf h}}_% {c_{gen}+1:n}-{\\bf z}_{c_{gen}+1:n}^{(2)})\\|_{2}^{2}}-\\nu r_{socp}}{d\\nu} $ $ = $ $  0 $ </equation> <equation> $ \\iff\\sigma\\frac{\\nu\\|{\\bf g}\\|_{2}^{2}-\\nu\\|\\bar{{\\bf h}}_{c_{gen% }+1:n}\\|_{2}^{2}+\\bar{{\\bf h}}_{c_{gen}+1:n}^{T}{\\bf z}_{c_{gen}+1:n}^{(2)}}{% \\sqrt{\\|{\\bf g}\\|_{2}^{2}\\nu^{2}-\\|\\nu\\bar{{\\bf h}}_{c_{gen}+1:n}-{\\bf z}_{c_{% gen}+1:n}^{(2)})\\|_{2}^{2}}}=r_{socp}. $ $ \\iff\\sigma\\frac{\\nu\\|{\\bf g}\\|_{2}^{2}-\\nu\\|\\bar{{\\bf h}}_{c_{gen% }+1:n}\\|_{2}^{2}+\\bar{{\\bf h}}_{c_{gen}+1:n}^{T}{\\bf z}_{c_{gen}+1:n}^{(2)}}{% \\sqrt{\\|{\\bf g}\\|_{2}^{2}\\nu^{2}-\\|\\nu\\bar{{\\bf h}}_{c_{gen}+1:n}-{\\bf z}_{c_{% gen}+1:n}^{(2)})\\|_{2}^{2}}} $ $ = $ $  r_{socp}. $ </equation> </equationgroup> Let \\newline <equationgroup> <equation> $  a_{gen}=\\sigma\\frac{\\|{\\bf g}\\|_{2}^{2}-\\|\\bar{{\\bf h}}_{c_{gen}% +1:n}\\|_{2}^{2}}{r_{socp}} $ $  a_{gen} $ $ = $ $ \\sigma\\frac{\\|{\\bf g}\\|_{2}^{2}-\\|\\bar{{\\bf h}}_{c_{gen}+1:n}\\|_{% 2}^{2}}{r_{socp}} $ </equation> <equation> $  b_{gen}=\\sigma\\frac{\\bar{{\\bf h}}_{c_{gen}+1:n}^{T}{\\bf z}_{c_{% gen}+1:n}^{(2)}}{r_{socp}}. $ $  b_{gen} $ $ = $ $ \\sigma\\frac{\\bar{{\\bf h}}_{c_{gen}+1:n}^{T}{\\bf z}_{c_{gen}+1:n}^% {(2)}}{r_{socp}}. $ </equation> </equationgroup> Then combining ( [@ref:LABEL:eq:compwgen4] ) and ( [@ref:LABEL:eq:compwgen5] ) one obtains \\newline <equation> $ (a_{gen}\\nu+b_{gen})^{2}=\\|{\\bf g}\\|_{2}^{2}\\nu^{2}-\\|\\nu\\bar{{\\bf h}}_{c_{gen% }+1:n}-{\\bf z}_{c_{gen}+1:n}^{(2)})\\|_{2}^{2}. $ </equation> After solving ( [@ref:LABEL:eq:compwgen6] ) over $ \\nu $ we have \\newline <equation> $ \\nu=\\frac{-(a_{gen}b_{gen}-\\bar{{\\bf h}}_{c_{gen}+1:n}^{T}{\\bf z}_{c_{gen}+1:n% }^{(2)})-\\sqrt{(a_{gen}b_{gen}-\\bar{{\\bf h}}_{c_{gen}+1:n}^{T}{\\bf z}_{c_{gen}% +1:n}^{(2)})^{2}-\\frac{b_{gen}^{2}+\\|{\\bf z}_{c_{gen}+1:n}^{(2)}\\|_{2}^{2}}{(a% _{gen}^{2}-\\|{\\bf g}\\|_{2}^{2}+\\|\\bar{{\\bf h}}_{c_{gen}+1:n}\\|_{2}^{2})^{-1}}}% }{a_{gen}^{2}-\\|{\\bf g}\\|_{2}^{2}+\\|\\bar{{\\bf h}}_{c_{gen}+1:n}\\|_{2}^{2}}. $ </equation> Given the structure of $ a_{gen} $ and $ b_{gen} $ ( [@ref:LABEL:eq:compwgen7] ) can be simplified a bit.", "However, we find it more appealing to work with ( [@ref:LABEL:eq:compwgen7] ).", "Combining ( [@ref:LABEL:eq:compwgen2] ), ( [@ref:LABEL:eq:compwgen3] ), and ( [@ref:LABEL:eq:compwgen7] ) one obtains the following equation (rather an inequality) that can be used to determine $ c_{gen} $ (essentially $ c_{gen} $ is the largest natural number such that the left-hand side of the equation below is less than $ 1 $ ; since we will assume a large dimensional scenario we will instead of any of the inequalities below write an equality; this will make writing much easier).", "\\newline <equation> $ \\bar{{\\bf h}}_{c_{gen}}\\frac{-(a_{gen}b_{gen}-\\bar{{\\bf h}}_{c_{gen}+1:n}^{T}{% \\bf z}_{c_{gen}+1:n}^{(2)})-\\sqrt{(a_{gen}b_{gen}-\\bar{{\\bf h}}_{c_{gen}+1:n}^% {T}{\\bf z}_{c_{gen}+1:n}^{(2)})^{2}-\\frac{b_{gen}^{2}+\\|{\\bf z}_{c_{gen}+1:n}^% {(2)}\\|_{2}^{2}}{(a_{gen}^{2}-\\|{\\bf g}\\|_{2}^{2}+\\|\\bar{{\\bf h}}_{c_{gen}+1:n% }\\|_{2}^{2})^{-1}}}}{a_{gen}^{2}-\\|{\\bf g}\\|_{2}^{2}+\\|\\bar{{\\bf h}}_{c_{gen}+% 1:n}\\|_{2}^{2}}=1.", "$ </equation> Let $ c_{gen} $ be the solution of ( [@ref:LABEL:eq:compwgen8] ).", "Then \\newline <equation> $ \\nu_{gen}=\\frac{-(a_{gen}b_{gen}-\\bar{{\\bf h}}_{c_{gen}+1:n}^{T}{\\bf z}_{c_{% gen}+1:n}^{(2)})-\\sqrt{(a_{gen}b_{gen}-\\bar{{\\bf h}}_{c_{gen}+1:n}^{T}{\\bf z}_% {c_{gen}+1:n}^{(2)})^{2}-\\frac{b_{gen}^{2}+\\|{\\bf z}_{c_{gen}+1:n}^{(2)}\\|_{2}% ^{2}}{(a_{gen}^{2}-\\|{\\bf g}\\|_{2}^{2}+\\|\\bar{{\\bf h}}_{c_{gen}+1:n}\\|_{2}^{2}% )^{-1}}}}{a_{gen}^{2}-\\|{\\bf g}\\|_{2}^{2}+\\|\\bar{{\\bf h}}_{c_{gen}+1:n}\\|_{2}^% {2}}. $ </equation> From ( [@ref:LABEL:eq:genlasso5] ) one then has \\newline <equation> $ \\|{\\bf w}_{gen}\\|_{2}=\\sigma\\frac{\\|\\nu_{gen}\\bar{{\\bf h}}_{c_{gen}+1:n}-{\\bf z% }_{c_{gen}+1:n}^{(2)}\\|_{2}}{\\sqrt{\\|{\\bf g}\\|_{2}^{2}\\nu_{gen}^{2}-\\|\\nu_{gen% }\\bar{{\\bf h}}_{c_{gen}+1:n}-{\\bf z}_{c_{gen}+1:n}^{(2)}\\|_{2}^{2}}}. $ </equation> Combination of ( [@ref:LABEL:eq:compwgen8] ), ( [@ref:LABEL:eq:compwgen9] ), and ( [@ref:LABEL:eq:compwgen10] ) is conceptually enough to determine $ \\|{\\bf w}_{gen}\\|_{2} $ .", "What is left to be done is a computation of all unknown quantities that appear in ( [@ref:LABEL:eq:compwgen8] ), ( [@ref:LABEL:eq:compwgen9] ), and ( [@ref:LABEL:eq:compwgen10] ).", "We will below show how that can be done.", "As mentioned earlier what we will present substantially relies on what was shown in [@bib:StojnicCSetam09] and we assume a familiarity with the procedure presented there.", "\\newline The first thing to resolve is ( [@ref:LABEL:eq:compwgen8] ).", "Since all random quantities concentrate we will be dealing (as in [@bib:StojnicCSetam09] ) with the expected values.", "To compute $ c_{gen} $ in ( [@ref:LABEL:eq:compwgen8] ) we will need the following expected values \\newline <equation> $ E\\|{\\bf g}\\|_{2}^{2},E\\|\\bar{{\\bf h}}_{c_{gen}+1:n}\\|_{2}^{2},E(\\bar{{\\bf h}}_% {c_{gen}+1:n}^{T}{\\bf z}_{c_{gen}+1:n}^{(2)}).", "$ </equation> Clearly, since components of $ {\\bf g} $ are i.i.d. standard normals one easily has \\newline <equation> $ E\\|{\\bf g}\\|_{2}^{2}=m.", "$ </equation> Let $ c_{gen}=(1-\\theta)n $ where $ \\theta $ is a constant independent of $ n $ .", "Then as shown in [@bib:StojnicCSetam09] \\newline <equation> $ \\lim_{n\\rightarrow\\infty}\\frac{E\\|\\bar{{\\bf h}}_{c_{gen}+1:n}\\|_{2}^{2}}{n}=% \\frac{1-\\beta_{w}}{\\sqrt{2\\pi}}\\left(\\sqrt{2\\pi}+2\\frac{\\sqrt{2(\\mbox{erfinv}(% \\frac{1-\\theta}{1-\\beta_{w}}))^{2}}}{e^{(\\mbox{erfinv}(\\frac{1-\\theta}{1-\\beta% _{w}}))^{2}}}-\\sqrt{2\\pi}\\frac{1-\\theta}{1-\\beta_{w}}\\right)+\\beta_{w}, $ </equation> where we of course recall that $ \\beta_{w}=\\frac{k}{n} $ .", "Also, as shown in [@bib:StojnicCSetam09] \\newline <equation> $ \\lim_{n\\rightarrow\\infty}\\frac{E(\\bar{{\\bf h}}_{c_{gen}+1:n}^{T}{\\bf z}_{c_{% gen}+1:n}^{(2)})}{n}=\\left((1-\\beta_{w})\\sqrt{\\frac{2}{\\pi}}e^{-(\\mbox{erfinv}% (\\frac{1-\\theta}{1-\\beta_{w}}))^{2}}\\right).", "$ </equation> The only other thing that we will need in order to be able to compute $ c_{gen} $ (besides the expectations from ( [@ref:LABEL:eq:compwgenexp1] )) is the following inequality related to the behavior of $ \\bar{{\\bf h}}_{c_{gen}} $ .", "Again, as shown in [@bib:StojnicCSetam09] \\newline <equation> $ P(\\sqrt{2}\\mbox{erfinv}((1+\\epsilon_{1}^{\\bar{{\\bf h}}_{c_{gen}}})(\\frac{1-% \\theta}{1-\\beta_{w}}))\\leq\\bar{{\\bf h}}_{c_{gen}})\\leq e^{-\\epsilon_{2}^{\\bar{% {\\bf h}}_{c_{gen}}}n}, $ </equation> where $ \\epsilon_{1}^{\\bar{{\\bf h}}_{c_{gen}}}>0 $ is an arbitrarily small constant and $ \\epsilon_{2}^{\\bar{{\\bf h}}_{c_{gen}}} $ is a constant dependent on $ \\epsilon_{1}^{\\bar{{\\bf h}}_{c_{gen}}} $ but independent of $ n $ (essentially one only needs this direction in ( [@ref:LABEL:eq:compwgen8] ); however, a similar reverse holds as well).", "\\newline At this point we have all the necessary ingredients to determine $ c_{gen} $ and consequently $ \\nu_{gen} $ and $ \\|{\\bf w}_{gen}\\|_{2} $ (of course in a random setup determining $ c_{gen} $ , $ \\nu_{gen} $ , and $ \\|{\\bf w}_{gen}\\|_{2} $ does not really make sense; what we really mean is determining their concentrating points).", "The following corollary then provides a systematic way of doing so.", "\\newline <theorem> Corollary 1 .", "Assume the setup of Theorems [@ref:LABEL:thm:mainlasso] and [@ref:LABEL:thm:genlasso] .", "Let $ \\bar{{\\bf h}} $ be as defined in ( [@ref:LABEL:eq:defhbar] ) and let $ r_{socp}^{(sc)}=\\lim_{n\\rightarrow\\infty}\\frac{r_{socp}}{\\sqrt{n}} $ . Let $ \\alpha=\\frac{m}{n} $ and $ \\beta_{w}=\\frac{k}{n} $ be fixed.", "Consider the following \\newline <equationgroup> <equation> $  A(\\theta)=\\lim_{n\\rightarrow\\infty}\\frac{Ea_{gen}}{\\sqrt{n}}=% \\sigma\\frac{\\alpha-\\frac{1-\\beta_{w}}{\\sqrt{2\\pi}}\\left(\\sqrt{2\\pi}+2\\frac{% \\sqrt{2(\\mbox{erfinv}(\\frac{1-\\theta}{1-\\beta_{w}}))^{2}}}{e^{(\\mbox{erfinv}(% \\frac{1-\\theta}{1-\\beta_{w}}))^{2}}}-\\sqrt{2\\pi}\\frac{1-\\theta}{1-\\beta_{w}}% \\right)-\\beta_{w}}{r_{socp}^{(sc)}}=\\sigma\\frac{\\alpha-D(\\theta)}{r_{socp}^{(% sc)}} $ $  A(\\theta) $ $ = $ $ \\lim_{n\\rightarrow\\infty}\\frac{Ea_{gen}}{\\sqrt{n}}=\\sigma\\frac{% \\alpha-\\frac{1-\\beta_{w}}{\\sqrt{2\\pi}}\\left(\\sqrt{2\\pi}+2\\frac{\\sqrt{2(\\mbox{% erfinv}(\\frac{1-\\theta}{1-\\beta_{w}}))^{2}}}{e^{(\\mbox{erfinv}(\\frac{1-\\theta}% {1-\\beta_{w}}))^{2}}}-\\sqrt{2\\pi}\\frac{1-\\theta}{1-\\beta_{w}}\\right)-\\beta_{w}% }{r_{socp}^{(sc)}}=\\sigma\\frac{\\alpha-D(\\theta)}{r_{socp}^{(sc)}} $ </equation> <equation> $  B(\\theta)=\\lim_{n\\rightarrow\\infty}\\frac{Eb_{gen}}{\\sqrt{n}}=% \\sigma\\frac{\\left((1-\\beta_{w})\\sqrt{\\frac{2}{\\pi}}e^{-(\\mbox{erfinv}(\\frac{1-% \\theta_{w}}{1-\\beta_{w}}))^{2}}\\right)}{r_{socp}^{(sc)}}=\\sigma\\frac{C(\\theta)% }{r_{socp}^{(sc)}} $ $  B(\\theta) $ $ = $ $ \\lim_{n\\rightarrow\\infty}\\frac{Eb_{gen}}{\\sqrt{n}}=\\sigma\\frac{% \\left((1-\\beta_{w})\\sqrt{\\frac{2}{\\pi}}e^{-(\\mbox{erfinv}(\\frac{1-\\theta_{w}}{% 1-\\beta_{w}}))^{2}}\\right)}{r_{socp}^{(sc)}}=\\sigma\\frac{C(\\theta)}{r_{socp}^{% (sc)}} $ </equation> <equation> $  F(\\theta)=\\sqrt{2}\\mbox{erfinv}(\\frac{1-\\theta}{1-\\beta_{w}}), $ $  F(\\theta) $ $ = $ $ \\sqrt{2}\\mbox{erfinv}(\\frac{1-\\theta}{1-\\beta_{w}}), $ </equation> </equationgroup> where \\newline <equationgroup> <equation> $  C(\\theta)=\\lim_{n\\rightarrow\\infty}\\frac{E(\\bar{{\\bf h}}_{(1-% \\theta)n+1:n}^{T}{\\bf z}_{(1-\\theta)n+1:n}^{(2)})}{n}=\\left((1-\\beta_{w})\\sqrt% {\\frac{2}{\\pi}}e^{-(\\mbox{erfinv}(\\frac{1-\\theta_{w}}{1-\\beta_{w}}))^{2}}\\right) $ $  C(\\theta) $ $ = $ $ \\lim_{n\\rightarrow\\infty}\\frac{E(\\bar{{\\bf h}}_{(1-\\theta)n+1:n}^% {T}{\\bf z}_{(1-\\theta)n+1:n}^{(2)})}{n}=\\left((1-\\beta_{w})\\sqrt{\\frac{2}{\\pi}% }e^{-(\\mbox{erfinv}(\\frac{1-\\theta_{w}}{1-\\beta_{w}}))^{2}}\\right) $ </equation> <equation> $  D(\\theta)=\\lim_{n\\rightarrow\\infty}\\frac{E\\|\\bar{{\\bf h}}_{(1-% \\theta)n+1:n}\\|_{2}^{2}}{n}=\\frac{1-\\beta_{w}}{\\sqrt{2\\pi}}\\left(\\sqrt{2\\pi}+2% \\frac{\\sqrt{2(\\mbox{erfinv}(\\frac{1-\\theta}{1-\\beta_{w}}))^{2}}}{e^{(\\mbox{% erfinv}(\\frac{1-\\theta}{1-\\beta_{w}}))^{2}}}-\\sqrt{2\\pi}\\frac{1-\\theta}{1-% \\beta_{w}}\\right)+\\beta_{w}. $ $  D(\\theta) $ $ = $ $ \\lim_{n\\rightarrow\\infty}\\frac{E\\|\\bar{{\\bf h}}_{(1-\\theta)n+1:n}% \\|_{2}^{2}}{n}=\\frac{1-\\beta_{w}}{\\sqrt{2\\pi}}\\left(\\sqrt{2\\pi}+2\\frac{\\sqrt{2% (\\mbox{erfinv}(\\frac{1-\\theta}{1-\\beta_{w}}))^{2}}}{e^{(\\mbox{erfinv}(\\frac{1-% \\theta}{1-\\beta_{w}}))^{2}}}-\\sqrt{2\\pi}\\frac{1-\\theta}{1-\\beta_{w}}\\right)+% \\beta_{w}. $ </equation> </equationgroup> Let $ \\hat{\\theta} $ be the solution of \\newline <equation> $ F(\\theta)\\frac{-(A(\\theta)B(\\theta)-C(\\theta))-\\sqrt{(A(\\theta)B(\\theta)-C(% \\theta))^{2}-(B(\\theta)^{2}+\\theta)(A(\\theta)^{2}-\\alpha+D(\\theta))}}{A(\\theta% )^{2}-\\alpha+D(\\theta)}=1.", "$ </equation> Then the concentrating points of $ \\nu_{gen} $ , $ \\|{\\bf w}_{gen}\\|_{2} $ , and $ \\xi_{prim}^{(gen)}(\\sigma,{\\bf g},{\\bf h},r_{socp}) $ in Theorem [@ref:LABEL:thm:genlasso] can be determined as \\newline <equationgroup> <equation> $  E\\nu_{gen}=\\frac{-(A(\\hat{\\theta})B(\\hat{\\theta})-C(\\hat{\\theta}% ))-\\sqrt{(A(\\hat{\\theta})B(\\hat{\\theta})-C(\\hat{\\theta}))^{2}-(B(\\hat{\\theta})% ^{2}+\\hat{\\theta})(A(\\hat{\\theta})^{2}-\\alpha+D(\\hat{\\theta}))}}{A(\\hat{\\theta% })^{2}-\\alpha+D(\\hat{\\theta})} $ $  E\\nu_{gen} $ $ = $ $ \\frac{-(A(\\hat{\\theta})B(\\hat{\\theta})-C(\\hat{\\theta}))-\\sqrt{(A(% \\hat{\\theta})B(\\hat{\\theta})-C(\\hat{\\theta}))^{2}-(B(\\hat{\\theta})^{2}+\\hat{% \\theta})(A(\\hat{\\theta})^{2}-\\alpha+D(\\hat{\\theta}))}}{A(\\hat{\\theta})^{2}-% \\alpha+D(\\hat{\\theta})} $ </equation> <equation> $  E\\|{\\bf w}_{gen}\\|_{2}=\\sigma\\sqrt{\\frac{(E\\nu_{gen})^{2}D(\\hat{% \\theta})-2E\\nu_{gen}C(\\hat{\\theta})+\\hat{\\theta}}{\\alpha(E\\nu_{gen})^{2}-((E% \\nu_{gen})^{2}D(\\hat{\\theta})-2E\\nu_{gen}C(\\hat{\\theta})+\\hat{\\theta})}} $ $  E\\|{\\bf w}_{gen}\\|_{2} $ $ = $ $ \\sigma\\sqrt{\\frac{(E\\nu_{gen})^{2}D(\\hat{\\theta})-2E\\nu_{gen}C(% \\hat{\\theta})+\\hat{\\theta}}{\\alpha(E\\nu_{gen})^{2}-((E\\nu_{gen})^{2}D(\\hat{% \\theta})-2E\\nu_{gen}C(\\hat{\\theta})+\\hat{\\theta})}} $ </equation> <equation> $ \\lim_{n\\rightarrow\\infty}\\frac{E\\xi_{prim}^{(gen)}(\\sigma,{\\bf g}% ,{\\bf h},r_{socp})}{\\sqrt{n}}=\\sigma\\sqrt{\\alpha(E\\nu_{gen})^{2}-((E\\nu_{gen})% ^{2}D(\\hat{\\theta})-2E\\nu_{gen}C(\\hat{\\theta})+\\hat{\\theta})}-E\\nu_{gen}r_{% socp}^{(sc)}. $ $ \\lim_{n\\rightarrow\\infty}\\frac{E\\xi_{prim}^{(gen)}(\\sigma,{\\bf g}% ,{\\bf h},r_{socp})}{\\sqrt{n}} $ $ = $ $ \\sigma\\sqrt{\\alpha(E\\nu_{gen})^{2}-((E\\nu_{gen})^{2}D(\\hat{\\theta% })-2E\\nu_{gen}C(\\hat{\\theta})+\\hat{\\theta})}-E\\nu_{gen}r_{socp}^{(sc)}. $ </equation> </equationgroup> \\newline </theorem> <proof> Proof.", "Follows from Theorem [@ref:LABEL:thm:genlasso] and the discussion presented above.", "\u220e \\newline </proof> The results from the above corollary can be then used to compute parameters of interest in our derivation for particular values of $ \\beta_{w} $ , $ \\alpha $ , $ \\sigma $ , and $ r_{socp} $ .", "We conducted massive numerical experiments and found that the results one can get through them are in firm agreement (as they should be) with what the presented theory predicts.", "This paper is above all intended to be an introductory presentation of a framework for the analysis of the SOCP algorithms and we therefore refrain from a substantial discussion related to the results obtained through the numerical experiments and their agreement with the theory.", "We instead defer such a discussion to several forthcoming papers.", "Just to give an idea how powerful the introduced mechanism is we, in the next subsection, present only a small sample of the conducted numerical experiments.", "\\newline </subsubsection> <subsubsection> <title> 2.4.4 Numerical experiments </title> Using ( [@ref:LABEL:eq:compwgenthmcond1] ), ( [@ref:LABEL:eq:compwgenthmcond2] ), ( [@ref:LABEL:eq:compwgenthmcgen] ), and ( [@ref:LABEL:eq:compwgenthmnuwgenxiprim] ) one can then for any $ r_{socp} $ , any $ \\sigma $ , and any pair $ (\\alpha,\\beta_{w}) $ (that is below fundamental characterization ( [@ref:LABEL:eq:fundl1] )) determine the value of $ E\\|{\\bf w}_{socp}\\|_{2} $ as well as the concentrating points of all other quantities in our derivations.", "We will split the presentation of the numerical results in four parts.", "To demonstrate the precision of our technique in the first couple of experiments we will run both SOCP from ( [@ref:LABEL:eq:socp] ) as well as ( [@ref:LABEL:eq:compwgen1] ).", "In some of the later experiment sets we will instead focus solely on SOCP from ( [@ref:LABEL:eq:socp] ) whose performance analysis is actually the leading topic of this paper.", "\\newline {1) Random examples from low $ (\\alpha,\\beta_{w}) $ regime} \\newline Under low $ (\\alpha,\\beta_{w}) $ regime we consider pairs $ (\\alpha,\\beta_{w}) $ that are well below the fundamental characterization ( [@ref:LABEL:eq:fundl1] ).", "We ran $ 500 $ times ( [@ref:LABEL:eq:compwgen1] ) for $ \\alpha=\\{0.3,0.5,0.7\\} $ , $ n=1000 $ , $ \\sigma=1 $ , and $ r_{socp}=\\sqrt{m}=\\sqrt{\\alpha n} $ and various randomly chosen values of $ \\beta_{w} $ .", "In parallel, we ran $ 500 $ times ( [@ref:LABEL:eq:socp] ) with the same parameters, except that ( [@ref:LABEL:eq:socp] ) was run for $ n=400 $ .", "Also, since the non-zero components of $ \\tilde{{\\bf x}} $ can not really be made infinite we set them to be $ \\frac{40}{\\sqrt{n}} $ when generating ( [@ref:LABEL:eq:systemnoise] ) (we could/should have set them higher but this already works fairly well).", "The results we obtained for $ E\\nu_{gen} $ , $ E\\xi_{prim}^{(gen)}(\\sigma,{\\bf g},{\\bf h},r_{socp}) $ , $ E\\|{\\bf w}_{gen}\\|_{2} $ , $ Ef_{obj} $ , and $ E\\|{\\bf w}_{socp}\\|_{2} $ through these experiments are presented in Table [@ref:LABEL:tab:simlowerrandom] .", "The theoretical values for any of these quantities in any of the simulated scenarios are given in parallel as bolded numbers.", "We observe a solid agreement between the theoretical predictions and the results obtained through numerical experiments.", "\\newline {2) Specific examples in low $ (\\alpha,\\beta_{w}) $ regime} \\newline {a) $ r_{socp}=r_{socp}^{(opt)}=\\sigma\\sqrt{(\\alpha-\\alpha_{w})n} $} \\newline We also ran a carefully designed set of experiments intended to show a specific behavior of the SOCP from ( [@ref:LABEL:eq:socp] ) and the above theoretical predictions.", "Namely, for a pair $ (\\alpha,\\beta_{w}) $ instead of choosing $ r_{socp} $ as $ \\sqrt{m}=\\sqrt{\\alpha n} $ (which is, as discussed in Section [@ref:LABEL:sec:back] , how one could do it if solely based on statistics of $ {\\bf v} $ ) we chose $ r_{socp}=\\sigma\\sqrt{(\\alpha-\\alpha_{w})n} $ , where $ \\alpha_{w} $ is the one that corresponds to $ \\beta_{w} $ in the fundamental characterization ( [@ref:LABEL:eq:fundl1] ).", "As discussed in Section [@ref:LABEL:sec:optrsocp] this choice could in certain sense be optimal.", "Moreover, as discussed in [@bib:StojnicGenLasso10] this choice of $ r_{socp} $ should make the norm-2 of the error vector in ( [@ref:LABEL:eq:socp] ) no worse (larger) than the one that can be obtained via a couple of LASSO algorithms considered in [@bib:StojnicGenLasso10] .", "We then considered the contour LASSO line from [@bib:StojnicGenLasso10] that corresponds to the norm-2 of the error vector equal to $ 2 $ and from that line we chose three pairs $ (\\alpha,\\beta_{w}) $ (see Table [@ref:LABEL:tab:simlowerspec] ) for which we then ran ( [@ref:LABEL:eq:socp] ) (for the completeness and easiness of following we present the LASSO contour lines again in Figure [@ref:LABEL:fig:lassoweakthr] ; in fact as argued in Section [@ref:LABEL:sec:optrsocp] and [@bib:StojnicGenLasso10] with $ r_{socp} $ as above the performance of SOCP from ( [@ref:LABEL:eq:socp] ) can also be characterized by these lines, i.e. it is not really necessary to refer to them as LASSO contour lines, one may as well refer to them as SOCP contour lines!).", "Now, further, we will again set $ \\sigma=1 $ .", "Based on results of [@bib:StojnicGenLasso10] it is then easy to see that on the contour line that corresponds to the norm-2 of the error vector equal to $ 2 $ , $ r_{socp}=\\sqrt{0.2m} $ .", "We ran ( [@ref:LABEL:eq:socp] ) $ 200 $ times with $ n=400 $ .", "We also in parallel for the same set of parameters ran ( [@ref:LABEL:eq:compwgen1] ).", "To get a bit better concentration results we ran ( [@ref:LABEL:eq:compwgen1] ) $ 500 $ times with $ n=5000 $ .", "Obtained results are presented in Table [@ref:LABEL:tab:simlowerspec] .", "The theoretical values for any of the simulated quantities in any of the simulated scenarios are again given in parallel as bolded numbers.", "We again observe a solid agreement between the theoretical predictions and the results obtained through numerical experiments.", "\\newline {b) Varying $ r_{socp} $ from $ \\sqrt{0.2m} $ to $ \\sqrt{m} $} \\newline To observe how the norm-2 of the error vector changes with a change in $ r_{socp} $ we conducted a set of experiments where we chose the same three pairs $ (\\alpha,\\beta_{w}) $ as in the previous set but varied $ r_{socp} $ .", "We varied $ r_{socp} $ over set $ \\{\\sqrt{0.2m},\\sqrt{0.6m},\\sqrt{m}\\} $ .", "This time we only focused on SOCP and ran only ( [@ref:LABEL:eq:socp] ).", "We ran ( [@ref:LABEL:eq:socp] ) $ 200 $ times with $ n=400 $ .", "The obtained results are presented in Table [@ref:LABEL:tab:simlowervarsocp] .", "Again, the theoretical predictions are given in parallel in bold.", "We again observe a solid agreement between the the theoretical predictions and numerical results.", "Also, from Table [@ref:LABEL:tab:simlowervarsocp] one can see that as $ r_{socp} $ decreases from $ \\sqrt{m} $ to $ \\sqrt{0.2m} $ , $ E\\|{\\bf w}_{socp}\\|_{2} $ decreases as well.", "\\newline {2) Specific examples in high $ (\\alpha,\\beta_{w}) $ regime} \\newline {a) $ r_{socp}=r_{socp}^{(opt)}=\\sigma\\sqrt{(\\alpha-\\alpha_{w})n} $} \\newline We also ran a carefully designed set of experiments intended to show a specific behavior of the SOCP from ( [@ref:LABEL:eq:socp] ) and the above theoretical predictions in \u201chigh\u201d $ (\\alpha,\\beta_{w}) $ regime (under \u201chigh\u201d $ (\\alpha,\\beta_{w}) $ regime we of course assume pairs of $ (\\alpha,\\beta_{w}) $ that are relatively close to the fundamental characterization).", "We again for a pair $ (\\alpha,\\beta_{w}) $ instead of choosing $ r_{socp} $ as $ \\sqrt{m}=\\sqrt{\\alpha n} $ chose it based on the SOCP/LASSO contour lines.", "This time, though, we considered the contour line from [@bib:StojnicGenLasso10] (or Figure [@ref:LABEL:fig:lassoweakthr] ) that corresponds to the norm-2 of the error vector equal to $ 3 $ and from that line we chose three pairs $ (\\alpha,\\beta_{w}) $ (see Table [@ref:LABEL:tab:simhigherspec] ) for which we then ran ( [@ref:LABEL:eq:socp] ).", "As usual to make the scaling smoother we set $ \\sigma=1 $ .", "Based on results from Section [@ref:LABEL:sec:optrsocp] and [@bib:StojnicGenLasso10] it is then easy to see that $ r_{socp}=\\sqrt{0.1m} $ .", "To get a bit better concentration results (the pairs of $ (\\alpha,\\beta_{w}) $ are now fairly close to the fundamental characterization) we ran ( [@ref:LABEL:eq:socp] ) $ 200 $ times with $ n=2000 $ and in parallel we ran ( [@ref:LABEL:eq:compwgen1] ) $ 200 $ times with $ n=10000 $ for the same set of other parameters.", "The obtained results are presented in Table [@ref:LABEL:tab:simhigherspec] .", "The theoretical values for any of the simulated quantities in any of the simulated scenarios are again given in parallel as bolded numbers.", "We again observe a solid agreement between the theoretical predictions and the results obtained through numerical experiments.", "\\newline {b) Varying $ r_{socp} $ from $ \\sqrt{0.1m} $ to $ \\sqrt{m} $ } \\newline We also conducted a set of high regime experiments that are analogous to the varying $ r_{socp} $ in the lower regime.", "We maintained the structure of the experiments as in the lower regime with a different way of choosing three pairs $ (\\alpha,\\beta_{w}) $ .", "As above we chose them from the LASSO contour line that corresponds to the norm-2 of the error vector that is equal to $ 3 $ (this is of course the same as in Table [@ref:LABEL:tab:simhigherspec] ).", "Again, as above one has $ r_{socp}=\\sigma\\sqrt{(\\alpha-\\alpha_{w})n}=\\sqrt{0.1m} $ (we again for the simplicity of scaling assume $ \\sigma=1 $ ).", "We then varied $ r_{socp} $ over set $ \\{\\sqrt{0.1m},\\sqrt{0.5m},\\sqrt{m}\\} $ and again focused only on SOCP and ran ( [@ref:LABEL:eq:socp] ).", "We ran ( [@ref:LABEL:eq:socp] ) $ 200 $ times with $ n=2000 $ ."]], "target": "The obtained results are presented in Table . The theoretical predictions are given in parallel in bold. The results obtained through numerical experiments are again in a solid agreement with the theoretical predictions. Also, as it was the case in lower regime, one can see again that as $ r_{socp} $ decreases from $ \\sqrt{m} $ to $ \\sqrt{0.1m} $ , $ E\\|{\\bf w}_{socp}\\|_{2} $ decreases as well."}, {"tabular": ["  Data Set  &  No. of  &  Class  &  Data Set  &  No.of  &  Class  &  Data Set  &  No.of  &  Class ", " features  &  Distribution  &  features  &  Distribution  &  features  &  Distribution ", " {australian}  &  14  &  383:307  &  {house-votes}  &  16  &  168:267  &  {pima}  &  8  &  268:500 ", " {bcw}  &  9  &  458:241  &  {ionosphere}  &  34  &  225:126  &  {sonar}  &  60  &  97:111 ", " {crx}  &  15  &  307:383  &  {kr-vs-kp}  &  36  &  1669:1527  &  {monks-3}  &  6  &  228:204 ", " {transfusion}  &  4  &  178:570  &  {mammographic}  &  5  &  445:516  &  {spect}  &  22  &  212:55 ", " {german}  &  24  &  700:300  &  {monks-1}  &  6  &  216:216  &  {parkinsons}  &  22  &  147:48 ", " {wdbc}  &  30  &  212:357  &  {monks-2}  &  6  &  290:142  &  {tic-tac-toe}  &  9  &  626:332 ", " {bands}  &  36  &  228:312  &    &    &    &    &    &    "], "ref_sec": [["<section> <title> 1 Introduction </title>  Traditionally, a classification task is to assign items (instances) in a data-set to target categories (classes) based on classifier(s) learnt by training instances.", "In binary classification there are only two classes or categories and all instances in the data set will be assigned one of them.", "The target of a classification problem is trying to design classifiers which make error-free assignments.", "\\newline The ROC graph is a technique for visualizing, organizing and selecting classifiers based on their performance [@bib:fawcett2006introduction] .", "A salient topic in ROC analysis is to generate ROC curves for varying discriminative thresholds over the output of the classifier [@bib:fawcett2006introduction] , and ROC curves have been used widely in many areas.", "Actually, over the course of the past 40 years, ROC technique has been widely applied in many research and application areas, such as signal detection [@bib:egan1975signal] , medical decision making [@bib:sox1988medical] , diagnostic systems [@bib:swets1988measuring] .", "\\newline Though ROC curve works well in many cases, recently attention of the research is also drawn towards another perspective of ROC analysis, namely ROC convex hull (ROCCH).", "ROCCH pays more attention to the convex hull of a set of points (hard classifiers) obtained either from sever curves (i.e., soft classifiers) or itself (hard classifier).", "A classifier is potentially optimal, if and only if it is a component of ROCCH, in other words, ROCCH could provide better choices than a single ROC curve to specific environments.", "The significance of ROCCH in ROC analysis is that for test data sets with different skewed class distributions or misclassification costs, it is always possible to choose suitable classifiers by iso-performance lines which is translated by operating conditions of classifiers and used to identify a portion of the ROCCH [@bib:provost2001robust] .", "Consequently, ROCCH is emphasized in this paper and we will focus on searching a group of independently hard classifiers to maximize the ROCCH performance rather try to maximize the area under the ROC curve (AUC) of a single soft classifier.", "\\newline Essentially, ROCCH is the collection of all potentially optimal classifiers in a given set of classifiers, so ROCCH maximization is to find a group of classifiers with their performance approximating the top and the left axes as near as possible in ROC space.", "However, ROCCH maximization is not an easy task, there are not many works focusing on how to maximize the ROCCH though it is a really important topic in classification problems.", "Generally, the exist works could be reviewed into two categories, ROC geometric analysis based machine learning methods and multi-objective optimization strategies based evolutionary computation methods for ROCCH maximization.", "\\newline Fawcett et al. [@bib:fawcett2001using] employed C4.5 and Rule Learning (RL) systems to induce decision rules in ROC space and its advanced version PRIE was introduced in [@bib:fawcett2008prie] .", "It was a straight way to analysis the geometrical properties to generate decision rules to maximize the ROC performance.", "However, the procedure easily gets trapped in local optima.", "\\newline The concavity problem in ROC analysis was researched by Flach et al. [@bib:flach2003repairing] who demonstrated how to detect and repair concavities in ROC curves.", "The basic idea of that work is that if a point in the concavity can be mirrored to a better point which could perform well beyond the original ROC curve. But it is not a general method to maximize the ROC performance.", "\\newline ROCCER was introduced by Prati et al. in [@bib:prati2005roccer] .", "It was argued that ROCCER is less dependent on the previously induced rules compared with set covering algorithms to construct rule sets that have a convex hull in ROC space.", "However, it adopted an association rule learner to generate new rules to cover the instance space as full as possible.", "It is too easy to fall into overfitting, because it needs many rules to cover the space which is similar with a decision tree with a very high height.", "\\newline The Neyman-Pearson lemma as the theoretical basis for finding the optimal combination of classifiers to maximize the ROCCH is given in [@bib:Barreno_Cardenas_Tygar_2008] .", "In contrast to the similar technique in [@bib:flach2003repairing] , it not only focuses on repairing but it also pays attention on improving if there was on concavity.", "For a given rule set, the method proposed by [@bib:Barreno_Cardenas_Tygar_2008] can be efficient to combine these rules using {AND} and {OR} to get the optimum rule subset.", "However, as mentioned above, it misses schemes for generating new rules in the global rule set searching.", "\\newline To maximize ROCCH is searching a group of classifiers to maximize the ROCCH performance ideally would yield classifiers that simultaneously minimize the {fpr} and maximize the {tpr} , i.\u2009e. that are located as much to the left and to the top of the ROC space as possible.", "However, it is very hard to optimize {fpr} and {tpr} simultaneously because they are conflicting targets.", "From this perspective, ROCCH maximization problem is similar to multi-objective optimization problem.", "\\newline Zhao [@bib:zhao2007multi] proposed specific non-dominated relationship involved into multi-objective optimization framework to optimize {tpr} and $ 1- $ {fpr} .", "However, it paid more attention on cost-sensitive classification and made many rules by information of costs of misclassification to rank the individuals in its multi-objective genetic programming.", "First, it is not a general method for ROCCH maximization because it only focused on cost-sensitive problem.", "Second, two data sets involved in experiments are too few to evaluation the proposed method.", "\\newline Bhowan et al. searched the Pareto front to maximize the accuracy of each minority class with unbalanced data set [@bib:bhowan2009multi] , and they also employed multi-objective optimization techniques to evolve diverse ensembles using genetic programming to maximize the classification performance in [@bib:bhowan2012evolving] .", "\\newline Wang et al. investigated investigated some EMOAs such as NSGA-II [@bib:deb2002fast] , MOEA/D [@bib:zhang2007moea] , SMS-EMOA [@bib:Beume20071653] and Approximation-Guided Evolutionary Multi-objective Algorithm (AG-EMOA) [@bib:Bringmann] .", "These different evolutionary multi-objective optimization frameworks had been combined with genetic programming to maximize ROC performance [@bib:wang2012multiobjective] .", "\\newline However, ROCCH is different from Pareto front though it was reported they were similar to each other [@bib:fawcett2004roc] .", "ROCCH is the collection of points which construct the convex hull of existing classifiers in ROC space, and Pareto front is the collection of points that is the first level sorted by dominance relationship.", "Though evolutionary multi-objective algorithms(EMOAs) have been successfully used into ROCCH maximization, these EMO techniques do not take into account a special characteristic of ROCCH.", "That is by mixing two classifiers we can take any two real classifiers to construct any virtual classifier with its performance as a point along the line connected by above two points [@bib:fawcett2004roc] .", "Consequently, hard classifiers in concave parts of the Pareto front can always be replaced by classifier combinations that yield dominating points.", "The computational resources for the approximation of concave parts are thus better spent on the accurate approximation of only those parts of the Pareto front that are part of the convex hull.", "\\newline In [@bib:shan2009multi,DavoodiMonfared20111435,ZapotecasMartinez:2010] , convex hull concept of was employed into EMOAs to make the sort fast or maintain a well-distributed set of non-dominated solutions.", "These work are good to supply some ideas of convex hull based sort.", "In [@bib:CococcioniCHEA] and [@bib:ducange2010multi] , convex hull-based ranking involved with evolutionary multi-objective optimization and fuzzy rule-based binary classifiers to maximize ROOCH in ROC space.", "However, the number of levels was pre-defined as three without explaining in first work and the second one was considered as bi-objectives optimization, which were accuracy of classification and complexity of classifier rules.", "\\newline Moreover, instead of designing algorithms based on Pareto dominance compliant performance indicators, such as the hypervolume indicator as done in [@bib:Beume20071653] and in [@bib:igel2007covariance] , it seems more promising to directly target the algorithm towards the maximization of the area under the convex hull (AUCH).", "\\newline In this paper, we utilize Genetic Programming (GP) combined multi-objective techniques to get the optimal ROCHH.", "Two strategies will be represented, the first is the convex hull-based without redundancy sort to make the population of GP into several levels such as non-dominated sort in NSGA-II, the second is using area-based contribution to select the survivors in the same level, actually we use $ \\mu $ + $ \\mu $ selection strategy as [@bib:igel2007covariance] .", "We show that convex hull-based without redundancy sort plays a key role in multi-objective genetic programming (MOGP) for maximizing ROCCH performance and area-based contribution selection scheme also can improve the performance.", "\\newline This paper is organized as follows: Section [@ref:LABEL:section:rochhmo] will discuss the relationship between ROCCH optimization and traditional multi-objective optimization in detail.", "Convex hull-based multi-objective genetic programming (CH-MOGP) will be described in Section [@ref:LABEL:section:chmogp] .", "Experiments are studied in Section [@ref:LABEL:section:experiment] and shows the advantages of our new algorithm.", "Section [@ref:LABEL:section:confusion] gives the conclusions and a discussion on the important aspects and the future perspectives of this work.", "\\newline  </section>"], ["<section> <title> 2 ROC Convex Hull and Multi-objective Optimization </title>  <subsection> <title> 2.1 What is ROCCH? </title> Basically, ROC analysis concerns the confusion matrix for the outputs of a classifier, in which we can analysis the performance by measuring different metrics such as accuracy, precise, specificity, sensitivity and some others.", "ROC graph (Left side of Fig. [@ref:LABEL:fig:rocspace] ) is plotted upon Y axis and X axis respectively taken {tpr} and {fpr} , which are also defined from the confusion matrix.", "Each classifier can be mapped in the ROC graph by its performance.", "Essentially, ROCCH is the collection of all potentially optimal classifiers in a given set of classifiers(Right side of Fig. [@ref:LABEL:fig:rocspace] ).", "Furthermore, a classifier is potentially optimal if and only if it lies on the convex hull of the set of points in ROC space [@bib:fawcett2006introduction] .", "\\newline </subsection> <subsection> <title> 2.2 ROCCH maximization problem and multi-objective optimization problem </title> The target of ROOCH maximization problem essentially aims at searching a group of solutions (classifiers)to approximate the upmost line and the leftmost line in ROC space as closely as possible.", "However, it is conflicting to minimize $ fpr $ and maximize $ tpr $ simultaneously because if the classifier labels more instances as positives, it will produce less negatives and vice versa.", "Generally speaking, ROCCH maximization is considered as a multi-objective optimization problem from this perspective and we can describe it as follows: \\newline <equationgroup> <equation> $ \\textnormal{maximize}F(x)=(f_{tpr}(x),f_{1-fpr}(x)) $ $ \\textnormal{maximize}F(x) $ $ = $ $ (f_{tpr}(x),f_{1-fpr}(x)) $ </equation> <equation> $ \\textnormal{subject to}x\\in\\Omega $ $  x\\in\\Omega $ </equation> </equationgroup> \\newline In Eq. [@ref:LABEL:mop] , $ x $ is a classifier and $ F(x) $ is a vector function for $ fpr $ and $ tpr $ of the classifier.", "An important term in MOP is {dominance} which can be defined as: Let $ u=(u_{1},\\dots,u_{m}) $ , $ v=(v_{1},\\dots,v_{m}) $ be two vectors, $ u $ is said to {dominate} $ v $ if $ u_{i}\\leq v_{i} $ for all $ i=1{\\dots}m $ , and $ u\\neq v $ , this is noted as $ u\\prec v $ .", "If $ u $ and $ v $ can not dominate each other, we say that $ u $ and $ v $ are {nondominated} .", "The nondominated set is a set that each item does not dominate any another one.", "A point $ x^{\\star} $ is called {Pareto optimal} if there is no $ x\\in\\Omega $ such that $ F(x) $ dominates $ F(x^{\\star}) $ [@bib:zhang2007moea,WGOEB] .", "Pareto set (PS) is the collection of all Pareto optimal points.", "The Pareto front is the set of all the Pareto objective vectors $ PF=\\{F(x)|x\\in PS\\} $ .", "\\newline Most evolutionary multi-objective algorithms involves the pair-wise based dominance to describe the relationship of two solutions.", "However, we get a special character in ROCCH maximization in ROC space.", "Fig. [@ref:LABEL:fig:ROCCHParetoFront] shows the convex hull part and Pareto front for all points.", "Obviously, convex hull is different from the Pareto front though they were argued that they are similar to each other [@bib:flach2010roc] .", "For example, points $ a,b,c $ in Fig. [@ref:LABEL:fig:ROCCHParetoFront] are non-dominated set in traditional multi-objective optimization problem, however, the classifier along the line connected by $ a $ and $ c $ would dominate $ b $ .", "That is the special character in ROC maximization problem which makes ROCCH maximization is beyond traditional multi-objective optimization.", "However, we need to design some new techniques for searching a group of classifiers with maximum ROCCH.", "\\newline </subsection> <subsection> <title> 2.3 Nondominated sort does harm to EMOAs in ROCCH maximization </title> The root reason for why we want to get the convex hull rather than Pareto front is that two classifiers will produce any classifiers with their ROC performance which is along the line connected by two point representing for the performance for previous two classifiers in ROC space [@bib:fawcett2004roc] .", "As shown in left side of Fig. [@ref:LABEL:fig:doharm] , classifiers with performance at point $ d $ and $ b $ can be used to construct any virtual classifier with its performance at $ e $ along the line connected by $ d $ and $ b $ .", "That is a special and important character in ROCCH maximization problem.", "\\newline In the right side of Fig. [@ref:LABEL:fig:doharm] , all the points are nondominated to each other and belong to the convex hull expect for point $ a $ .", "However, if we take crowding-distance selection or hyper-volume contribution based selection to select one individual to be discarded from the population, obviously, point $ a $ will be selected to survive rather than point $ b $ though point $ a $ is not on the convex hull.", "Actually, there are two phenomenons we need pay attention to, one is the sort strategy and the other is the selection scheme.", "Besides, suitable sort strategy and selection scheme are should be considered in EMOAs for ROCCH not matter which classifier is involved.", "\\newline </subsection> <subsection> <title> 2.4 The motivation and ideas for new multi-objective algorithms for ROCCH maximization </title> We need to think about how to use the special character of ROCCH to make multi-objective optimization techniques more efficient to solve the ROCCH maximization problem.", "The main techniques for MOP is how to rank the population to select the solutions to survive in next generation.", "The mostly common rank approach includes two steps, one is sorting the population into several levels indicating the priority level, after that, a selection scheme is used to choose winners from solutions at the same level.", "In ROCCH maximization problem, firstly, convex hull-based idea will considered into sorting strategy, however, because of the critical concept of convex hull, it would make the diversity decrease fast in the evolutionary process, so we design convex hull-based sorting without redundancy to sort the population.", "Another idea is to use area-base selection scheme because the target is to maximize the area under the convex hull insteading of hypervolume or crowding-distance.", "Convex hull-based sorting without redundancy and area-based selection scheme will be descried in detail in Section [@ref:LABEL:section:chmogp] .", "\\newline </subsection>  </section>"], ["<section> <title> 3 Convex Hull-based Multi-objective Genetic Programming (CH-MOGP) </title>  In this section, we will describe our proposed convex hull-based multi-objective genetic programming to maximize ROCCH.", "Firstly, convex hull-based sorting without redundancy approach is used to rank the individuals in the union population into several levels which represent different priorities to survive as described in NSGA-II.", "Secondly, as the target is to maximize the area under the convex hull (AUCH) rather than the hypervolume mentioned in SMS-EMOA, and area-based indicator is designed to calculate the contribution of each individual to AUCH maximization.", "One major of disadvantage of ( $ \\mu $ + 1) selection strategy was employed in SMS-EMOA and AG-EMOA is that it needs to call fast-nondominated sorting $ \\mu $ times to select $ \\mu $ offsprings.", "In [@bib:igel2007covariance] , an approximate scheme ( $ \\mu $ + $ \\mu $ ) is proposed to make the selection process faster, and this idea has been adopted in CH-MOGP.", "\\newline <subsection> <title> 3.1 Convex hull-based without redundancy sorting </title> <float> {Convex hull-based-sorting-without-redundancy} ( $ Q $ , $ r $ ) \\ $ Q\\neq\\emptyset $ \\ \\ $ Q $ is a solution set \\ \\ $ r $ is the reference point \\ \\ ch-based-sorting-without-redundancy \\ \\ $ i $ = 0 \\ \\ while $ Q\\neq\\emptyset $ do \\ \\ $ T $ = $ Q\\cup\\{r\\} $ \\ \\ $ \\textbf{F}_{i} $ = Jarvis-Algorithm( $ T $ ) [@bib:jarvis1973identification] \\ \\ $ \\textbf{F}_{i} $ = Elimination( $ \\textbf{F}_{i} $ ) //", "Some points in $ \\textbf{F}_{i} $ are not interesting and removed \\ \\ $ Q=Q-\\textbf{F}_{i} $ \\ \\ $ i=i+1 $ \\ \\ end while \\ </float> \\newline First of all, we introduce convex hull-based without redundancy sorting in this subsection.", "The main idea is too keep the diversity of the population by force, that means, each redundant solutions will be put into an archive to be random selected to survive into the next generation if there is not enough non-redundant solutions to fill the whole population full.", "Non-redundant solutions with not good performance have chance to be kept by discarding the redundant solutions with good performance to make high diversity, and this could avoid that the solutions at the convex hull being copied a lot at the selection phase in evolutionary multi-objective optimization.", "As described in Alg.", "[@ref:LABEL:algreduce] , the population will be split into redundant part and the other part which is sorted by convex hull-based sorting into several levels and the redundant part is taken as the last level which is the candidates by random selecting.", "\\newline In Fig. [@ref:LABEL:fig:chullrankingwithout] , the first and second graphs gives the illustration for convex hull-based sorting with and without redundancy.", "All the redundant individuals will be discarded into the last level and selected random to the next generation if it is necessary.", "\\newline <float> {DeltaArea} ( $ Q $ ) \\ $ Q\\neq\\emptyset $ \\ \\ $ Q $ is a solution set \\ \\ DeltaArea \\ \\ $ m=sizeof(Q) $ \\ \\ E is performance of $ Q $ \\ \\ $ \\textbf{DeltaH}_{1},...,\\textbf{DeltaH}_{m}\\leftarrow 0 $ \\ \\ if $ m<3 $ then \\ \\ Set $ \\textbf{DeltaH}_{1},...,\\textbf{DeltaH}_{m}\\leftarrow\\infty $ \\ \\ else \\ \\ Set $ \\textbf{DeltaH}_{1},\\textbf{DeltaH}_{m}\\leftarrow\\infty $ \\ \\ for $ 2\\leq i\\leq sizeof(Q)-1 $ do \\ \\ $ \\textbf{DeltaH}_{i} $ = 0.5 $ \\times $ det(( $ \\textbf{E}_{i} $ - $ \\textbf{E}_{i-1} $ ) $ \\circ $ ( $ \\textbf{E}_{i+1} $ - $ \\textbf{E}_{i-1} $ )) \\ \\ end for \\ \\ while $ sizeof(Q)>2 $ do \\ \\ $ r\\leftarrow argmin\\{\\textbf{DeltaH}\\} $ \\ \\ $ Q\\leftarrow Q\\texttt{\\char 92}\\{Q_{r}\\} $ \\ \\ Update( $ \\textbf{DeltaH}_{r-1} $ , $ \\textbf{DeltaH}_{r+1} $ ) \\ \\ end while \\ \\ end if \\ \\ Return ( DeltaH ) \\ </float> \\newline <float> {Reduce} ( $ Q $ , $ N $ ) \\ $ Q\\neq\\emptyset $ \\ \\ $ Q $ is a solution set \\ \\ $ N $ is the number of solutions will be discarded \\ \\ Reduce \\ \\ $ F=empty $ \\ \\ Split $ Q $ into two subpopulation $ U $ and $ R $ // $ R $ is the collection of redundant individuals \\ \\ if $ sizeof(R)>=N $ then \\ \\ $ F\\leftarrow $ Random select $ N $ solutions from $ R $ \\ \\ $ Q\\leftarrow U\\cup R\\texttt{\\char 92}F $ \\ \\ else \\ \\ $ F\\leftarrow R $ \\ \\ $ {\\Re_{1},...,\\Re_{v}}\\leftarrow $ $ Convexhull $ - $ based $ - $ sort $ - $ without $ - $ redundancy(Q) $ \\ \\ for $ i=v...1 $ do \\ \\ if $ sizeof(F) $ + $ sizeof(\\Re_{i})<N $ then \\ \\ $ F\\leftarrow F\\cup\\Re_{i} $ \\ \\ $ U=U\\texttt{\\char 92}\\Re_{i} $ \\ \\ else \\ \\ break \\ \\ end if \\ \\ end for \\ \\ $ T\\leftarrow Select $ $ (N-sizeof(F)) $ solutions with minial $ DeltaArea(\\Re_{i}) $ \\ \\ $ F\\leftarrow F\\cup T $ \\ \\ $ U\\leftarrow U\\texttt{\\char 92}T $ \\ \\ $ Q\\leftarrow U $ \\ \\ end if \\ \\ Return ( $ Q $ ) \\ </float> \\newline </subsection> <subsection> <title> 3.2 Area-based Selection Scheme </title> <equation> $ \\Delta area=\\frac{det((\\textbf{X}-\\textbf{L})\\circ(\\textbf{U}-\\textbf{X}))}{2} $ </equation> \\newline In this subsection, we describe our area-based indicator for selection scheme in the new EMOA.", "The reason for why area-based and not hypervolume-based contribution is adopt is we need to maximize the area under the convex hull.", "Area-based indicator is more directly and efficiently.", "In the third graph of Fig. [@ref:LABEL:fig:chullrankingwithout] , it shows the novel area calculation for two dimensions.", "The contribution of one point $ x $ with its performance vector X to the area is the area of triangle constructed by the point with its predecessor $ l $ and successor $ u $ with performance vector L and U .", "Alg. [@ref:LABEL:deltahypervolume] gives the procedure of calculating of the novel area contribution.", "Eq. [@ref:LABEL:equ:area] gives the equation to how to calculate the area contribution of each point to its convex hull front.", "\\newline <float> {CH-MOGP} ( $ Max,N $ ) \\ $ Max>0,N>0 $ \\ \\ $ Max $ is the maximum of evaluations \\ \\ $ N $ is the population size \\ \\ CH-MOGP \\ \\ $ P_{0}=init $ \\ \\ $ t=0 $ \\ \\ $ m=0 $ \\ \\ while $ m<Max $ do \\ \\ $ Q_{t}=empty $ \\ \\ for $ i=1:N $ do \\ \\ $ q_{i}\\leftarrow $ Operators on $ P_{t} $ \\ \\ $ Q_{t}\\leftarrow Q_{t}+q_{i} $ \\ \\ end for \\ \\ $ P_{t+1}\\leftarrow Reduce(P_{t}\\cup{Q_{t}}) $ \\ \\ $ t\\leftarrow t+1 $ \\ \\ $ m\\leftarrow m+N $ \\ \\ end while \\ </float> \\newline </subsection> <subsection> <title> 3.3 CH-MOGP </title> Alg. [@ref:LABEL:algchmoea] describes the CH-MOGP algorithm.", "The framework is very similar with SMS-EMOA and NSGA-II.", "However, we employ convex hull-based sorting without redundancy approach to rank the individuals into different levels.", "( $ \\mu+\\mu $ ) scheme is adopted into CH-MOGP to maximize the ROC performance.", "Because the target is to maximize area under the convex hull, area-based selection is designed insteading of hypervolume-based contribution to keep the survivors with high area-based contribution.", "\\newline In Alg. [@ref:LABEL:algchmoea] , first of all, the population size and the maximum of evaluations are given.", "Initial population is constructed by a group of solutions represented by genetic decision trees [@bib:jin2000fgp] using ramped-half-and-half method [@bib:poli2008field] .", "To generate the offsprings, two operators are employed and described in detail in [@bib:wang2011memetic] .", "The selection part of CH-MOGP are operated by two schemes like other EMOAs, one is how to sort the population into different levels and the other is how to rank the solutions at the same level.", "Convex hull-based without redundancy sorting and area-based selection scheme play the main role to the selection part of CH-MOGP.", "To reduce the time of calling sorting approach, we also take ( $ \\mu $ + $ \\mu $ ) scheme not ( $ \\mu $ + 1) in SMS-EMOA.", "\\newline </subsection>  </section>"], ["<section> <title> 4 Experimental Studies </title>  <subsection> <title> 4.1 Data Set </title> Nineteen data sets are selected from the UCI repository [@bib:WP27] and described in Table [@ref:LABEL:DataSets] ."]], "target": "Actually, we choose another three large-scaled data sets described in the last row of Table to make more solid results. In this paper, we focus on binary classification problems, so all the data sets are two-class problems. Balanced and imbalanced benchmark data sets are carefully selected. The scale in terms of the number of instances of these data sets ranges from hundreds to thousands."}, {"tabular": ["  model  &  D  &  D+2  &  D+4  &  D+6  &  D+8 ", " top-1  &  34.5  &  34.0  &  33.9  &  34.0  &  34.2 ", " top-5  &  13.9  &  13.6  &  13.4  &  13.5  &  13.6  "], "ref_sec": [["<section> <title> 1 Introduction </title>  Convolutional neural networks (CNNs) [@bib:LeCun1989,Krizhevsky2012] have recently brought in revolutions to the computer vision area.", "Deep CNNs not only have been continuously advancing the image classification accuracy [@bib:Krizhevsky2012,Sermanet2014,Zeiler2014,Chatfield2014,He2014,Simonyan2014,Szegedy2014] , but also play as generic feature extractors for various recognition tasks such as object detection [@bib:Girshick2014,He2014] , semantic segmentation [@bib:Girshick2014,Hariharan2014] , and image retrieval [@bib:Krizhevsky2012,Razavian2014] .", "\\newline Most of the recent advanced CNNs are more time-consuming than Krizhevsky \\etal \u2019s [@bib:Krizhevsky2012] original architecture in both training and testing.", "The increased computational cost can be attributed to the increased width (numbers of filters) [@bib:Sermanet2014,Zeiler2014,Chatfield2014] , depth (number of layers) [@bib:Simonyan2014,Szegedy2014] , smaller strides [@bib:Sermanet2014,Zeiler2014,Simonyan2014] , and their combinations.", "Although these time-consuming models are worthwhile for advancing the state of the art, they can be unaffordable or unnecessary for practical usages.", "For example, an on-line commercial search engine needs to response to a request in real-time; a cloud service is required to handle thousands of user-submitted images per second; even for off-line processes like web-scale image indexing, the system needs to handle tens of billions of images in a few days.", "Increasing the computational power of the hardware can partially relief these problems, but will take very expensive commercial cost.", "Furthermore, on smartphones or portable devices, the low computational power (CPUs or low-end GPUs) limits the speed of the real-world recognition applications.", "{So in industrial and commercial scenarios, engineers and developers are often faced with the requirement of constrained time budget} .", "\\newline Besides the test-time demands, the off-line training procedure can also be constrained by affordable time cost.", "The recent models [@bib:Chatfield2014,He2014,Simonyan2014,Szegedy2014] take a high-end GPU or multiple GPUs/clusters one week or several weeks to train, which can sometimes be too demanding for the rapidly changing industry.", "Moreover, even if the purpose is purely for pushing the limits of accuracy (like for the ImageNet competition [@bib:Russakovsky2014] ), the maximum tolerable training time is still a major bottleneck for experimental research.", "While the time budget can be loose in this case, it is worthwhile to understand which factors can gain more improvement.", "\\newline This paper investigates the accuracy of CNN architectures at constrained time cost during both training and testing stages.", "Our investigations involve the depth, width, filter sizes, and strides of the architectures.", "Because the time cost is constrained, the differences among the architectures must be exhibited as trade-offs between those factors.", "For example, if the depth is increased, the width and/or filter sizes need to be properly reduced.", "In the core of our designs is \u201clayer replacement\u201d - a few layers are replaced with some other layers that preserve time cost.", "Based on this strategy, we progressively modify a model and investigate the accuracy through a series of controlled experiments.", "This not only results in a more accurate model with the same time cost as a baseline model, but also facilitates the understandings of the impacts of different factors to the accuracy.", "\\newline From the controlled experiments, we draw the following empirical observations about the depth.", "(1) The network depth is clearly of high priority for improving accuracy, {even if the width and/or filter sizes are reduced to compensate the time cost} .", "This is not a straightforward observation even though the benefits of depth have been recently demonstrated [@bib:Simonyan2014,Szegedy2014] , because in previous comparisons [@bib:Simonyan2014] the extra layers are added without trading off other factors, and thus increase the complexity.", "(2) While the depth is important, the accuracy gets stagnant or even degraded if the depth is overly increased.", "This is observed even if width and/filter sizes are not traded off (so the time cost increases with depth).", "\\newline Through the investigations, we obtain a model that achieves 11.8% top-5 error (10-view test) on ImageNet [@bib:Deng2009] and only takes 3 to 4 days training on a single GPU.", "Our model is more accurate and also faster than several competitive models in recent papers [@bib:Zeiler2014,Chatfield2014,He2014] .", "Our model has 40% less complexity than \u201c {AlexNet} \u201d [@bib:Krizhevsky2012] and 20% faster actual GPU speed, while has 4.2% lower top-5 error.", "\\newline  </section>"], ["<section> <title> 2 Related Work </title>  Recently there has been increasing attention on accelerating the test-time speed of CNNs [@bib:Mamalet2012,Denton2014,Jaderberg2014] .", "These methods approximate and simplify the trained networks, with some degradation on accuracy.", "These methods do not address the training time because they are all post-processes of the trained networks.", "Besides, when the testing time budget is given by demand, it is still desirable to find the pre-trained model subject to certain time constraints, because the speedup ratio of these acceleration methods is limited.", "These methods should also be applicable in our models, further speeding up the testing stage.", "\\newline Constraining the network complexity is a way of understanding the impacts of the factors in the network designs.", "In [@bib:Eigen2013] , the accuracy of tied/untied CNNs is evaluated with various width, depth, and numbers of parameters.", "The tied (recursive) network is designed for strictly fixing the number of parameters.", "In contrast, our paper investigates the accuracy while fixing the time complexity.", "Our paper focuses on the untied (non-recursive) CNNs trained on ImageNet, which can be more useful as generic feature extractors.", "We also investigate factors like filter sizes and strides.", "\\newline Most recently, Szegedy \\etal [@bib:Szegedy2014] propose the \u201cinception\u201d layer.", "Their model achieves the best accuracy in ILSVRC 2014 [@bib:Russakovsky2014] while its theoretical complexity is merely 50% more than AlexNet.", "The inception layer is a \u201cmulti-path\u201d design that concatenates several convolutional layers of various filter sizes/numbers.", "The principal of choosing these hyper-parameters requires further investigation, and the influence of each branch remains unclear.", "In our paper, we only consider \u201csingle-path\u201d designs with no parallel convolutional layers, which are already faced with abundance of choices.", "\\newline  </section>"], ["<section> <title> 3 Prerequisites </title>  <subsection> <title> 3.1 A Baseline Model </title> Our investigation starts from an eight-layer model similar to an Overfeat model [@bib:Sermanet2014] that is also used in [@bib:Chatfield2014,He2014] .", "It has five convolutional ( {conv} ) layers and three fully-connected ( {fc} ) layers.", "The input is a 224 $ \\times $ 224 color image with mean subtracted.", "The first convolutional layer has 64 7 $ \\times $ 7 filters with a stride 2, followed by a 3 $ \\times $ 3 max pooling layer with a stride 3.", "The second convolutional layer has 128 5 $ \\times $ 5 filters, followed by a 2 $ \\times $ 2 max pooling layer with a stride 2.", "The next three convolutional layers all have 256 3 $ \\times $ 3 filters.", "A spatial pyramid pooling (SPP) layer [@bib:He2014] is used after the last convolutional layer.", "The last three layers are two 4096-d fc layers and a 1000-d fc layer, with softmax as the output.", "All the convolutional/fc layers (except the last fc) are with the Rectified Linear Units (ReLU) [@bib:Nair2010,Krizhevsky2012] .", "We do not apply local normalization.", "The details are in Table [@ref:LABEL:tab:models] (A).", "This model is \u201cnarrower\u201d (with fewer numbers of filters) than most previous models [@bib:Krizhevsky2012,Howard2013,Sermanet2014,Zeiler2014,He2014] .", "\\newline We train the model on the 1000-category ImageNet 2012 training set [@bib:Deng2009,Russakovsky2014] .", "The details of training/testing are in Sec. [@ref:LABEL:sec:impl] , which mostly follow the standards in [@bib:Krizhevsky2012] .", "We train this model for 75 epochs, which take about 3 days.", "The top-1/top-5 error is 37.4/15.9 using the 10-view test [@bib:Krizhevsky2012] .", "\\newline In the following we will design new models with the same time complexity as this model.", "We start from this model due to a few reasons.", "Firstly, this model mostly follows the popular \u201c3-stage\u201d designs as in [@bib:Krizhevsky2012,Howard2013,Sermanet2014,Zeiler2014,He2014] - the first stage is a single convolutional layer having a few number of large-size filters (7 $ \\times $ 7 or 11 $ \\times $ 11) with pooling, the second stage is a single convolutional layer with 5 $ \\times $ 5 filters with pooling, and the third stage is a cascade of 3 $ \\times $ 3 convolutional layers.", "So we expect that the observations in this paper will apply for other similar models.", "Secondly, our baseline model has fewer filters than most previous models, so it is faster for training/testing.", "\\newline Nevertheless, even though in this paper we are based on this model, some of the following observations should remain mostly valid, because the observations are drawn from several variants of the models.", "\\newline </subsection> <subsection> <title> 3.2 Time Complexity of Convolutions </title> The total time complexity of all convolutional layers is: \\newline <equation> $ O\\left(\\sum_{l=1}^{d}n_{l-1}\\cdot s_{l}^{2}\\cdot n_{l}\\cdot m_{l}^{2}\\right) $ </equation> Here $ l $ is the index of a convolutional layer, and $ d $ is the depth (number of convolutional layers).", "$ n_{l} $ is the number of filters (also known as \u201cwidth\u201d) in the $ l $ -th layer.", "$ n_{l-1} $ is also known as the number of input channels of the $ l $ -th layer.", "$ s_{l} $ is the spatial size (length) of the filter.", "$ m_{l} $ is the spatial size of the output feature map.", "Note this time complexity applies to both training and testing time, though with a different scale.", "The training time per image is roughly three times of the testing time per image (one for forward propagation and two for backward propagation).", "\\newline The time cost of fc layers and pooling layers is not involved in the above formulation.", "These layers often take 5-10% computational time.", "Without further complicating the investigation, we fix the input/output dimensions of the fc and pooling layers in all the models.", "We only consider the trade-offs among the convolutional layers.", "\\newline The theoretical time complexity in Eqn.", "( [@ref:LABEL:eq:time] ), rather than the actual running time, will be the base of our network designs, because the actual running time can be sensitive to implementations and hardware.", "Even so, most of our models in this paper have actual running time that scales nicely with the theoretical complexity.", "\\newline </subsection>  </section>"], ["<section> <title> 4 Model Designs by Layer Replacement </title>  Designing a model under a constrained complexity is a complicated problem, because there are several factors involved in Eqn.", "( [@ref:LABEL:eq:time] ).", "We simplify the cases by designing a series of \u201clayer replacement\u201d - at each time a few layers are replaced by some other layers that preserve complexity, without changing the rest layers.", "For the design of a layer replacement, we study the trade-offs between two factors with the rest factors unchanged.", "With all these put together, we progressively modify the models and investigate the changes of accuracy through controlled experiments.", "\\newline Without further complicating the cases, we will mainly trade off the factors inside a \u201cstage\u201d - a \u201cstage\u201d is defined as those layers between two nearby pooling layers.", "We will fix the numbers of output filters of each stage, so also fix the numbers of input channels to the next stage.", "\\newline <subsection> <title> 4.1 Trade-offs between Depth and Filter Sizes </title> We first investigate the trade-offs between depth $ d $ and filter sizes $ s $ .", "We replace a larger filter with a cascade of smaller filters.", "We denote a layer configuration as: \\newline <equation> $ n_{l-1}\\cdot s_{l}^{2}\\cdot n_{l} $ </equation> which is also its theoretical complexity (with the feature map size temporarily omitted).", "An example replacement can be written as the complexity involved: \\newline <equationgroup> <equation> $  256\\cdot 3^{2}\\cdot 256 $ $  256\\cdot 3^{2}\\cdot 256 $ </equation> <equation> $ \\Rightarrow 256\\cdot 2^{2}\\cdot 256+256\\cdot 2^{2}\\cdot 256.", "$ $ \\Rightarrow $ $  256\\cdot 2^{2}\\cdot 256+256\\cdot 2^{2}\\cdot 256.", "$ </equation> </equationgroup> This replacement means that a 3 $ \\times $ 3 layer with 256 input/output channels is replaced by two 2 $ \\times $ 2 layers with 256 input/output channels.", "After the above replacement, the complexity involved in these layers is nearly unchanged (slightly reduces as $ (2^{2}+2^{2})/(3^{2})=8/9 $ ).", "The model B in Table [@ref:LABEL:tab:models] is from the model A using this replacement.", "\\newline Similarly, we replace the 5 $ \\times $ 5 filters by two 3 $ \\times $ 3 filters.", "The complexity involved is: \\newline <equationgroup> <equation> $  64\\cdot 5^{2}\\cdot 128 $ $  64\\cdot 5^{2}\\cdot 128 $ </equation> <equation> $ \\Rightarrow 64\\cdot 3^{2}\\cdot 128+128\\cdot 3^{2}\\cdot 128.", "$ $ \\Rightarrow $ $  64\\cdot 3^{2}\\cdot 128+128\\cdot 3^{2}\\cdot 128.", "$ </equation> </equationgroup> This replacement also approximately preserves the complexity (increasing by $ \\sim $ 8% of this layer).", "The models C and D in Table [@ref:LABEL:tab:models] are from A and B using this replacement.", "We can further replace the 5 $ \\times $ 5 filters by four 2 $ \\times $ 2 filters: \\newline <equationgroup> <equation> $  64\\cdot 5^{2}\\cdot 128 $ $  64\\cdot 5^{2}\\cdot 128 $ </equation> <equation> $ \\Rightarrow 64\\cdot 2^{2}\\cdot 128+(128\\cdot 2^{2}\\cdot 128)% \\times 3.", "$ $ \\Rightarrow $ $  64\\cdot 2^{2}\\cdot 128+(128\\cdot 2^{2}\\cdot 128)\\times 3.", "$ </equation> </equationgroup> The model E in Table [@ref:LABEL:tab:models] is from B with this replacement.", "\\newline In case when 2 $ \\times $ 2 filters are used, the feature map size cannot be preserved strictly.", "To address this issue, we consider two sequential 2 $ \\times $ 2 layers: we use no padding in the first 2 $ \\times $ 2 layer, and pad one pixel (on each side) in the next 2 $ \\times $ 2 layer.", "This reduces the feature map size by one pixel after the first layer, but restores the original feature map size after the second layer.", "This also slightly reduces the complexity of using 2 $ \\times $ 2 layers due to the reduced feature map sizes in the first layer.", "Because we cannot strictly preserve time complexity, we also show the complexity (relative to A) of the models in Table [@ref:LABEL:tab:models] .", "\\newline Fig. [@ref:LABEL:fig:depth_size] summarizes the relations among the models A, B, C, D, and E. Table [@ref:LABEL:tab:models] shows their detailed structures and the top-1/top-5 errors (10-view test).", "These results show that the depth is more important than the filter sizes.", "When the time complexity is roughly the same, the deeper networks with smaller filters show better results than the shallower networks with larger filters.", "\\newline </subsection> <subsection> <title> 4.2 Trade-offs between Depth and Width </title> Next we investigate the trade-offs between depth $ d $ and width $ n $ .", "We increase depth while properly reducing the number of filters per layer, without changing the filter sizes.", "We replace the three 3 $ \\times $ 3 layers in A with six 3 $ \\times $ 3 layers (model F).", "The complexity involved is: \\newline <equationgroup> <equation> $  128\\cdot 3^{2}\\cdot 256+(256\\cdot 3^{2}\\cdot 256)\\times 2 $ $  128\\cdot 3^{2}\\cdot 256+(256\\cdot 3^{2}\\cdot 256)\\times 2 $ </equation> <equation> $ =128\\cdot 3^{2}\\cdot 160+(160\\cdot 3^{2}\\cdot 160)\\times 4+160% \\cdot 3^{2}\\cdot 256.", "$ $ = $ $  128\\cdot 3^{2}\\cdot 160+(160\\cdot 3^{2}\\cdot 160)\\times 4+160% \\cdot 3^{2}\\cdot 256.", "$ </equation> </equationgroup> Here we fix the number of the input channels (128) of the first layer, and the number of output filters (256) of the last layer, so avoid impacting the previous/next stages.", "With this replacement, the width reduces from 256 to 160 (except the last layer).", "This replacement is designed such that it exactly preserves the complexity, so we use the notation \u201c $ = $ \u201d.", "\\newline We can also replace the three 3 $ \\times $ 3 layers in A with nine 3 $ \\times $ 3 layers (model G): \\newline <equationgroup> <equation> $  128\\cdot 3^{2}\\cdot 256+(256\\cdot 3^{2}\\cdot 256)\\times 2 $ $  128\\cdot 3^{2}\\cdot 256+(256\\cdot 3^{2}\\cdot 256)\\times 2 $ </equation> <equation> $ =(128\\cdot 3^{2}\\cdot 128)\\times 8+128\\cdot 3^{2}\\cdot 256.", "$ $ = $ $ (128\\cdot 3^{2}\\cdot 128)\\times 8+128\\cdot 3^{2}\\cdot 256.", "$ </equation> </equationgroup> This replacement also exactly preserves the complexity.", "\\newline We also consider a replacement in stage 2 on the models C and D. We replace the two 3 $ \\times $ 3 layers by four layers.", "The complexity involved is \\newline <equationgroup> <equation> $  64\\cdot 3^{2}\\cdot 128+128\\cdot 3^{2}\\cdot 128 $ $  64\\cdot 3^{2}\\cdot 128+128\\cdot 3^{2}\\cdot 128 $ </equation> <equation> $ \\Rightarrow(64\\cdot 3^{2}\\cdot 64)\\times 3+64\\cdot 3^{2}\\cdot 128.", "$ $ \\Rightarrow $ $ (64\\cdot 3^{2}\\cdot 64)\\times 3+64\\cdot 3^{2}\\cdot 128.", "$ </equation> </equationgroup> This gives the models H and I. \\newline Fig. [@ref:LABEL:fig:depth_width] summarizes the relations among the models of A, F, G, H, and I. Table [@ref:LABEL:tab:models] shows the details and the top-1/top-5 error.", "We find that increasing the depth leads to considerable gains, even the width needs to be properly reduced.", "The models F and G has 14.8 and 14.7 top-5 error respectively, much lower than the error of A (15.9).", "The models H and I also have lower error than C and D. \\newline It is worth noticing that G can also be considered as a deeper and narrower version of F (see Fig. [@ref:LABEL:fig:depth_width] ).", "But G is only better than F marginally.", "The improvement due to increased depth becomes saturated.", "We will further investigate a related issue in Sec. [@ref:LABEL:sec:deeper] .", "\\newline </subsection> <subsection> <title> 4.3 Trade-offs between Width and Filter Sizes </title> We can also fix the depth and investigate the trade-offs between width and filter sizes.", "Some of the above models can actually be considered as undergone such replacement.", "\\newline The models B and F exhibit this trade-off on the last six convolutional layers: \\newline <equationgroup> <equation> $  128\\cdot 2^{2}\\cdot 256+(256\\cdot 2^{2}\\cdot 256)\\times 5 $ $  128\\cdot 2^{2}\\cdot 256+(256\\cdot 2^{2}\\cdot 256)\\times 5 $ </equation> <equation> $ \\Rightarrow 128\\cdot 3^{2}\\cdot 160+(160\\cdot 3^{2}\\cdot 160)% \\times 4+160\\cdot 3^{2}\\cdot 256.", "$ $ \\Rightarrow $ $  128\\cdot 3^{2}\\cdot 160+(160\\cdot 3^{2}\\cdot 160)\\times 4+160% \\cdot 3^{2}\\cdot 256.", "$ </equation> </equationgroup> This means that the first five 2 $ \\times $ 2 layers with 256 filters are replaced with five 3 $ \\times $ 3 layers with 160 filters.", "The errors are comparable: 35.7/14.9 for B and 35.5/14.8 for F. \\newline Similarly, the models E and I have this trade-off: \\newline <equationgroup> <equation> $ (64\\cdot 3^{2}\\cdot 64)\\times 3+64\\cdot 3^{2}\\cdot 128 $ $ (64\\cdot 3^{2}\\cdot 64)\\times 3+64\\cdot 3^{2}\\cdot 128 $ </equation> <equation> $ \\Rightarrow 64\\cdot 2^{2}\\cdot 96+96\\cdot 2^{2}\\cdot 128+(128% \\cdot 2^{2}\\cdot 128)\\times 2.", "$ $ \\Rightarrow $ $  64\\cdot 2^{2}\\cdot 96+96\\cdot 2^{2}\\cdot 128+(128\\cdot 2^{2}% \\cdot 128)\\times 2.", "$ </equation> </equationgroup> This means that the four 3 $ \\times $ 3 layers with 64 filters (128 in the last) are replaced with four 2 $ \\times $ 2 layers with 128 filters.", "The top-1/top-5 errors are also comparable (33.8/13.3 for E and 33.9/13.5 for I).", "\\newline Fig. [@ref:LABEL:fig:width_size] shows the relations of these models.", "Unlike the depth that has a high priority, the width and filter sizes (3 $ \\times $ 3 or 2 $ \\times $ 2) do not show apparent priorities to each other.", "\\newline </subsection> <subsection> <title> 4.4 Is Deeper Always Better? </title> The above results have shown the priority of depth for improving accuracy.", "With the above trade-offs, we can have a much deeper model if we further decrease width/filter sizes and increase depth.", "However, in experiments we find that the accuracy is stagnant or even reduced in some of our very deep attempts.", "There are two possible explanations: (1) the width/filter sizes are reduced overly and may harm the accuracy, or (2) overly increasing the depth will degrade the accuracy even if the other factors are not traded.", "To understand the main reason, {in this subsection we do not constrain the time complexity} but solely increase the depth without other changes.", "\\newline We add several duplicates of the last convolutional layers on the model D. Each extra layer has 256 2 $ \\times $ 2 filters.", "The other layers are not modified, so the time cost will be increased."]], "target": "Table shows the error \\vs the depth. We find that the errors not only get saturated at some point, but get worse if going deeper. We also find that the degradation is not due to over-fitting, because the training errors are also clearly worse. Similar phenomena are also observed when 3 $ \\times $ 3 filters are added in the models C and G."}, {"tabular": ["  Phase  &  Density ( $ \\mathrm{\\AA}^{-3}) $  &  Temperature (K) ", " Solid  &  $ 3.040\\times 10^{-2} $  &  59.90 ", " Liquid  &  $ 2.026\\times 10^{-2} $  &  119.8 ", " Gas  &  $ 7.599\\times 10^{-3} $  &  359.4  "], "ref_sec": [["<section> <title> 1 Introduction </title>  The size, scope, and realism of simulations of physical systems continue to grow with increases in computing power, but not without new challenges for developers of simulation codes.", "Increases in computing capability seem only to come with such additional burdens as distributed memory and distributed (and, more recently, heterogeneous) processing capacity.", "From the perspective of working physicists, it can seem that the physics itself recedes ever further away as their efforts are channeled towards developing and tailoring codes to the specific features of these increasingly complex high-performance machines.", "In this environment, the availability of well-designed codes with broadly applicable physics capabilities is increasingly valuable to researchers.", "\\newline Even physicists working directly on code development prefer to focus on numerical algorithms, problem setup and testing, shepherding production runs to completion, and data analysis and interpretation; but large-scale physics simulations on distributed-memory supercomputers require more basic utilitarian functionality as well.", "Examples include handling of physical units and constants; display to the screen or standard output device; message passing; I/O to disk; and runtime parameter management and usage statistics.", "A distributed-memory message passing environment complicates many of these tasks.", "While necessary for production simulations, this sort of functionality is not central to the physics problem to be simulated, and therefore is often treated in an ad-hoc way as an afterthought or annoying nuisance\u2014sometimes with inconsistent results and time-consuming consequences.", "\\newline Like other developers of physics simulation codes, we have had to implement such utilitarian functionality in our code GenASiS ( Gen eral A strophysical Si mulation S ystem), a new code under development for the simulation of core-collapse supernovae on the world\u2019s leading capability supercomputers.", "Initial capabilities of GenASiS for a refinable mesh and hydrodynamics from an astrophysics perspective, with emphasis on test results rather than code features, have been reported elsewhere [@bib:Cardall2014] .", "\u2018General\u2019 denotes the capacity of the code to include and refer to multiple algorithms, solvers, and physics and numerics choices with the same abstracted names and/or interfaces.", "In GenASiS this is accomplished with features of Fortran 2003 that support the object-oriented programming paradigm (e.g. [@bib:Reid2007] ).", "\u2018Astrophysical\u2019 roughly suggests\u2014over-broadly, at least initially\u2014the types of systems at which the code is aimed, and", "the kinds of physics and solvers it will include. \u2018Simulation System\u2019 indicates that the code is not a single program, but a collection of modules, organized as classes, that can be invoked by a suitable driver program set up to characterize and initialize a particular problem.", "While we are initially and primarily developing and using GenASiS for astrophysics problems, our goal is to address even the most utilitarian parts of the code with enough care and generality to hopefully make them useful for physics simulations in other fields.", "\\newline The object-oriented approach we apply to even the basic utilitarian parts of the code facilitates the generality, extensibility, and maintainability that potentially make portions of GenASiS useful beyond our initial target application.", "Object-oriented programming involves several interrelated principles.", "Abstraction identifies the major concepts required by a program, without specifying the details of implementation.", "Encapsulation bundles together and controls access to the data (or members ) and actions (or methods ) associated with a particular concept into a self-contained unit\u2014a class .", "(The class itself is just a kind of template or glorified type specification; an instance of the class, basically a variable declared with that type specification, is called an object .", "In terms of an analogy with intrinsic data types, a class is to an object as the intrinsic data type real is to a variable declared as real .) Done well, abstraction and encapsulation lead to decoupling \u2014the separation of code into building blocks that are as independent and reusable as possible.", "Reusability is further enhanced by polymorphism , which enables multiple versions or implementations of the same basic concept to be referred to and used interchangeably.", "Closely related is inheritance, which allows new ( child ) classes to be formed from prior ( parent ) classes: existing members and methods are retained or modified, and new members and methods can be added.", "\\newline The high-level structure of the core of GenASiS is sketched in Fig. [@ref:LABEL:fig:GenASiS_Structure] .", "\\newline Solid lines outline relationships in the source code directory hierarchy, and dashed arrows indicate compilation dependencies.", "Modules and Programs are the two highest-level divisions shown.", "Modules comprises the classes forming the central functionality of GenASiS .", "Each class is defined in a single Fortran module .", "Fortran program s\u2014drivers that invoke these classes, and begin the execution of some particular computational task\u2014are collected under Programs .", "\\newline Modules , and the divisions UnitTests and Examples under Programs , all contain divisions labeled Physics , Mathematics , and Basics .", "On the Modules side, these categories contain class implementations.", "Dual to these under UnitTests on the Programs side, in almost complete one-to-one correspondence, are \u2018unit tests\u2019 that exercise the capabilities and provide example usage of the individual classes.", "The programs in the Physics , Mathematics , and Basics divisions under Examples integrate the use of many classes in their respective divisions.", "For instance, the example problems discussed in Sec. [@ref:LABEL:sec:ExampleProblems] are found in the Basics division of Examples .", "As indicated by the dashed dependency arrows, the unit tests and integrative examples first depend upon their corresponding classes: Basics examples and unit tests depend on Basics classes; Mathematics examples and unit tests depend on Mathematics classes, and through these also on Basics classes; and so on.", "\\newline Programs also has another division\u2014 Applications \u2014intended for drivers aimed at purposes beyond unit tests and integrative examples, ranging from the solution of simple physical test problems to the execution of production-scale multiphysics research simulations.", "These ultimately depend on all Modules \u2014 Physics , Mathematics , and Basics .", "\\newline Our main purpose in this paper is to describe and make available the Basics division of GenASiS , which contains some utilitarian functionality for large-scale simulations on distributed-memory supercomputers.", "(For the place of Basics in the overall scheme of GenASiS , refer again to Fig. [@ref:LABEL:fig:GenASiS_Structure] .) \\newline Its content, as illustrated in the left diagram of Fig. [@ref:LABEL:fig:Basics_Structure] , includes the divisions VariableManagement , Display , MessagePassing , FileSystem , and Runtime .", "Two of these\u2014 Display and Runtime \u2014are framed with boxes of thinner linewidth; these are \u2018leaf\u2019 divisions in the sense that they contain no further subdirectories, but only individual files defining classes.", "The right diagram in Fig. [@ref:LABEL:fig:Basics_Structure] shows the structure within VariableManagement , which includes the leaf divisions Specifiers , ArrayOperations , ArrayArrays , and VariableGroups .", "The middle right diagram in Fig. [@ref:LABEL:fig:Basics_Structure] shows the structure within MessagePassing , which includes the leaf divisions MessagePassingBasics , PointToPoint , and Collective .", "The middle left diagram in Fig. [@ref:LABEL:fig:Basics_Structure] shows the structure within FileSystem , which includes the leaf divisions FileSystemBasics , GridImageBasics , CurveImages , StructuredGridImages , UnstructuredGridImages , and PointGridImages .", "In these illustrations the compilation order is from bottom to top; thus the dependencies essentially flow in reverse, from top to bottom.", "\\newline Before proceeding further we say a bit more about the way in which the classes in GenASiS are arranged and accessed.", "In discussing the organization of GenASiS \u2014sketched in part in Figs. [@ref:LABEL:fig:GenASiS_Structure] and [@ref:LABEL:fig:Basics_Structure] \u2014we refer to the boxed entities as \u2018divisions\u2019 of the code.", "They are not classes, but subdirectories in the source code tree; thus they comprise and compose hierarchically arranged groups of classes.", "However, even though these divisions are not classes per se, we nevertheless mirror their hierarchical structure in Fortran module s that make groups of classes more convenient to access, as follows.", "In each directory there is a file defining a Fortran module whose name corresponds to the division or directory, and which contains all the classes (for a leaf division) or groups of classes (for a non-leaf division) within that directory.", "For instance, in the Specifiers directory (see the right diagram in Fig. [@ref:LABEL:fig:Basics_Structure] ) there is a file Specifiers.f90 defining a module Specifiers containing Fortran use statements invoking each individual class in that directory.", "Then the classes in the ArrayOperations directory (the next to be compiled) need only the statement use Specifiers to access all the classes in that grouping.", "Similarly, the directory VariableManagement contains a file VariableManagement.f90 as follows: \\newline \\ module VariableManagement \\ \\ use Specifiers \\ \\ use ArrayOperations \\ \\ use ArrayArrays \\ \\ use VariableGroups \\ \\ end module VariableManagement \\ Then, in the classes grouped under, for example, Display (see the left diagram in Fig. [@ref:LABEL:fig:Basics_Structure] ), the statement use VariableManagement suffices to access everything contained within that division.", "\\newline We describe the functionality available through Basics in the context of example problems, which also allow us to illustrate our object-oriented approach utilizing features of Fortran 2003.", "These examples and our object-oriented approach are presented in Section [@ref:LABEL:sec:ExampleProblems] .", "Basics functionality is illustrated more particularly and systematically in Section [@ref:LABEL:sec:BasicsFunctionality] .", "Section [@ref:LABEL:sec:Building] provides instructions for compiling and building the examples and unit test programs.", "We conclude in Section [@ref:LABEL:sec:Conclusion] .", "\\newline  </section>"], ["<section> <title> 2 Example Problems and Object-Oriented Solutions </title>  We present two categories of nontrivial example problems for which solutions are built upon GenASiS Basics functionality: fluid dynamics and molecular dynamics.", "These are fundamentally different models, requiring the solution of different equations, using different techniques and different parallelization strategies.", "Nevertheless, GenASiS Basics serves as an excellent basis for coding solutions in both cases.", "In developing these examples we foreshadow future releases of the Mathematics and Physics portions of GenASiS by illustrating in a simple way the object-oriented mechanisms of inheritance and polymorphism that we use to separate lower-level coding of generic, reusable solvers from higher-level coding for specific physical systems.", "\\newline In the case of the first example problem described below we present some explicit code.", "We do not show every detail, but only selected snippets that illustrate some points we wish to make about our object-oriented approach and, in Section [@ref:LABEL:sec:BasicsFunctionality] , the functionality available in GenASiS Basics classes.", "All example programs are included in the accompanying submission to the CPC Program Library.", "\\newline <subsection> <title> 2.1 Fluid Dynamics </title> We present two kinds of fluid dynamics problems.", "The first is the periodic advection of a plane wave in mass density in one, two, or three position space dimensions (1D, 2D, 3D).", "The second is a Riemann problem (also 1D, 2D, 3D) which ventures beyond periodic advection to full fluid evolution with shocks and reflecting boundary conditions.", "\\newline A few underlying classes provide a generic mathematical foundation for both types of fluid dynamics problems.", "These are: (a) a mesh\u2014a discretization of space into cubic \u2018cells\u2019\u2014that can be domain-decomposed into \u2018bricks\u2019 assigned to the several processes of a distributed-memory parallel program ( DistributedMeshForm ); (b) a template for a set of conserved fields, associated primitive and auxiliary fields, and some basic routines associated with them ( ConservedFieldsTemplate ); (c) a finite-volume conservation law stepper ( ConservationLawStepForm ); and (d) a template for the evolution of a set of conserved fields, for many time steps over a finite time ( ConservationLawEvolutionTemplate ).", "We implement each of these with a Fortran module defining a \u2018class\u2019 as understood in the object-oriented programming paradigm.", "Subsequent classes extend and use these to flesh out the details of particular physical models, and set up and solve particular problems; see Fig. [@ref:LABEL:fig:FluidDynamics] .", "\\newline As it would be outside our main scope and purpose here\u2014which is to introduce GenASiS Basics functionality\u2014we do not discuss these fluid dynamics classes or the numerical methods they implement in any detail, though some snippets will be shown in Sec. [@ref:LABEL:sec:BasicsFunctionality] as we illustrate in more systematic and bottom-up fashion the functionality made available in Basics .", "The full example codes are included in the accompanying submission to the CPC Program Library.", "By way of summary, we note that the heart of the finite-volume solver, ConservationLawStepForm , uses piecewise linear reconstruction with an MC (monotonized central) slope limiter to get cell face values; an HLL (Harten-Lax-van Leer) Riemann solver to obtain fluxes from the left and right values at cell interfaces; and a second-order Runge-Kutta time step.", "Information and references on these aspects can be found in Ref.", "[@bib:Cardall2014] .", "\\newline <subsubsection> <title> 2.1.1 Plane Wave Advection </title> In order to simulate the periodic advection of a plane wave in mass density, for two different waveforms, we need to implement classes further specifying the necessary physics and setting up these particular problems.", "Referring again to Fig. [@ref:LABEL:fig:FluidDynamics] , and continuing the enumeration above, these are (e) a realization of ConservedFieldsTemplate for the specific case of a pressureless fluid ( PressurelessFluidForm ); (f) an extension of ConservationLawEvolutionTemplate to a template for the evolution of a generic plane wave ( PlaneWaveAdvectionTemplate ); and (g) realizations of PlaneWaveAdvectionTemplate for two different waveforms ( SineWaveAdvectionForm and SawtoothWaveAdvectionForm ).", "\\newline We gain perspective with a top-down discussion of this example, beginning with the program outlined in Listing [@ref:LABEL:lst:Program_SWA_Outline] , which performs the advection of a sine wave.", "\\newline <float> Outline of program SineWaveAdvection .", "\\ program SineWaveAdvection \\ \\ \\ use Basics \\ \\ use SineWaveAdvection_Form !", "\u2013 See Listing \\ ref {lst : Module_SWA_Outline } \\ \\ implicit none \\ \\ \\ type ( SineWaveAdvectionForm ) :: SWA !\u2013 See Listing \\ ref {lst : Module_SWA_Outline } \\ \\ \\ !", "\u2013 Program initialization omitted ; see Section \\ ref {sec : Runtime } \\ \\ \\ c all SWA % Initialize !\u2013 See Listings \\ ref {lst : Module_SWA_Outline } and \\ ref {lst : Subroutine_Initialize_SWA } \\ \\ c all SWA % Evolve !\u2013 See Listing \\ ref {lst : Module_CLE_Outline } \\ \\ \\ !\u2013 Program finalization omitted \\ \\ \\ end program SineWaveAdvection \\ </float> \\newline The statement use Basics in line 3 gives access to all the classes in the code divisions pictured in Fig. [@ref:LABEL:fig:Basics_Structure] .", "In line 7 we declare an object SWA of class SineWaveAdvectionForm , whose definition is available through the use statement in line 4.", "The subroutine calls in lines 11 and 12 are invocations of methods belonging to the object SWA .", "\\newline Evidently we must consider the class SineWaveAdvectionForm defined in the module sketched in Listing [@ref:LABEL:lst:Module_SWA_Outline] to begin to see how the highest-level tasks\u2014initializing and evolving the problem\u2014are implemented.", "\\newline <float> Outline of module SineWaveAdvection_Form .", "Used at line 4 of Listing [@ref:LABEL:lst:Program_SWA_Outline] .", "\\ module SineWaveAdvection_Form \\ \\ \\ use Basics \\ \\ use PlaneWaveAdvection_Template !", "\u2013 See Listing \\ ref {lst : Module_PWA_Outline } \\ \\ implicit none \\ \\ private \\ \\ \\ type , public , extends ( PlaneWaveAdvectionTemplate ) & \\ \\ :: SineWaveAdvectionForm \\ \\ type ( MeasuredValueForm ) :: Offset , Amplitude \\ \\ c ontains \\ \\ procedure , private , pass :: Initialize_SWA !\u2013 See Listing \\ ref {lst : Subroutine_Initialize_SWA } \\ \\ generic , public :: Initialize => Initialize_SWA \\ \\ procedure , public , pass :: Waveform \\ \\ final :: Finalize \\ \\ end type SineWaveAdvectionForm \\ \\ \\ c ontains \\ \\ \\ !", "\u2013 Definitions of {\\ tt subroutine } s {\\ tt Initialize \\ _SWA } and {\\ tt Finalize }, and {\\ tt function Waveform }, omitted \\ \\ \\ end module SineWaveAdvection_Form \\ </float> \\newline Two members of SineWaveAdvectionForm are declared in line 10, the Offset and Amplitude of the sine wave.", "(An offset is included because the plane wave is in the density, and our PressurelessFluidForm methods expect the density to be positive.) Offset and Amplitude are objects of class MeasuredValueForm ; as we will discuss in Section [@ref:LABEL:sec:Specifiers] , this allows them to have a unit of measure associated with them.", "Lines 12-14 are declarations related to two of the methods of SineWaveAdvectionForm .", "The generic statement in line 13 provides a public ly available alias Initialize for the private method Initialize_SWA declared in line 12; as we shall see later, the method Initialize will be overloaded to alias other subroutines as well.", "The final routine declared in line 15 is automatically executed when an object of SineWaveAdvectionForm goes out of existence; such routines are typically used for \u2018garbage collection\u2019 (for instance, to deallocate any allocatable members, in order to avoid memory leaks).", "\\newline Of the two methods invoked in lines 11 and 12 of Listing [@ref:LABEL:lst:Program_SWA_Outline] , only one\u2014 Initialize \u2014is made publicly available in Listing [@ref:LABEL:lst:Module_SWA_Outline] (via line 13); where is Evolve ? Consider the indication in line 8 of Listing [@ref:LABEL:lst:Module_SWA_Outline] that this class extends the class PlaneWaveAdvectionTemplate outlined in Listing [@ref:LABEL:lst:Module_PWA_Outline] .", "\\newline <float> Outline of module PlaneWaveAdvection_Template .", "Used at line 4 of Listing [@ref:LABEL:lst:Module_SWA_Outline] .", "\\ module PlaneWaveAdvection_Template \\ \\ \\ use Basics \\ \\ use C onservationLawEvolution_Template \\ \\ use PressurelessFluid_Form \\ \\ implicit none \\ \\ private \\ \\ \\ type , public , extends ( C onservationLawEvolutionTemplate ), abstract :: & \\ \\ PlaneWaveAdvectionTemplate \\ \\ c ontains \\ \\ procedure , private , pass :: Initialize_PWA !\u2013 See Listing \\ ref {lst : Subroutine_Initialize_PWA } \\ \\ generic , public :: Initialize => Initialize_PWA \\ \\ procedure ( WaveformInterface ), public , pass , deferred ::", "Waveform \\ \\ end type PlaneWaveAdvectionTemplate \\ \\ \\ !", "\u2013 Declaration of the {\\ tt interface } for {\\ tt function WaveformInterface } omitted \\ \\ \\ c ontains \\ \\ \\ !", "\u2013 Definition of {\\ tt subroutine Initialize \\ _PWA } omitted \\ \\ \\ end module PlaneWaveAdvection_Template \\ </float> \\newline In turn, note in line 9 of Listing [@ref:LABEL:lst:Module_PWA_Outline] that PlaneWaveAdvectionTemplate extends the class ConservationLawEvolutionTemplate , outlined in Listing [@ref:LABEL:lst:Module_CLE_Outline] .", "\\newline <float> Outline of module ConservationLawEvolution_Template .", "Used at line 4 of Listing [@ref:LABEL:lst:Module_PWA_Outline] .", "\\ module C onservationLawEvolution_Template \\ \\ \\ use Basics \\ \\ use DistributedMesh_Form \\ \\ use C onservedFields_Template \\ \\ use C onservationLawStep_Form \\ \\ implicit none \\ \\ private \\ \\ \\ type , public , abstract :: C onservationLawEvolutionTemplate \\ \\ !", "\u2013 Declaration of several members omitted \\ \\ type ( DistributedMeshForm ) :: DistributedMesh \\ \\ c lass ( C onservedFieldsTemplate ), allocatable :: C onservedFields \\ \\ type ( C onservationLawStepForm ) :: C onservationLawStep \\ \\ c ontains \\ \\ procedure , private , pass :: Initialize_CLE \\ \\ generic , public :: Initialize => Initialize_CLE \\ \\ procedure , public , pass :: Evolve \\ \\ end type PlaneWaveAdvectionTemplate \\ \\ \\ c ontains \\ \\ \\ !", "\u2013 Definition of {\\ tt subroutine } {\\ tt Initialize \\ _CLE } omitted \\ \\ \\ subroutine Evolve ( C LE ) \\ \\ c lass ( C onservationLawEvolutionTemplate ), intent ( inout ) :: C LE \\ \\ !", "\u2013 Declaration of local variables and other statements omitted \\ \\ end subroutine Evolve \\ \\ \\ end module C onservationLawEvolution_Template \\ </float> \\newline The sequential extensions of ConservationLawEvolutionTemplate to PlaneWaveAdvectionTemplate to SineWaveAdvectionForm provide an initial look at the object-oriented principles of inheritance and polymorphism.", "The idea in this particular example is that ConservationLawEvolutionTemplate contains members and methods needed to evolve any set of conserved fields.", "The extension PlaneWaveAdvectionTemplate adds members and methods useful to solve the advection of a plane wave for any waveform, while SineWaveAdvectionForm adds only the elements necessary to specify that the waveform be a sine wave.", "PlaneWaveAdvectionTemplate , and in turn SineWaveAdvectionForm , are said to inherit all the members declared and suggested by lines 11-14 of Listing [@ref:LABEL:lst:Module_CLE_Outline] .", "SineWaveAdvectionForm also inherits the methods declared in lines 12-14 of Listing [@ref:LABEL:lst:Module_PWA_Outline] and lines 16-18 of Listing [@ref:LABEL:lst:Module_CLE_Outline] .", "\\newline This inheritance is what allows, for instance, the object SWA of class SineWaveAdvectionForm declared in line 7 of the program in Listing [@ref:LABEL:lst:Program_SWA_Outline] to invoke, in line 12, the method Evolve of its ancestor ConservationLawEvolutionForm .", "The outline of Evolve is shown in lines 25-28 of Listing [@ref:LABEL:lst:Module_CLE_Outline] .", "The declaration of the argument CLE with the class keyword instead of the type keyword in line 26 makes CLE a polymorphic variable, which means that this argument can be any extension (or extension of extension, and so on) of ConservationLawEvolutionForm .", "This is what allows a SineWaveAdvectionForm object\u2014ultimately, SWA declared in line 7 of Listing [@ref:LABEL:lst:Program_SWA_Outline] \u2014to be the first argument of Evolve .", "\\newline In this particular example, PlaneWaveAdvectionTemplate and ConservationLawEvolutionTemplate are abstract , as specified in lines 9 and 10 of Listings [@ref:LABEL:lst:Module_PWA_Outline] and [@ref:LABEL:lst:Module_CLE_Outline] respectively.", "In the case of PlaneWaveAdvectionTemplate this is required by the fact that its method Waveform is deferred (as declared in line 14 of Listing [@ref:LABEL:lst:Module_PWA_Outline] ).", "This means that, unlike Initialize_PWA declared in line 12\u2014which is defined or \u2018fleshed out\u2019 in this class definition, as suggested by line 21\u2014the method Waveform is only given an interface , or specification of its argument list, as suggested by line 17.", "In this example the method Waveform is not fleshed out until PlaneWaveAdvectionTemplate is extend ed to SineWaveAdvectionForm in Listing [@ref:LABEL:lst:Module_SWA_Outline] , in which the previous deferred declaration is said to be overridden by the new declaration in line 14, and the method is defined as suggested in line 20.", "(The importance of procedure overriding for a powerful manifestation of polymorphism will be emphasized below.) \\newline ConservationLawEvolutionTemplate in Listing [@ref:LABEL:lst:Module_CLE_Outline] is declared abstract for a different reason.", "In line 13, the allocatable member ConservedFields is declared with the class keyword rather than the type keyword.", "We saw one use of the class keyword previously, in connection with a subroutine argument.", "The class keyword is also allowed in the declaration of an allocatable variable, which means that it can be allocated as the base type given in the declaration or any extension thereof .", "In this particular case, the member ConservedFields in line 13 cannot be allocated as the declared base type ConservedFieldsTemplate , because this type happens to be abstract \u2014having deferred methods\u2014and concrete instances of abstract types are not allowed.", "In the present example we use the extension PressurelessFluidForm of ConservedFieldsTemplate , which extends a generic template for a set of conserved fields to one with mass density, momentum density, and velocity fields.", "Allocation of the ConservedFields member as PressurelessFluidForm does not occur in ConservationLawEvolutionTemplate , which does not have access to any non-abstract extensions of ConservedFieldsTemplate .", "Instead, this allocation occurs in an extension of ConservationLawEvolutionTemplate that does have access to PressurelessFluidForm ; see line 6 of Listing [@ref:LABEL:lst:Subroutine_Initialize_PWA] for Initialize_PWA , which fits in line 21 of Listing [@ref:LABEL:lst:Module_PWA_Outline] for PlaneWaveAdvectionTemplate .", "We are not required to declare ConservationLawEvolutionTemplate as abstract , because it does not have any deferred methods; but we choose to do so in order to force the creation of an extension (in this case PlaneWaveAdvectionTemplate ), one with access to a non-abstract extension of ConservedFieldsTemplate (in this case PressurelessFluidForm ), which can therefore be responsible for allocating and initializing the ConservedFields member.", "\\newline <float> subroutine Initialize_PWA .", "Fits in line 21 of Listing [@ref:LABEL:lst:Module_PWA_Outline] .", "Called at line 13 of Listing [@ref:LABEL:lst:Subroutine_Initialize_SWA] .", "\\ subroutine Initialize_PWA ( PWA , DensityUnit ) \\ \\ \\ c lass ( PlaneWaveAdvectionTemplate ), intent ( inout ) :: PWA \\ \\ type ( MeasuredValueForm ), intent ( in ) :: DensityUnit \\ \\ \\ allocate ( PressurelessFluidForm :: PWA % C onservedFields ) \\ \\ !", "\u2013 Initialization of {\\ tt PWA \\% C onservedFields } omitted \\ \\ \\ !", "\u2013 Local variable declarations and assignments omitted \\ \\ \\ !", "\u2013 {\\ tt N } is a {\\ tt real } array c ontaining density values in the mesh c ells , and {\\ tt X }, {\\ tt Y }, and {\\ tt Z } are {\\ tt real } \\ \\ !", "\\ arrays c ontaining the C artesian c oordinates of the mesh c ells ; whereas {\\ tt K } is the wave vector \\ \\ !", "\\ of the plane wave , a three - element {\\ tt real } array \\ \\ \\ N = PWA % Waveform ( K ( 1 ) * X + K ( 2 ) * Y + K ( 3 ) * Z ) \\ \\ \\ end subroutine Initialize_PWA \\ </float> \\newline We now return to the overloading of the Initialize method of SineWaveAdvectionForm , called in line 11 of the main program in Listing [@ref:LABEL:lst:Program_SWA_Outline] .", "While not apparent in Listing [@ref:LABEL:lst:Program_SWA_Outline] , there is a difference behind the scenes between the two method invocations in lines 11 and 12.", "We have seen that the subroutine Evolve is declared as a method and defined in SineWaveAdvectionForm \u2019s ancestor ConservationLawEvolutionTemplate in Listing [@ref:LABEL:lst:Module_CLE_Outline] .", "In contrast, there is no subroutine named Initialize in SineWaveAdvectionForm or its ancestors.", "Instead, the method name Initialize is overloaded by virtue of the generic statements in line 13 of Listing [@ref:LABEL:lst:Module_SWA_Outline] , line 13 of Listing [@ref:LABEL:lst:Module_PWA_Outline] , and line 17 of Listing [@ref:LABEL:lst:Module_CLE_Outline] , making Initialize an alias for three different subroutine s named Initialize_SWA , Initialize_PWA , and Initialize_CLE .", "This is possible because these three routines have different argument lists, which allows the system to resolve any particular call (i.e. route it to the correct subroutine ) by finding the aliased routine with the appropriate argument list.", "To see how this works, consider the subroutine s Initialize_SWA and Initialize_PWA in Listings [@ref:LABEL:lst:Subroutine_Initialize_SWA] and [@ref:LABEL:lst:Subroutine_Initialize_PWA] , which would fall in line 20 of Listing [@ref:LABEL:lst:Module_SWA_Outline] of SineWaveAdvectionForm and line 21 of Listing [@ref:LABEL:lst:Module_PWA_Outline] of PlaneWaveAdvectionTemplate respectively.", "\\newline <float> subroutine Initialize_SWA.", "Fits in line 19 of Listing [@ref:LABEL:lst:Module_SWA_Outline] .", "Called at line 11 of Listing [@ref:LABEL:lst:Program_SWA_Outline] .", "\\ subroutine Initialize_SWA ( SWA ) \\ \\ \\ c lass ( SineWaveAdvectionForm ), intent ( inout ) :: SWA \\ \\ type ( MeasuredValueForm ) :: OffsetUnit , AmplitudeUnit \\ \\ \\ OffsetUnit = UNIT % IDENTITY \\ \\ AmplitudeUnit = UNIT % IDENTITY \\ \\ SWA % Offset = 2.0 _KDR * OffsetUnit \\ \\ SWA % Amplitude = 1.0 _KDR * AmplitudeUnit \\ \\ !", "\u2013 Statements overriding these defaults with values read from parameter file \\ \\ !", "\\ \\ or c ommand line omitted \\ \\ \\ c all SWD % Initialize ( AmplitudeUnit ) !", "\u2013 See Listing \\ ref {lst : Subroutine_Initialize_PWA } \\ \\ \\ end subroutine Initialize_SWA \\ </float> \\newline Notice that Initialize_SWA (Listing [@ref:LABEL:lst:Subroutine_Initialize_SWA] )", "has only one argument while Initialize_PWA (Listing [@ref:LABEL:lst:Subroutine_Initialize_PWA] ) has two.", "This difference allows the call in line 11 of the program in Listing [@ref:LABEL:lst:Program_SWA_Outline] to be resolved to Initialize_SWA (with the object SWA to the left of the % symbol passed as the only argument), and the call in line 13 of Listing [@ref:LABEL:lst:Subroutine_Initialize_SWA] to be resolved to Initialize_PWA (with SWA as the first argument and AmplitudeUnit as the second argument).", "\\newline Finally, we return to the deferred method Waveform of PlaneWaveAdvectionTemplate in Listing [@ref:LABEL:lst:Module_PWA_Outline] in order to get a glimpse of the power of polymorphism manifest by procedure overriding in association with class inheritance.", "Line 15 of Listing [@ref:LABEL:lst:Subroutine_Initialize_PWA] for subroutine Initialize_PWA calls the method Waveform that sets the plane wave values in the mesh cells (see also the comment in lines 11-13).", "As far as PlaneWaveAdvectionTemplate is concerned, this function could return anything; its definition is deferred, with only an interface specified (lines 14 and 17 of Listing [@ref:LABEL:lst:Module_PWA_Outline] ). But when the argument PWA is an instance of SineWaveAdvectionForm , the function call resolves to the overriding version of Waveform specified by lines 14 and 20 of Listing [@ref:LABEL:lst:Module_SWA_Outline] of SineWaveAdvectionForm , returning values of a sine wave. And similar to the sine wave case in Listings [@ref:LABEL:lst:Program_SWA_Outline] and [@ref:LABEL:lst:Module_SWA_Outline] , we have also implemented a very short and simple driver program SawtoothWaveAdvection and module SawtoothWaveAdvection_Form .", "In this case, when the argument PWA in Listing [@ref:LABEL:lst:Subroutine_Initialize_PWA] is an instance of SawtoothWaveAdvectionForm , that class\u2019 method Waveform is called and returns the values of a sawtooth wave.", "\\newline This simple example illustrates that the principles of inheritance and polymorphism\u2014embodied here in the mechanisms of type extension and method overriding\u2014make it much easier to allow lower-level code (in this example, PlaneWaveAdvectionTemplate ) to access higher-level code (in this example, the Waveform routines specific to SineWaveAdvectionForm and SawtoothWaveAdvectionForm ).", "\\newline The reason this language functionality is important for our purposes in GenASiS is that it greatly facilitates, for instance, the separation of the Mathematics and Physics divisions of GenASiS .", "For example, solvers for generic classes of equations can be written in Mathematics , and then invoked later by a range of different systems whose details are specified in Physics .", "This tremendously enhances the ease and transparency with which one can develop (for example) versatile and widely-applicable solvers.", "\\newline In fact we do just this sort of thing with the conservation law solver in this plane wave advection example.", "To this point\u2014taking a top-down perspective\u2014we have only discussed driver programs (Listing [@ref:LABEL:lst:Program_SWA_Outline] for the sine wave case), an abstract class for the initialization and periodic advection of a plane wave of arbitrary waveform (Listing [@ref:LABEL:lst:Module_PWA_Outline] ), and specific implementations of this template class (Listing [@ref:LABEL:lst:Module_SWA_Outline] for the sine wave case).", "The latter two are classes (f) and (g) in the list of classes in the first paragraph of this subsubsection.", "Continuing backwards through this list, classes (b)-(e) use the mechanism of polymorphism through inheritance to separate lower-level generic code from higher-level specific code.", "In particular, (c) and (d) constitute a solver for a generic system governed by conservation laws, which draws on certain properties specified in the abstract template (b)\u2014for instance, indices of \u2018primitive\u2019 and \u2018conserved\u2019 variables, and deferred methods for converting between these sets of variables, computing raw fluxes, setting boundary conditions, and so on.", "Class (e) is a higher-level realization of template (b) for a specific system, a pressureless fluid.", "It specifies mass density and velocity as primitive variables, and fills in routines that were deferred (specified only by interface) in template (b).", "\\newline We now show results from SineWaveAdvection and SawtoothWaveAdvection in 1D, 2D, and 3D.", "In the left panels of Figs. [@ref:LABEL:fig:SineWaveAdvection] and [@ref:LABEL:fig:SawtoothWaveAdvection] we plot the mass density of the initial conditions of SineWaveAdvection and SawtoothWaveAdvection , respectively.", "In the right panels we plot the relative error $ \\delta $ against the initial condition after one period of evolution, where $ \\delta=\\left(\\chi_{t}-\\chi_{t0}\\right)/\\chi_{t0} $ for every cell, and $ \\chi_{t} $ and $ \\chi_{t0} $ are the values of the density at the initial initial and final state, respectively.", "For these figures we have used a resolution of 128 cells in each spatial dimension.", "For SawtoothWaveAdvection , we have set the number of wavelengths to two, overriding the default value of one in the driver program via a command line option (see Section [@ref:LABEL:sec:Runtime] for this feature; see also Section [@ref:LABEL:sec:Building] for examples on how these programs were run).", "\\newline Problems with smooth flows like the SineWaveAdvection example can be used to check that the order of convergence of the numerical implementation is as expected for the numerical method employed.", "The error of the solution as a function of mesh resolution yields the convergence rate of the numerical solution.", "Here we use the $ L_{1} $ -norm relative error, given by \\newline <equation> $ L_{1}\\left(\\chi\\right)=\\frac{{\\sum_{i,j,k}}\\left|\\chi\\left(x_{i},% y_{j},z_{k}\\right)-\\chi_{0}\\left(x_{i},y_{j},z_{k}\\right)\\right|}{% {\\sum_{i,j,k}}\\left|\\chi_{0}\\left(x_{i},y_{j},z_{k}\\right)\\right|}, $ </equation> where $ \\chi_{0} $ is the \u2018known\u2019 value used as a standard for comparison.", "For the SineWaveAdvection program, $ \\chi_{0} $ and $ \\chi $ are the mass density at initial conditions and after one period of evolution, respectively.", "In Eq. [@ref:LABEL:eq:L1-Norm_Error] the summation is over all cells on the computational domain.", "Thus the $ L_{1} $ norm gives a single number as a quantitative error measure for a particular mesh resolution.", "The left panel of Fig. [@ref:LABEL:fig:SineWaveAdvectionConvergnce] shows that SineWaveAdvection converges to second order as expected.", "\\newline Finally, we consider the scaling behavior of SineWaveAdvection as the number of processes is increased.", "The weak scaling of a parallel program is the extent to which it maintains its parallel efficiency as the number of processes is increased, while keeping the amount of work per process constant.", "Parallel efficiency is the ratio of program\u2019s elapsed time with a particle number of processes over the elapsed time with a baseline number of processes.", "In the right panel of Fig. [@ref:LABEL:fig:SineWaveAdvectionConvergnce] we show the weak scaling of the SineWaveAdvection example on Darter [@bib:Fahey2014] , a Cray XC30 supercomputer at JICS.", "For this test every process was assigned $ 64^{3} $ cells.", "The total elapsed time is normalized by the number of time steps required to complete one period of evolution.", "The total elapsed time of this example program is output to the screen at the program\u2019s conclusion as provided by a class in the Runtime division of GenASiS Basics (see Section [@ref:LABEL:sec:Runtime] for this feature).", "\\newline </subsubsection> <subsubsection> <title> 2.1.2 Riemann Problem </title> In solving the Riemann problem we reuse most of the code used to solve the plane wave advection problem discussed in Sec. [@ref:LABEL:sec:PlaneWaveAdvection] (see also Fig. [@ref:LABEL:fig:FluidDynamics] ).", "The Riemann problem takes place in a box with reflecting boundaries, and features initial conditions that consist of two regions with different densities and pressures separated by a discontinuity.", "The classes listed in the second paragraph of Sec. [@ref:LABEL:sec:FluidDynamics] \u2014(a) DistributedMeshForm , (b) ConservedFieldsTemplate , (c) ConservationLawStepForm , and (d) ConservationLawEvolutionTemplate \u2014are sufficiently generic and \u2018pure mathematics\u2019 in nature to be used directly without modification.", "\\newline We also reuse one of the classes mentioned in the first paragraph of Sec. [@ref:LABEL:sec:PlaneWaveAdvection] .", "In particular, we extend (e) PressurelessFluidForm \u2014which includes mass density and velocity\u2014to PolytropicFluidForm , which adds a few variables such as pressure, internal energy, and parameters associated associated with a polytropic equation of state.", "PolytropicFluidForm also overrides many of the methods of PressurelessFluidForm .", "But even these overriding methods are able to reuse code in most cases, by first calling the overridden methods of PressurelessFluidForm to handle the portions relevant to its variables (mass density, velocity, etc.), and then separately addressing only the variables new to PolytropicFluidForm (pressure, energy, etc.).", "\\newline On the other hand, the code defining the specific problem must be replaced, but this is a small line count compared to the code as a whole.", "In particular, (f) PlaneWaveAdvectionTemplate and its extensions (g) SineWaveAdvectionForm and SawtoothWaveAdvectionForm are completely replaced by a new class RiemannProblemForm , which specifies reflecting boundary conditions and sets the initial conditions for this problem.", "\\newline We now show results from RiemannProblem in 1D, 2D, and 3D.", "The default initial conditions for the RiemannProblem are those typically known as the Sod shock tube problem [@bib:SOD1978] for 1D, which we have also extended to 2D and 3D.", "Figure [@ref:LABEL:fig:RiemannProblem] plots the initial (left panels) and final state (right panels) of the fluid at $ t=0.25 $ .", "\\newline </subsubsection> </subsection> <subsection> <title> 2.2 Molecular Dynamics </title> We present two kinds of molecular dynamics problems.", "The first is the equilibrium of bulk argon; we examine the solid, liquid, and gas phases with a periodic box as proxy for an infinite system, using the Lennard-Jones potential.", "The second is the gravitational collapse of an isolated system of particles (e.g. \u2018stars\u2019) which evolves to a state satisfying the virial theorem.", "\\newline A few underlying classes provide a generic mathematical foundation for both types of molecular dynamics problems.", "These are: (a) a function that returns a random number drawn from a Gaussian distribution ( NormalRandomNumberFunction ); (b) a class that provides for storage of the positions and velocities of a set of particles \u2018owned\u2019 by a particular process ( ParticlesForm ); (c) a class whose members include an instance of ParticlesForm , along with members and methods associated with some basic observables associated with a collection of particles, with the expectation that the totality of particles is divided among the several processes of a distributed-memory parallel program ( DistributedParticlesForm ); and (d) a template for the evolution of an instance of DistributedParticlesForm or one of its extensions, whose methods include integration of Newton\u2019s laws of motion for all particles (including a parallelized computation of the total force on each particle), and a deferred method specifying a particular force law ( ParticleDynamicsTemplate ).", "Subsequent classes extend and use these to flesh out the details of particular physical models, and set up and solve particular problems; see Fig. [@ref:LABEL:fig:MolecularDynamics] .", "\\newline As it would be outside our main scope and purpose here\u2014which is to introduce GenASiS Basics functionality\u2014we do not discuss these molecular dynamics example classes or the numerical methods they implement in any detail.", "The full example codes are included in the accompanying submission to the CPC Program Library.", "Here we provide only a brief overview.", "\\newline For purposes of this example we take the simple-minded approach of computing the pairwise forces between all particles, such that the work grows as the square of the number of particles.", "As a reflection of this, for the results presented below we chose numbers of particles and processes that gave similar wall-clock runtimes for each run (up to about 1.2 hours on Darter, the Cray XC30 at NICS): 2048, 6912, 16384, and 32000 particles, with 1, 12, 64, and 250 processes respectively.", "The parallelization strategy involves giving each process its equal share of particles, all of which may be anywhere in the computational domain (a periodic box in the case of argon equilibrium, and an \u2018infinite\u2019 box in the case of cluster formation by gravitational collapse).", "The parallelization of the force computation involves a \u2018ring algorithm,\u2019 in which each process has storage for \u2018guest particles\u2019 from another process in addition to the local particles it \u2018owns.", "\u2019 The force computation begins with each process copying the positions of its local particles to the guest particle storage, so that the contribution to the forces on each of its local particles by all the other local particles can be computed.", "Then each process sends the positions in its guest particle storage to the \u2018next\u2019 process, and their contributions are added to the forces on the local particles.", "This is repeated until all processes have computed the contributions from all the other processes\u2019 particles.", "We have implemented the \u2018leapfrog\u2019 and \u2018velocity-Verlet\u2019 approaches as options for integrating the particles\u2019 equations of motion; once the forces are computed, these position and velocity updates are trivially parallel and comparatively inexpensive.", "In the case of a periodic box, the \u2018minimum image convention\u2019 is employed, in which only the particle or its nearest periodic image is used in computing the force components.", "In the case of the gravitational force, which has no repulsion, we use potential softening at small scales by way of regularization of close encounters (i.e. avoiding very tight binaries, the detailed following of which would cause debilitatingly small time steps).", "For further information on molecular dynamics and N-body problems, see for instance Refs.", "[@bib:Thijssen2007Computational-P,Pang2006An-Introduction,Bodenheimer2007Numerical-Metho] .", "\\newline <subsubsection> <title> 2.2.1 Argon Equilibrium </title> In order to simulate the solid, liquid, and gas phases of argon, we need to implement classes further specifying the necessary physics and setting up this particular problem.", "Referring again to Fig. [@ref:LABEL:fig:MolecularDynamics] , and continuing the enumeration above, these are (e) an extension of DistributedParticlesForm that sets up particles on a Bravais lattice (in this case FCC, i.e. face-centered cubic) with a Maxwell-Boltzmann velocity distribution corresponding to a chosen temperature ( LatticeParticlesForm ); (f) an extension of ParticleDynamicsTemplate that adds the setup, computation, and recording of observables relevant to an atomic system ( LatticeDynamicsTemplate ); and (g) an extension of LatticeDynamicsTemplate that finally specifies a particular force law ( LennardJonesDynamicsForm ).", "The short and simple driver program ArgonEquilibrium initializes and evolves an instance of LennardJonesDynamicsForm , and an associated parameter file includes the force parameters appropriate for argon (see Sec. [@ref:LABEL:sec:FileSystemBasics] ).", "\\newline We now show results obtained with ArgonEquilibrium .", "We ran simulations of the solid, liquid, and gas phases using values for number density and temperature given in Table [@ref:LABEL:table:Phases] .", "\\newline Each simulation ran for 10000 time steps.", "The initial and final configurations of runs with 2048 particles are shown in Fig. [@ref:LABEL:fig:ArgonPointMesh] .", "\\newline From this figure it is evident that only the solid phase retains its initial FCC lattice configuration.", "\\newline Figure [@ref:LABEL:fig:ArgonEnergiesTemperature] shows that the simulations begin with an equilibration phase.", "\\newline Because the initial conditions do not represent equilibrium, there is an almost immediate exchange of kinetic and potential energy apparent in the earliest moments in left panel of Fig. [@ref:LABEL:fig:ArgonEnergiesTemperature] .", "The temperature\u2014computed from the kinetic energy via the equipartition relation, and shown for the gas phase in the right panel of Fig. [@ref:LABEL:fig:ArgonEnergiesTemperature] \u2014accordingly departs from the input \u2018target\u2019 temperature value.", "(The right panel also illustrates the reduction in fluctuations that comes by increasing the number of particles in the simulation.) The first third of each simulation is devoted to achieving equilibrium at the target temperature by periodically rescaling the velocities so as to reflect the target temperature.", "This process is visible as \u2018steps\u2019 in the energies plotted in the left panel, including the total energy.", "It is apparent that the system has settled into equilibrium at the desired target temperature well within the first third of the simulation, after which no further velocity rescalings are performed.", "Thermodynamic averages can then be computed as time averages of the data generated in the last two-thirds of the simulation.", "\\newline While the fact that only the solid phase retains the lattice structure of its initial condition can be directly apprehended from Fig. [@ref:LABEL:fig:ArgonPointMesh] , the correlation functions shown in Fig.", "[@ref:LABEL:fig:DiffusionCorrelation] distinguish all three phases from one another.", "\\newline The correlation function is essentially the probability of finding a particle at a given distance from another particle.", "It is time-averaged from the final two-thirds of the simulations.", "The correlation functions for all three phases are zero out to a distance corresponding to the repulsive hard core of the Lennard-Jones potential.", "All three phases then feature a peak\u2014albeit reduced in amplitude as one goes from solid to liquid to gas\u2014corresponding to the bottom of the attractive portion of the Lennard-Jones potential.", "Beyond this, the correlation function for the solid phase shows pronounced peaks corresponding to the distances between particles in the lattice structure.", "In the liquid phase there are a few additional peaks, more evenly spaced, less pronounced, and decaying more quickly with distance.", "The gas phase has very little structure beyond the first peak, as the random motions of particles less influenced by the potential quickly result in an average density.", "The correlation function is computed to larger distances for simulations with more particles, because a larger number of particles at a given density corresponds to a larger periodic box.", "\\newline The lower right panel of Fig. [@ref:LABEL:fig:DiffusionCorrelation] , showing the mean square displacement of particles from their initial positions, also distinguishes between the phases.", "The linear behavior is characteristic of diffusion, with the slope as diffusion constant; not surprisingly, particles in the gas phase diffuse more rapidly than those in the liquid phase, while particles in the solid phase do not diffuse at all, showing zero mean square displacement around their original lattice positions.", "The early quadratic behavior of the gas and liquid phases shown in insets represents initial free particle motion for times shorter than the typical collision time.", "\\newline </subsubsection> <subsubsection> <title> 2.2.2 Cluster Formation </title> In order to simulate the gravitational collapse of an isolated system of particles, we implement some alternative extensions to the generic classes listed in the second paragraph of Sec. [@ref:LABEL:sec:MolecularDynamics] .", "Continuing the enumeration there, and referring again to Fig. [@ref:", "LABEL:fig:MolecularDynamics] , these are (e) an extension of DistributedParticlesForm that sets up particles with a Gaussian spatial distribution, as well as a Maxwell-Boltzmann velocity distribution corresponding to an initial kinetic energy much smaller than the initial potential energy ( ClusterParticlesForm ); (f) an extension of ParticleDynamicsTemplate that adds the setup, computation, and recording of observables relevant to an isolated cluster ( ClusterDynamicsTemplate ); and (g) an extension of ClusterDynamicsTemplate that finally specifies a particular force law ( GravitationalDynamicsForm ).", "The short and simple driver program ClusterFormation initializes and evolves an instance of GravitationalDynamicsForm .", "\\newline We now show results for ClusterFormation .", "Unlike ArgonEquilibrium , in which dimensioned parameters corresponding to the real world were used, we set up an idealized dimensionless problem.", "The initial particle position components were drawn from a Gaussian distribution with a standard deviation of 0.5.", "The particle mass was chosen such that the total mass was 1.0.", "The standard deviation of the velocity components was chosen such that the initial kinetic energy was roughly 3% of the initial potential energy.", "Each simulation ran for 10,000 time steps.", "\\newline Figures [@ref:LABEL:fig:ClusterFormationZoomOut] - [@ref:LABEL:fig:ClusterFormationEnergies] illustrate and summarize our ClusterFormation runs.", "Figure [@ref:LABEL:fig:ClusterFormationZoomOut] shows the initial and final conditions of a run with 2048 particles.", "\\newline The initial cluster of particles (Fig. [@ref:LABEL:fig:ClusterFormationZoomOut] left), having much less kinetic than potential energy, undergoes gravitational collapse such that by the end of the simulation there is a dense core, but also a number of particles that seem to be unbound and escaping to infinity (Fig. [@ref:LABEL:fig:ClusterFormationZoomOut] right).", "The box framing the inner part of the domain is the same size as that in the magnified view of Fig. [@ref:LABEL:fig:ClusterFormation] , which shows a time sequence of particle positions.", "\\newline The sequence of selected frames in Fig. [@ref:LABEL:fig:ClusterFormation] can be examined alongside the time series in Fig. [@ref:LABEL:fig:ClusterFormationEnergies] .", "\\newline In the initial state, most of the particles ( $ 2\\sigma $ worth, with position components in Gaussian distributions) are within a radius of $ 1.0 $ (Fig. [@ref:LABEL:fig:ClusterFormation] upper left).", "This is only within a factor of order unity of the \u2018cluster size\u2019 $ -GM^{2}/U $ (where $ G $ is the gravitational constant, $ M $ is the sum of all the particles\u2019 masses, and $ U $ is the gravitational potential energy of the system) at $ t=0 $ in the left panel of Fig. [@ref:LABEL:fig:ClusterFormationEnergies] ; this is because this cluster length scale is only a figure of merit intended to convey changes in the size of the cluster during the simulation.", "Collapse is well underway by $ t=0.75 $ (Fig. [@ref:LABEL:fig:ClusterFormation] upper right).", "Maximum compression has almost been achieved at $ t=1.0 $ (Fig. [@ref:LABEL:fig:ClusterFormation] middle left), but it is clear that this configuration \u2018overshoots\u2019 the final situation.", "The middle right panel of Fig. [@ref:LABEL:fig:ClusterFormation] at $ t=1.5 $ still shows a dense core, but also a significant \u2018halo\u2019 of outward-moving particles that have rebounded from the initial collapse.", "After this initial rebound, the nature of the system does not change much between $ t=3.0 $ (Fig. [@ref:LABEL:fig:ClusterFormation] lower left) and $ t=10 $ (Fig. [@ref:LABEL:fig:ClusterFormation] lower right).", "\\newline The values of kinetic energy $ T $ and potential energy $ U $ at the end of the simulation in the right panel of Fig. [@ref:LABEL:fig:ClusterFormationEnergies] seem consistent with the virial theorem $ 2T=-U $ for a gravitationally bound system.", "The total energy stays constant as expected, with the only minor visible deviation occurring at the most dynamic moment of maximum compression.", "There is not a great deal of variation between the curves in Fig. [@ref:LABEL:fig:ClusterFormationEnergies] for runs with different numbers of particles.", "One perhaps notable exception is that the curves for the run with the largest number of particles (the red curve in the left panel, for instance) seem to be capturing a regular \u2018ring down\u2019 oscillation between $ t=2 $ and $ t=4 $ that the runs with fewer numbers of particles may not be able to capture.", "There may be a slight slope to the kinetic and potential energy curves between $ t=3 $ and $ t=10 $ ; this might reflect a slow settling to final equilibrium, but we also note that the virial theorem applies to a bound system, whereas the energy values calculated here apparently include a relatively small but perhaps non-negligible number of unbound particles escaping to infinity.", "\\newline </subsubsection> </subsection>  </section>"], ["<section> <title> 3 Illustrations of Basics Functionality </title>  With the example problems in Sec. [@ref:LABEL:sec:ExampleProblems] in mind, including the more detailed top-down discussion of the PlaneWaveAdvection example, we describe some of the functionality available in Basics .", "In particular we discuss in bottom-up fashion (i.e. in order of compilation) the code divisions appearing in Fig. [@ref:LABEL:fig:Basics_Structure] .", "\\newline <subsection> <title> 3.1 VariableManagement </title> VariableManagement is the first code division appearing in the diagram on the left side of Fig. [@ref:LABEL:fig:Basics_Structure] .", "It is one of the Basics divisions that is not a leaf division.", "Its subdivisions Specifiers , ArrayOperations , ArrayArrays , and VariableGroups are shown in the rightmost diagram of Fig. [@ref:LABEL:fig:Basics_Structure] .", "We discuss each in turn.", "\\newline <subsubsection> <title> 3.1.1 Specifiers </title> The first division of VariableManagement is Specifiers , which contains classes used in the specification of number and character variables.", "This includes some default parameters to be used as kind and len specifiers for Fortran intrinsic data types.", "It also includes mathematical and physical constants and a means of dealing with units.", "\\newline Consider for example the precision and range of numerical variables, which can be specified with a Fortran kind parameter.", "Note for instance the declaration of some of the members of ConservationLawEvolutionTemplate , which falls in line 11 in Listing [@ref:LABEL:lst:Module_CLE_Outline] : \\newline \\ real ( KDR ) :: C ourantFactor , StartTime , FinishTime , \u2026 \\ where $ \\ldots $ represents additional members not listed here.", "The parameter KDR is a nickname, provided by Specifiers , for KIND_DEFAULT % REAL , one of the members of the object KIND_DEFAULT declared and initialized in Specifiers .", "Fortran kind parameters also specify the precision and range of explicit numbers, as in the 2.0_KDR in line 8 of Listing [@ref:LABEL:lst:Subroutine_Initialize_SWA] .", "Normally KIND_DEFAULT % REAL (and therefore its nickname KDR ) is set to correspond to double precision; but if one wanted to execute a program in, say, single precision (and assuming all real variables were declared to be of kind KDR ), changing this single parameter would accomplish the task.", "\\newline Specifiers also provides means of dealing with units, primarily for initialization and output.", "These means include the class MeasuredValueForm and the singleton object UNIT .", "The class MeasuredValueForm has a real member Number and a character member Unit that specifies a unit of measure (e.g. \u2019m\u2019 for meters).", "The members of the singleton UNIT are objects of MeasuredValueForm for many units of measure.", "\\newline There is an additional wrinkle when instances of MeasuredValueForm are used in particular as units of measure, as opposed to typical measured values.", "Units of measure make use of an additional character member Label , not used in typical measured quantities.", "Consider for instance UNIT % CENTIMETER as an example.", "The need for an extra member Label (relative to the primary members Number and Unit of MeasuredValueForm ) arises from that fact that in its internal computations on plain real variables, GenASiS assumes all numbers are given in terms of a single fundamental unit.", "To be specific, a geometrized system of units is currently employed in which the meter is fundamental.", "In terms of this fundamental unit, 1 cm = $ 10^{-2} $ m. The right-hand side of this expression is reflected in the MeasuredValueForm members Number and Unit , i.e. UNIT % CENTIMETER % Number = 1.0e-2_KDR and UNIT % CENTIMETER % Unit = \u2019m\u2019 ; while the Label member is associated with the unit in question, i.e. UNIT % CENTIMETER % Label = \u2019cm\u2019 .", "The members Number and Unit of MeasuredValueForm are used to convert a number into or out of GenASiS \u2019 fundamental unit on input or output, while the Label member is used to label the output of a number in the given unit.", "\\newline We are now in a position to see how initializations with units can be coded directly.", "(Examples of initialization with units when reading numbers from parameter files or as command line options, and of output with units to the screen or to a file, will be discussed in connection with the FileSystem , Runtime , and Display portions of Basics in Sections [@ref:LABEL:sec:FileSystem] , [@ref:LABEL:sec:Runtime] , and [@ref:LABEL:sec:Display] respectively.) Note the declaration of the members Offset and Amplitude in line 10 of Listing [@ref:LABEL:lst:Module_SWA_Outline] for SineWaveAdvectionForm .", "These members, of type MeasuredValueForm , are initialized in lines 6-11 of Listing [@ref:LABEL:lst:Subroutine_Initialize_SWA] .", "Default, dimensionless values are assigned in lines 6-9.", "Default values with units could be implemented with some other member of the UNIT singleton instead of UNIT % IDENTITY ; as it is, non-default values, including units if desired, can be set through a parameter file or by command line options as noted in lines 10-11.", "Arithmetic and assignment operations involving instances of MeasuredValueForm , such as in lines 8 and 9, behave in expected ways: the explicit real number multiplies the Number member on the right-hand side.", "The multiplication of an explicit number by a member of the UNIT singleton puts the result into GenASiS \u2019 fundamental working unit, so that it can properly be used in subsequent calculations.", "(In the assignment of a MeasuredValueForm to a real , the Unit member is simply ignored.) \\newline Finally, we note that mathematical and physical constants can be accessed through the singleton CONSTANT .", "For instance, the number $ \\pi $ is referenced by the expression CONSTANT % PI in the Waveform function hinted at by line 20 of Listing [@ref:LABEL:lst:Module_SWA_Outline] for SineWaveAdvectionForm .", "\\newline </subsubsection> <subsubsection> <title> 3.1.2 ArrayOperations </title> The second division of VariableManagement is ArrayOperations , which includes some basic operations on arrays.", "Some of these\u2014such as Clear and Copy \u2014are merely overloaded names for subroutine s whose arguments are intrinsic data types stripped of any potentially complicating contexts or attributes that might inhibit compiler optimization.", "The purpose of these wrappers is to expose elementary variables to the compiler so that it might use fast methods (such as a system call to set values in memory) it might not otherwise employ if, say, the array has the pointer attribute, is deeply embedded in a complicated expression involving a sequence of derived type member references, etc.", "Other array operations have a more substantial purpose, such as Sort and Search .", "The Search operation, for instance, is used in PressurelessFluidForm \u2014an extension of ConservedFieldsTemplate \u2014in the fluid dynamics examples in Sec. [@ref:LABEL:sec:FluidDynamics] .", "ConservedFieldsTemplate has a rank-1 integer array member iaConserved containing indices denoting conserved variables.", "The extension PressurelessFluidForm has an integer member CONSERVED_DENSITY which is an index or flag whose value identifies that particular variable.", "In the expression \\newline \\ c all Search ( C F % iaConserved , C F % C ONSERVED_DENSITY , iDensity ) \\ these members of an instance CF of PressurelessFluidForm are the first two arguments\u2014the inputs\u2014of the Search command.", "Upon return, the last argument iDensity is set to the index of CF % iaConserved for which CF % iaConserved == CF % CONSERVED_DENSITY .", "(In this case equality is sought, but for a general real or integer array A and a corresponding scalar Value as the first two arguments, the last argument iValue is set upon return such that A ( iValue ) <= Value < A ( iValue + 1 ) .) \\newline </subsubsection> <subsubsection> <title> 3.1.3 ArrayArrays </title> The next division of VariableManagement is ArrayArrays , which contains classes that can be used to form arrays of arrays.", "One application is the construction of so-called \u2018ragged arrays.", "\u2019 In the rank-two case a ragged array can be conceptualized (following the Fortran row-major perspective) as a matrix whose columns have different numbers of elements.", "This can be useful when one wants an indexed collection of rank-one arrays that are not all the same length.", "Even if they are all the same length, one might want to conceptualize the data primarily as a collection of rank-one objects rather than as a rank-two array.", "\\newline For instance, in the fluid dynamics examples in Sec. [@ref:LABEL:sec:FluidDynamics] , the class DistributedMeshForm has a member \\newline \\ type ( ArrayReal_1D_Form ), dimension ( 3 ) :: Edge \\ which is a three-element array of 1D arrays, each of which contains the coordinate values of the cell edges in one of the three spatial dimensions.", "Having an \u2018array of arrays\u2019 allows for loops over dimension while simultaneously accommodating different numbers of cells in each dimension, should the user so desire.", "The class ArrayReal_1D_Form has an allocatable rank-one real array member Value ; thus, for an instance DM of DistributedMeshForm , the expression \\newline \\ DM % Edge ( iD ) % Value ( iE ) \\ is the edge value iE in dimension iD .", "The method Initialize of ArrayReal_1D_Form is overloaded; possible alternative arguments include an array shape, an explicit array to be copied, or another ArrayReal_1D_Form to be copied.", "In addition to ArrayReal_1D_Form , ArrayArrays includes analogous classes for arrays of other types and ranks.", "\\newline </subsubsection> <subsubsection> <title> 3.1.4 VariableGroups </title> The final division of VariableManagement is VariableGroups , which contains some infrastructure we use extensively in handling collections of variables, especially sets of related physical fields.", "In particular, the class VariableGroupForm includes both metadata about the variables (names, units, etc.) and storage for the variable data itself.", "The data storage is a rank-two real array member Value of VariableGroupForm ; the first dimension indexes separate points, and the second dimension indexes individual variables.", "The \u2018points\u2019 enumerated by the first index can mean very different things in different problems.", "In the fluid dynamics examples in Sec. [@ref:LABEL:sec:FluidDynamics] , each row represents a cell in the mesh, with the columns representing different physical fields on the mesh.", "In the molecular dynamics examples in Sec. [@ref:LABEL:sec:MolecularDynamics] , each row represents a different particle, and each column represents a position or velocity component of the particle.", "\\newline An example from the fluid dynamics classes in Sec. [@ref:LABEL:sec:FluidDynamics] is the member \\newline \\ type ( VariableGroupForm ) :: Position \\ of class DistributedMeshForm .", "The call \\newline \\ c all DM % Position % Initialize & \\ \\ ( [ DM % nProperCells + DM % nGhostCells , 3 ], & \\ \\ NameOption = \u2019Position\u2019 , & \\ \\ VariableOption = [ \u2019Center_X \u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423 \u2019 , & \\ \\ \u2019Center_Y \u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423 \u2019 , & \\ \\ \u2019Center_Z \u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423 \u2019 ], & \\ \\ UnitOption = DM % C oordinateUnit , C learOption = . true . ) \\ initializes this member of an instance DM of class DistributeMeshForm .", "The first argument is an array with two elements specifying the shape of the data storage.", "The first element gives the total number of cells in the section of the distributed mesh assigned to a given process: the number of \u2018proper\u2019 cells on which computation is performed, plus the number of \u2018ghost\u2019 cells containing data from neighboring processes (needed for differences, derivatives, etc.).", "The second element specifies that there be three variables or fields in this variable group.", "The second, third, and fourth arguments are optional metadata specifications: the name of the variable group, the names of the variables (in this case the position components of the cell centers), and an array of type MeasuredValueForm specifying the units associated with each of the three variables.", "The last optional argument instructs that the data array should be \u2018cleared\u2019 (initialized to 0.0).", "\\newline It is often convenient to use aliases to access the variable data in an instance of VariableGroupForm .", "Consider the following Fortran 2003 associate construct in PlaneWaveAdvectionTemplate from the examples in Sec. [@ref:LABEL:sec:PlaneWaveAdvection] : \\newline \\ associate & \\ \\ ( X => DM % Position % Value ( :, 1 ), & \\ \\ Y => DM % Position % Value ( :, 2 ), & \\ \\ Z => DM % Position % Value ( :, 3 ), & \\ \\ VX => PF % Value ( :, PF % VELOCITY ( 1 ) ), & \\ \\ VY => PF % Value ( :, PF % VELOCITY ( 2 ) ), & \\ \\ VZ => PF % Value ( :, PF % VELOCITY ( 3 ) ), & \\ \\ N => PF % Value ( :, PF % C OMOVING_DENSITY ) ) \\ \\ \\ !", "\u2013 Statements reference X ( : ), Y ( : ), Z ( : ), etc .", "as 1 D arrays \\ \\ \\ end associate \\ We encountered the variable group DM % Position in the previous paragraph; here the aliases X , Y , and Z can be referenced as 1D arrays for the components of the mesh cell positions.", "The other aliases correspond to physical fields in an instance PF of PressurelessFluidForm ; this is an extension of ConservedFieldsTemplate , which in turn is an extension of VariableGroupForm .", "Thus PressurelessFluidForm retains all the functionality of VariableGroupForm , and can be directly used in a call to any procedure with a polymorphic VariableGroupForm argument, while also adding members and methods pertaining to this particular group of physical variables.", "The members PF % VELOCITY and PF % COMOVING_DENSITY are integers whose values identify the columns in an instance of PressurelessFluidForm containing those variables.", "Use of the aliases given above for geometric and physical fields greatly simplifies expressions used in setting initial conditions in PlaneWaveAdvectionTemplate .", "\\newline An instance of VariableGroupForm can have its own storage, as in the Position member of DistributedMeshForm above; or point to existing storage, perhaps for the purpose of selecting only a subset of variables from an existing variable group.", "For instance, in the fluid dynamics examples in Sec. [@ref:LABEL:sec:FluidDynamics] , the size of output files can be reduced by only writing a few select fluid variables rather than all of them.", "Consider the initialization \\newline \\ c all PF % Output % Initialize & \\ \\ ( PF , SelectedOption = [ PF % C OMOVING_DENSITY , PF % VELOCITY ], & \\ \\ VectorOption = [ \u2019Velocity \u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423\u2423 \u2019 ], & \\ \\ VectorIndicesOption = VectorIndices ) \\ of a VariableGroupForm member Output of an instance PF of PressurelessFluidForm .", "As we have seen previously, overloading of the Initialize method allows variation in behavior.", "In this case the object PF % Output uses PF itself (the first argument) as the basis of its initialization, and in fact will set a pointer to PF \u2019s existing data storage rather than allocate its own and perform a copy operation.", "But as the second argument suggests, only the listed \u2018primitive\u2019 variables are \u2018selected\u2019 in the new variable group PF % Output ; conserved variables and any auxiliary variables in PF are to be omitted for output purposes.", "The GenASiS I/O classes (see Sec. [@ref:LABEL:sec:FileSystem] ) to which PF % Output is ultimately sent will then act only on the \u2018selected\u2019 variables.", "(This example also illustrates the optional specification of \u2018vectors\u2019 in a variable group, through arguments specifying the names and index triplets of the variables composing the vector components, for the purpose of generating vector field visualizations.) \\newline One may wonder why the trouble should be taken to use a class like VariableGroupForm rather than work directly with explicitly declared arrays of variables.", "One reason is that individual variable storage for a field on an irregular, unstructured, and/or adaptive multidimensional mesh cannot reside in a rank-2 or rank-3 array isomorphic to a logically Cartesian grid, as would be possible with a simple single-level rectangular mesh.", "Another reason is that our simple VariableGroupForm construct greatly simplifies and renders more generic many tasks.", "For instance, as just mentioned, the file system facilities described later are written in terms of variable groups, so that a user need not program such operations in detail for the many sets of variables that may arise in different problems.", "Other examples discussed in future papers include the exchange of ghost cell data in multidimensional meshes, and the prolongation and restriction of data in multilevel meshes, which also are handled in terms of variable groups.", "In writing versatile and reusable code it is much better to be able to simply set up a variable group rather than have to continually rewrite (for example) explicit ghost exchange or I/O library calls for the many different variables that may appear in, or be added to or removed from, various applications.", "\\newline Finally, we mention a couple of other classes in the code division VariableGroups .", "A class PackedVariableGroupForm can be used to collect selected rows and columns of a VariableGroupForm into contiguous memory for more efficient processing.", "Another class, VariableGroupArrayMetadataForm , contains metadata about an array of variable groups, for instance the numbers of variables in each group as well as the total number of variables in all groups.", "\\newline </subsubsection> </subsection> <subsection> <title> 3.2 Display </title> After VariableManagement comes the leaf division Display (see the diagram on the left side of Figure [@ref:LABEL:fig:Basics_Structure] ), which contains infrastructure for displaying messages and variables to the standard output in a uniform and orderly way.", "The two central features available through Display are the singleton object CONSOLE , which has members that are used to control output to the standard display (\u2018stdout\u2019); and the heavily overloaded routine Show , which manages the hassles of displaying various data types with their respective formatting.", "\\newline One issue that arises in a distributed-memory parallel computing environment is that, absent intervention by the programmer, output emerges from all processes in an asynchronous manner.", "This typically results in unordered output of unwanted redundancy.", "The use of if statements can avoid this by allowing only a particular process to output to the screen, but a proliferation of if statements with every print statement becomes tedious (and difficult to change for debugging purposes if a global variable specifying the process to be displayed is not used).", "Thus CONSOLE has a member DisplayRank used by the Show command to display output only from the process of the chosen rank, without the programmer having to continually supply if statements.", "The DisplayRank member of the CONSOLE singleton can be set through its method SetDisplayRank .", "Alternatively, instead of calling SetDisplayRank one can supply an optional argument DisplayRankOption to the Show command specifying, for that call only, which process should be visible.", "\\newline Another issue related to output is that different pieces of information are of different levels of interest.", "In a production run one might want minimal output, while more verbose output may be useful for debugging.", "CONSOLE has a member Verbosity that addresses this need.", "It can be set to one of several levels denoted, in descending order of importance, by the CONSOLE members ERROR , WARNING , INFO_1 , ..., INFO_7 .", "In a call to Show one of these same members can be given as an optional argument IgnorabilityOption ; if the importance is thus tagged as less than Verbosity , the output is suppressed.", "In the absence of IgnorabilityOption an importance of INFO_1 is assumed.", "\\newline The Show command provides a very simple interface for a wide range of formatted and labeled output.", "There are provisions for intrinsic data types, objects of MeasuredValueForm , and arrays of all these.", "An object of MeasuredValueForm can be supplied to modify the units (including the unit label itself) in which real or MeasuredValueForm numbers are displayed.", "Consider for instance the statements \\newline \\ c all Show ( DM % MinCoordinate , \u2019MinCoordinate\u2019 ) \\ \\ c all Show ( DM % MaxCoordinate , \u2019MaxCoordinate\u2019 ) \\ which would display, as pure numbers (albeit with the given string labels), the real members MinCoordinate and MaxCoordinate of an instance DM of DistributedMeshForm .", "But we saw in Sec. [@ref:LABEL:sec:VariableGroups] that DistributedMeshForm has a member CoordinateUnit of type MeasuredValueForm ; this can be used in the calls \\newline \\ c all Show ( DM % MinCoordinate , DM % C oordinateUnit , \u2019MinCoordinate\u2019 , & \\ \\ C ONSOLE % INFO_3 ) \\ \\ c all Show ( DM % MaxCoordinate , DM % C oordinateUnit , \u2019MaxCoordinate\u2019 , & \\ \\ C ONSOLE % INFO_3 ) \\ in order that the output be converted and appropriately labeled according to the unit specified by DM % CoordinateUnit .", "In these latter Show statements we have also added an ignorability argument.", "The output will only appear from the process of rank CONSOLE % DisplayRank , and only if CONSOLE % Verbosity is high enough to include INFO_3 .", "\\newline </subsection> <subsection> <title> 3.3 MessagePassing </title> The next division of Basics is MessagePassing (see the diagram on the left side of Fig. [@ref:LABEL:fig:Basics_Structure] ), which contains some classes that abstract the data and methods useful for working in a message passing parallel computing environment.", "As shown in the middle right diagram in Fig. [@ref:LABEL:fig:Basics_Structure] , it has three subdivisions: MessagePassingBasics , PointToPoint , and Collective .", "\\newline Before describing the leaf divisions under MessagePassing we note that one may ask why we bother to create and use message passing classes when MPI (Message Passing Interface) [@bib:Gropp1999] is well-established, widely used, and seems likely to remain the dominant library for message passing in distributed-memory applications.", "Our message passing classes do indeed use MPI \u2018under the hood,\u2019 but in principle could be given other back ends (such as the lower-level Cray GNI application programming interface) without modifying their references in higher-level code.", "Our interfaces are streamlined relative to direct MPI calls while providing methods for common use cases relevant to our system, and thus provide a simpler experience for the user.", "In terms of object-oriented design patterns [@bib:Gamma1994] , our MessagePassing classes provide a \u2018fa\u00e7ade\u2019 that shields the user from concerns such as the kind and size of data types being sent and received (e.g. single precision vs. double precision, and beyond, in the case of real data) and declaring many variables\u2014handles, buffers, tags, and so on\u2014for use in MPI calls.", "In our approach many of these needed variables are encapsulated in the classes we define.", "Finally, we note that the MPI library is mostly procedural in orientation and uses Fortran 77 semantics for its routines, sometimes requiring more arguments than necessary if the equivalent modern Fortran interfaces were available (though the MPI-3 standard, which is not yet in wide use at this writing, may improve on this).", "Our interfaces relieve the user of these burdens.", "\\newline <subsubsection> <title> 3.3.1 MessagePassingBasics </title> We mention two classes in this division: CommunicatorForm and PortalHeaderForm .", "\\newline Inspired (obviously) by the notion of an MPI communicator, an instance of CommunicatorForm connects a group of processes in a distributed-memory parallel program.", "The members of CommunicatorForm include its Handle , Size , and Rank .", "Its method Initialize is overloaded to provide for either starting the message passing system, i.e. initializing MPI and thus creating the \u2018global\u2019 or \u2018world\u2019 communicator; or for creating a \u2018subcommunicator\u2019 that connects only a subset of the processes connected by a parent communicator.", "The method Synchronize pauses all processes until they reach the call to this method (i.e. an MPI \u2018barrier\u2019).", "Another method of CommunicatorForm is Abort , which provides an emergency exit from distributed-memory program execution.", "Normal termination of the message passing system (i.e. finalization of MPI) occurs when the CommunicatorForm object representing the global communicator goes out of existence.", "\\newline The purpose of PortalHeaderForm is to have a object that handles some basic information about data exchange with a set of processes, such as the numbers of source and target processes; arrays containing the ranks of those processes; and arrays containing the numbers of \u2018chunks\u2019 of data to be received from and sent to those processes.", "\\newline </subsubsection> <subsubsection> <title> 3.3.2 PointToPoint </title> A single message from one process to another is a \u2018point-to-point\u2019 communication.", "The division PointToPoint provides classes for single incoming and outgoing messages of different intrinsic data types, as well as for arrays of such messages.", "Here we illustrate methods for arrays of messages, used in the exchange of ghost cell data in the DistributedMeshForm class of the fluid dynamics examples in Sec. [@ref:LABEL:sec:FluidDynamics] .", "This class includes the members \\newline \\ type ( IncomingMessageArrayRealForm ), allocatable :: & \\ \\ IncomingPrevious , IncomingNext \\ \\ type ( OutgoingMessageArrayRealForm ), allocatable :: & \\ \\ OutgoingPrevious , OutgoingNext \\ used to receive data from and send data to processes owning the \u2018previous\u2019 and \u2018next\u2019 bricks of cells in each dimension.", "\\newline By way of illustration, the call \\newline \\ c all DM % IncomingPrevious % Initialize & \\ \\ ( DM % C ommunicator , spread ( TAG_IN_PREV , 1, PHP % nSources ), & \\ \\ PHP % Source , PHP % nChunksFrom * VM % nVariablesTotal ) \\ initializes the first of these message array members.", "The first argument is an object of CommunicatorForm .", "The second argument is an array of tags labeling incoming messages from \u2018previous\u2019 processes, and the third argument is an array of the ranks of the processes from which messages are to be received; in both of these members of an instance PHP of PortalHeaderForm (see Sec. [@ref:LABEL:sec:MessagePassingBasics] ) are used.", "The final argument is an array containing the number of values to be received from each process, calculated from another member of PHP and a member of an instance VM of VariableGroupArrayMetadataForm (Sec. [@ref:LABEL:sec:VariableGroups] ).", "Here PHP % nChunksFrom is an array of the numbers of ghost cells from each source process, and the scalar VM % nVariablesTotal is the total number physical variables in the array of variable groups to be exchanged.", "The call \\newline \\ c all DM % IncomingPrevious % Receive \\ posts receives for all the incoming messages from \u2018previous\u2019 processes.", "Similar calls for DM % IncomingNext initialize and post receives for the incoming messages from \u2018next\u2019 processes.", "\\newline An example of the initialization of an outgoing message array is \\newline \\ c all DM % OutgoingPrevious % Initialize & \\ \\ ( DM % C ommunicator , spread ( TAG_OUT_PREV , 1, PHP % nTargets ), & \\ \\ PHP % Target , PHP % nChunksTo * VM % nVariablesTotal ) \\ which is analogous to the above example for an incoming message array, but with targets replacing sources, etc.", "Element iE of the rank-1 buffer to be sent to a target process associated with the index iD is set with a statement of the form \\newline \\ DM % OutgoingPrevious % Message ( iD ) % Value ( iE ) = \u2026 \\ where $ \\ldots $ is an expression for the scalar value to be sent.", "With all elements of the buffer set, the call \\newline \\ c all DM % OutgoingPrevious % Send ( iD ) \\ sends the individual message iD .", "(All messages in an outgoing array can be sent by a similar call without a target index, but it is typically advantageous to load the buffer for a single message and then send it right away before moving to the next message.) Similar calls for DM % OutgoingNext initialize and send messages to the \u2018next\u2019 processes.", "\\newline A \u2018wait\u2019 must be executed before data can be retrieved from a message for which a receive has been previously posted.", "In this case, the call \\newline \\ c all DM % IncomingPrevious % Wait ( iD ) \\ waits for the message from the \u2018previous\u2019 process specified by the index iD .", "(In many cases it will be advantageous to use a different overloading we provide for the Wait method, the \u2018wait any\u2019 variant which returns the index of the first message whose data is available to read from its buffer.) Upon return from a wait call, buffer values are stored into appropriate variables with a statement of the form \\newline \\ \u2026 = DM % IncomingPrevious % Message ( iD ) % Value ( iE ) \\ where $ \\ldots $ represents a scalar variable to which is to be assigned element iE of the incoming buffer from message iD .", "Similar calls for DM % IncomingNext wait for and extract values from messages from the \u2018next\u2019 processes.", "Finally, perfunctory waits \\newline \\ c all DM % OutgoingPrevious % Wait \\ \\ c all DM % OutgoingNext % Wait \\ for the outgoing messages to finish sending complete the exchange, here in the version that waits for all messages at once.", "\\newline </subsubsection> <subsubsection> <title> 3.3.3 Collective </title> A communication that involves a group of processes is said to be \u2018collective\u2019.", "The division Collective contains classes that provide for collective communications involving different intrinsic data types.", "The collective communication class for a given data type is generic in the sense that its members are sufficient for broadcast, gather, scatter, and combined scatter-gather communications, and for collection operations (i.e. reductions of data); thus all of these are simply implemented as methods of a single class.", "For the gather and reduction methods, the absence or presence of the optional argument RootOption in the initialization of a collective communication object determines whether or not the gathered or reduced value is distributed to all processes.", "This option must also be used in connection with broadcast and scatter calls: by definition, these communications involve a \u2018root\u2019 process that is the source of data to be sent to all the processes in the group.", "\\newline As an example of a collective communication in our fluid dynamics examples in Sec. [@ref:LABEL:sec:FluidDynamics] , the class ConservationLawEvolutionTemplate executes a global reduction in determining the time step.", "The call \\newline \\ c all C O % Initialize ( C , nOutgoing = [ 1 ], nIncoming = [ 1 ] ) \\ initializes an instance CO of CollectiveOperationRealForm .", "The first argument is an instance of CommunicatorForm .", "The second and third arguments specify the sizes of the outgoing and incoming buffers.", "(Here only a single value will be involved in the reduction; arrays are used for the nOutgoing and nIncoming arguments to allow for the possibility of scatter and gather operations with different numbers of elements sent to or received from different processes.)", "The absence of the RootOption means that all processes will receive the reduced value.", "The statement \\newline \\ C O % Outgoing % Value ( 1 ) = TimeStepLocal \\ sets the outgoing buffer value as the minimum time step as determined from the data local to a particular process.", "The global reduction is accomplished by \\newline \\ c all C O % Reduce ( REDUCTION % MIN ) \\ in which the argument specifies, via reference to a member of our REDUCTION singleton, the type of reduction operation to be performed.", "Finally, the statement \\newline \\ C LE % TimeStep = C O % Incoming % Value ( 1 ) \\ copies the reduced value to the member TimeStep of the instance CLE of ConservationLawEvolutionTemplate .", "\\newline </subsubsection> </subsection> <subsection> <title> 3.4 FileSystem </title> As seen in the diagram on the left side of Fig. [@ref:LABEL:fig:Basics_Structure] , the next division of Basics is FileSystem , whose classes handle I/O to disk.", "The middle left diagram in Fig. [@ref:LABEL:fig:Basics_Structure] shows divisions FileSystemBasics , GridImageBasics , CurveImages , StructuredGridImages , UnstructuredGridImages , and PointGridImages .", "The major classes in FileSystem used to create and interact with files of various kinds have the term \u2018stream\u2019 in their names, connoting a source or sink of data flow.", "The names of classes used to store grids and data intended for visualization and/or checkpoint/restart include the term \u2018image,\u2019 which evokes both colloquial and computing connotations (respectively \u2018picture\u2019 and \u2018a computer file capturing the contents of some computing element\u2014hard drive, system memory, etc.", "\u2019).", "\\newline <subsubsection> <title> 3.4.1 FileSystemBasics </title> In addition to some lower-level functionality, the classes in this division include two kinds of streams.", "\\newline ParametersStreamForm is used to read scalar or one-dimensional array parameters of various datatypes from a text file, each line of which contains an entry of the form [name]=[value] or [name]=[value1],[value2],... .", "Units may be attached to real values with a tilde  followed by an all-caps string denoting one of the units available in UNIT_Singleton (see Section [@ref:LABEL:sec:Specifiers] ).", "For example, a parameter file associated with the ArgonEquilibrium program of Sec. [@ref:LABEL:sec:ArgonEquilibrium] contains the entries \\newline \\ ParticleMass =39.95 ATOMIC_MASS_UNIT \\ \\ EnergyParameter =1.032 e -2 ELECTRON_VOLT \\ \\ LengthParameter =3.405 ANGSTROM \\ which specify the particle mass and Lennard-Jones potential parameters appropriate for argon.", "Parameters need not appear in the file in the order searched for by the program; indeed the absence of a parameter, or even the absence of the parameter file, will not in themselves cause a program exit or crash (though the lack of sensible default values for absent parameters certainly might).", "Our example programs in Sec. [@ref:LABEL:sec:ExampleProblems] do not directly invoke objects of ParametersStreamForm , but instead use a higher-level parameter search facility described in Section [@ref:LABEL:sec:Runtime] that looks both in a parameter file and at command line options.", "\\newline A second type of stream in FileSystemBasics is TableStreamForm , which reads simple two-dimensional tables of data from a text file.", "\\newline </subsubsection> <subsubsection> <title> 3.4.2 GridImageBasics </title> The remaining divisions in FileSystem , beginning with GridImageBasics , provide a fa\u00e7ade facilitating interaction with sophisticated I/O libraries.", "In principle different libraries may be implemented as \u2018back ends.", "\u2019 For this initial release we have implemented the Silo library as the back end (see Sec. [@ref:LABEL:sec:Building] ), which is convenient for use with the visualization package VisIt.", "\\newline The major class of interest in this division is GridImageStreamForm , which handles some basic aspects of I/O that are the same regardless of the type(s) of grid(s) to be written.", "Its members and methods allow one to, for instance, open (for reading or writing) or close a file; make a directory (virtual within a file or actual in the file system, which may depend on the I/O library back end); list an existing file\u2019s contents; and automatically track filename numbering for consecutive write operations (e.g. for periodic output in time-dependent evolution).", "For example, \\newline \\ type ( GridImageStreamForm ) :: GridImageStream \\ is one of the members of the class DistributedMeshForm used by our fluid dynamics example problems in Sec. [@ref:LABEL:sec:FluidDynamics] .", "The method SetImage of DistributedMeshForm contains the lines \\newline \\ associate ( GIS => DM % GridImageStream ) \\ \\ c all GIS % Initialize & \\ \\ ( Name , C ommunicatorOption = DM % C ommunicator , & \\ \\ WorkingDirectoryOption = OutputDirectory ) \\ \\ c all GIS % Open ( GIS % ACCESS_SET_GRID ) \\ in which DM is the passed DistributedMeshForm object.", "Initialization of this stream for parallel I/O includes a name for a file or series of files, an instance of CommunicatorForm (see Sec. [@ref:LABEL:sec:MessagePassingBasics] ), and optionally a string specifying an output directory.", "Once initialized, an object of GridImageStreamForm can be opened for various purposes, with the argument used in calling the Open method specifying the mode of access to the stream object.", "In this case the purpose is only to set properties of the grid (see the following subsections); no files are to be written, created, or read with this access mode.", "With the mode ACCESS_CREATE , a file number counter associated with the stream object is incremented and appended to the Name argument to form the filename, and then a new file is created with this filename.", "This access mode also implies read and write access to the newly created file.", "Other possible access modes are ACCESS_WRITE , which opens an existing file for writing by appending new data to the end of the file; and ACCESS_READ , which opens an existing file for reading only.", "Once the tasks associated with the stream are finished, the call \\newline \\ c all GIS % C lose \\ \\ end associate !", "\u2013 GIS \\ flushes any data associated with the stream and closes the file.", "\\newline </subsubsection> <subsubsection> <title> 3.4.3 CurveImages </title> The class CurveImageForm is used write a one-dimensional grid and data on it, which can be used to generate curves in a Cartesian $ xy $ plane.", "Data for the 1D plots in Figs. [@ref:LABEL:fig:SineWaveAdvection] (upper panels), [@ref:LABEL:fig:SawtoothWaveAdvection] (upper panels), [@ref:LABEL:fig:RiemannProblem] (upper panels), [@ref:LABEL:fig:ArgonEnergiesTemperature] , [@ref:LABEL:fig:DiffusionCorrelation] , and [@ref:LABEL:fig:ClusterFormationEnergies] were generated using this class.", "An example of its use is the member \\newline \\ type ( C urveImageForm ) :: C urveImage \\ of the class DistributedMeshForm used by our fluid dynamics example problems in Sec. [@ref:LABEL:sec:FluidDynamics] .", "If an instance DM of class DistributedMeshForm is used as a 1D mesh, the lines \\newline \\ associate ( C I => DM % C urveImage ) \\ \\ c all C I % Initialize ( GIS ) \\ \\ c all C I % SetGrid & \\ \\ ( \u2019Curves\u2019 , DM % Edge ( 1 ), DM % nProperCells , & \\ \\ oValue = DM % nGhostLayers ( 1 ), & \\ \\ C oordinateUnitOption = DM % C oordinateUnit ( 1 ) ) \\ \\ do iVG = 1, size ( VG ) \\ \\ c all C I % AddVariableGroup ( VG ( iVG ) ) \\ \\ end do \\ \\ end associate !", "\u2013 C I \\ are executed between the opening and closing of the instance of GridImageStreamForm (aliased as GIS ) just discussed in Sec. [@ref:LABEL:sec:GridImageBasics] , in the method SetImage of DistributedMeshForm .", "Initialization requires an instance of GridImageStreamForm .", "The call to CI % SetGrid specifies a directory, gives the spatial coordinates of the mesh points, and includes an optional argument specifying the units of those coordinates.", "The argument VG is an array of class VariableGroupForm (see Sec. [@ref:LABEL:sec:VariableGroups] ); sending them to the method CI % AddVariableGroup makes the variable data and metadata (names, units, etc.) specified by the variable groups available to the CurveImageForm methods for inclusion in the file as supported by the I/O library back end, which in turn allows a visualization package that supports the format to display it directly.", "(For instance, the units in Fig. [@ref:LABEL:fig:ArgonEnergiesTemperature] were automatically included by the visualization package VisIt without intervention of the user beyond including units in the appropriate variable groups in the ArgonEquilibrium program.) With these preparations, all that is needed is the call \\newline \\ c all DM % C urveImage % Write & \\ \\ ( TimeOption = TimeOption , C ycleNumberOption = C ycleNumberOption ) \\ which is made in the DM % Write method.", "Again, as supported by the I/O library back end and visualization package, time and cycle number can be optionally saved and displayed.", "\\newline </subsubsection> <subsubsection> <title> 3.4.4 StructuredGridImages and UnstructuredGridImages </title> The classes StructuredGridImageForm and UnstructuredGridImageForm are used to write and read two- or three-dimensional grids and data on them, which can be used to generate various kinds of multidimensional plots.", "Using the member \\newline \\ type ( StructuredGridImageForm ) :: GridImage \\ of the class DistributedMeshForm used by our fluid dynamics example problems in Sec. [@ref:LABEL:sec:FluidDynamics] , data were generated for the 2D and 3D plots in Figs. [@ref:LABEL:fig:SineWaveAdvection] (middle and lower panels), [@ref:LABEL:fig:SawtoothWaveAdvection] (middle and lower panels), and [@ref:LABEL:fig:RiemannProblem] (middle and lower panels).", "A structured grid may be rectilinear or curvilinear, but is in either case \u2018logically rectangular\u2019: the quadrilateral or cuboid cells filling the domain can be indexed with a pair or triplet of integers, with a single list of coordinates in each dimension sufficing (together with the integer values of an index pair or triplet) to specify the edge values of any particular cell.", "In an unstructured grid\u2014which can be more irregular in shape\u2014the coordinates of each individual node must be given, together with connectivity information that specifies how the nodes are arranged into cells.", "Use of structured and unstructured grids follows that just exemplified in Sec. [@ref:LABEL:sec:CurveImages] in connection with curves: a SetGrid method provides the information needed to set up a particular kind of grid, an AddVariableGroup method attaches dependent variables to the grid, and a Write method commits the data to a file.", "\\newline </subsubsection> <subsubsection> <title> 3.4.5 PointGridImages </title> The class PointGridImageForm is used to write a list of positions that can be displayed as points.", "Using the members \\newline \\ type ( PointGridImageForm ) :: GridImagePosition , GridImagePositionBox \\ of the class DistributedParticlesForm used by our molecular dynamics example problems in Sec. [@ref:LABEL:sec:MolecularDynamics] , data were generated for the plots in Figs. [@ref:LABEL:fig:ArgonPointMesh] , [@ref:LABEL:fig:ClusterFormationZoomOut] , and [@ref:LABEL:fig:ClusterFormation] .", "(Member GridImagePosition is used to plot particles\u2019 absolute positions, while member GridImagePositionBox is used to plot positions restricted to a periodic box.) Like DistributedMeshForm discussed in connection with the curve and structured grid examples discussed in the preceding subsections, DistributedParticlesForm also has SetImage and Write methods. And similar to its curve, structured grid, and unstructured grid counterparts, PointGridImageForm has SetGrid , AddVariableGroup , and Write methods.", "One difference, however, is that SetGrid need only be called once in our fluid dynamics examples, in which the mesh remains unchanged throughout the simulation, whereas SetGrid must be called before every call to Write in the point mesh case because the particle positions change with every time step.", "\\newline </subsubsection> </subsection> <subsection> <title> 3.5 Runtime </title> The last division of Basics is the leaf division Runtime (see the diagram on the left side of Fig. [@ref:LABEL:fig:Basics_Structure] ), which provides some generic functionality associated with running programs.", "Included are a function that returns wall time, a command that displays memory usage, a command that initializes the random seed, and a class that handles command line options (newly accessible to programs in Fortran 2003).", "\\newline The user can access most of this functionality through the object PROGRAM_HEADER declared in PROGRAM_HEADER_Singleton , which requires initialization.", "For instance, our example program SineWaveAdvection contains the lines \\newline \\ allocate ( PROGRAM_HEADER ) \\ \\ c all PROGRAM_HEADER % Initialize ( \u2019SineWaveAdvection\u2019 ) \\ which appear at line 9 of Listing [@ref:LABEL:lst:Program_SWA_Outline] .", "The argument to the Initialize method is the program name.", "By default, PROGRAM_HEADER appends a short string indicating the dimensionality (e.g. \u2019_1D\u2019 , \u2019_2D\u2019 , etc.) to the program name.", "Because many applications of GenASiS are expected to be written rather generically for anywhere from one to three space position space dimensions, the default behavior is to determine the dimensionality from a parameter file or command line option, or failing to find these, default to three dimensions.", "PROGRAM_HEADER has the global (\u2018world\u2019) communicator as one of its members, and intialization of this singleton takes care of initialization of MPI (see Section [@ref:LABEL:sec:MessagePassing] ).", "The UNIT (see Section [@ref:LABEL:sec:Specifiers] ) and CONSOLE (see Section [@ref:LABEL:sec:Display] ) singletons are also initialized.", "Once PROGRAM_HEADER is intialized its method ShowStatistics can be called at any time to display total elapsed time, the amount of time spent in I/O, and current memory usage.", "\\newline Another PROGRAM_HEADER method of significant interest is GetParameter .", "Our general approach to parameters is to (1) set a reasonable default value in the code itself, (2) look for an overriding user-specified value in a parameter file, and (3) look for a higher-priority user-specified value among any command line options.", "The PROGRAM_HEADER % GetParameter method follows this philosophy: it informs the user of the existing (presumably default) value, and then sequentially looks for and resets that value if it is found in a parameter file and/or on the command line (with appropriate notifications along the way).", "The parameter file searched by default is one whose filename consists of the program name with the suffix \u2019_Program_Parameters\u2019 appended; alternatively, a ParametersStream object (see Section [@ref:LABEL:sec:FileSystem] ) associated with some other parameter file can be supplied as an optional argument.", "Our fluid and molecular dynamics example problems in Sec. [@ref:LABEL:sec:ExampleProblems] make heavy use of this functionality.", "For instance, there is a file ArgonEquilibrium_3D_Program_Parameters automatically found and read by our example program ArgonEquilibrium .", "In Sec. [@ref:LABEL:sec:FileSystemBasics] we showed three lines from that file, specifying the particle mass and Lennard-Jones potential parameters appropriate for argon.", "This file also contains the default number density and temperature values \\newline \\ NumberDensity =7.599 e -3 NUMBER_DENSITY_ANGSTROM \\ \\ TargetTemperature =359.4 KELVIN \\ which correspond to the gas phase."]], "target": "The program can be run with alternate values, such as those listed in Table for the liquid and solid phases, either by changing the entries in the parameter file, or by including expressions like the above as command line options."}, {"tabular": ["    &  Source Model  &  Target Model ", "  &  VGG16  &  WRN-28-2  &  R18 +Adam  &  R18 +SGD  &  R18 +Adam+Reg  &  R18 +SGD+Reg ", "  &  20%  &  30%  &  40%  &  20%  &  30%  &  40%  &  20%  &  30%  &  40%  &  20%  &  30%  &  40%  &  20%  &  30%  &  40%  &  20%  &  30%  &  40% ", " RSB  &  77.3  &  80.3  &  82.6  &  79.1  &  82.4  &  84.7  &  74.1  &  77.3  &  80.8  &  80.1  &  84.1  &  86.2  &  86.7  &  89  &  90.4  &  84.8  &  87.8  &  89.3 ", " Coreset  &  76.7  &  79.9  &  82.4  &  79.1  &  82.9  &  83.7  &  74.4  &  78.8  &  81.1  &  80.1  &  84  &  86.5  &  86.4  &  88.9  &  90.3  &  85.1  &  87.2  &  89.2 ", " VAAL  &  77.0  &  80.3  &  82.4  &  78.9  &  82.7  &  84.1  &  75.7  &  79.6  &  81.5  &  79.6  &  83.8  &  86.4  &  86.6  &  88.9  &  90.5  &  84.9  &  87.7  &  89.3 ", " QBC  &  77.2  &  80.3  &  81.6  &  78.1  &  82.9  &  84.9  &  74.3  &  77.8  &  80.6  &  79.9  &  83.6  &  86.1  &  86.6  &  88.9  &  90.1  &  85.1  &  87.6  &  89.3  "], "ref_sec": [["<section> <title> 1 Introduction </title>  Active learning (AL) is a machine learning paradigm that promises to help reduce the burden of data annotation by intelligently selecting a subset of informative samples from a large pool of unlabeled data that are relatively more conducive for learning.", "In AL, a model trained with a small amount of labeled seed data is used to parse through the unlabeled data to select the subset that should be sent to an annotator (called oracle in AL literature).", "To select such a subset, AL methods rely on exploiting the latent-space structure of samples, model uncertainty, or other such heuristics.", "The promise of reducing annotation cost has brought a surge in recent AL research [@bib:VAAL_sinha2019variational] , [@bib:coreset_sener2018active] , [@bib:Ensembles_Beluch2018ThePO] , [@bib:DBAL_gal2017deep] , [@bib:BatchBald_DBLP:journals/corr/abs-1906-08158] , [@bib:BGALD_DBLP:journals/corr/abs-1904-11643] , [@bib:yoo2019learning_loss_for_AL] , and with it, a few outstanding issues.", "\\newline First , the results reported for RSB vary significantly between studies.", "For example, using $ 20\\% $ labeled data of CIFAR10, the difference between RSB performance reported by [@bib:yoo2019learning_loss_for_AL] and [@bib:BGALD_DBLP:journals/corr/abs-1904-11643] is $ 13\\% $ under identical settings.", "Second , the results reported for the same AL method can vary across studies: using VGG16 [@bib:vgg16_simonyan2014very] on CIFAR100 [@bib:cifar10_dataset_krizhevsky2009learning] with $ 40\\% $ labeled data, [@bib:coreset_sener2018active] reports $ \\approx 55\\% $ classification accuracy whereas [@bib:VAAL_sinha2019variational] reports $ 47.01\\% $ for [@bib:coreset_sener2018active] .", "Third , recent AL studies have been inconsistent with each other.", "For example, [@bib:coreset_sener2018active] and [@bib:Adv_AL_DBLP:journals/corr/abs-1802-09841] state that diversity-based AL methods consistently outperform uncertainty-based methods, which were found to be worse than the random sampling baseline (RSB).", "In contrast, recent developments in uncertainty based studies [@bib:yoo2019learning_loss_for_AL] suggest otherwise.", "\\newline In addition to these issues, results using a new AL method are often reported on simplistic datasets and tested under limited experimental conditions, with an underlying assumption that the relative performance gains using an AL method would be maintained under changes in the experimental conditions.", "These issues with reporting of AL results has spurred a recent interest in benchmarking of AL methods and recent NLP and computer vision studies have raised a number of interesting questions [@bib:inspire1_DBLP:journals/corr/abs-1807-04801] , [@bib:inspire2_prabhu2019sampling] , [@bib:mittal2019parting] .", "With the goal of improving the reproducibility and robustness of AL methods, in this study we evaluate the performance of these methods for image classification compared to a RSB in a fair experimental environment.", "The contributions of this study are as follows.", "\\newline Contributions : Through a comprehensive set of experiments performed using our PyTorch-based AL evaluation toolkit we compare different AL methods including state-of-the-art diversity-based, uncertainty-based, and committee-based methods [@bib:VAAL_sinha2019variational] , [@bib:coreset_sener2018active] , [@bib:Ensembles_Beluch2018ThePO] , [@bib:DBAL_gal2017deep] and a well-tuned RSB.", "We demonstrate that: 1) results with our RSB are higher across a range of experiments than previously reported, 2) state-of-the-art AL methods achieve a marginal gain over our RSB under narrow combination of experimental conditions (e.g. a specific architecture), which vanishes with changes in experimental conditions (e.g. using a different architecture for classifier), 3) variance in evaluation metric (accuracy) across repeated runs on the same set of data, or on different fold of initial labeled data, can lead to incorrect conclusions where accuracy gain using an AL method may be observed within the margin of error of accuracy measurement, 4) a bit surprisingly, our experiments also show that these performance gains vanish when the neural networks are well-regularized, and none of the evaluated AL methods performs better than our RSB 5) the variance in accuracy achieved using AL methods is substantially lower in consistent repeated training runs with a well-regularized model, suggesting that such a training regime is unlikely to effect misleading results in AL experiments, 6) finally, we conclude the paper with a set of guidelines on experimental evaluation of a new AL method, and provide a PyTorch-based AL toolkit to facilitate this.", "\\newline  </section>"], ["<section> <title> 2 Pool-Based Active Learning Methods </title>  Contemporary pool-based AL methods can be broadly classified into: (i) uncertainty based [@bib:VAAL_sinha2019variational] , [@bib:DBAL_gal2017deep] , [@bib:BatchBald_DBLP:journals/corr/abs-1906-08158] , (ii) diversity based [@bib:coreset_sener2018active] , [@bib:Adv_AL_DBLP:journals/corr/abs-1802-09841] , and (iii) committee based [@bib:Ensembles_Beluch2018ThePO] .", "AL methods also differ in other aspects, for example, some AL methods use the task model (e.g. model trained for image classification) within their sampling function [@bib:DBAL_gal2017deep] , [@bib:coreset_sener2018active] , where as others use different models for task and sampling functions [@bib:VAAL_sinha2019variational] , [@bib:Ensembles_Beluch2018ThePO] .", "These methods are discussed in detail next.", "\\newline Notations : Starting with an initial set of labeled data $ L_{0}^{0} $ = $ \\{(x_{i},y_{i})\\}_{i=1}^{N_{L}} $ and a large pool of unlabeled data $ U_{0}^{0} $ = $ \\{x_{i}\\}_{i=1}^{N_{U}} $ , pool-based AL methods train a model $ \\Phi_{0} $ .", "A sampling function $ \\Psi(L_{0}^{0},U_{0}^{0},\\Phi_{0}) $ then evaluates $ x_{i}\\in U_{0} $ , and selects $ k $ (budget size) samples to be labeled by an oracle.", "The selected samples with oracle-annotated labels are then added to $ L_{0}^{0} $ , resulting in an extended $ L_{0}^{1} $ labeled set, which is then used to retrain $ \\Phi $ .", "This cycle of sample-annotate-train is repeated until the sampling budget is exhausted or a satisficing metric is achieved.", "AL sampling functions evaluated in this study are outlined next.", "\\newline <subsection> <title> 2.1 Model Uncertainty on Output (UC) </title> The method in [@bib:uncertainty_lewis1994sequential] ranks the unlabeled datapoints, $ x_{i}\\in U $ in a descending order based on their scores given by $ \\max_{j}\\Phi(x_{i});j\\in\\{1\\dots C\\} $ , where $ C $ is the number of classes, and chose the top $ k $ samples.", "Typically this approach focuses on the samples in $ U $ for which the softmax classifier is least confident.", "\\newline </subsection> <subsection> <title> 2.2 Deep Bayesian Active Learning (DBAL) </title> [@bib:DBAL_gal2017deep] train the model $ \\Phi $ with dropout layers and use Monte carlo dropout to approximate the sampling from posterior.", "For our experiments, we used the two most reported acquisitions i .", "e ., max entropy and Bayesian Active Learning by Disagreement (BALD).", "The max entropy method selects the top $ k $ datapoints having maximum entropy ( $ \\operatorname*{arg\\,max}_{i}\\mathbb{H}[P(\\textbf{y}|x_{i})];\\forall x_{i}\\in U% _{0} $ ) where the posterior is given by, $ P(\\textbf{y}|x_{i})=\\sum_{j=1}^{T}\\frac{1}{T}P(\\textbf{y}|x_{i},\\phi_{j}) $ ;where $ T $ denotes number of forward passes through the model, $ \\Phi $ .", "BALD selects the top $ k $ samples that increase the information gain over the model parameters i .", "e ., $ \\operatorname*{arg\\,max}_{i}\\mathbb{I}[P(\\textbf{y},\\Phi|x_{i},L_{0})];\\forall x% _{i}\\in U_{0} $ .", "We implement DBAL as described in [@bib:DBAL_gal2017deep] where probability terms in information gain is evaluated using above equation.", "\\newline </subsection> <subsection> <title> 2.3 Center of Gravity (CoG) </title> Uncertainty in unlabeled datapoints is estimated in terms of the euclidean distance from the centre of gravity ( $ z_{\\text{cog}} $ ) in the latent space.", "We define the COG as: $ z_{\\text{cog}}=\\sum_{i=1}^{N_{L}+N_{U}}\\frac{\\Phi^{l}({x_{i}})}{|N_{L}+N_{U}|} $ , where $ \\Phi^{l}(x_{i}) $ denotes the $ l^{\\text{th}} $ layer activations of the model $ \\Phi $ for $ x_{i} $ .", "Using this distance estimate, we select the top $ k $ farthest datapoints from CoG. For our experiments, we use the penultimate layer activations.", "\\newline </subsection> <subsection> <title> 2.4 Coreset </title> [@bib:coreset_sener2018active] exploit the geometry of datapoints and choose samples that provide a cover to all datapoints.", "Essentially, their algorithm tries to find a set of points (cover-points), such that distance of any datapoint from its nearest cover-point is minimized.", "They proposed two sub-optimal but efficient solutions to this NP-Hard problem: coreset-greedy and coreset-MIP (Mixed Integer programming), coreset-greedy is used to initialize coreset-MIP.", "For our experiments, following [@bib:yoo2019learning_loss_for_AL] , we implement coreset-greedy since it achieves comparable performance while being significantly compute efficient.", "\\newline </subsection> <subsection> <title> 2.5 Variational Adversarial Active Learning (VAAL) </title> [@bib:VAAL_sinha2019variational] combined a VAE [@bib:VAE_kingma] and a discriminator [@bib:Goodfellow:2014:GAN:2969033.2969125] to learn a metric for AL sampling.", "VAE encoder is trained on both $ L\\text{and }U $ , and the discriminator is trained on the latent space representations of $ L\\text{and }U $ to distinguish between seen ( $ L $ ) and unseen ( $ U $ ) images.", "Sampling function selects samples from $ U $ with lowest discriminator confidence (to be seen) as measured by output of discriminator\u2019s softmax.", "Effectively, samples that are most likely to be unseen based on the discriminator\u2019s output", "are chosen.", "\\newline <float> AL Training Schedule \\ Input $ AL_{iter} $ , Budget size $ k $ and Oracle, $ \\mathcal{A} $ \\ \\ Split $ \\mathcal{D}\\rightarrow\\{T_{r},T_{s},V\\} $ \\ \\ Split $ T_{r}\\rightarrow\\{L_{0}^{0},U_{0}^{0}\\} $ \\ \\ Train a base classifier, $ \\mathcal{B} $ using only $ L_{0}^{0} $ \\ \\ $ \\phi=\\mathcal{B} $ \\ \\ while i $ \\in\\{0\\dots AL_{iter}\\} $ do \\ \\ sample $ \\{x_{j}\\}_{j=1}^{k}\\in U_{i} $ using $ \\Psi(L_{0}^{i},U_{0}^{i},\\phi) $ \\ \\ $ \\{x_{j},y_{j}\\}_{j=1}^{k}\\leftarrow\\{x_{j},\\mathcal{A}(x_{j})\\}_{j=1}^{k} $ \\ \\ $ i\\leftarrow i+1 $ \\ \\ $ L_{0}^{i}\\leftarrow L_{0}^{i}\\cup\\{x_{j},y_{j}\\}_{j=1}^{k} $ \\ \\ $ U_{0}^{i}\\leftarrow U_{0}^{i}\\setminus\\{x_{j},y_{j}\\}_{j=1}^{k} $ \\ \\ $ \\phi\\leftarrow $ Initialize randomly \\ \\ while convergence do \\ \\ Train $ \\phi $ using only $ L_{0}^{i} $ \\ \\ end while \\ \\ end while \\ </float> \\newline </subsection> <subsection> <title> 2.6 Ensemble Variance Ratio Learning </title> Proposed by [@bib:Ensembles_Beluch2018ThePO] , this is a query-by-committee (QBC) method that uses a variance ratio computed by $ v=1-f_{m}/N $ to select the sample set with the largest dispersion ( $ v $ ), where $ N $ is the number of committee members (CNNs), and $ f_{m} $ is the number of predictions in the modal class category.", "Variance ratio lies in 0\u20131 range and can be treated as an uncertainty measure.", "We note that it is possible to formulate several AL strategies using the ensemble e.g. BALD, max-entropy, etc.", "Variance ratio was chosen for this study because it was shown by authors to lead to superior results.", "For training the CNN ensembles, we train 5 models with VGG16 architecture but a different random initialization.", "Further, following [@bib:Ensembles_Beluch2018ThePO] , the ensembles are used only for sample set selection, a separate task classifier is trained in fully-supervised manner to do image classification.", "\\newline </subsection>  </section>"], ["<section> <title> 3 Regularization and Active Learning </title>  In a ML training pipeline comprising data\u2013model\u2013metric and training tricks, regularization can be introduced in several forms.", "In neural networks, regularization is commonly applied using parameter norm penalty (metric), dropout (model), or using standard data augmentation techniques such as horizontal flips and random crops (data).", "However, parameter norm penalty coefficients are not easy to tune and dropout effectively reduces model capacity to reduce the extent of over-fitting on the training data, and requires the drop probability to be tuned.", "On the other hand, several recent studies in semi-supervised learning (SSL) have shown promising new ways of regularizing neural networks to achieve impressive gains.", "While it isn\u2019t surprising that these regularization techniques help reduce generalization error, most AL studies have overlooked them.", "We believe this is because of a reasonable assumption that if an AL method works better than random sampling, then its relative advantage should be maintained when newer regularization techniques and training tricks are used.", "Since regularization is critical for low-data training regime of AL where the massively-overparameterized model can easily overfit to the limited training data, we investigate the validity of such assumptions by applying regularization techniques to the entire data\u2013model\u2013metric chain of neural network training.", "\\newline Specifically, we employ parameter norm penalty, random augmentation (RA) [@bib:cubuk2019randaugment] , stochastic weighted averaging (SWA) [@bib:SWA_izmailov2018averaging] , and shake-shake (SS) [@bib:shake-shake_gastaldi2017shake] .", "In RA, a sequence of $ n $ randomly chosen image transforms are sequentially applied to the training data, with a randomly chosen distortion magnitude ( $ m $ ) which picks a value between two extremes.", "For details of extreme values used for each augmentation choice, we refer the reader to work of [@bib:cubuk2018autoaugment] .", "SWA is applied on the model by first saving $ e $ snapshots of model during the time-course of optimization, and then averaging the snapshots as a post-processing step.", "For SS experiments, we utilize the publicly available pytorch implementation .", "The hyper-parameters associated with these techniques as well as experiments and results with regularization applied to neural network training with AL-selected sample sets are discussed in Sec. [@ref:LABEL:Regularization] .", "\\newline  </section>"], ["<section> <title> 4 Implementation Details </title>  We perform experiments on CIFAR10, CIFAR100, and ImageNet by following the training schedule summarized in Alg.", "[@ref:LABEL:alg:train_schedule] .", "Given a dataset $ \\mathcal{D} $ , we split it into train ( $ T_{r} $ ), validation ( $ V $ ), and test ( $ T_{s} $ ) sets.", "The train set is further divided into the initial labeled ( $ L_{0} $ ) and unlabeled ( $ U_{0} $ ) sets.", "A base classifier $ \\mathcal{B} $ is first trained, followed by iterations of sample-annotate-train process using various AL methods.", "Model selection is done by choosing the best performing model on the validation set.", "For a fair comparison, a consistent set of experimental settings is used across all methods.", "Dataset-specific training details are discussed next.", "\\newline Learning rate ( $ lr $ ) and weight decay ( $ wd $ ) were tuned using grid search, and set as follows for individual datasets.", "CIFAR10 : optimizer=Adam [@bib:adam_DBLP] , $ lr=5e^{-4} $ , $ wd=5e^{-4} $ , input pre-processed using random horizontal flip ( $ p=0.5 $ ) and normalization (divide by 255).", "CIFAR100 : optimizer=Adam, $ lr=5e^{-4} $ and $ wd=0 $ for AL iterations and $ lr=5e^{-5} $ and $ wd=0 $ for base classifier that was trained on $ L_{0} $ , input pre-processed using random crop (pad=4) followed by horizontal flip ( $ p=0.5 $ ) and normalization (divide by 255).", "ImageNet : optimizer=SGD, $ wd=3e^{-4} $ .", "We train the base classifier on $ L_{0} $ for $ 200 $ epochs where $ lr=0.1 $ with a linear warm-up schedule (for first $ 5 $ epochs) followed by decaying the $ lr $ by a factor of $ 10 $ on epoch number: $ \\{140,160,180\\} $ .", "For AL iterations we fine-tune the best model (picked by validation set accuracy) from previous iteration for $ 100 $ epochs where $ lr=1e^{-2} $ which gets decayed by a factor of $ 10 $ on epoch number: $ \\{35,55,80\\} $ .", "Further, we choose the best model based on a realistically small validation set ( i .", "e ., $ 12811 $ images) following [@bib:S4L_DBLP:journals/corr/abs-1905-03670] .", "The input is pre-processed using random crops resized to $ 224 $ x $ 224 $ followed by horizontal flip (p=0.5) and normalized to zero mean and one standard deviation using statistics of initial $ 10\\% $ partition.", "\\newline Architecture : We use VGG16 [@bib:vgg16_simonyan2014very] with batchnorm [@bib:batchnorm_ioffe2015batch] , 18-layer ResNet [@bib:resnet_he2016deep] , and 28-layer 2-head Wide-ResNet (WRN-28-2) [@bib:wide_resnet_zagoruyko2016wide] in our experiments.", "For both target architectures we use ^(,) .", "For CIFAR10/100 models we set the number of neurons in penultimate fully-connected layer of VGG16 to $ 512 $ as in .", "\\newline Regularization Hyper-parameters: CIFAR10, $ lr $ = $ 5e^{-4} $ ; $ wd=0 $ and CIFAR100, $ lr $ = $ 5e^{-5} $ ; $ wd=0 $ .", "Adam optimizer is used for both datasets.", "RA parameters are: CIFAR10: n=1, m=5, CIFAR100: n=1, m=2, ImageNet: n=2, m=9.", "We empirically select the SWA hyperparameters as: CIFAR 10/100: SWA LR: $ 5e^{-4} $ and frequency: $ 50 $ .", "Imagenet: SWA LR: $ 1e^{-5} $ and frequency: $ 50 $ .", "These parameters are selected after performing a grid search and kept consistent across experiments.", "We always train a model from scratch in each AL iteration except for Imagenet due to its heavy compute budget.", "\\newline Implementation of AL methods : We developed a PyTorch-based toolkit to evaluate the AL methods in a unified implementation.", "AL methods can be cast into two categories based on whether or not AL sampling relies on the task model (classifier network).", "For example, coreset uses the latent space representations learnt by task model to select the sample set, whereas VAAL relies on a separate VAE-discriminator network to select the samples, independent of the task model.", "In our implementation, we abstract these two approaches in a sampling function that may use the task model if required by the AL method.", "Each AL method was implemented using a separate sampling function, by referencing author-provided code if it was available.", "Using command line arguments, the toolkit allows the user to configure various aspects of training such as architecture used for task model, AL method, size of initial labeled set, size of acquisition batch, number of AL iterations, hyper-parameters for task model training and AL sampling and number of repetitions.", "\\newline  </section>"], ["<section> <title> 5 Experiments and Results </title>  All experiments were performed using 2 available nVidia DGX-1 servers, with each experiment utilizing 1\u20134 GPUs out of available 8 GPUs on each server.", "All codes were written in Python using PyTorch and other libraries in addition to third-party codebases.", "We plan to release our codebase on GitHub soon, for early-access please contact the authors.", "\\newline <subsection> <title> 5.1 Variance in Evaluation Metrics </title> Training a neural network involves many stochastic components including parameter initialization, data augmentation, mini-batch selection, and batchnorm whose parameters change with mini-batch statistics.", "These elements can lead to a different optima thus resulting in varying performances across different runs of the same experiment.", "To evaluate the variance in classification accuracy caused by different initial labeled data, we draw five random initial labeled sets ( $ L_{0}\\dots L_{4} $ ) with replacement.", "Each of these five sets were used to train the base model, initialized with random weights, 5 times; a total of 25 models were trained for each AL method to characterize variance within-sample-sets and between-sample-sets.", "\\newline From the results summarized in Fig. [@ref:LABEL:fig:cifar_five_lSets_stats] , we make the following observations: (i) A standard deviation of 1 to 2.5 % in accuracy among different AL methods, indicating that out of chance, it is possible to achieve seemingly better results.", "(ii) In contrast to previous studies, our extensive experiments indicate that compared to RSB, no AL method achieves strictly better classification accuracy.", "At times, RSB appears to perform marginally better; for example, it achieves best mean accuracy of $ 80.36\\% $ (on CIFAR10 with $ 30\\% $ labeled data) and $ 35.72\\% $ (on CIFAR100 with $ 20\\% $ labeled data), whereas the second best performance is given by DBAL and VAAL i .", "e ., $ 80.25\\% $ and $ 35.59\\% $ respectively.", "(iii) Our results averaged over 25 runs in Fig. [@ref:LABEL:fig:cifar_five_lSets_stats] (f) and (l) indicate that no method performs clearly better than others.", "An ANOVA and pairwise multiple comparisons test with Tukey-Cramer FWER correction revealed that no AL method\u2019s performance was significantly different from RSB.", "This provides a strong evidence and need to repeat an experiment over multiple runs to demonstrate true effectiveness of an AL method.", "\\newline </subsection> <subsection> <title> 5.2 Differing Experimental Conditions </title> Next, we compare AL methods and RSB by modifying different experimental conditions for annotation batch size, size of validation set and class imbalance.", "\\newline Annotation Batch Size ( $ b $ ) : Following previous studies, we experiment with annotation batch size ( $ b $ ) equal to 5%, and 10% of the overall sample count ( $ L+U $ ).", "\\newline Results in Fig. [@ref:LABEL:fig:cifar_budgetsize_exp] (corresponding table in supplementary section) show that VAAL and UC perform marginally better than the RSB, although this is inconsistent.", "For example, on CIFAR100 at $ 20\\% $ labeled data, and $ b=10\\% $ , VAAL performs marginally better than most of the AL methods (Fig. [@ref:LABEL:fig:cifar_five_lSets_stats] (l)).", "This is in contrast to results with $ b=5\\% $ (Fig. [@ref:LABEL:fig:cifar_budgetsize_exp] ).", "We therefore conclude that no AL method offers consistent advantage over others under different budget size settings.", "\\newline Validation Set Size : During training, we select the best performing model on the validation set ( $ V $ ) to report the test set ( $ T_{s} $ ) results.", "To evaluate the sensitivity of AL results to the size of $ V $ , we perform experiments on CIFAR100 with three different $ V $ sizes: $ 2\\% $ , $ 5\\% $ , and $ 10\\% $ of the total samples ( $ L+U $ ).", "From results in Table [@ref:LABEL:tab:valSet_exps_cifar100] , we do not observe any appreciable trend in accuracy with respect to the size of $ V $ .", "For example, the RSB achieves a mean accuracy of $ 49.8\\% $ , $ 49.1\\% $ , and $ 48.4\\% $ , respectively, for the best model selected using $ 2\\% $ , $ 5\\% $ and $ 10\\% $ of the training data as $ V $ .", "We conclude that AL results do not change significantly with the size of $ V $ , and a small $ V $ set can work for model selection in low-data regimes such as AL, freeing up more data for training the task model; a similar observation was made in a recent SSL study [@bib:S4L_DBLP:journals/corr/abs-1905-03670] .", "\\newline Class Imbalance : Here, we evaluate the robustness of different AL methods on imbalanced data.", "For this, we construct $ L_{0} $ on CIFAR100 dataset, to simulate long tailed distribution of classes by following a power law, where the number of samples of 100 classes are given by $ \\text{samples}[i]=a+b*\\exp^{\\alpha x} $ where $ i\\in\\{1\\dots 100\\};a=100,x=i+0.5,\\alpha=-0.046\\text{and }b=400 $ .", "The resulting sample count per class is normalized to construct a probability distribution.", "Models were trained using previously described settings, with the exception of loss function which was set to weighted cross entropy.", "The results in Fig. [@ref:LABEL:fig:class_imbalance_cif100_exp] show that for the first two AL iterations, RSB achieves the highest mean accuracy ( $ n=5 $ ), and is surpassed by DBAL in the last iteration.", "More importantly, we notice that AL methods demonstrate different degree of change in the imbalanced class setting, without revealing a clear trend in the plot.", "In contrast to the previously reported observations that found AL methods robust to class imbalance in the dataset, we conclude that AL methods do not outperform RSB.", "\\newline </subsection> <subsection> <title> 5.3 Regularization </title> With the motivation stated in section 3, we evaluate the effectiveness of advanced regularization techniques (RA and SWA) in the context of AL using CIFAR10 and CIFAR100 datasets.", "All experimental settings were used as previously reported, with the exception of number of epochs which was increased to 150 (from 100).", "We empirically observed that unlike $ \\ell_{2}- $ regularization, which requires careful tuning, RA and SWA work fairly well with changes in their hyper-parameters.", "We therefore do not use $ \\ell_{2}- $ regularization in these experiments where RA and SWA was applied.", "\\newline Fig. [@ref:LABEL:fig:cifar_randaug_swa_exp] compares different AL methods with RSB on CIFAR10/100 datasets.", "We observe that models trained with RA and SWA consistently achieve significant performance gains across all AL iterations and exhibit appreciably-smaller variance across multiple runs of the experiments.", "Our regularized random-sampling baselines on $ 40\\% $ labeled data achieves mean accuracy of $ 89.73\\% $ and $ 57.16\\% $ respectively on CIFAR10 and CIFAR100.", "We note that using RSB, for CIFAR10, a model regularized using RA and SWA with 20% of training data achieves over 4% higher accuracy compared to a model trained without RA and SWA using much larger 40% of the training data.", "Similarly for CIFAR100, the RSB 20%-model with regularization performs comparably to the 40%-model without regularization.", "Therefore, we consider regularization to be a valuable addition to the low-data training regime of AL, especially given that it significantly reduces the variance in evaluation metric and helps avoid misleading conclusions.", "\\newline An ablative study to show individual contribution of each regularization technique towards overall performance gain is given in Table [@ref:LABEL:tab:SWA_RANDAUG_10_percent_data] .", "The results indicate that both RA and SWA show a significant combined gain of $ \\approx 10 $ %.", "We also experimented with Shake-Shake (SS) [@bib:shake-shake_gastaldi2017shake] in parallel to RA and SWA, and observed that it significantly increases the runtime, and is not robust to model architectures.", "We therefore chose RA & SWA over SS in our experiments.", "\\newline </subsection> <subsection> <title> 5.4 Transferability and Optimizer Settings </title> In principle, the sample sets drawn by an AL method should be agnostic to the task model\u2019s architecture, and a change in the architecture should maintain consistent performance trends for the AL method.", "We conduct an experiment by storing the indices of sample set drawn in an AL iteration on the source network, and use them to train the target network.", "We consider VGG16 as the source, and ResNet18 (RN18) [@bib:resnet_he2016deep] & WRN-28-2 [@bib:wide_resnet_zagoruyko2016wide] as the target architectures.", "From Table [@ref:LABEL:tab:cifar10_transfer_experiments] , we observe that the trend in AL gains is architecture dependent.", "On CIFAR10 with RN18 using Adam, VAAL achieves higher accuracy than RSB.", "However, this relative gain vanishes with RA and SWA.", "Further, there was no discernible trend in results using WRN-28-2 or VGG16 architectures.", "\\newline To evaluate whether the choice of optimizer played a role in VAAL\u2019s performance using RN18 with Adam, we repeated the training with SGD.", "We note the followings (Table [@ref:LABEL:tab:cifar10_transfer_experiments] ): (i) RSB (and other methods) achieved a higher mean accuracy when trained using SGD compared to Adam ( $ 74.1\\% $ vs $ 80.1\\% $ ) on RN18 using $ 20\\% $ CIFAR10 labeled data.", "Further, RN18 with SGD performs comparably against WRN-28-2 with Adam i .", "e ., $ 80.1\\% $ vs $ 79.1\\% $ . (ii) Using Adam, both VAAL and coreset perform favorably against RSB.", "However, with SGD, the results are comparable.", "\\newline </subsection> <subsection> <title> 5.5 Active Learning on ImageNet </title> Compared to CIFAR10/100, ImageNet is more challenging with larger sample count, 1000 classes and higher resolution images.", "We compare coreset, VAAL and RSB on ImageNet.", "We were unable not evaluate QBC due to prohibitive compute cost of training an ensemble of 5 CNN models.", "The details for training hyper-parameters are in supplementary section.", "Results with and without regularization (RA, SWA) are shown in Table [@ref:LABEL:tab:RA_SWA_Imagenet_exp] .", "Using ResNext-50 architecture [@bib:resnext_50_xie2017aggregated] and following the settings of [@bib:S4L_DBLP:journals/corr/abs-1905-03670] ), we achieve improved baseline performances compared to the previously reported results [@bib:Ensembles_Beluch2018ThePO,VAAL_sinha2019variational] .", "From table [@ref:LABEL:tab:RA_SWA_Imagenet_exp] , we observe that both AL methods performed marginally better than RSB though ImageNet experiments are not repeated for multiple runs due to prohibitive compute requirements.", "\\newline </subsection> <subsection> <title> 5.6 Additional Experiments </title> Noisy Oracle : In this experiment, we sought to evaluate the stability of regularized network to labels from a noisy oracle.", "We experimented with two levels of oracle noise by randomly permuting labels of 10% and 20% of samples in the set drawn by random sampling baseline at each iteration.", "From results in Table [@ref:LABEL:tab:NOISY_cifar_10_swa_randaug] , we found that the drop in accuracy for the model regularized by RA and SWA was nearly half ( $ 3\\% $ ) compared with the model trained without these regularizations ( $ 6\\% $ ) on both $ 30\\% $ and $ 40\\% $ data splits.", "Our findings suggest that the noisy pseudolabels generated for the unlabelled set $ U $ by model $ \\phi $ , when applied in conjunction with appropriate regularization, should help improve model\u2019s performance.", "Additional results using AL methods in this setting are shared in the supplementary section.", "Active Learning Sample Set Overlap : For interested readers, we discuss the extent of overlap among the sample sets drawn by AL methods in the supplementary section.", "\\newline </subsection>  </section>"], ["<section> <title> 6 Discussion </title>  Under-Reported Baselines : We note that several recent AL studies show baseline results that are lower than the ones reproduced in this study.", "Table [@ref:LABEL:tab:baselines_vs_reported_results] summarizes our RSB results with comparisons to some of the recently published AL methods, under similar training settings.", "Based on this observation, we emphasize that comparison of AL methods must be done under a consistent set of experimental settings.", "Our observations confirm and provide a stronger evidence for a similar conclusion drawn in [@bib:mittal2019parting] , and to a less related extent, [@bib:oliver2018realistic_ssl] .", "Different from [@bib:mittal2019parting] though, we demonstrate that: (i) relative gains using AL method are found under a narrow combination of experimental conditions, (ii) such gains are not statistically meaningful over random baseline, (iii) more distinctly, we show that the performance gains vanish when a well-regularized training strategy is used.", "\\newline The Role of Regularization : Regularization helps reduce generalization error and is particularly useful in training overparameterized neural networks with low data.", "We show that both RA and SWA can achieve appreciable gain in performance at the expense of a small computational overhead.", "We observed that along with learning rate (in case of SGD), regularization was one of the key factors in reducing the error while being fairly robust to its hyperparameters (in case of RA and SWA).", "We also found that any trend of consistent gain observed with an AL method over RSB on CIFAR10/100 disappears when the model is well-regularized.", "Models regularized with RA and SWA also exhibited smaller variance in evaluation metric compared to the models trained without them.", "With these observations, we recommend that AL methods be also tested using well-regularized model to ensure their robustness.", "Lastly, we note that there are multiple ways to regularize the data-model-metric pipeline, we focus on data and model side regularization using techniques such as RA and SWA, though it is likely that other combination of newer regularization techniques will lead to similar results.", "We do believe that with their simplicity and applicability to a wide variety of model (as compared to methods such as shake-shake), RA and SWA can be effectively used in AL studies without significant hyperparameter tuning.", "\\newline Using Unlabeled Set in Training : Some recent methods such as VAAL use $ U $ set to train another network as part of their sampling routine.", "We argue that for such models, a better baseline comparison would be from the semi-supervised learning (SSL) literature.", "We note that some of the current SSL methods such as UDA [@bib:UDA_xie2019unsupervised] have reported very strong results ( $ 94.71\\% $ on CIFAR10 with $ 8\\% $ labeled training data).", "These results suggest that large number of noisy labels are relatively more helpful in reducing the generalization error as compared to the smaller percentage of high quality labels.", "Further commentary on this topic can be found in [@bib:mittal2019parting] .", "\\newline AL Methods Compared To Strong RSB : Compared to the well-regularized RSB, state-of-the-art AL methods evaluated in this paper do not achieve any noticeable gain.", "We believe that reported AL results in the literature were obtained with insufficiently-regularized models, and the gains reported for AL methods are often not because of the superior quality of selected samples."]], "target": "As shown in Table , the fact that a change in model architecture can change the conclusions being drawn suggests that transferability experiments should be essential to any AL study. Similarly we observed that a simple change in optimizer or use of regularization can influence the conclusions. The highly-sensitive nature of AL results using neural networks therefore necessitates a comprehensive suite of experimental tests."}, {"tabular": ["  Scenes  &  Img  &  Size  &  BB_min  &  BB_max ", " Dave-straight1  &  54  &  $ 455\\times 256 $  &  $ 21\\times 22 $  &  $ 41\\times 49 $ ", " Dave-curve1  &  34  &  $ 455\\times 256 $  &  $ 29\\times 32 $  &  $ 51\\times 49 $ ", " Udacity-straight1  &  22  &  $ 640\\times 480 $  &  $ 48\\times 29 $  &  $ 66\\times 35 $ ", " Udacity-curve1  &  80  &  $ 640\\times 480 $  &  $ 51\\times 51 $  &  $ 155\\times 156 $ ", " Kitti-straight1  &  20  &  $ 455\\times 1392 $  &  $ 56\\times 74 $  &  $ 121\\times 162 $ ", " Kitti-straight2  &  21  &  $ 455\\times 1392 $  &  $ 80\\times 46 $  &  $ 247\\times 100 $ ", " Kitti-curve1  &  21  &  $ 455\\times 1392 $  &  $ 64\\times 74 $  &  $ 173\\times 223 $  "], "ref_sec": [["<section> <title> I Introduction </title>  Deep Neural Networks (DNNs) are being widely applied in many autonomous systems for their state-of-the-art, even human-competitive accuracy in cognitive computing tasks.", "One such domain is autonomous driving, where DNNs are used to map the raw pixels from on-vehicle cameras to the steering control decisions [@bib:chen2015deepdriving,px2] .", "Recent end-to-end learning frameworks make it even possible for DNNs to learn to self-steer from limited human driving datasets [@bib:dave] .", "\\newline Unfortunately, the reliability and correctness of systems adopting DNNs as part of their control pipeline have not been formally guaranteed.", "In practice, such systems often misbehave in unexpected or incorrect manners, particularly in certain corner cases due to various reasons such as overfitted/underfitted DNN models, biased training data, or incorrect runtime parameters.", "Such misbehaviors may cause severe consequences given the safety-critical nature of autonomous driving.", "A recent example of tragedy is that an Uber self-driving car struck and killed an Arizona pedestrian because the autopilot system made an incorrect control decision that \u201c {it didn\u2019t need to react right away} \u201d when the victim was crossing the road at night.", "Even worse, recent DNN testing research has shown that DNNs are rather vulnerable to intentional adversarial inputs with perturbations [@bib:carlini2016towards,kurakin2016adversarial,moosavi2016deepfool,papernot2016limitations,szegedy2013intriguing] .", "The root cause of adversarial inputs and how to systematically generate such inputs are being studied in many recent DNN testing works [@bib:eykholt2018robust,ccs16,eykholt2018robust,carlini2016towards,deepgauge,deepxplore,deeptest,deeproad] .", "While these works propose various testing techniques that prove to be effective, particularly for autonomous driving, they mainly focus on generating {digital} adversarial perturbations, which may never happen in physical world.", "The only exception is a recent set of works [@bib:eykholt2018robust,ccs16] , which take first step in printing robust physical perturbations that lead to misclassification of static physical objects (i.e., printouts in [@bib:robuster] , human face in [@bib:ccs16] , and stop sign in [@bib:eykholt2018robust] ).", "Our work seeks to further enhance physical-world testing of autonomous driving by enhancing test effectiveness during a realistic, continuous driving process.", "Focusing on generating adversarial perturbations on any single snapshot of any misclassified physical object is unlikely to work in practice, as any real-world driving scenario may encounter driving conditions (e.g., viewing angle/distance) that are dramatically different from those in that static single-snapshot view.", "\\newline In this paper, we propose a systematic physical-world testing approach, namely DeepBillboard, targeting at a quite common and practical continuous driving scenario: an autonomous vehicle drives by roadside billboards.", "DeepBillboard contributes to the systematic generation of adversarial examples for misleading steering angle when perturbations are added to roadside billboards in either a {digital} or {physical} manner.", "Note that the basic idea can also be directly generalized to a variety of other physical entities/surfaces besides just billboards along the roadside, e.g., a graffiti painted on a wall; in this work, we choose the roadside billboards as our targeted physical driving scenario for several practical considerations: (1) Billboards are available to rent for advertising everywhere.", "Attackers who rent billboards can customize their sizes and contents, as illustrated in Fig. [@ref:LABEL:fig:rent] ; (2) Billboards are usually considered irrelevant or benign to the safety of transportation, and there are no strict rules regulating the appearance of a billboard; (3) Billboards are usually large enough to read by drivers and thus dashcams for cars with different distances, viewing angles, and light conditions; (4) An attacker may easily construct a physical world billboard to affect the steering decision of driving-by autonomous vehicles without others noticing, e.g., the actual core adversarial painting can only be a part of the entire billboard while the other parts of the billboard can still look normal, e.g., some bottom text bar showing \u201cArt Museum This Saturday\u201d.", "\\newline The objective of DeepBillboard is to generate a single adversarial billboard image that may mislead the steering angle of an autonomous vehicle upon every single frame captured by onboard dashcam during the process of driving by a billboard.", "To generate effective perturbations, a major challenge is to cover a set of image frames exhibiting different conditions, including distance to the billboard, viewing angle, and lighting.", "Simply applying existing DNN testing techniques [@bib:deepxplore,deeptest,deeproad] to generate digital perturbations upon any specific frame clearly does not work in this case, because a realistic driving scene may not incur any frame with same or similar conditions (e.g., inserting sky black holes as done in the recent award-winning DeepXplore work [@bib:deepxplore] ).", "Besides, the effectiveness of single frame perturbation may be not effective, since a mis-steering upon a frame may be quickly corrected by the next frame.", "\\newline To resolve this critical challenge, we develop a robust and resilient joint optimization algorithm, which generates a printable billboard image with perturbations that may mislead the steering angle upon every single frame captured by the dashcam during the entire driving process.", "To maximize the adversarial effectiveness, we develop various techniques to minimize interferences among per-frame-perturbations, and design the algorithm towards achieving global optimality considering all frames.", "Moreover, by inputting videos that record the process of driving by a roadside billboard with different driving patterns (e.g., driving speed and route), our algorithm can be easily tuned to generate printable adversarial image that is robust and resilient considering various physical world constraints such as changing environmental conditions and pixel printability due to printer hardware constraints.", "\\newline Contributions.", "Considering such a real-world driving scenario and developing a corresponding digital and physical adversarial test generation method yield obvious advantages in terms of test effectiveness: the possibility, degree, and duration of misled steering decisions of any driving-by vehicles due to the adversarial billboards can be reliably increased.", "Our key contributions are summarized as follow.", "\\newline <list> \\ We propose a novel angle of testing autonomous driving systems in the physical world that can be easily deployed.", "\\newline \\ \\ We introduce a robust joint optimization method to systematically generate adversarial perturbations that can be patched on roadside billboards both {digitally} and {physically} to consistently mislead steering decisions of an autonomous vehicle driving by the billboard with different driving patterns.", "\\newline \\ \\ We propose new evaluation metrics and methodology to measure the test effectiveness of perturbations for steering models in both digital and physical domains.", "\\newline \\ \\ We prove the robustness and effectiveness of DeepBillboard through conducting extensive experiments with both digital perturbations and physical case studies.", "The digital experimental results show that DeepBillboard is effective for various steering models and scenes, being able to mislead the average steering angle up to 41.93 degree under various scenarios.", "The physical case studies further demonstrate that DeepBillboard is sufficiently robust and resilient for generating physical-world adversarial billboard tests for real-world driving under various weather conditions, being able to mislead the average steering angle error from 4.86 up to 26.44 degree. {To the best of our knowledge, this is the first study demonstrating the possibility of generating realistic and continuous physical-world tests for practical autonomous driving scenarios.} \\newline \\ </list> \\newline  </section>"], ["<section> <title> II Background and Related Work </title>  DNN in Autonomous Driving.", "An autonomous driving system captures surrounding environmental data via multiple sensors (e.g. camera, Radar, Lidar) as inputs, processes these data with DNNs and outputs control decisions (e.g. steering).", "In this paper, we mainly focus on the steering angle component with camera inputs and steering angle outputs, as adopted in NVIDIA Dave [@bib:dave] .", "\\newline Convolutional Neural Network (CNN), which is efficient at analyzing visual imagery, is the most widely used DNN for steering angle decisions.", "Similar to regular neural networks, CNNs are composed of multiple layers and pass information through layers in a feed-forward way.", "Among all layers, the convolutional layer is a key component in CNNs, which performs convolution with kernels on the output of previous layers and sends the feature maps to successor layers.", "Different from another widely used DNN architecture \u2013 Recurrent Neural Networks (RNNs) which is a kind of neural network with feedback connections, CNN-based steering model makes steering decisions based only on the currently captured image.", "In this paper, we focus on the testing of CNN steering models and leave RNN testing as future work.", "We nonetheless note that DeepBillboard can be adapted to apply to RNN testing.", "Intuitively, this can be achieved by modifying the gradient calculation method according to RNN\u2019s specific characteristics.", "\\newline Digital Adversarial Examples.", "Recent research shows that deep neural network classifier can be tested and further fooled by adversarial examples [@bib:carlini2016towards,kurakin2016adversarial,moosavi2016deepfool,papernot2016limitations,szegedy2013intriguing] .", "Such testing can be performed in both black-box [@bib:papernot2016transferability,papernot2017practical] and white-box [@bib:carlini2016towards,kurakin2016adversarial,moosavi2016deepfool,papernot2016limitations,szegedy2013intriguing] settings.", "Goodfellow et al. proposed the fast gradient method that applies a first-order approximation of the loss function to construct adversarial samples [@bib:goodfellow6572explaining] .", "Optimization-based methods have also been proposed to create adversarial perturbations for targeted attacks [@bib:carlini2016towards,liu2016delving] .", "Meanwhile, the recent DeepTest [@bib:deeptest] and DeepRoad [@bib:deeproad] techniques transform original images to generate adversarial images via simple affine/filter transformations or Generative Adversarial Networks (GANs) [@bib:gan] .", "Overall, these methods contribute to understanding digital adversarial examples, and the generated adversarial examples may never exist in reality (e.g., the rainy driving scenes generated by DeepTest [@bib:deeptest] and DeepRoad [@bib:deeproad] are still far from real-world scenes).", "By contrast, our work examines physical perturbations on real objects (billboards) under dynamic conditions such as changing distances and view angles.", "\\newline Physical Adversarial Examples.", "Kurakin et al. showed that adversarial examples, when photoed by a smartphone camera, can still lead to misclassification [@bib:kurakin2016adversarial] .", "Athalye et al. introduced an attacking algorithm to generate physical adversarial examples that are robust to a set of synthetic transformations [@bib:athalye2017synthesizing] .", "They further created 3D-printed replicas of perturbed objects [@bib:athalye2017synthesizing] .", "The main differences between aforementioned works and our work include: (1) Previous works only use a set of synthetic transformations during optimization, which can miss subtle physical effects; while our work can sample from both synthetic transformations and various real-world physical conditions.", "(2) Our work modifies real-world true-sized objects; and (3) Our work targets the testing of realistic and continuous driving scenarios.", "\\newline Sharif et al. presented dodging and impersonation attacks for DNN-based face recognition systems by printing adversarial perturbations on the eyeglasses frames [@bib:ccs16] .", "Their work demonstrated successful physical attacks in relatively stable physical conditions with little variation in pose, distance/angle from the camera, and lighting.", "This contributes an interesting understanding of physical examples in stable environments.", "However, environmental conditions can vary widely in general and can contribute to reducing the effectiveness of perturbations.", "Therefore, we choose the inherently unconstrained environment of drive-by billboards classification.", "In our work, we explicitly design our perturbations to be effective in the presence of diverse and continuous physical-world conditions (particularly, large distances/angles and resolution changes).", "\\newline Lu et al. performed experiments with physical adversarial examples of road sign images against detectors and show that current detectors cannot be attacked [@bib:lu2017no] .", "Several more recent works have demonstrated adversarial examples against detection/segmentation algorithms digitally [@bib:DBLP:journals/corr/XieWZZXY17,metzen2017universal,cisse2017houdini] .", "The most recent work for attacking autonomous driving systems are the works conducted by Eykholt and Evtimov et al. They showed that physical robust attacks can be constructed for road signs classifiers [@bib:eykholt2018robust] , and such attacks can be further extended to attack YOLO detectors [@bib:eykholt2017note] .", "Our work differs from such works due to the fact that: (1) we target attacking steering models by constructing printable perturbations on drive-by billboards, which can be anywhere and have much more impacts than road signs; (2) our proposed algorithm considers a sequence of contiguous frames captured by dashcams with gradually changing distances and viewing angles, and seeks to maximize the possibility and the degree of misleading the steering angles of an autonomous vehicle driving by our adversarial roadside billboard; and (3) we introduce a new joint optimization algorithm to efficiently generate such attacks both digitally and physically.", "\\newline  </section>"], ["<section> <title> III Generating Adversarial Billboards </title>  <subsection> <title> III-A Adversarial Scenarios </title> The goal of DeepBillboard is to mislead the steering angle of an autonomous vehicle, causing off-tracking from the central of the lane by painting the adversarial perturbation on the billboard alongside the road.", "Our targeted DNNs are CNN-based steering models [@bib:dave,dave2,dave3,cg32,rambo] , without involving detection/segmentation algorithms.", "The steering model takes images captured by dashcam as inputs, and outputs steering angle decisions.", "\\newline We use off-tracking distance to measure the test effectiveness (i.e., the strength of steering misleading), which has been applied in Nvidia\u2019s Dave [@bib:dave] system to trigger human interventions.", "Assume the vehicle\u2019s speed is $ v $ m/s, the decision frequency of using DNN inference is $ i $ second(s), the ground truth steering angle is $ \\alpha $ , and the misleading steering angle is $ \\alpha^{\\prime} $ , then the off-tracking distance is calculated by $ v\\cdot i\\cdot\\sin(\\alpha^{\\prime}-\\alpha) $ .", "In potential physical world attack, the speed of the vehicle usually are not controllable by the tester/attacker.", "Thus we use steering angle error which is the steering angle divergence between ground truth and misled steering to measure the test effectiveness.", "\\newline Instead of misleading the steering decision only at a fixed distance and view angle, which may be hardly captured by a driving-by vehicle, we consider the actual driving-by scenario.", "Specifically, when a vehicle is driving towards the billboard, we seek to generate a physical adversarial billboard that may mislead the steering decision upon a sequence of dashcam-captured frames viewing from different distances and angles.", "The number of captured frames clearly depends on the FPS of the dashcam and the time used for the vehicle to drive from the starting position till physically passing the billboard.", "Considering such a real-world dynamic driving scenario yields obvious advantage in terms of attacking strength: the possibility and the degree of misled steering decisions of any driving-by vehicles due to the adversarial billboards can be reliably increased.", "We emphasize that this consideration also fundamentally differentiate the algorithmic design of DeepBillboard from applying simpler strategies such as random search, average/max value-pooling, different order etc.", "Applying such simpler methods would improve misleading angle for a single frame yet lowering the overall objective.", "After a few iterations, such methods hardly improve the objective.", "\\newline </subsection> <subsection> <title> III-B Evaluating Matrices </title> Our evaluating metrics aim to reflect the attacking strength and possibility .", "Vehicles may pass by our adversarial billboard with different speeds and slightly different angles, which may impact the number of image frames captured by the camera and the billboard layout among different frames.", "Assume X\u0302={$ x_{0} $ , $ x_{1} $ , $ x_{2} $ , \u2026 , $ x_{n} $ } denotes an exhaustive set of image frames possibly captured by a drive-by vehicle with any driving pattern (e.g., driving speed and route), then frames captured by any drive-by vehicle are clearly a subset $ X\\subseteq $ X\u0302. Our objective is to generate the physical printable billboard which can affect (almost) every frame in X\u0302, such that any subset $ X $ corresponding to a potential real-world driving scenario may have a maximized chance to be affected.", "To meet this objective, we define two evaluating metrics denoted $ M_{0} $ , $ M_{1} $ as follows.", "\\newline $ M_{0} $ measures the mean angle error (MAE) for every frame in X\u0302: \\newline <equation> $ M_{0}=\\operatorname*{Av\\,g}_{0<i<\\lVert\\hat{X}\\lVert}(f(x_{i}^{\\prime})-f(x_{i% })), $ </equation> where $ f(\\cdot) $ denotes the prediction result of the targeted steering model, $ x^{\\prime} $ denotes the perturbed frame.", "This metric measures the average strength of attacks to the frame super set.", "A larger $ M_{0} $ intuitively would imply a higher chance and a larger error of misleading the steering angle during the process of driving by the billboard.", "\\newline $ M_{1} $ measures the percentage of frames in X\u0302 whose angle error exceeds a predefined threshold, denoted by $ \\tau $ .", "$ \\tau $ can be calculated based on the physical driving behavior.", "A formal definition of $ M_{1} $ is given by: \\newline <equation> $ M_{1}=\\frac{\\|\\{x_{i}|f(x_{i}^{\\prime})-f(x_{i})>\\tau,0<i<\\|\\hat{X}\\|\\}\\|}{\\|% \\hat{X}\\|}. $ </equation> For example, if we want to mislead a 40MPH autonomous vehicle by an off-track distance of one meter within a time interval of 0.2 seconds, then $ \\tau $ can be calculated as 16.24 according to the above equation.", "We mainly adopt $ M_{1} $ as an evaluating metric for our physical-world case studies, as $ M_{1} $ can clearly reflect the number of frames that incur unacceptable steering decisions (e.g., those that may cause accidents) given any reasonable predefined threshold according to safety stands in practice.", "\\newline </subsection> <subsection> <title> III-C Challenges </title> Physical attacks on an object should be able to work under changing conditions and remain effective at fooling the classifier.", "We structure our discussion of these conditions using our targeted billboard classification.", "A subset of these conditions can also be applied to other types of physical learning systems such as drones and robots.", "\\newline Spatial Constraints.", "Existing adversarial algorithms mostly focus on perturbing digital images and add adversarial perturbations to all parts of the image, including background imagery (e.g., sky).", "However, for a physical billboard, the attacker cannot manipulate the background imagery other than the billboard area.", "Furthermore, the attacker cannot assume that there exists a fixed background imagery as it will change depending on the distance and viewing angle of the dashcam of a drive-by vehicle.", "\\newline Physical Limits on Imperceptibility.", "An attractive feature of existing adversarial learning algorithms is that their perturbations to a digital image are often small in magnitude such that the perturbations are almost imperceptible to a casual observer.", "However, when transferring such minimal perturbations to a real world physical image, we must ensure that a camera is able to perceive the perturbations.", "Therefore, there are physical constraints on perturbation imperceptibility, which is also dependent on the sensing hardware.", "\\newline Environmental Conditions.", "The distance and angle of a camera in a drive-by autonomous vehicle with respect to a billboard may consistently vary.", "The captured frames that are fed into a classifier are taken at different distances and viewing angles.", "Therefore, any perturbation that an attacker physically adds to a billboard must be able to survive under such dynamics.", "Other impactful environmental factors include changes in lighting/weather conditions and the presence of debris on the camera or on the billboard.", "\\newline Fabrication Error. To physically print out an image with all constructed perturbations, all perturbation values must be valid colors that can be printed in the real world.", "Furthermore, even if a fabrication device, such as a printer, can produce certain colors, there may exist certain pixel mismatching errors.", "\\newline Context Sensitivity.", "Every frame in X\u0302 must be perturbed considering its context in order to maximize the overall attacking strength (maximizing $ M_{0} $ for instance).", "Each perturbed frame can be mapped to a printable adversarial image with a certain view angle and distance.", "Each standalone frame has its own optimal perturbation.", "However, we need to consider all frames\u2019 context to generate an ultimate single printable adversarial image that is globally optimal w.r.t.", "all frames.", "\\newline In order to physically attack deep learning classifiers, an attacker should account for the above physical world constraints, for otherwise the effectiveness of perturbations can be significantly weakened.", "\\newline </subsection> <subsection> <title> III-D The Design of DeepBillboard </title> We design DeepBillboard, which generates a single printable image that can be pasted on a roadside billboard by analyzing given driving videos where vehicles drive by a roadside billboard with different driving patterns, for continuously misleading the steering angle decision of any drive-by autonomous vehicle.", "DeepBillboard starts with generating perturbations for every frame $ f_{i} $ of a given video without considering frame context and other physical conditions.", "We then describe how to update the algorithm to resolve the aforementioned physical world challenges.", "We finally describe the algorithmic pseudocode of DeepBillboard in detail.", "We note that it may not be practically possible to construct the exhaustive set of image frames (i.e. X\u0302), possibly captured by a drive-by vehicle with any driving pattern (e.g., driving speed and route).", "Nonetheless, processing a larger number of driving videos will clearly strengthen the testing effectiveness of DeepBillboard due to a larger X\u0302, at the cost of increased time complexity.", "\\newline The single frame adversarial example generation searches for a perturbation $ \\sigma $ to be added to the input x such that the perturbed input $ x^{\\prime}=x+\\sigma $ can be predicted by the targeted DNN steering model $ f(\\cdot) $ as \\newline <equation> $ max\\quad H(f(x+\\delta),A_{x}), $ </equation> where $ H $ is a chosen distance function and $ A_{x} $ is the ground truth steering angle.", "To solve the above constrained optimization problem, we reformulate it in the Lagrangian-relaxed form similar to prior work [@bib:eykholt2018robust,ccs16] : \\newline <equation> $ \\operatorname*{arg\\,min}_{\\delta}(-L(f(x+\\delta),A_{x})), $ </equation> where $ L $ is the loss function which measures the difference between the model\u2019s prediction and ground truth $ A_{x} $ .", "The attacking scenario in this paper can be treated as inference dodging which aims to not being correctly inferred.", "\\newline Joint Loss Optimization.", "As discussed earlier, our objective is to generate a single adversarial image that may mislead the steering angle of an autonomous vehicle upon every single frame the dashcam may capture during driving by the billboard.", "The appearance of the adversarial billboard may vary when being viewed from different angles and distances.", "As a result, to meet the objective, we need to generate one single printable adversarial perturbation that can mislead every single frame captured during the driving-by process.", "This is clearly an optimization problem beyond a single image.", "It is thus necessary to consider all frames jointly since one modification on the billboard affects all frames.", "To this end, the problem becomes finding a single perturbation $ \\Delta $ that optimizes Eq. [@ref:LABEL:eq:obj] for every image $ x $ in an image set $ X $ .", "We formalize this perturbation generation as the following optimization problem.", "\\newline <equation> $ \\operatorname*{arg\\,min}_{\\Delta}\\sum_{0<i<\\|X\\|}(-L(f(x_{i}+p_{i}(\\Delta)),A_% {x})), $ </equation> where $ p_{i} $ is the projection function of printable perturbation $ \\Delta $ into every single frame $ i $ .", "\\newline Handling Overlapped Perturbations.", "Every single frame may generate a set of perturbations which is composed of multiple pixels to be updated on the ultimate printable adversarial image.", "Perturbations of multiple frames may encounter overlapped pixels, which may produce interferences among those frames.", "To maximize the attacking strength, DeepBillboard seeks to minimize the overlapped perturbations among multiple frames by only updating a fixed number of $ k $ pixels for each single frame in order.", "The $ k $ pixels are those that have the most impact on misleading the steering decision.", "We assume the final adversarial billboard image covering $ n $ dashcam-captured frames is composed of $ m $ pixels.", "$ k $ is a value satisfying $ n\\cdot k<m $ , which helps reduce the overall chance of perturbation overlapping among frames.", "For each overlapped pixel, we update it by greedily choosing a value that maximizes the objective metric (e.g., $ M_{0} $ ).", "\\newline Enhancing Perturbation Printability.", "For the perturbation to work in the physical world, each perturbed pixel needs to be a printable value by existing printer hardware.", "Let $ P\\subset[0,1]^{3} $ be the set of printable RGB triples.", "We define non-printability score (NPS) of a pixel to reflect the maximum distance between this pixel and any pixel in $ P $ .", "A larger NPS value would imply a smaller chance of accurately printing out the corresponding pixel.", "Our algorithm thus seeks to minimize NPS as part of the optimization.", "We define the NPS of a pixel $ p^{\\prime} $ as: \\newline <equation> $ NPS(p^{\\prime})=\\prod_{p\\in P}|p^{\\prime}-p|.", "$ </equation> We generalize the definition of NPS of a perturbation as the sum of NPS values of all the pixels in this perturbation.", "\\newline Adjust Color Difference under Various Environment Conditions.", "For different environmental conditions, the observable color of the same pixel belonging to the billboard image may look different in the video captured by a dashcam.", "Such difference may impact the adversarial efficacy under different conditions.", "In our physical world experiments, we pre-fill the entire billboard with unicolor $ p=\\{r,g,b\\} $ .", "Under a specific environment condition $ e $ , its actual color shown in camera may become $ p^{\\prime}=\\{r^{\\prime},g^{\\prime},b^{\\prime}\\} $ .", "Based on our experiments, we observe that such color differences of pixels in the same image are almost the same.", "To simplify the problem, we introduce a color adjustment function $ ADJ_{i}=d_{i}(p,p^{\\prime}) $ for each image $ x_{i} $ to adjust the color difference.", "\\newline <float> Generating attacks for maximizing $ M_{0} $ \\ IMGS $ \\triangleright $ List of images of the same scene \\ \\ COORD $ \\triangleright $ List of coordinates of billboards in IMGS \\ \\ ITER $ \\triangleright $ Number of enhance iterations \\ \\ BSZ $ \\triangleright $ Number of images in a batch \\ \\ ADJ $ \\triangleright $ List of adjustment for environment factors \\ \\ DIM $ \\triangleright $ Dimensions of printable perturbation \\ \\ function Generate \\ \\ perturb = COLOR_INIT(DIM) \\ \\ pert_data = zero(BSZ, DIM) \\ \\ for i in ITER do \\ \\ random.shuffle(IMGS) \\ \\ for j in range(0,len(IMGS), BSZ) do \\ \\ batch = IMGS[j, j+BSZ] \\ \\ pert_data.clear \\ \\ for x in batch do \\ \\ grad = $ \\partial obj/\\partial x $ \\ \\ grad = DOMAIN_CONSTRNTS(grad) \\ \\ pert_data[x] = REV_PROJ(grad, ADJ) \\ \\ pert_data = HANDLE_OVERLAP(pert_data) \\ \\ atmpt_pert = pert_data $ \\cdot $ s + perturb \\ \\ atmpt_pert = NPS_CTL(atmp_per, ADJ) \\ \\ atmpt_imgs := UPDATE_IMGS(atmpt_pert, COORD) \\ \\ this_diff = CALC_DIFF(atmp_imgs) \\ \\ if this_diff $ > $ last_diff or rand $ < $ $ SA $ then \\ \\ perturb = APPLY(perturb) \\ \\ imgs := UPDATE_IMGS(perturb, COORD) \\ \\ last_diff = this_diff \\ \\ return perturb \\ </float> \\newline Algorithm overview.", "The procedure of DeepBillboard for generating an adversarial billboard image is illustrated in Fig. [@ref:LABEL:fig:calib] .", "To generate an adversarial billboard image, we first pre-fill the billboard with unicolor, and paint its four corners with contrasting colors for the purpose of (1) locating the coordinates of the billboard digitally, and (2) getting the color adjustment function $ ADJ_{i} $ .", "Then we record video using dashcam and drive by the billboard with different driving behaviors (e.g., different driving speeds and driving patterns) along the road.", "Then we send the pre-recorded videos to our algorithm as inputs to generate the printable adversarial billboard image.", "As discussed earlier, inputting a larger number of driving videos will clearly strengthen the testing effectiveness of DeepBillboard, at the cost of increased time complexity.", "\\newline The pseudocode of our adversarial algorithm is illustrated in Alg.", "[@ref:LABEL:alg:generate] .", "Our algorithm is essentially iteration-based.", "In each iteration, we first obtain perturbation proposals for a batch of randomly chosen images according to their gradients which reflect the influence of every pixel to the final objective.", "We then greedily apply only those proposed perturbations that may lead to better adversarial effect.", "We apply a sufficient number of iterations to maximize steering angle divergence and the perturbation robustness.", "\\newline As seen at the beginning of Alg. [@ref:LABEL:alg:generate] , the inputs include: a list of frames in the pre-recorded videos, a list of coordinates of the four corners in the billboard in every frame, number of enhancing iterations, batch size, a list of color adjustment factors, and the dimension of targeted digital perturbation.", "As illustrated in Alg.", "[@ref:LABEL:alg:generate] , $ perturb $ is a printable perturbation matrix that is composed of RGB pixel values (line 2).", "We use $ COLOR\\_INIT $ to pre-fill the printable perturbation matrix with one unicolor $ c\\in\\{0|255\\}^{3} $ .", "Based on our extensive digital and physical experiments, using unicolor-prefilled matrix may result in better results and faster convergence.", "According to our experiments, gold, blue, and green are the most efficient unicolors for our testing purposes.", "$ pert\\_data $ is a list of matrices which store the proposed perturbations for a batch of images (line 2).", "Lines 4 to 21 loop through enhanced iterations which aim to maximize the adversarial effectiveness and the perturbation robustness.", "At line 5, we randomly shuffle the processing order of captured frames.", "The purpose is to avoid quick convergence to a non-optimal point at early video frames (similar to deep neural network training).", "Starting from line 6, we loop over all the images which are split into batches.", "For each image batch, we initialize and clear $ pert\\_data $ (lines 7-8) before looping over every single image inside the batch (line 9).", "\\newline For each image $ x $ within a batch, we calculate its gradient which is the partial derivative [@bib:goodfellow6572explaining] of object function to input image (line 10).", "By iteratively changing $ x $ using gradient ascent, the object function can be easily maximized.", "We note that we can intentionally mislead targeted steering model to steer left or right by selecting positive or negative value of gradient.", "We then apply domain constraints to the gradient (line 11) to ensure that we only update the pixels belonging to the corresponding area of the billboard, and the pixel values after gradient ascent are within a certain range (e.g., 0 to 255).", "In the implementation, as discussed earlier, we introduce a parameter $ k $ to only apply top $ k $ gradient values that have the most impact on adversarial efficacy.", "This is to reduce the overlapped perturbations among all images.", "Different from the saliency map used in JSMA [@bib:jsma] which represents the confidence score of $ x $ being classified into targeted class for the current image, we consider the influence to the joint objective function for all images in this scene, seeking to maximize the average steering angle difference from ground truth.", "After constraining the applicable gradients, we project the gradient values for each image $ x $ to the proposed perturbations of the batched images (line 12).", "ADJ (i.e., the input list of adjustments for environment factors) is used to correct color difference for different lighting conditions.", "For example, if a pure yellow color $ (255,255,0) $ becomes $ (200,200,0) $ , then $ ADJ $ is set to be $ (55,55,0) $ .", "When projected to the physical billboard, the gradient value should be increased by $ (55,55,0) $ .", "\\newline After all images in the batch get their gradients, there may exist overlapped perturbations among them.", "That is, for each pixel corresponding to overlapped perturbations, it may have multiple proposed update values for the ultimate printable adversarial example.", "To handle such overlaps (line 13), we implemented three methods: (1) update the overlapped pixels with the max gradient value among proposed perturbations, (2) update the overlapped pixels with the sum of all gradient values, and (3) update the overlapped pixels with one of the proposed values that has the greatest overall influence to the objective function.", "Then at line 14, we calculate the proposed update $ atmpt\\_pert $ by adding gradients to the current physical perturbation $ perturb $ .", "After color corrections and non-printable score control (line 15), the proposed perturbations for the physical billboard are projected to the images according to the coordinates (line 16).", "We calculate the total steering angle difference for perturbed images (line 17).", "If the proposed perturbations can improve the objective, or meet the simulated annealing [@bib:van1987simulated] , indicated by $ SA $ .", "we accept the proposed perturbations (line 19) and update all images with these perturbations (line 20).", "Then we record the current iteration\u2019s total steering angle divergence and use it as the starting point in the next iteration.", "When all enhanced iterations are finished, we return the physical perturbation $ perturb $ as the resultant output.", "We note that, although our major goal is to generate physical perturbations, the output can be directly patched to digital images as well.", "\\newline </subsection>  </section>"], ["<section> <title> IV Evaluation </title>  In this section, we evaluate the efficacy of DeepBillboard both digitally and physically for various steering models and road scenes.", "\\newline <subsection> <title> IV-A Experiment Setup </title> Datasets and Steering Models.", "We use four pre-trained popular CNNs as targeted steering models, which have been widely used in autonomous driving testing [@bib:deeproad,deeptest,deepxplore,deepgauge] .", "Specifically, we test three models based on the DAVE self-driving car architecture from NVIDIA, denoted as Dave_V1 [@bib:dave1] , Dave_V2 [@bib:dave2] , Dave_V3 [@bib:dave3] , and the Epoch model [@bib:cg32] from the Udacity challenge [@bib:udacity] .", "Specifically, Dave_V1 is the original CNN architecture presented in NVIDIA\u2019s Dave system [@bib:dave] .", "Dave_V2 [@bib:dave2] is a variation of Dave_V1 which normalizes the randomly initialized network weights and removes the first batch normalization layer.", "Dave_V3 [@bib:dave3] is another publicly available steering model which modifies the original Dave model by removing two convolution layers and one fully connected layer, and inserting two dropout layers among the three fully connected layers.", "As the pre-trained Epoch weights are not publicly available, we train it following the instructions provided by the corresponding authors using the Udacity self-driving Challenge dataset [@bib:udacity] .", "\\newline The datasets used in our experiments include: (1) Udacity self-driving car challenge dataset [@bib:udacity] which contains 101,396 training images captured by a dashboard mounted camera of a driving car and the simultaneous steering wheel angle applied by the human driver for each image; (2) Dave testing dataset [@bib:dave-data] which contains 45,568 images recorded by a GitHub user to test the NVIDIA Dave model; and (3) Kitti [@bib:kitti] dataset which contains 14,999 images from six different scenes captured by a VW Passat station wagon equipped with four video cameras.", "\\newline The dataset used for our physical case studies consists of videos recorded by a tachograph mounted behind the windshield of a driving car for driving by a pre-placed roadside billboard on campus.", "We use aforementioned pre-trained steering models to predict every frame, and use the resultant steering angle decisions as the ground truth.", "\\newline Experiment Design.", "Based on our discussion from Section [@ref:LABEL:sec:metrics] , we evaluate the efficacy of our algorithm by measuring the Average Angle Errors of all frames in a scene, both digitally and physically.", "\\newline For digital tests, our scene selection criteria is that the billboard should appear entirely in the first frame with more than 400 pixels and partially disappear in the last frame.", "We then randomly select seven scenes that satisfy this criteria from aforementioned datasets, and evaluate on all the selected scenes.", "The selected scenes in each dataset cover both straight and curved lane scenarios.", "Since all these datasets do not contain coordinates of billboards, we have to label the four corners of billboards in every frame of the selected scenes.", "To make the labeling process semi-automated, we use the motion tracker functionality of Adobe After Effects [@bib:aftereffects] to automatically track the movement of billboard\u2019s four corners among consecutive frames.", "We then perform necessary adjustments for certain frames whose coordinates are not accurate enough."]], "target": "We list the statistics about all the studied scenes in Table , where the first column lists the names of scenes, the second column shows the number of images in every scene, the third to fifth columns indicate the resolutions of images and the min/max sizes of billboards in each scene. In digital tests, there is no color adjustment under different environmental conditions. The final adversarial example is patched into every frame according to the projection function. Then we use the steering models to predict the patched images and compare them against the ground-truth steering decisions recorded in the given datasets."}, {"tabular": ["    &  Integrator  &  Number of Elements  &  $ \\Delta t^{m} $ [ps]  &  CPU [s] ", " Tier  &  0  &  1  &  2  &  3  &  4  &  0  &  ", " reson-r7.5-Slab  &  LSERK4-CPLTS  &  240  &  288  &  -  &  -  &  -  &  1.24  &  2403 ", " LSERK4-CPLTS-mS  &  240  &  288  &  -  &  -  &  -  &  1.24  &  4051 ", " LSERK4  &  528  &  -  &  -  &  -  &  -  &  1.24  &  4013 ", " LF2-LTS  &  240  &  288  &  -  &  -  &  -  &  0.55  &  1207 ", " LF2  &  528  &  -  &  -  &  -  &  -  &  0.55  &  1917 ", " LF2full-CPLTS  &  160  &  96  &  272  &  -  &  -  &  0.55  &  2381 ", " LF2full  &  528  &  -  &  -  &  -  &  -  &  0.55  &  3615 ", " rcs-1m $ \\* $  &  LSERK4-CPLTS  &  4535  &  57279  &  157  &  -  &  -  &  2.1  &  3733 ", " LSERK4  &  61971  &  -  &  -  &  -  &  -  &  2.1  &  8613 ", " LF2-LTS  &  522  &  8614  &  52798  &  37  &  -  &  0.95  &  963 ", " LF2  &  61971  &  -  &  -  &  -  &  -  &  0.95  &  4348 ", " LF2full-CPLTS  &  114  &  2155  &  7411  &  34521  &  17770  &  0.95  &  1851 ", " LF2full  &  61971  &  -  &  -  &  -  &  -  &  0.95  &  8642  "], "ref_sec": [["<section> <title> 1 Introduction </title>  Many time\u2013stepping algorithms have been proposed in order to improve the performance of Discontinuous Galerkin (DG) based schemes by increasing the maximum time step while preserving stability.", "There are usually two kinds of strategies used for this purpose: to use implicit schemes [@bib:dolean10,piperno07] or, to use a explicit local time\u2013stepping (LTS) technique [@bib:alvarez12,pebernet08,dosopoulos10,piperno07,montseny08,diaz09,grote10,kaser07] .", "The advantage of LTS schemes versus implicit strategies is that the former can be used recursively and easily paralellized.", "This leads to an improvement in performance independent of the relative size distribution of the elements in the mesh.", "Additionally, time integration algorithms may have other constraints on the time\u2013step arising from accuracy considerations and other inherent time scales such as in dispersive media [@bib:sarmany13] or when hybridized with network/lumped elements models [@bib:zhao12] , LTS techniques can also contribute to mitigate these problems in a simple and straightforward way.", "\\newline When a second order convergent spatial discretization is used, the most commonly used time integration method is the second\u2013order leapfrog (LF2) algorithm.", "Several authors [@bib:pebernet08,dosopoulos10] use a LF2-LTS scheme proposed by Montseny [@bib:montseny08] consisting on using the last known values of the fields on the larger time stepped region each time that the smaller one needs a field value.", "Piperno [@bib:piperno07] adopts a similar approach based on a Verlet scheme.", "Alvarez [@bib:alvarez12,alvarez13,alvarez13b,alvarez13c,alvarez13d] contributed with a novel approach to perform LTS in LF2 schemes whereby an interpolation between the fields is used in an interface between the larger and smaller time\u2013stepped regions.", "A rigorous demonstration of the stability and dispersive properties of these schemes is still an open problem.", "\\newline Diaz and Grote [@bib:diaz09,grote10] implemented a rigorous study on the stability and dispersion of LF-LTS high\u2013order schemes applied to the second\u2013order wave equation by means of an eigenvalue analysis.", "They found that the LTS introduces numerical dispersion and can produce instabilities if the global time step is not slightly reduced with respect to a classic implementation.", "The authors also found that the global stability could be improved by enlarging the smaller time\u2013stepped region.", "\\newline For higher order methods, explicit Runge-Kutta (RK) algorithms [@bib:hesthaven07,sarmany07,alvarez10,angulo11,niegemann09,diehl10] seem to be preferred with respect to LF schemes [@bib:fahs09b] .", "Despite of their popularity, there are less works in the literature related to RK-LTS than to LF-LTS.", "For Maxwell\u2019s equations RK-LTS algorithms usually rely on interpolations at the interfaces using previously computed solutions [@bib:hesthaven07] or arbitrary high-order derivatives (ADER) schemes [@bib:kaser07,taube09,liu10] .", "\\newline In this paper we present a novel LTS technique that can be applied to a large variety of time integration algorithms.", "It does not need interpolation between computed solutions and nor directly uses any previously known values.", "Numerical results showing comparisons with analytical solutions for applications on a second\u2013order Leap-Frog (LF2) and on a fourth\u2013order Low Storage Explicit Runge\u2013Kutta scheme (LSERK4) are shown to demonstrate the advantages of the proposed LTS technique.", "\\newline  </section>"], ["<section> <title> 2 Discontinuous Galerkin Semidiscretization </title>  Maxwell\u2019s curl equations for source\u2013less homogeneous media can be written as \\newline <equationgroup> <equation> $ \\vec{\\nabla}\\times\\vec{E}=-\\mu\\partial_{t}\\vec{H} $ $ \\vec{\\nabla}\\times\\vec{E} $ $ =-\\mu\\partial_{t}\\vec{H} $ </equation> <equation> $ \\vec{\\nabla}\\times\\vec{H}=\\varepsilon\\partial_{t}\\vec{E} $ $ \\vec{\\nabla}\\times\\vec{H} $ $ =\\varepsilon\\partial_{t}\\vec{E} $ </equation> </equationgroup> For simplicity, in our discussion we will assume that $ \\varepsilon $ and $ \\mu $ do not vary in the computational domain, and use a system of units where $ \\varepsilon=\\mu=1 $ .", "\\newline We tessellate the computational domain with $ k=1,\\ldots,K $ non\u2013overlapping tetrahedrons.", "In each of those, we apply the Discontinuous Galerkin\u2019s formalism [@bib:hesthaven07,alvarez10,angulo11] to obtain \\newline <equationgroup> <equation> $ \\mathcal{M}_{k}\\partial_{t}{\\mathbf{E}}_{k}(t)+\\mathcal{S}_{k}{% \\mathbf{H}}_{k}(t)-\\sum_{f}\\mathcal{F}_{kf}{\\mathbf{H}}_{kf}^{*}(t)=0 $ $ \\mathcal{M}_{k}\\partial_{t}{\\mathbf{E}}_{k}(t) $ $ +\\mathcal{S}_{k}{\\mathbf{H}}_{k}(t)-\\sum_{f}\\mathcal{F}_{kf}{% \\mathbf{H}}_{kf}^{*}(t)=0 $ </equation> <equation> $ \\mathcal{M}_{k}\\partial_{t}{\\mathbf{H}}_{k}(t)+\\mathcal{S}_{k}{% \\mathbf{E}}_{k}(t)-\\sum_{f}\\mathcal{F}_{kf}{\\mathbf{E}}_{kf}^{*}(t)=0 $ $ \\mathcal{M}_{k}\\partial_{t}{\\mathbf{H}}_{k}(t) $ $ +\\mathcal{S}_{k}{\\mathbf{E}}_{k}(t)-\\sum_{f}\\mathcal{F}_{kf}{% \\mathbf{E}}_{kf}^{*}(t)=0 $ </equation> </equationgroup> With $ \\mathcal{M} $ being the mass matrix, $ \\mathcal{S} $ the spatial semidiscretization of the curl operator and $ \\mathcal{F}_{f} $ the lift operator for face $ f $ .", "$ {\\mathbf{E}} $ and $ {\\mathbf{H}} $ are column vectors containing all the degrees of freedom for the electric and magnetic field respectively.", "$ {\\mathbf{E}}^{*} $ and $ {\\mathbf{H}}^{*} $ are the numerical fluxes.", "\\newline We define a state vector $ \\mathbf{q}_{k}=[{\\mathbf{E}}_{k}\\ {\\mathbf{H}}_{k}]^{T} $ containing all the $ N_{k} $ degrees of freedom of element $ k $ .", "With this definition, we can rewrite system ( [@ref:LABEL:eq:MaxwellGalerkin] ) as a single equation that governs the time evolution of the system, \\newline <equation> $ \\partial_{t}\\mathbf{q}_{k}(t)=-(\\mathcal{M}^{q}_{k})^{-1}\\left({\\mathcal{S}}^{% q}_{k}\\mathbf{q}_{k}(t)-\\sum_{f}\\mathcal{F}^{q}_{kf}\\left(\\mathcal{E}_{kf}% \\mathbf{q}_{k}(t)-\\mathcal{E}_{kf_{+}}\\mathbf{q}_{kf_{+}}(t)\\right)\\right) $ </equation> The DG method gives us some freedom in the selection of the operators $ \\mathcal{E}_{kf} $ and $ \\mathcal{E}_{kf_{+}} $ as long as it respects the properties of consistency, continuity, and monotonicity needed for the numerical flux [@bib:shu10] .", "If this operator is block diagonal with all its components being $ 1/2 $ , we will say that the semi-discrete scheme is using a centered flux and therefore is numerically non-dissipative [@bib:sherwin99,alvarez12] .", "On the other hand, if these operators are non-block diagonal we will say that the flux is being penalized and therefore the semi-discrete scheme is numerically dissipative.", "We will mostly focus on a particular case of penalized flux: the upwind flux [@bib:alvarez10,alvarez12d] , coming from the solution of the Riemann problem.", "\\newline When using penalized fluxes some dissipation is introduced and more operations are needed to compute the flux terms.", "However, introducing such penalization is known to improve numerical dispersion and suppress spurious modes [@bib:montseny08,alvarez10,alvarez12,hesthaven07,hesthaven04] .", "Thus, the penalized fluxes approach is usually preferred for simulations not requiring very long integration times.", "\\newline To simplify the discussion further we will change the basis of the vector space using an invertible operator $ \\mathcal{P}_{k} $ on equation ( [@ref:LABEL:eq:evolution] ) that diagonalizes only the locally applied operators, \\newline <equation> $ \\mathcal{W}_{k}=-\\mathcal{P}_{k}^{-1}(\\mathcal{M}^{q}_{k})^{-1}(\\mathcal{S}^{q% }_{k}-\\sum_{f}\\mathcal{F}^{q}_{kf}\\mathcal{E}_{kf})\\mathcal{P}_{k} $ </equation> We can also define the eigenmodes as \\newline <equation> $ \\mathbf{p}_{k}=\\mathcal{P}^{-1}_{k}\\mathbf{q}_{k} $ </equation> and the external operators as \\newline <equation> $ \\mathcal{V}_{kf}=-\\mathcal{P}_{k}^{-1}(\\mathcal{M}^{q})_{k}^{-1}\\mathcal{F}^{q% }_{kf}\\mathcal{E}_{kf_{+}}\\mathcal{P}_{k} $ </equation> This change of basis let us write equation ( [@ref:LABEL:eq:evolution] ) in the following compact form \\newline <equation> $ \\partial_{t}\\mathbf{p}_{k}(t)=\\mathcal{W}_{k}\\mathbf{p}_{k}(t)+\\sum_{f}% \\mathcal{V}_{kf}\\mathbf{p}_{kf_{+}}(t) $ </equation> \\newline  </section>"], ["<section> <title> 3 Time integration </title>  In the following discussion, we will focus on two time integration methods that are also the most popular choices in conjunction with DG semidiscretizations.", "\\newline <subsection> <title> 3.1 Second-order Leap-Frog (LF2) </title> The second-order leap-frog method [@bib:dosopoulos12] is applied by alternately evolving the $ {\\mathbf{E}}^{n} $ and $ {\\mathbf{H}}^{n+1/2} $ fields, arbitrarily defined at times $ t_{n} $ and $ t_{n}+\\Delta t/2 $ respectively.", "This implies that we do not have a fully defined state vector in the sense of eq. ( [@ref:LABEL:eq:evolution] ) for a given time $ t $ .", "To obtain the future values from a present state the following algorithm is applied \\newline <equationgroup> <equation> $ {\\mathbf{E}}^{n+1}={\\mathbf{E}}^{n}+\\Delta t\\ \\mathcal{L}_{h}% \\left({\\mathbf{H}}^{n+1/2},{\\mathbf{E}}^{n}\\right) $ $ {\\mathbf{E}}^{n+1} $ $ ={\\mathbf{E}}^{n}+\\Delta t\\ \\mathcal{L}_{h}\\left({\\mathbf{H}}^{n+% 1/2},{\\mathbf{E}}^{n}\\right) $ </equation> <equation> $ {\\mathbf{H}}^{n+3/2}={\\mathbf{H}}^{n+1/2}+\\Delta t\\ \\mathcal{L}_{% h}\\left({\\mathbf{E}}^{n},{\\mathbf{H}}^{n+1/2}\\right) $ $ {\\mathbf{H}}^{n+3/2} $ $ ={\\mathbf{H}}^{n+1/2}+\\Delta t\\ \\mathcal{L}_{h}\\left({\\mathbf{E}}% ^{n},{\\mathbf{H}}^{n+1/2}\\right) $ </equation> </equationgroup> With $ \\mathcal{L}_{h} $ being a function representing the result of applying the spatial semi\u2013discretization.", "When centered fluxes are used, the operator $ \\mathcal{L}_{h} $ only uses $ {\\mathbf{H}}^{n+1/2} $ or $ {\\mathbf{E}}^{n} $ as arguments.", "This implies that the scheme is reversible in time and will preserve energy as long as the time step used is below a maximum value $ \\Delta t_{k} $ set by a CFL-like condition [@bib:dolean10,piperno07,dosopoulos12] .", "\\newline </subsection> <subsection> <title> 3.2 Low-Storage Explicit Runge\u2013Kutta (LSERK4) </title> The second method that we will use in our discussion is the five-stage fourth-order Explicit Runge-Kutta method (LSERK4) [@bib:hesthaven07,diehl10] .", "This method states that for a given vector representing the state of the system, i.e. $ \\mathbf{p}_{k}(t)=\\mathbf{p}^{n}_{k} $ we can find an approximate solution state $ \\mathbf{p}_{k}(t+\\Delta t)=\\mathbf{p}_{k}^{n+1} $ applying the following algorithm \\newline <equationgroup> <equation> $ \\mathbf{p}_{k}^{(0)}=\\mathbf{p}_{k}^{n}, $ $ \\mathbf{p}_{k}^{(0)} $ $ =\\mathbf{p}_{k}^{n}, $ </equation> <equation> $ \\mathbf{r}^{(i)}=a_{i}\\mathbf{r}^{(i-1)}+\\Delta t\\left(\\mathcal{W% }_{k}\\mathbf{p}_{k}^{(i-1)}+\\sum_{f}\\mathcal{V}_{kf}\\mathbf{p}_{kf_{+}}^{(i-1)% }\\right), $ $ \\mathbf{r}^{(i)} $ $ =a_{i}\\mathbf{r}^{(i-1)}+\\Delta t\\left(\\mathcal{W}_{k}\\mathbf{p}_% {k}^{(i-1)}+\\sum_{f}\\mathcal{V}_{kf}\\mathbf{p}_{kf_{+}}^{(i-1)}\\right), $ </equation> <equation> $ \\mathbf{p}_{k}^{(i)}=\\mathbf{p}_{k}^{(i-1)}+b_{i}\\mathbf{r}^{(i)}, $ $ \\mathbf{p}_{k}^{(i)} $ $ =\\mathbf{p}_{k}^{(i-1)}+b_{i}\\mathbf{r}^{(i)}, $ </equation> <equation> $ \\mathbf{p}_{k}^{(n+1)}=\\mathbf{p}_{k}^{(5)} $ $ \\mathbf{p}_{k}^{(n+1)} $ $ =\\mathbf{p}_{k}^{(5)} $ </equation> </equationgroup> with $ i\\in[1,...,5] $ and the coefficients $ a_{i} $ , $ b_{i} $ and $ c_{i} $ taking the values indicated in Table [@ref:LABEL:table:rkConstants] .", "The LSERK4 scheme is one of the most used methods in high\u2013order Discontinuous Galerkin semi\u2013discretizations, because it introduces low dispersion and dissipation.", "Contrary to other RK implementations, the low\u2013storage version requires the storage of only two times the number of degrees of freedom in the scheme at the expense of one additional stage.", "RK methods are constrained by the spectra of the operator $ \\mathcal{W}_{k} $ , i.e. all the eigenvalues of $ \\mathcal{W}_{k} $ must lie inside of the stability region of the RK scheme.", "Consequently, the time step must be chosen sufficiently small, e.g. for a nodal basis the following inequality must hold [@bib:hesthaven07] \\newline <equation> $ \\Delta t_{k}\\leq\\frac{C}{c_{k}}\\min_{i}\\frac{\\Delta r_{ki}}{2} $ </equation> where $ \\min_{i}\\Delta r_{ki} $ indicates the minimum distance between nodes in element $ k $ and $ c_{k} $ is the maximum speed of light in the element $ k $ .", "\\newline Despite its many advantages, LSERK4 has a high computational cost and the numerical dissipation it introduces can be a factor depending on the application.", "\\newline </subsection>  </section>"], ["<section> <title> 4 The Causal\u2013Path LTS technique </title>  In this section we introduce the Causal\u2013Path technique as a novel way of performing LTS in different time integration techniques.", "We require two basic properties for the time integration technique: \\newline <list> \\ It has to provide a fully defined state $ \\mathbf{q}_{k}(t) $ for each element.", "\\newline \\ \\ The next state $ \\mathbf{q}_{k}(t+\\Delta t) $ can be explicitly computed from a neighbourhood of elements.", "\\newline \\ </list> \\newline As a first step we will organize the elements in different groups, called tiers, according to their time steps denoted as $ \\Delta t^{m} $ .", "An element $ k $ will belong to a tier $ m=[0,\\ldots,N_{m}-1] $ if its maximum time step $ \\Delta t_{k} $ is such that \\newline <equation> $ \\Delta t^{m}\\leq\\Delta t_{k}<\\Delta t^{m+1} $ </equation> \\newline In order to compute the next time step, we need to use the field values at local and neighbor elements, $ \\mathbf{p}^{(m,i-1)}_{k} $ and $ \\mathbf{p}^{(m,i-1)}_{kf_{+}} $ .", "If there is no connection with other elements belonging to a lower tier, we can evolve all the elements in $ m $ using their $ \\Delta t^{m} $ .", "However, in the border between a tier $ m $ and $ m+1 $ we can not apply the direct algorithm because the value $ \\mathbf{p}^{(m,i-1)}_{k^{m}f_{+}}=\\mathbf{p}^{(m,i-1)}_{k^{m+1}} $ has not been computed.", "\\newline The strategy that we propose is to compute the values $ \\mathbf{p}^{(m,i-1)}_{k^{m+1}} $ using $ \\Delta t^{m-1}=h_{i-1}\\Delta t^{m} $ as time step wherever they are necessary.", "If to do that, we need additional neighbour values that have not been computed, and we recursively apply this idea until a known value is found.", "Thus, starting from $ m=0 $ we can compute all the stages needed to evolve it before starting with the tier $ m=1 $ and so on.", "Finally, the values $ \\mathbf{p}^{(m,i-1)}_{k^{m+1}} $ are casted aside and the upper tiers uses the original values from the lower tier.", "\\newline To compute the next time step values in each of the $ N_{m} $ tiers we may need to compute $ N_{s} $ stages in all the elements of tier $ m $ .", "We will also need to compute intermediate stages between the stages in the $ m+1 $ tier.", "So, in order to avoid a possible interleaving with other higher tiers, we impose that the $ (N_{s}-1) $ \u2013depth neighbourhood of a tier $ m $ is only composed of elements belonging to tier $ m+1 $ or $ m-1 $ .", "This additional condition for the tier assortment is illustrated in Figure [@ref:LABEL:fig:meshneighbourhood] .", "\\newline The implementation of this algorithm may seem difficult at a first glance; however, the recursive nature of the algorithm allows us to make use of recursive calls to the function used to evolve the system.", "Every time the function is called, we pass the information about the tier in which this is being computed and the time step that has to be applied.", "So starting from a call to evolve the $ N_{m} $ tier for a given time step $ \\Delta t $ , the function will recursively call itself on each of the stages of the algorithm passing $ N_{m}-1 $ and $ h_{i}\\Delta t $ as arguments and evolving its corresponding tier elements.", "This technique also requires that the degrees of freedom in the region being interfaced are saved in the higher tier.", "Note that no interpolation of field values is necessary and only past field values generated by the discretization itself are utilized.", "\\newline In the next sections we describe two examples of the CPLTS technique, applied to the LF2 and LSERK4 algorithms, together with illustrations to clarify the concepts.", "\\newline <subsection> <title> 4.1 LF2-CPLTS </title> Since the LF2 performs iterations using a single stage we can create any distribution of $ N_{s} $ intermediate stages in the higher tiers to fit the evaluations needed by the smaller tiers.", "The time\u2013steps of the intermediate stages would then be $ h_{i}\\Delta t^{m+1,i}=\\Delta t^{m} $ , with $ h_{i}>0 $ and the restriction $ \\sum_{i}^{N_{s}}h_{i}=1 $ .", "The choice of $ h_{i}=1/N_{s} $ would be the most favourable in terms of computational cost.", "Figure [@ref:LABEL:fig:lf-cplts] shows an schematic view of this scheme applied to the case $ h_{1}=h_{2}=1/2 $ .", "Note that this freedom in choosing $ h_{i} $ is an improvement compared with the Montseny\u2019s scheme [@bib:montseny08] , which is constrained by the condition $ \\Delta t^{m}=\\Delta t^{m-1}(1+2k) $ .", "This is also an improvement with respect to the Verlet\u2013Piperno\u2019s scheme [@bib:piperno07] in which $ \\Delta t^{m+1}=2\\Delta t^{m} $ , and it allows our scheme to adapt to the different transitions as necessary; however, for the sake of simplicity we will not consider these cases here.", "\\newline On the other hand, we need both values of $ {\\mathbf{E}} $ and $ {\\mathbf{H}} $ at same time instants in order to find a fully\u2013defined state of the system at any given stage $ \\mathbf{p}^{(m,i)} $ .", "In other words, we can\u2019t apply this LTS technique computing only $ {\\mathbf{E}}(t_{n}) $ and $ {\\mathbf{H}}(t_{n}+\\Delta t/2) $ because to compute the intermediate value of a lower tier, let\u2019s say $ {\\mathbf{E}}^{m-1}(t_{n}+\\Delta t^{m}/2) $ we would need the values of the magnetic field $ {\\mathbf{H}}^{m}(t_{n}) $ .", "To overcome this issue we need to apply LF2 twice, doubling the computational costs with respect to the conventional approach.", "\\newline When we apply this scheme to a non\u2013dissipative semi\u2013discretization (e.g. DG with centered flux) we find that the scheme is unstable showing growing high\u2013frequency numerical modes.", "\\newline The introduction of a penalized flux solves this problem through higher frequency damping [@bib:sarmany07,sherwin99] .", "\\newline </subsection> <subsection> <title> 4.2 LSERK4-CPLTS </title> When the CPLTS technique is applied to an LSERK4 we note that the stages are not evenly distributed in time.", "As a result, we apply a variable time step in the lower tiers (Figure [@ref:LABEL:fig:rk-lts] ).", "The values for $ \\Delta t^{m} $ and $ \\Delta t^{m-1} $ are chosen such that equation ( [@ref:LABEL:eq:maxTimeStep] ) is always enforced and therefore \\newline <equation> $ \\max_{i}(h_{i})\\Delta t^{m}=\\Delta t^{m-1} $ </equation> with $ \\max_{i}(h_{i}) $ being the maximum stage size (for LSERK4 $ \\max_{i}(h_{i})=h_{4}=c_{5}-c_{4}=0.336026\\simeq 1/3 $ ).", "Whenever we compute intermediate stages in higher tiers we satisfy this condition because in higher tiers this condition is less restrictive.", "However, every time we apply this division, $ N_{s} $ times more computational operations are needed to get a speed-up of about three times in the higher tier region.", "So, if the largest tier region is not at least $ 5/3 $ times larger than the smallest we won\u2019t see any appreciable global speed-up.", "\\newline For this reason it seems preferable to organize the time tiers with $ \\Delta t^{m-1}=\\Delta t^{m}/N_{s} $ rather than with the maximum stage size criteria.", "By doing this, we are computing an stage in the lower tier region with a time-step bigger than is strictly allowed based on a conventional CFL-like criterion for the associated direct algorithm, which could be a source of potential instability.", "On the other hand, the smaller stages in the lower tier compute the solution using a time-step smaller than the maximum allowed and thus introducing an additional numerical dissipation.", "We may then wonder if the additional dissipation introduced by the smaller stages offsets the potential for instability introduced by the larger.", "Note that as long as these effects are mostly kept limited to high frequency components (which are under-resolved anyway) the solution accuracy should not be impacted.", "In the next sections we perform some tests to assess the practical validity of this approach.", "\\newline </subsection>  </section>"], ["<section> <title> 5 Numerical Results </title>  In this section we present comparisons between results using the proosed CPLTS technique, the LF2-LTS technique introduced by Montseny [@bib:montseny08] , classical implementations of the algorithms, and analytical solutions.", "\\newline For all cases we use nodal basis of order $ P=2 $ and numerical upwind fluxes as described in [@bib:hesthaven07,alvarez10,angulo11,otin10] .", "This implies that we are using 60 degrees of freedom per element.", "The implementation has been performed with an in--house C++ code with OpenMP parallelization .", "GiD was used to obtain meshes and for pre and post-processing .", "Simulations for the reflection and resonance problems were performed using a single processor laptop with Intel(R) Core(TM)2 Duo CPU T9400 @ 2.53GHz processor and running Ubuntu 12.04 LTS.", "The RCS problem were run in a desktop computer with an Intel(R) Core(TM) i7-3960X CPU @ 3.30GHz processor with 12 cores and Ubuntu 10.04 LTS.", "\\newline <subsection> <title> 5.1 Reflection caused by a non-homogeneous mesh </title> The first example we present is an study of the numerical reflection caused by differences in the mesh size, a similar type of analysis can be found in [@bib:chilton08,donderici08] .", "This type of analysis is important for LTS because it quantifies a source of additive noise on the results.", "Figure [@ref:LABEL:fig:mesh-for-reflection] shows the meshes used, together with an isometric view of the boundary conditions employed.", "A plane wave excitation with $ z $ -polarization is introduced in one of the ends of the computational domain and the other end is backed by an Silver-Mueller absorbing (SMA) boundary condition.", "The side\u2013walls of the domain are Perfect Electric Conducting (PEC) and Perfect Magnetic Conducting (PMC) boundary conditions at the $ xy $ and $ xz $ planes respectively.", "The mesh is $ 1\\ \\text{m} $ long from one end to the other.", "The coarse cell size is $ 7.5\\ \\text{cm} $ and the cell sizes in the finer region vary from $ 0.1 $ to $ 0.5\\ \\text{cm} $ .", "\\newline Figures [@ref:LABEL:fig:refl-coeff-r15] , [@ref:LABEL:fig:refl-coeff-r75] and [@ref:LABEL:fig:refl-coeff-slab] show the reflection coefficient in a range of frequencies.", "The closer the values are to zero the better are the properties of the scheme.", "We observe that for this case the LF2 with a fully defined state (LF2full) exhibits slightly better properties than the classic LF2 scheme.", "A possible explanation for this is that the incident wave is resolved using more time steps.", "In LF2-LTS and LF2-CPLTS, we observe some additional degradation when compared to the classic LF2 schemes.", "The CPLTS exhibits less reflection than the Montseny\u2019s LTS, the difference growing with the ratio between the coarser and finer mesh.", "The three LSERK4 figures exhibit a better behaviour than the LF2, as expected due to the higher order of the time integration technique.", "When the maximum stage is used for the tier assortment, we observe a higher degradation in the low-frequency regime, probably because more time\u2013stepping operations are being performed.", "The results for the LSERK4-CPLTS are very encouraging as we see little differences between the use the LSERK4-CPLTS technique and the classic LSERK4.", "Table [@ref:LABEL:table:tierAssortingPWRefl] shows data corresponding to the tier assortment and computational times.", "As expected, the LF2-CPLTS is able of create more tiers than LF2-LTS because it only needs a ratio of two between maximum time step sizes.", "The CPU times for this simulation are listed for reference only and are not quite representative because the time employed to compute the excitation at the boundaries and the initialization is significant when compared with the operations performed to evolve the elements.", "\\newline </subsection> <subsection> <title> 5.2 PEC cavity resonances </title> As a second example we show comparisons of evolving a spatially uncorrelated random field (white noise) to study the resonances of a $ 1 $ m PEC cavity, in a similar way as done in [@bib:kim11] .", "The mesh used is depicted in Figure [@ref:LABEL:fig:slab-r7c5] with PEC boundaries at the ends rather than SMA.", "The resonance frequencies are obtained by performing the Fourier transform of the electric field evolution after $ 250\\ \\text{ns} $ at a point separated $ 0.3\\text{m} $ from one of the boundaries.", "Figure [@ref:LABEL:fig:resonances] show the eigenfrequencies obtained by the simulations together with the exact ones (black dashed vertical lines).", "The LF2 schemes don\u2019t show any particular difference with respect to their dispersive properties.", "The differences in amplitude between LF2 and LF2full can be attributed to the different initial treatment of fields.", "The LSERK4 schemes exhibit a similar behaviour in frequency but we observe additional attenuation when the CPLTS is used.", "When the tiers are assorted using the maximum stage criteria the attenuation is reduced.", "No late time stabilities were observed in any of the simulations."]], "target": "Table shows data corresponding to the tier assortment and computational times. The CPU times show a clear improvement with the LSERK4-CPLTS algorithm while the gains for the LF2-LTS are more moderate. LF2-CPLTS does not perform better than the LF2."}, {"tabular": ["  \\backslashbox TgtModel  &  ASR $ {}_{en} $  &  SLT $ {}_{ende} $  &  SLT $ {}_{enfr} $  &  SLT $ {}_{enes} $ ", " en  &  45.45  &  20.84  &  23.34  &  24.85 ", " +Adapt  &  57.76  &  28.04  &  28.29  &  27.00 ", " de  &  6.77  &  10.48  &  8.43  &  8.05 ", " +Adapt  &  5.58  &  15.48  &  9.41  &  8.13 ", " fr  &  10.85  &  10.66  &  19.10  &  14.06 ", " +Adapt  &  8.27  &  13.48  &  25.04  &  14.46 ", " es  &  6.75  &  8.14  &  9.83  &  14.23 ", " +Adapt  &  5.64  &  10.61  &  11.25  &  19.26  "], "ref_sec": [["<section> <title> 1 Introduction </title>  Research in end-to-end Spoken Language Translation (SLT) has appeared as an effective alternative to the classical cascade approach, i.e. the concatenation of Automatic Speech Recognition (ASR) plus Machine Translation (MT) systems.", "The advantages of the end-to-end systems are mainly accounting for a significant reduction of inference time; direct use of prosodic features; and more relevantly, the avoidance of concatenation errors.", "Even though in recent years new datasets have been released, the available data for SLT is still limited, compared to cascade approaches that rely on ASR and MT datasets.", "Especially the latter, that can usually be orders of magnitude larger.", "In parallel, Multilingual Neural Machine Translation (MultiNMT) systems have been improving the quality of translation, mostly for low-resource pairs that learn from high-resource ones, even allowing the translation of language pairs never seen in training, zero-shot translation.", "These architectures can vary the degree of shared information between the different languages, from universal models where a single model processes all languages [@bib:johnson2017google] to completely language-specific parameters relying on the cross-lingual mapping of several languages into a shared representation space [@bib:escolano:2020] .", "\\newline In this work, we propose to couple a speech encoder to the MultiNMT architecture based on language-specific encoders-decoders, extending the previous MultiNMT system to a new data modality and allowing the system to translate between several languages benefiting from the abundance of MultiNMT data available both in terms of quantity and number of languages.", "We only require parallel data from speech utterances in one language and one of the languages supported by the MultiNMT system.", "Once trained, we can account for Multilingual SLT (MultiSLT) between the added spoken language to any other languages already in the system, without any explicit training for the task, and without parallel data for those directions, zero-shot MultiSLT.", "The two main contributions of this paper are: (1) The training of a zero-shot MultiSLT system using a trained MultiNMT system, both from bilingual SLT and monolingual ASR corpora.", "(2) The use of Adapter networks [@bib:bapna2019simple] for SLT, which leads to significant improvements, +1 BLEU points in performance for both the proposed MultiSLT system and the state-of-the-art ASR and SLT end-to-end baseline systems.", "\\newline  </section>"], ["<section> <title> 2 Background </title>  In this section, we describe the previous work that is used as starting point of our proposal, which are the SLT architecture of the S-transformer [@bib:di-gangi-etal-2019-enhancing] and the MultiNMT architecture of language-specific encoder-decoders [@bib:escolano:2020] .", "\\newline <subsection> <title> 2.1 S-Transformer for SLT </title> The S-Transformer [@bib:di-gangi-etal-2019-enhancing] presents a specific adaptation of the Transformer [@bib:vaswani2017attention] ,which is based on 3 main modifications of its encoder, while keeping the same decoder.", "First, it downsamples the speech input, which is much longer than text inputs, with convolutional neural networks [@bib:dong:2018] .", "Second, it models the bidimensional nature of the audio spectrogram with two-dimensional (2D) components.", "Finally, it adds a distance penalty to the attention creating an attention bias towards short-range dependencies [@bib:sperber:2018] .", "This S-Transformer has been shown to outperform previous baseline systems both in translation quality and computational efficiency and it has established a new state-of-the-art performance on end-to-end Speech Translation for six language pairs.", "Previous works [@bib:9004003] have shown that this architecture is extendable to a multilingual setting, but never on the Zero-Shot scenario.", "\\newline </subsection> <subsection> <title> 2.2 Language-Specific Encoders and Decoders for MultiNMT </title> This approach [@bib:escolano:2020] trains a separate encoder and decoder for each of the $ N $ languages available without requiring multi-parallel corpus but requiring parallel corpus among all $ {N(N-1)} $ translation directions.", "This approach jointly trains independent encoders and decoders for each translation direction.", "In this joint training, the main difference from standard pairwise training is that, in this case, there is only one encoder and decoder for each language, which is used for all translation directions involving that language.", "The approach does not share any parameter across modules from different languages.", "This means that this architecture can be incrementally extended to new modules (other languages or modalities) without retraining the entire system and requiring only parallel data from the new language or modality to one of the languages already in the system.", "Therefore, this is allowing for zero-shot translation from the added language and the other $ N-1 $ languages in the system.", "This approach has been shown to outperform the MultiNMT shared encoder/decoder architecture [@bib:escolano:2020] .", "\\newline </subsection>  </section>"], ["<section> <title> 3 Proposed Methodology </title>  In this work we extend the language specific encoder-decoder architecture to spoken modules.", "For this purpose, we adapt the S-transformer to be compatible with the language specific encoder-decoder architecture and therefore accounting for zero-shot MultiSLT.", "\\newline <subsection> <title> 3.1 Speech Encoder </title> There are several adaptations that the speech encoder based on the S-Transformer described in section [@ref:LABEL:sec:stransf] requires to be compatible with the already existing language-specific encoder-decoders described in section [@ref:LABEL:sec:langspe] .", "First, we use an extra 2D-convolutional layer, in order to further reduce the length of the input speech, resulting in a total of 3 2D-convolutional layers, as in previous works [@bib:hannun:2019] .", "Each convolution halves the temporal dimension of the input waveform resulting in an 8 times shorter sequence, 2 times longer than the text target on average, measured on the validation set.", "Second, we make use of an Adapter module at the end of the speech encoder.", "This modeling technique has been previously proposed for NLP [@bib:bapna2019simple,artetxe2019cross] and used in a variety of tasks including multilingual ASR with a single model [@bib:google:2019] .", "Previous works use this technique to finetune a frozen model for specific languages.", "In our case, we start from a pretrained encoder and decoder and using an Adapter during training allows to better learn the difference in the intermediate representation created by the speech encoder and the language-specific encoder-decoders, helping the model to overcome multimodal differences.", "The Adapter module consists in a step of layer normalization followed by a projection step with ReLU non linear activation.", "This step projects the hidden encoder\u2019s hidden representation into adifferent dimensionality space.", "This representation is projected back to the original dimensionality by a second feedforward layer.", "This process tries to capture new information in a feature richer space while trying to maintain that information when recovering the original dimensionality.", "The final encoder representation will be the sum of the self-attention encoder and adapter outputs, as a residual connection, see Figure [@ref:LABEL:fig:architecture] for a block diagram of the proposed architecture.", "\\newline </subsection> <subsection> <title> 3.2 Adding Speech Modules to the MultiNMT architecture based on Language-Specific Encoders and Decoders </title> Since parameters are not shared between the independent encoders and decoders, the joint training enables the addition of new languages and modalities without the need to retrain the existing modules.", "Let us say we want to add speech of the language $ N $ (hereinafter, $ N_{speech} $ ).", "To do so, we must have parallel data between $ N_{speech} $ and any language in the system.", "For illustration, let us assume that we have $ N_{speech}-I $ parallel data.", "Then, we can set up a new bilingual system with language $ N_{speech} $ as source and language $ I $ as target.", "To ensure that the representation produced by this new pair is compatible with the previously jointly trained system, we use the previous $ I $ decoder ( $ d_{I} $ ) as the decoder of the new $ N_{speech} $ - $ I $ system and we freeze it.", "During training, we optimize the cross-entropy between the generated tokens and $ I $ reference data but update only the parameters of to the $ N_{speech} $ encoder ( $ e_{{N_{speech}}} $ ).", "By doing so, we train $ e_{{N_{speech}}} $ not only to produce good quality translations but also to produce similar representations to the already trained languages.", "\\newline </subsection>  </section>"], ["<section> <title> 4 Experimental Framework </title>  Experiments are runned as follows.", "We build a baseline system which consists in an end-to-end ASR and SLT architecture based on the S-Transformer [@bib:di-gangi-etal-2019-enhancing] .", "Using the pre-trained ASR English encoder, we train the SLT systems from English-to-German, French and Spanish (Baseline); we then alternatively add our proposed architecture presented in section [@ref:LABEL:sec:propo] of the language-specific architecture (+LangSpec) or the Adapter module (+Adapt).", "Finally, we add the combination of both proposed architectures (+LangSpec/Adapt).", "\\newline <subsection> <title> 4.1 Data </title> We use Must-C as speech dataset [@bib:di-gangi-etal-2019-enhancing] , which is a multilingual set extracted from TED talks from English to 8 different languages.", "Must-C is the largest corpus in the languages that we are aiming at (English, German, French, Spanish).", "The amount of transcribed hours varies from 385 to 504, depending on the language pair.", "We are using the English transcribed speeches and the bilingual data on pairs English-German, English-French, English-Spanish.", "We use the training, validation and test splits that Must-C provides.", "Multilingual validation and test sets have around 1.4K and 2.5K sentences (respectively) varying on the language pair.", "The MultiNMT system of language-specific encoder-decoders is trained on the EuroParl corpus [@bib:Europarl] in English to German, French, Spanish as training data, with 2 million parallel sentences among all combinations of these four languages (without being multi-parallel).", "As validation and test set, we used newstest2012/2013 , respectively, from WMT13, which is multi-parallel across all the above languages.", "All data were preprocessed using standard Moses scripts [@bib:koehn2007moses] .", "Note that speech and text data are from different domains, studying the effect on this matter is out-of-scope of this paper.", "\\newline </subsection> <subsection> <title> 4.2 Parameters </title> Our MultiNMT system is based on language-specific encoder-decoders, and we use the same architecture proposed in [@bib:escolano:2020] .", "This architecture uses the transformer with 6 attention blocks for both encoder and decoder and 8 attention-heads each and 512 hidden dimensions, with the only modification of adding Layer Normalization as the last step of both encoder and decoder.", "Experimental results showed adding this step to the original textual space helped model convergence during the SLT training.", "Our baseline SLT architecture follows the parameters in [@bib:di-gangi-etal-2019-enhancing] .", "The basic parameters are identical to the MultiNMT system, with the only modifications to the original system of using 3 2D-convolutional layers (instead of 2) and 2048 hidden dimensions for the feedforward layers to match the MultiNMT model.", "The addition of a 3rd 2D-convolutional helped the model learn the mapping to the pre-trained space.", "On average, over the validation set, this layer reduced the input speech sequence from 4 times longer to just 2 times longer than text, which can help trained decoder attend the input.", "\\newline Finally, we tested several alternatives to projection size for the Adapter module to see their effect on the task.", "Figure [@ref:LABEL:adapter-size] shows the performance of the model for the tasks of English to Spanish, French, and German SLT compared to each respective bilingual baseline system.", "When over-parametrizing the space, we observe an improvement in performance in all cases, especially for 4096 dimensions.", "We tested our models until 9120 dimensions as it was the biggest size we could use without out of memory errors on a single GPU.", "Experiments show that the models obtain their best performance (close to or slightly better than the baseline system) at 4096 dimensions for German and French and 9120 dimensions for Spanish.", "\\newline </subsection>  </section>"], ["<section> <title> 5 Results </title>  Table [@ref:LABEL:results-de] shows that adding a new speech encoder to the MultiNMT language-specific architecture (+LangSpec) is possible by using the existing architecture of the S-Transformer and supervised training with one of the languages in the system (English (en), German (de), French (fr), Spanish (es)).", "We are training a new English speech encoder with a frozen target module of the MultiNMT language-specific architecture (either en, de, fr or es).", "BLEU results vary from 10.80 in English-to-German up to 19.10 in English-to-French.", "These results are improved in a large amount when adding the Adapter (+Adapt), up to almost 6 points BLEU (English-to-French).", "In fact, the Adapter by itself consistently improves in all directions on SLT models.", "\\newline Note that the LangSpec coupling is allowing for zero-shot translation with other languages available in the MultiNMT system."]], "target": "Table shows the results in zero-shot (except for the numbers in italics that show supervised results) with and without Adapt. Zero-shot is achieved whenever we train the English speech encoder with any text decoder available in the LangSpec architecture (en, de, fr, es). For example, column ASR $ _{en} $ is showing zero-shot from English to German, French and Spanish. In this case, we are training the English speech encoder with the English text decoder (using the ASR $ _{en} $ model), and we have zero-shot with English speech to German (target (tgt) de), French (tgt fr) and Spanish (tgt es). We observe that higher zero-shot results are achieved when training on the French text decoder. The Adapter consistently improves in all translation directions on SLT models."}, {"tabular": ["  Model  &  FP32  &  CBDNet  &  Bitrate ", " top-1(%)  &  top-5(%)  &  size-MB  &  top-1(%)  &  top-5(%)  &  size-MB ", " ResNet-18  &  66.41  &  87.37  &  44.57  &  65.27(-1.14)  &  86.62(-0.75)  &  7.31  &  5.25 ", " VGG-16  &  68.36  &  88.44  &  527.7  &  67.36(-1.00)  &  87.81(-0.63)  &  90.3  &  5.47 ", " DenseNet-121  &  74.41  &  92.14  &  30.11  &  73.13(-1.28)  &  91.29(-0.85)  &  5.38  &  5.72  "], "ref_sec": [["<section> <title> Introduction </title>  With the remarkable improvements of Convolutional Neural Networks (CNNs), varied excellent performance has been achieved in a wide range of pattern recognition tasks, such as image classification [@bib:krizhevsky2012imagenet,szegedy2015going,he2015deep,densenet] , object detection [@bib:girshick2014rich,ren2015faster,shen2017dsod] and semantic segmentation [@bib:long2015fully,badrinarayanan2017segnet] , etc.", "A well-performed CNN based systems usually need considerable storage and computation power to store and calculate millions of parameters in tens or even hundreds of CNN layers.", "Therefore,the deployment of CNNs to some resource limited scenarios is hindered, especially low-power embedded devices in the emerging Internet-of-Things (IoT) domain.", "\\newline Many efforts have been devoted to optimizing the inference resource requirement of CNNs, which can be roughly divided into three categories according to the life cycle of deep models.", "First, design-time network optimization considers designing efficient network structures from scratch in a handcraft way such as MobileNet [@bib:howard2017mobilenets] , interlacing/shuffle networks [@bib:zhang2017igc,zhang2017shufflenet] , or even automatic search way such as NASNet [@bib:zoph2016neural] , PNASNet [@bib:liu2017pnas] .", "Second, training-time network optimization tries to simplify the pre-defined network structures on neural connections [@bib:han-learning,deep-compression] , filter structures [@bib:Wen2016Learning,Li2016Pruning,liu2017learning] , and even weight precisions [@bib:hashnet,binarynet,rastegari2016xnor] through regularized retraining or fine-tuning or even knowledge distilling [@bib:hinton2015distilling] .", "Third, deploy-time network optimization tries to replace heavy/redundant components/structures in pre-trained CNN models with efficient/lightweight ones in a training-free way.", "Typical works include low-rank decomposition [@bib:Denton2014Exploiting] , spatial decomposition [@bib:Jaderberg2014Speeding] , channel decomposition [@bib:Zhang2016Accelerating] and network decoupling [@bib:guo2018nd] .", "\\newline To produce desired outputs, it is obvious that the first two categories of methods require a time-consuming training procedure with full training-set available, while methods of the third category may not require training-set, or in some cases require a small dataset (e.g., 5000 images) to calibrate some parameters.", "The optimization process can be typically done within dozens of minutes.", "Therefore, in case that the customers can\u2019t provide training data due to privacy or confidential issues, it is of great value when software/hardware vendors help their customers optimize CNN based solutions.", "It also opens the possibility for on-device learning to compression, and online learning with new ingress data.", "In consequence, there is a strong demand for modern deep learning frameworks or hardware (GPU/ASIC/FPGA) vendors to provide deploy-time model optimizing tools.", "\\newline However, current deploy-time optimization methods can only provide very limited optimization (2 $ \\sim $ 4 $ \\times $ in compression/speedup) over original models.", "Meanwhile, binary neural networks [@bib:courbariaux2015binaryconnect,binarynet] , which aim for training CNNs with binary weights or even binary activations, attract much more attention due to their high compression rate and computing efficiency.", "However, binary networks generally suffer much from a long training procedure and non-negligible accuracy drops, when comparing to the full-precision (FP32) counterparts.", "Many efforts have been spent to alleviate this problem in training-time optimization [@bib:rastegari2016xnor,zhou2016dorefa] .", "This paper considers the problem from a different perspective via raising the question: is it possible to directly transfer full-precision networks into binary networks at deploy-time in a training-free way? We study this problem, and give a positive answer by proposing a solution named composite binary decomposition networks (CBDNet).", "Figure [@ref:LABEL:fig:cbd] illustrates the overall framework of the proposed method.", "The main contributions of this paper are summarized as below: \\newline <list> \\ We show that full-precision CNN models can be directly transferred into highly parameter and computing efficient multi-bits binary network models in a training-free way by the proposed CBDNet.", "\\newline \\ \\ We propose an algorithm to first expand full-precision tensors of each conv-layer with a limited number of binary tensors, and then decompose some conditioned binary tensors into two low-rank binary tensors.", "To our best knowledge, we are the first to study the network sparsity and the low-rank decomposition in the binary space.", "\\newline \\ \\ We demonstrate the effectiveness of CBDNet on different classification networks including VGGNet, ResNet, DenseNet as well as detection network SSD300 and semantic segmentation network SegNet.", "This verifies that CBDNet is widely applicable.", "\\newline \\ </list> \\newline  </section>"], ["<section> <title> Related Work </title>  <subsection> <title> Binary Neural Networks </title> Binary neural networks [@bib:courbariaux2015binaryconnect,binarynet,rastegari2016xnor] with high compression rate and great computing efficiency, have progressively attracted attentions owing to their great inference performance.", "\\newline Particularly, BinaryConnect (BNN) [@bib:courbariaux2015binaryconnect] binarizes weights to $ +1 $ and $ -1 $ and substitutes multiplications with additions and subtractions to speed up the computation.", "As well as binarizing weight values plus one scaling factor for each filter channel, Binary weighted networks (BWN) [@bib:rastegari2016xnor] extends it to XNOR-Net with both weights and activations binarized.", "DoReFaNet [@bib:zhou2016dorefa] binarizes not merely weights and activations, but also gradients for the purpose of fast training.", "However, binary networks are facing the challenge that accuracy may drops non-negligibly, especially for very deep models (e.g., ResNet).", "In spite of the fact that [@bib:hou2016loss] directly consider the loss to mitigate possible accuracy drops to mitigate during binarization, which gain more accurate results than BWN and XNOR-Net, it still has gap to the full-precision counterparts.", "A novel training procedure named stochastic quantization [@bib:dong2017sq] was introduced to narrow down such gaps.", "All these works belongs to the training-time optimization category in summary.", "\\newline </subsection> <subsection> <title> Deploy-time Network Optimization </title> Deploy-time network optimization tries to replace some heavy CNN structures in pre-trained CNN models with efficient ones in a training-free way.", "Low-rank decomposition [@bib:Denton2014Exploiting] exploits low-rank nature within CNN layers, and shows that fully-connected (FC) layers can be efficiently compressed and accelerated with low-rank approximations, while conv-layers can not.", "Spatial decomposition [@bib:Jaderberg2014Speeding] factorizes the $ k_{h} $ $ \\times $ $ k_{w} $ convolutional filters into a linear combination of a horizontal filter 1 $ \\times $ $ k_{w} $ and a vertical filter $ k_{h} $ $ \\times $ 1.", "Channel decomposition [@bib:Zhang2016Accelerating] decomposes one conv-layer into two layers, while the first layer has the same filter-size but with less channels, and the second layer uses a 1 $ \\times $ 1 convolution to mix output of the first one.", "Network decoupling [@bib:guo2018nd] decomposes the regular convolution into the successive combination of depthwise convolution and pointwise convolution.", "\\newline Due to its simplicity, deploy-time optimization has many potential applications for software/hardware vendors as aforementioned.", "However, it suffers from relatively limited optimization gains (2 $ \\sim $ 4 $ \\times $ in compression/speedup) over original full-precision models.", "\\newline </subsection> <subsection> <title> Binary Network Decomposition </title> Few existing works like us consider transferring full-precision networks into multi-bits binary networks in a training-free way.", "Binary weighted decomposition (BWD) [@bib:kamiya2017binary] takes each filter as a basic unit as BWN, and expands each filter into a linear combination of binary filters and a FP32 scalar.", "ABC-Net [@bib:lin2017abc] approximates full-precision tensor with a linear combination of multiple binary tensors and FP32 scalar weights during training-procedure to obtain multi-bits binary networks.", "Our method is quite different to these two works.", "We further consider the redundance and sparsity in the expanded binary tensors, and try to decompose binary tensors.", "The decomposition is similar to spatial decomposition [@bib:Jaderberg2014Speeding] but in the binary space.", "Hence, our binary decomposition step can also be viewed as binary spatial decomposition.", "\\newline </subsection>  </section>"], ["<section> <title> Method </title>  As is known, parameters of each conv-layer in CNNs could be represented as a 4-dimensional (4D) tensor.", "We take tensor as our study target.", "We first present how to expand full-precision tensors into a limited number of binary tensors.", "Then we show some binary tensors that fulfill certain conditions can be decomposed into two low-rank binary tensors, and propose an algorithm for that purpose.", "\\newline <subsection> <title> Tensor Binary Composition </title> Suppose the weight parameters of a conv-layer are represented by a 4D tensor $ \\mathbf{W}_{t}\\in\\mathbb{R}^{n\\times k\\times k\\times m} $ , where $ n $ is the number of input channels, $ m $ is the number of output channels, and $ k\\times k $ is the convolution kernel size.", "For each element $ w\\in{\\mathbf{W}}_{t} $ , we first normalize it with \\newline <equation> $ \\small\\tilde{w}=w/w_{max}, $ </equation> where $ w_{max}=\\max\\nolimits_{w_{i}\\in\\mathbf{W}_{t}}\\{|w_{i}|\\} $ .", "The normalized tensor is denoted as $ \\tilde{\\mathbf{W}}_{t} $ .", "The normalization makes every element $ \\tilde{w} $ within range $ [-1,1] $ .", "For simplicity, we denote the magnitude of $ \\tilde{w} $ as $ \\hat{w} $ , i.e., $ \\tilde{w} $ = $ sign(\\tilde{w})\\hat{w} $ , where $ sign(\\cdot) $ is a sign function which equals to -1 when $ \\tilde{w} $ $ < $ 0, otherwise 1. And $ \\hat{w} $ $ \\in $ $ [0,1] $ can be expressed by the composite of a series of fixed-point basis as \\newline <equation> $ \\small\\hat{w}\\approx\\sum\\nolimits_{i=0}^{J-2}a_{i}*2^{-i}, $ </equation> where $ a_{i} $ $ \\in $ $ \\{0,1\\} $ is a binary coefficient indicating whether certain power-of-two term is activated or not, $ i\\in\\{0,\\cdots,J-2\\} $ means totally $ J-1 $ bits is needed for the representation.", "When taking the sign bit into consideration, $ \\tilde{w} $ requires $ J $ bits to represent.", "\\newline Denote the magnitude of the normalized tensor as $ \\hat{\\mathbf{W}}_{t} $ .", "Tensor binary composition is a kind of tensor expansion, when each element of the tensor is binary expanded and expressed by the same bit rate $ J $ as \\newline <equation> $ \\small\\hat{\\mathbf{W}}_{t}\\approx\\sum\\nolimits_{i=0}^{J-2}A_{i}*2^{-i}, $ </equation> where $ A_{i}\\in\\{0,1\\}^{n\\times k\\times k\\times m} $ is 4D binary tensor.", "$ J $ will impact the approximation accuracy, while larger $ J $ gives more accurate results.", "We empirically study three different ImageNet CNN models.", "Figure [@ref:LABEL:fig:top1a5] shows that $ J=7 $ is already sufficiently good to keep a balance between the accuracy and the efficiency of the expansion.", "\\newline Different $ A_{i} $ may have different sparsity, which could be further utilized to compress the binary tensor.", "Figure [@ref:LABEL:fig:wd] illustrates the distribution of normalized weights $ \\tilde{w} $ in all the layers of ResNet-18, which looks like a Laplacian distribution, where most weight values concentrate in the range $ (-0.5,0.5) $ .", "This suggests that 1 is very rare in some binary tensor $ A_{i} $ with smaller $ i $ , since smaller $ i $ corresponds to bigger values in the power-of-two expansion.", "Figure [@ref:LABEL:fig:sparse] further shows the average sparsity of each binary tensor $ A_{i} $ , which also verifies that $ A_{i} $ with smaller $ i $ is much more sparse.", "Due to the sparsity of $ A_{i} $ , we next perform binary tensor decomposition to further reduce the computation complexity, as introduced in the next section.", "\\newline <subsubsection> <title> Binary expansion with \u03b1 scaling factor </title> The non-saturation direct expansion from FP32 to low-bits will yield non-negligible accuracy loss as shown in [@bib:tensorrt,googlewhitepaper] .", "A scaling factor is usually introduced and learnt to minimize the loss through an additional calibration procedure [@bib:tensorrt,googlewhitepaper] .", "Similarly, we impose a scaling factor $ \\alpha $ to Eq.( [@ref:LABEL:eq1] ) as \\newline <equation> $ \\small\\tilde{w}=\\alpha*w/w_{max}, $ </equation> where $ \\alpha\\geq 1 $ is a parameter to control the range of $ \\tilde{w}\\in[-\\alpha,\\alpha] $ .", "When the scaling factor $ \\alpha $ is allowed, the normalized weight $ \\hat{w}\\in[0,\\alpha] $ can be expressed with a composite of power-of-two terms as below: \\newline <equation> $ \\small\\hat{w}\\approx\\sum\\nolimits_{i=-q}^{J-q-2}a_{i}*2^{-i}, $ </equation> where $ q=\\lceil\\log_{2}{\\alpha}\\rceil $ and $ J $ also denotes the number bits of the weight, including $ J-1 $ bits for magnitude and 1 sign bit.", "The corresponding tensor form can be written as \\newline <equation> $ \\small\\hat{\\mathbf{W}}_{t}^{\\alpha}\\approx\\sum\\nolimits_{i=-q}^{J-q-2}A_{i}*2^% {-i}. $ </equation> Note that the scaling factor $ \\alpha $ will shift the power-of-two bases from $ \\{2^{0},2^{-1},\\cdots,2^{-J+2}\\} $ for Eq.( [@ref:LABEL:eq6] ) to $ \\{2^{q},\\cdots,2^{0},\\cdots,2^{-J+q+2}\\} $ for Eq.( [@ref:LABEL:BDWa] ).", "When $ \\alpha=1 $ , we have $ q=\\lceil\\log_{2}{\\alpha}\\rceil=0 $ , which makes Eq.( [@ref:LABEL:BDWa] ) reduce to the case without scaling factor as in Eq.( [@ref:LABEL:eq6] ).", "\\newline </subsubsection> </subsection> <subsection> <title> Binary Tensor Decomposition </title> We have shown that some binary tensors $ A_{i} $ are sparse.", "As sparse operations require specific hardware/software accelerators, it is not preferred in many practical usages.", "In deploy-time network optimization, researches show that full-precision tensor could be factorized into two much smaller and more efficient tensors [@bib:Jaderberg2014Speeding,Zhang2016Accelerating] .", "Here, we attempt to extend the spatial-decomposition [@bib:Jaderberg2014Speeding] to our binary case.", "\\newline For the simplicity of analysis, we flatten the 4D tensor $ \\hat{\\mathbf{W}}_{t}\\in\\mathbb{R}^{n\\times k\\times k\\times m} $ into the weight matrix $ \\mathbf{W}\\in\\mathbb{R}^{(n\\times k)\\times(k\\times m)} $ , so does for each $ A_{i} $ .", "Here the matrix height and width are $ n\\times k $ and $ k\\times m $ respectively.", "We then factorize a sparse matrix $ A $ into two smaller matrices as \\newline <equation> $ \\small A=B*C, $ </equation> where matrix $ B\\in\\{0,1\\}^{(n\\times k)\\times c} $ and matrix $ C\\in\\{0,1\\}^{c\\times(k\\times m)} $ .", "Note that our method is significantly different from the vector decomposition method [@bib:kamiya2017binary] , which keeps $ B $ binary, the other full-precision.", "On the contrary, we keep both $ B $ and $ C $ binary.", "This decomposition has the special meaning in conv-layers.", "It decomposes a conv-layer with $ k\\times k $ spatial filters into two layers\u2014one layer with $ k\\times 1 $ spatial filters and the other with $ 1\\times k $ spatial filters.", "Suppose the feature map size is $ h\\times w $ , then the number of operations is $ n\\times m\\times k^{2}\\times h\\times w $ for matrix $ A $ , while the number of operations reduces to $ (m+n)\\times c\\times k\\times h\\times w $ for $ B*C $ .", "We have the following lemma regarding to the difference before and after binary decomposition.", "\\newline <theorem> Lemma 1 (1) The computing cost ratio for $ A $ over $ B*C $ is $ \\nicefrac{{n\\times m\\times k}}{{c\\times(m+n)}} $ . (2) The bit-rate compression ratio from $ A $ to $ B*C $ is also $ \\nicefrac{{n\\times m\\times k}}{{c\\times(m+n)}} $ . (3) $ c<\\frac{n\\times m\\times k}{(m+n)} $ can yield real parameter and computing operation reduction.", "\\newline </theorem> <subsubsection> <title> Binary Matrix Decomposition </title> We first review the property of matrix rank: \\newline <equation> $ \\small rank(B*C)\\leq min\\{rank(B),rank(C)\\}. $ </equation> Comparing with binary matrix factorization methods [@bib:zhang2007binary,miettinen2010sparse] , which tend to minimize certain kind of loss like $ |A-B*C| $ and find matrices $ B $ and $ C $ iteratively, we attempt to decompose $ A $ into matrices $ B $ and $ C $ without any loss when $ c\\geq rank(A) $ is satisfied.", "\\newline <theorem> Theorem 1 If $ c\\geq rank(A) $ , binary matrix $ A\\in\\{0,1\\}^{(n\\times k)\\times(k\\times m)} $ can be losslessly factorized into binary matrices $ B\\in\\{0,1\\}^{(n\\times k)\\times c} $ and $ C\\in\\{0,1\\}^{c\\times(k\\times m)} $ .", "\\newline </theorem> Proof According to the Gaussian elimination method, matrix $ A $ can be converted to an upper triangular matrix $ D $ .", "Our intuition is to construct matrices $ B $ and $ C $ through the process of Gaussian elimination.", "Assume $ n\\leq m $ , $ P_{i} $ is the transform matrix representing the $ i $ -th primary transformation, matrix $ D $ can be expressed as: \\newline <equation> $ \\small D=\\prod_{i=0}^{k}P_{k-i}\\otimes A, $ </equation> where $ \\otimes $ is element-wise binary multiply operator so that \\newline <equation> $ \\small A\\otimes B=(A*B)\\mod 2.", "$ </equation> For simplicity, we use $ * $ instead of $ \\otimes $ here.", "As $ P_{i}\\in\\{0,1\\}^{(n\\times k)\\times(n\\times k)} $ is the permutation transform matrix, the inverse matrix of $ P_{i} $ exists.", "Therefore, $ A $ can be decomposed into the following form: \\newline <equation> $ \\small A=\\prod_{i=0}^{k}P_{i}^{-1}*D $ </equation> Since $ D $ only contains value $ 1 $ in the first $ r $ rows where $ r $ = $ rank(A) $ , $ D $ can be decomposed into two matrices: \\newline <equation> $ \\small D=\\left[\\begin{array}[]{c}D_{1}\\\\ 0\\end{array}\\right]=\\left[\\begin{array}[]{c}I\\\\ 0\\end{array}\\right]*\\left[\\begin{array}[]{c}D_{1}\\end{array}\\right] $ </equation> where $ D_{1} $ $ \\in $ $ \\{0,1\\}^{r\\times(k\\times m)} $ is the first $ r $ rows of matrix $ D $ , $ I $ is a $ r $ $ \\times $ $ r $ identity matrix.", "Then matrix A can be written as:", "\\newline <equation> $ \\small A=(\\prod_{i=0}^{k}P_{i}^{-1}*\\left[\\begin{array}[]{c}I\\\\ 0\\end{array}\\right])*\\left[\\begin{array}[]{c}D_{1}\\end{array}\\right].", "$ </equation> We then obtain the size $ (n\\times k)\\times r $ matrix $ B $ as $ B=\\prod_{i=0}^{k}P_{i}^{-1}*\\left[\\begin{array}[]{c}I\\\\ 0\\end{array}\\right] $ , and the size $ r\\times(k\\times m) $ matrix $ C $ as $ C=D_{1} $ exactly without any loss.", "This procedure also indicates that the minimum bottleneck parameter $ c $ is $ rank(A) $ , i.e., $ c\\geq rank(A) $ .", "This ends the proof.", "$ \\Box $ \\newline Based on this proof, we outline the binary matrix decomposition procedure in Algorithm [@ref:LABEL:algo-fact] .", "From the proof, we should also point out that the proposed binary decomposition is suitable for both conv-layers and FC-layers .", "Note that for binary permutation matrix $ P $ , its inverse matrix $ P^{-1} $ equals to itself, i.e., $ P^{-1}=P $ .", "Suppose $ h_{A} $ and $ w_{A} $ are height and width of matrix $ A $ , the computing complexity of $ B*P $ is just $ O(h_{A}) $ as $ P $ is a permutation matrix.", "The computing complexity of Algorithm [@ref:LABEL:algo-fact] is $ O(w_{A}\\times h_{A}^{2}) $ .", "\\newline </subsubsection> <subsubsection> <title> Losslessly Compressible of A i ? </title> Theorem [@ref:LABEL:th1] shows that only when $ c\\geq rank(A) $ , our method could produce lossless binary decomposition, while Lemma [@ref:LABEL:lemma1] shows that only $ c<\\frac{n\\times m\\times k}{(m+n)} $ could yield practical parameter and computing operations reduction.", "We have the following corollary: \\newline <theorem> Corollary 1 Binary matrix $ A $ is losslessly compressible based on Theorem [@ref:LABEL:th1] when $ rank(A)\\leq c<\\frac{n\\times m\\times k}{(m+n)} $ .", "\\newline </theorem> However, it is unknown which $ A_{i} $ in Eq.( [@ref:LABEL:eq6] ) was losslessly compressible before decomposition.", "The brute-force way is trying to decompose each $ A_{i} $ as in Eq.( [@ref:LABEL:eq7] ), and then keeping those satisfying Definition [@ref:LABEL:cor1] .", "This is obviously inefficient and impracticable.", "\\newline <float> Binary matrix decomposition Input: binary matrix $ A $ with size $ h_{A}\\times w_{A} $ Output: matrix rank $ r $ , matrix $ B $ , matrix $ C $ ( $ A=B*C $ ) \\newline \\ function BinaryMatDecomposition ( $ A $ ) \\ \\ if $ h_{A}\\leq w_{A} $ then \\ \\ $ A\\leftarrow A^{T} $ \\ \\ $ transpose=True $ \\ \\ end if \\ \\ $ r\\leftarrow 0 $ ; \\ \\ $ B\\leftarrow $ identity matrix $ h_{A}\\times w_{A} $ \\ \\ for $ c\\leftarrow $ 1 to $ w_{A} $ do \\ \\ $ l\\leftarrow $ first raw satisfy the constraints: $ A[l,c]=1 $ and $ l\\geq r+1 $ \\ \\ Reverse row $ l $ & $ r+1 $ , $ P\\leftarrow $ corresponding transition matrix \\ \\ $ r\\leftarrow r+1 $ ; \\ \\ $ B\\leftarrow B*P^{-1} $ \\ \\ for $ row\\leftarrow r+1 $ to $ h_{A} $ do \\ \\ if $ A[row,c]>0 $ then \\ \\ $ A[row,:]\\leftarrow(A[r,:]+A[row,:]) $ mod 2 \\ \\ $ P\\leftarrow $ corresponding transition matrix \\ \\ $ B\\leftarrow B*P^{-1} $ \\ \\ end if \\ \\ end for \\ \\ end for \\ \\ $ C\\leftarrow $ first $ r $ rows of A \\ \\ $ P\\leftarrow $ first $ r $ rows are identity matrix, other $ h_{A}-r $ rows are zeros.", "\\ \\ $ B\\leftarrow B*P $ \\ \\ if $ transpose $ then \\ \\ Return $ r $ , $ C^{T} $ , $ B^{T} $ \\ \\ else \\ \\ Return $ r $ , $ B $ , $ C $ \\ \\ end if \\ \\ end function \\ </float> \\newline Alternatively, we may use some heuristic cue to estimate which subset of $ \\{A_{i}\\} $ could be losslessly compressible .", "According to Figure [@ref:LABEL:fig:3] , $ A_{i} $ with smaller $ i $ is more sparse than that with bigger $ i $ .", "Empirically, more sparsity corresponds to smaller $ rank(A_{i}) $ .", "Based on Theorem [@ref:LABEL:th1] , this pushes us to seek the watershed value $ j $ so that those $ A_{i} $ where $ i\\leq j $ are losslessly compressible , while other $ A_{i} $ ( $ i>j $ ) are not.", "This requires introducing a variable into the definition of $ A_{i} $ , so that we choose the $ A_{i} $ defined by Eq.( [@ref:LABEL:BDWa] ) rather than Eq.( [@ref:LABEL:eq6] ).", "As is known, the $ A_{i} $ sequence defined by Eq.( [@ref:LABEL:BDWa] ) is $ \\{A_{-q},\\cdots,A_{0},\\cdots,A_{J-q-2}\\} $ where $ q=\\lceil\\log_{2}{\\alpha}\\rceil $ .", "Here, we seek for the optimal $ \\alpha $ , so that $ j=0 $ is the watershed, i.e., $ \\{A_{-q},\\cdots,A_{0}\\} $ are losslessly compressible .", "\\newline For simplicity, we still denote the flatten matrix of the tensor $ \\hat{\\mathbf{W}}_{t}^{\\alpha} $ in Eq.( [@ref:LABEL:BDWa] ) as $ \\mathbf{W} $ $ \\in $ $ \\mathbb{R}^{(n\\times k)\\times(k\\times m)} $ .", "We propose to use the indicator matrix described below for easy analysis.", "\\newline <theorem> Definition 1 The indicator matrix of $ \\mathbf{W} $ is defined as $ I_{w>\\beta}\\in\\{0,1\\}^{(n\\times k)\\times(k\\times m)} $ , in which the value at position $ (x,y) $ is $ I_{w>\\beta}[x,y]=I_{f}(\\mathbf{W}[x,y]>\\beta) $ , where $ \\beta\\in[0,\\alpha] $ is a parameter, $ I_{f}(\\cdot) $ is an element-wise indication function, which equals to 1 when the prediction is true, otherwise 0.", "\\newline </theorem> Based on this definition, $ A_{i} $ in Eq.( [@ref:LABEL:BDWa] ) can be written as \\newline <equation> $ \\small A_{i}[x,y]=I_{f}(\\lfloor\\frac{\\mathbf{W}[x,y]+\\Delta w}{2^{-i}}\\rfloor% \\mod 2=1), $ </equation> where $ \\Delta w=2^{-J+q+1} $ is the largest throwing-away power-of-two terms in Eq.( [@ref:LABEL:BDWa] ).", "\\newline Define $ w^{\\prime}=w+\\Delta w $ , according to Eq.( [@ref:LABEL:BDWa] ), the rank of matrix $ A_{0} $ can be expressed as: \\newline <equation> $ \\small\\begin{split} rank(A_{0})&=rank(\\sum\\nolimits_% {i=0}^{\\lceil n_{I}/2\\rceil-1}I_{(2i+1)\\leq w^{\\prime}<(2i+2)})\\\\ &=rank(\\sum\\nolimits_{i=1}^{n_{I}}I_{w^{\\prime}\\geq i}),\\end{split} $ </equation> where $ n_{I} $ = $ \\lceil\\alpha\\rceil $ .", "Based on the matrix rank property \\newline <equation> $ \\small rank(A+B)\\leq rank(A)+rank(B), $ </equation> we derive the upper bound of the $ rank(A_{i}) $ as: \\newline <equation> $ \\small rank(A_{0})\\leq\\sum\\nolimits_{i=1}^{n_{I}}{rank(I_{w^{\\prime}\\geq i})}. $ </equation> \\newline The empirical results show that when $ rank(I_{w^{\\prime}\\geq 1})\\leq 0.5*min\\{n\\times k,m\\times k\\} $ , and the rank of the indicator matrix satisfies the following constraints: \\newline <equation> $ \\small\\begin{split}&(1)\\frac{rank(I_{w^{\\prime}\\geq 2})}{rank(% I_{w^{\\prime}\\geq 1})}\\leq C_{0},\\\\ &(2)max\\{\\frac{rank(I_{w^{\\prime}\\geq i})}{rank(I_{w^{\\prime}% \\geq 1})}\\}\\leq C_{1},3\\leq i\\leq n_{I}.\\end{split} $ </equation> With the bound ( [@ref:LABEL:AiB] ), we get the bound of the rank by the indicator matrix $ I_{w^{\\prime}\\geq 1} $ : \\newline <equation> $ \\small rank(A_{0})\\leq((\\alpha-2)_{+}*C_{1}+C_{0}+1)*rank(I_{w^{\\prime}\\geq 1}), $ </equation> where $ (x)_{+} $ equals to $ x $ if $ x>0 $ , otherwise 0.", "We make some empirical study on ResNet-18, and find that $ C_{0}\\leq 0.2 $ in most of the layers as in Figure [@ref:LABEL:fig:2a] , $ C_{1}\\leq 0.05 $ in most of the layers as in Figure [@ref:LABEL:fig:3a] , and $ \\alpha\\leq 8 $ in all the layers.", "Hence, we have a simpler method to estimate $ rank(A_{0}) $ by: \\newline <equation> $ \\small\\begin{split} rank(A_{0})&\\leq(4*0.05+0.2+1)*% rank(I_{w^{\\prime}\\geq 1})\\\\ &=1.4*rank(I_{w^{\\prime}\\geq 1}).\\end{split} $ </equation> Therefore, we transfer the optimization of $ \\alpha $ to the optimization of $ rank(I_{w^{\\prime}\\geq 1}) $ .", "\\newline <float> Binary search $ \\alpha $ to satisfy rank condition.", "Input: weight matrix $ \\mathbf{W} $ , expected rank $ c $ Output: scalar value $ \\alpha $ \\newline \\ function ScalarValueSearch ( $ W $ , $ c $ ) \\ \\ $ min\\leftarrow 0 $ , $ max\\leftarrow $ max number of 1 value in a full rank matrix \\ \\ sort $ \\mathbf{W} $ in a descending order to a vector $ \\mathbf{v} $ \\ \\ while $ min\\leq max $ do \\ \\ $ center\\leftarrow(min+max)/2 $ \\ \\ $ \\alpha\\leftarrow 1/\\mathbf{v}[center] $ \\ \\ Compute indicator matrix $ I_{w\\geq 1} $ \\ \\ Compute rank $ r $ of $ I_{w\\geq 1} $ \\ \\ if $ r>c $ then \\ \\ $ max\\leftarrow center-1 $ \\ \\ else if $ r<c $ then \\ \\ $ min\\leftarrow center+1 $ \\ \\ else \\ \\ Return $ \\alpha $ \\ \\ end if \\ \\ end while \\ \\ Return $ \\alpha=1/\\mathbf{v}[max] $ \\ \\ end function \\ </float> \\newline </subsubsection> </subsection> <subsection> <title> A binary search algorithm for scaling factor a </title> In practice, we may give an expected upper bound $ c $ for $ rank(I_{w\\geq 1}) $ , and search the optimal $ \\alpha $ to satisfy \\newline <equation> $ \\small rank(I_{w\\geq 1})\\leq c. $ </equation> As $ w\\in[0,\\alpha] $ and $ \\alpha>1 $ , $ w\\in[0,1] $ only corresponds to $ 1/\\alpha $ portion of the whole range of $ w $ .", "\\newline Generally, weight matrix $ \\mathbf{W} $ should be sorted and traversed to compute the $ rank(I_{w\\geq 1}) $ .", "Instead of using this time-consuming method, we propose an efficient solution based on the binary search algorithm.", "\\newline Suppose that every element in weight matrix $ \\mathbf{W} $ has a unique value, we sort $ \\mathbf{W} $ to be a vector $ \\mathbf{v} $ with length $ N=n\\times m\\times k^{2} $ in descending order, so that $ \\mathbf{v}[1] $ is the largest element.", "Assume index $ i $ satisfy the constraint: $ \\mathbf{v}[i]\\geq 1>\\mathbf{v}[i+1] $ , then the indicator matrix can be expressed as: \\newline <equation> $ \\small I_{w\\geq 1}=I_{p(w)<i+1} $ </equation> where $ p(w) $ is the index position of the element $ w $ in array $ \\mathbf{v} $ , i.e., $ \\mathbf{v}[p(w)]=w $ .", "Hence, there are $ i $ ones in the indicator matrix while others are zeros.", "Comparing matrix $ I_{p(w)<i+1} $ with matrix $ I_{p(w)<i} $ , we have the property: \\newline <equation> $ \\small I_{p(w)<i+1}[x,y]=\\begin{cases}I_{p(w)<i}[x,y]&p(w[x,y])\\neq i\\\\ I_{p(w)<i}[x,y]+1&p(w[x,y])=i\\end{cases} $ </equation> With the property of Eq.( [@ref:LABEL:A+B] ), we get the relation between the indicator matrices with adjacent index: \\newline <equation> $ \\small|rank(I_{p(w)<i+1})-rank(I_{p(w)<i})|\\leq 1.", "$ </equation> This indicates that $ rank(I_{p(w)<i}) $ are continuous integers.", "Hence, $ i\\in[i_{1},i_{2}] $ always exists to satisfy below constraints: \\newline <equation> $ \\small\\begin{split}& rank(I_{p(w)<i})=c,\\\\ &(rank(I_{p(w)<i_{1}})-c)*(rank(I_{p(w)<i_{2}})-c)<0.\\end{split} $ </equation> Eq.( [@ref:LABEL:PforI] ) provides the property to design a binary search algorithm as outlined in Algorithm [@ref:LABEL:algo-rank] , which could find the local optimal of $ \\alpha $ when matrix $ \\mathbf{W} $ has some identical elements.", "The computing complexity of this algorithm is $ O(N\\log(N)) $ , where $ N=n\\times k^{2}\\times m $ .", "\\newline Here, $ c $ is a tunable hyper-parameter.", "In practice, we did not directly give $ c $ since different layers may require different $ c $ .", "Instead, we assume $ m\\geq n $ and define $ b=\\nicefrac{{c}}{{n\\times k}} $ as bottleneck ratio, since we reduce the neuron number from $ n\\times k $ to $ c $ by the binary matrix $ B $ .", "Then we tune $ b $ and make it constant over all the layers, while $ b<0.5 $ could provide compression effect.", "After getting $ \\alpha $ , we obtain $ q=\\lceil\\log_{2}{\\alpha}\\rceil $ , and only do binary decomposition for the subset $ \\{A_{-q},\\cdots,A_{0}\\} $ as they are losslessly compressible .", "\\newline </subsection>  </section>"], ["<section> <title> Experiments </title>  This section conducts experiments to verify the effectiveness of CBDNet on various ImageNet classification networks [@bib:ILSVRC15] , as well as object detection network SSD300 [@bib:Liu2016SSD] and semantic segmentation network SegNet [@bib:badrinarayanan2017segnet] .", "\\newline <subsection> <title> Classification Networks on ImageNet </title> The ImageNet dataset contains 1.2 million training images, 100k test images, and 50k validation images.", "Each image is classified into one of 1000 object categories.", "The images are cropped to $ 224\\times 224 $ before fed into the networks.", "We use the validation set for evaluation and report the classification performance via top-1 and top-5 accuracies."]], "target": "Table gives an overall performance of different evaluated networks. Note we decomposed all the conv-layers and FC-layers with the proposed method in this study. The resulted bit-rate is defined by $ 32\\times size(\\mathrm{CBDNet})/size(\\mathrm{FP32}) $ , where $ size(x) $ gives the model size of model $ x $ ."}, {"tabular": ["  Project  &  Method  &  Intent ", " Commons Lang  &  96%  &  97% ", " Freemind  &  92%  &  99% ", " JabRef  &  90%  &  82% ", " Jetty  &  83%  &  88% ", " JHotDraw  &  93%  &  84%  "], "ref_sec": [["<section> <title> I Introduction </title>  Research in software maintenance has shown that many programs contain a significant amount of duplicated (cloned) code.", "Such cloned code is considered harmful for tworeasons: (1) multiple, possibly unnecessary, duplicates of code increase maintenance costs [@bib:koschke-2007_KoschkeR-clone_survey,roy-2007-clone_survey] and, (2) inconsistent changes to cloned code can create faults and, hence, lead to incorrect program behavior [@bib:2009_juergens_inconsistens_clones] .", "The negative impact of clones on software maintenance is not due to copy&paste but caused by the semantic coupling of the clones.", "Hence, functionally similar code, independent of its origin, suffers from the same problems clones are known for.", "In fact, the re-creation of existing functionality can be seen as even more critical, since it is a missed reuse opportunity.", "\\newline The manual identification of functionally similar code is infeasible in practice due to the size of today\u2019s software systems.", "Tools are required to automatically detect similar code.", "An earlier study [@bib:juergens2010code] has shown that existing clone detection tools are limited to finding duplicated code, {i.\u2009e.} , they are not capable of finding redundant code that has been developed independently.", "As a result, we do not know to what degree real-world software systems contain similar code beyond the code clones that stem from copy&paste programming.", "Manual analysis of sample projects [@bib:juergens2010code] as well as anecdotal evidence, however, indicate that programs indeed contain many similarities not caused by copy&paste.", "Hence, we expected that tools for the automatic detection of similar code could prove as beneficial for quality assurance activities as the now widely-used clone detection tools.", "\\newline While the equivalence of two programs is undecidable in general, a straightforward approach to detect similar code relies on executing candidate code fragments with random input data and comparing the output values.", "This approach was pursued by Jiang&Su [@bib:jiang2009automatic] as well as us.", "While they successfully applied their approach for code in the Linux kernel, we were unable to produce significant results using this seemingly simple approach for diverse Java systems.", "As this differs from Jiang&Su\u2019s results, this paper details on our insights regarding challenges of dynamic detection of functionally similar code in Java programs.", "\\newline <subsubsection> <title> Research Problem </title> Functional duplication in software systems causes a multitude of problems for software maintenance.", "Although clone detection is a viable approach to find copied code, existing tools are not capable of finding similar code that was created independently [@bib:juergens2010code] .", "Jiang&Su [@bib:jiang2009automatic] developed a dynamic approach to identify functionally similar code in C systems and experienced a high detection rate for the Linux kernel.", "We implemented a similar approach for detecting functionally similar code fragments in Java systems.", "When analyzing five open-source Java systems, we got considerably lower detection rates.", "We found this to be caused by limitations of the approach when applied to Java systems.", "As a consequence, it is unclear if the dynamic approach can be applied in practice to detect functionally similar code fragments in object-oriented systems.", "\\newline </subsubsection> <subsubsection> <title> Contribution </title> While not a replication in the strict sense, this paper transfers Jiang&Su\u2019s work to object-oriented software implemented in Java.", "We describe our implementation of the dynamic detection approach, report on the detection results, and provide a detailed comparison of our approach and results to Jiang&Su\u2019s work.", "Moreover, we discuss the challenges of the dynamic detection approach for object-oriented systems and thereby provide a basis for a substantiated discussion as well as directions for further research.", "\\newline </subsubsection>  </section>"], ["<section> <title> II Terms & Definitions </title>  <subsubsection> <title> Simion </title> We define a {simion} as a functionally simi lar c o de fragme n t regarding I/O behavior [@bib:juergens2010code] .", "Two fragments are simions if they compute the same output for all input values.", "\\newline </subsubsection> <subsubsection> <title> Chunk </title> A {chunk} is a code fragment that is compared for functional similarity.", "It consists of a set of input parameters, a statement sequence, and a set of output parameters.", "\\newline </subsubsection> <subsubsection> <title> Clone </title> We define a {clone} as a syntactically similar code fragment typically resulting from copy&paste and potential modification.", "Several specific clone types were introduced that impose constraints on the differences between the code fragments [@bib:koschke-2007_KoschkeR-clone_survey] .", "{Type-1 clones} are clones that may differ in layout and comments.", "{Type-2 clones} may additionally differ in identifier names and literal values.", "{Type-3 clones} allow a certain amount of statement changes, deletions or insertions.", "{Type-4 clones} as defined by [@bib:roy-2007-clone_survey] are comparable to simions.", "However, we do not use the term type-4 clones, since the term \u201cclone\u201d implies that one instance is derived from the other, which is not necessarily the case for simions according to our definition.", "In addition to the known clone types we define for this paper {type-1.5 clones} as type-1 clones that may be subject to consistent variable renaming.", "The clone types form an inclusion hierarchy, {i.\u2009e.} , all type-2 clones are also type-3 clones, but there are type-3 clones that are not type-2 (and analogously for the other types).", "While clones may be (and often are) simions, even textually identical fragments of code may emit different behavior because of the type binding implied by the surrounding context.", "\\newline </subsubsection>  </section>"], ["<section> <title> III Dynamic Detection Approach </title>  Our approach for dynamically detecting functionally similar code fragments follows in principle that of Jiang&Su [@bib:jiang2009automatic] , {i.\u2009e.} , it is based on the fundamental heuristic that two functionally similar code fragments will produce the same output for the same randomly generated input.", "We exclude clones from the simion detection, however, since these can be found with existing clone detection tools.", "The main difference to the approach of Jiang&Su is that our approach targets object-oriented systems written in Java whereas they address C programs.", "The detection procedure can be divided in five principal phases that are executed in a pipeline fashion.", "We implemented a prototype of this pipeline based on our continuous quality assessment toolkit ConQAT .", "\\newline Figure [@ref:LABEL:fig:pipeline] illustrates the detection pipeline with its five phases, each consisting of several processing steps.", "The input of the pipeline is the source code of the analyzed projects and their referenced libraries.", "The output is a set of equivalence classes of functionally similar code fragments.", "The pipeline phases are detailed in the following sections.", "\\newline <subsection> <title> III-A Chunking </title> <subsubsection> <title> Chunk Extraction </title> This step extracts a set of chunks from each source code file, which are used as candidates for simion detection.", "For Java systems, which consist of classes, it is a challenge to extract those chunks, since a code fragment arbitrarily cut out of the source files will almost certainly not represent a compilable unit on its own.", "Therefore, we developed several strategies for extracting chunks from Java classes.", "The first challenge is to determine what will be the input and output parameters of the chunk.", "In a Java class, several types of variables occur.", "During the chunk extraction we derive the input and output parameters from the declared and referenced variables in the statements of the chunk with the following heuristics: \\newline <list> \\ Referenced {instance variables (non-static)} become input as well as output parameters.", "\\newline \\ \\ Referenced {class variables (static)} become neither input nor output variables, but are treated as a local variable as some of the called methods might rely on the value of these globals.", "\\newline \\ \\ {Local variables} with a scope that is nested in the statements of the chunk do not have to be considered as input or output variables.", "All other referenced local variables become output variables.", "Local variables that are referenced but not declared in the chunk statements also become input variables.", "\\newline \\ \\ {Method parameters} are treated in the same way as referenced but not declared local variables, {i.\u2009e.} , they become input and output parameters.", "They are also output parameters, as the code after the chunk might reference these values.", "\\newline \\ </list> \\newline Since we want to compare individual chunks for functional similarity, we have to be able to compile their statement sequences and execute them separately.", "We therefore process each statement sequence and apply several transformations to obtain a static function with the input and output parameters of the chunk.", "In certain cases, two chunks are generated for the same statement sequence.", "This is used to provide different input signatures for the same piece of code.", "For example, code referencing the attributes of the class could either use an input object of the class\u2019 type or inputs for each of the attributes, leading to different chunk signatures.", "There are cases when we cannot create a compilable chunk at all.", "Examples include statement sequences with branch statements ( {e.\u2009g.} , continue ) where the target of the branch statement is not in the sequence or calls to constructors of non-static inner classes.", "\\newline Non-static methods that could be static since they do not reference any non-static methods or fields are converted to static methods by adding the static keyword to the method declaration.", "Thereby potentially more chunks do not need an additional input parameter with the type of the surrounding class and thus represent a more generally reusable fragment.", "\\newline In case that a statement sequence contains a return statement, the resulting chunk gets a single output parameter with the type of the return value of the associated method.", "The code with the return statement is changed to a local variable declaration with the corresponding initialization followed by a jump to an exit label.", "\\newline The detection performance heavily depends on the number of chunks compared for similarity.", "Since Jiang&Su experienced performance problems due to high numbers of chunks, we developed three different chunk extraction strategies.", "\\newline The {sliding window} strategy extracts chunks by identifying all possible statement subsequences that represent valid AST fragments with a certain minimal length.", "Thus, it can cover cases where an arbitrary statement sequence is functionally similar to another one.", "However, the number of chunks is quadratic in the number of statements of a method.", "\\newline The {intent-based} strategy utilizes the programmer\u2019s intent by interpreting blank lines or comments in statement sequences as a logical separator between functional units.", "It extracts one chunk that includes all statements in a method and all chunks that can be formed from the statement sequences separated by blank or comment lines.", "\\newline The {method-based} strategy utilizes structuring of the code given by the methods and considers the statement sequences of all methods in the code.", "This strategy has a slight variation for determining the output parameters.", "If the method has a non-void return value the chunk gets one output parameter with the return type of the method.", "Otherwise the chunk gets one output parameter with the type of the surrounding class.", "\\newline </subsubsection> <subsubsection> <title> Type-1.5-Clone Filter </title> Although code clones are often simions w.r.t. to our definition, they are not useful as they could be more easily detected with a clone detection tool.", "The type-1.5-clone filter discards chunks that are type-1.5 clones.", "This removes the clones from the results and improves detection performance as these chunks are not further processed.", "\\newline </subsubsection> </subsection> <subsection> <title> III-B Input Generation </title> <subsubsection> <title> Parameter Permutation </title> To cover cases where two code fragments compute the same function but have different parameter ordering, we additionally apply a parameter permutation.", "For each chunk we generate additional chunks where all input parameters of the same type are permutated.", "To cope with combinatorial explosion, we constrain the number of additionally generated permutations to 25.", "\\newline </subsubsection> <subsubsection> <title> Input Value Generation </title> As Jiang&Su, we employ a random testing approach for generating input values.", "We generate code for constructing random values for the input parameters of the chunks.", "Since the chunks must be executed on the same inputs for comparison, the input generation uses a predefined set of values for primitive parameters (all Java primitives, their wrapper types, and String).", "Input values for parameters with composite types are generated with a recursive algorithm.", "All constructors (if any) of the type are used for creating instances.", "The algorithm is applied recursively to the parameters of the constructors, {i.\u2009e.} , again, random values are generated and passed as arguments to the constructor.", "In case of 1-dimensional arrays and Java 5 collections where the component type is known, we generate collections of several randomly chosen but fixed sizes and apply the algorithm recursively for the elements of the collection.", "For chunks with many parameters or constructors with many arguments, a large number of inputs would be generated by this approach.", "Therefore, we constrain the number of input values generated for one chunk to 100.", "\\newline </subsubsection> </subsection> <subsection> <title> III-C Compilation & Execution </title> <subsubsection> <title> Compilation </title> The {compilation} step wraps the chunk code with the input generation code and code for handling errors during execution and for storing the values of the output parameters after execution.", "If an exception occurred during the execution, a special output value {error} is used.", "Moreover, we generate code for handling non-terminating chunks ( {e.\u2009g.} , due to infinite loops), which stops execution after a timeout of 1 second.", "The code is compiled in a static method within a copy of the original class code and with all compiled project code and libraries on the classpath.", "This means that the statements in the chunk have access to all static methods and fields from the original context as well as their dependencies.", "\\newline </subsubsection> <subsubsection> <title> Execution </title> After compilation, the chunks are executed in groups of at most 20 in a separate Java Virtual Machine with a security manager configured.", "This ensures that the chunk execution does not have unwanted side effects ( {e.\u2009g.} , deletion of files).", "The result of the execution step is a list of execution data objects, which hold data about the chunk and a list of input and output values of the chunk execution.", "\\newline </subsubsection> </subsection> <subsection> <title> III-D Comparison </title> Preliminary experiments with the simion detection pipeline revealed that the majority of identified simions is not relevant.", "A large fraction are false positives, {i.\u2009e.} , our analysis identified them as functionally similar although they are not.", "This is caused by the random nature of the generated input data.", "Often two (or more) chunks are executed with input data that {triggers} only very specific execution paths.", "With respect to these paths the chunks are similar although they are not for reasonable input data.", "An example are two string processing methods where one trims the string (deletes leading and trailing whitespace) and one replaces characters with a unicode code point above 127 with the unicode escape sequence used in Java.", "If both chunks are executed for a string without leading or trailing whitespace and without characters outside the ASCII range, they both simply return the input string.", "As a result, they are identified as a simion.", "We address both problems, clones and false positives, with additional filter steps in our detection pipeline.", "\\newline <subsubsection> <title> Identity filter </title> The identity filter discards all chunks that implement the identity function, {i.\u2009e.} , for each input data set they return the same output data set.", "This heuristic excludes chunks for which the randomly generated input data is not capable of triggering {interesting} execution paths.", "\\newline </subsubsection> <subsubsection> <title> Equality filter </title> The equality filter discards all chunks that generate the same output data for all input data sets.", "The rationale behind this is that the chunk execution is apparently independent of the input data.", "Again, this is caused by the inherently limited quality of the randomly generated input data.", "An example is a chunk that has two input parameters: a string and an integer value $ i $ .", "If $ i $ is less than the length of the string (in characters), the chunk returns a new string with the first $ i $ characters of the input string.", "If $ i $ is greater than the length of the string, it returns the empty string.", "As this dependency between the two parameters is unknown to the input data generator it could possibly generate only data sets where $ i $ is greater than the length of the chosen string.", "All executions of the chunk return the empty string.", "\\newline </subsubsection> <subsubsection> <title> Output Comparison </title> The {output comparison} step uses the execution data objects to compare the chunks for functional similarity.", "Chunks that do not provide valid output data for at least 3 inputs ( {i.\u2009e.} , either throw an exception or have a time-out), are discarded at this step.", "To make the comparison performant we use a hash-based approach.", "For each chunk and each of its output variables, it computes an MD5 digest for the comparison.", "This requires only moderate space in memory even for large output data.", "This digest can be thought of as a functional fingerprint of the chunk regarding a projection to one of its output variables.", "To construct the MD5 digest, we transform each output object to a string representation and append it to the MD5 digest.", "We use the XStream XML serialization library to transform an arbitrary object into a string with its XML serialization.", "The MD5 digest of each chunk is used as a key into a hash map holding the chunks.", "If two chunks have the same MD5 digest, we have identified a pair of functionally similar code chunks.", "This is done for eliminating the otherwise quadratic effort of comparing all chunks for equal MD5 digests.", "While hash collisions could lead to false positives, we consider the comparison correct, since collisions are very unlikely in practice.", "The result of the output comparison step is a set of equivalence classes of chunks with similar functionality.", "\\newline </subsubsection> </subsection> <subsection> <title> III-E Post Processing </title> <subsubsection> <title> Subsumption filter </title> An additional filter discards simions that are entirely covered by a larger simion (in terms of its length and position in the source code).", "For example, if two methods are identified to be simions, it is usually not worth reporting that parts of them are also simions.", "\\newline </subsubsection> <subsubsection> <title> Type-3 clone filter </title> Additionally, at the end of the pipeline a type-3-clone clone detector is run to determine which of the simions could also be detected by a type-3-clone-detector, {i.\u2009e.} , a clone detector that takes into account insertion, modification, and deletion of a certain amount of statements.", "The filter calculates the statement-level edit-distance between chunks and is configured to filter all chunks with an edit-distance less or equal to 5.", "Both clone filters are implemented with ConQAT\u2019s clone detection algorithm [@bib:2009_juergens_clonedetective] .", "We cannot filter type-3 clones earlier, as they are not guaranteed to be functionally equivalent.", "Thus, it is unclear which of the instances should be filtered as each could be a potential simion of another chunk.", "\\newline </subsubsection> </subsection>  </section>"], ["<section> <title> IV Study Design </title>  <subsection> <title> IV-A Research Questions </title> RQ 1: How large is the simion detection problem? \\newline As a first step, we are interested to characterize the problem we want to solve.", "We look at the number of chunks extracted by our approach.", "\\newline RQ 2: How do technical challenges affect the detection?", "\\newline To compare the functionality of two chunks, we need to transform them into executable code pieces and generate useful test cases.", "This provokes a number of technical challenges that affect the detection approach.", "This includes the generation of input values for the chunks, which need to be meaningful to trigger interesting functionality.", "Furthermore, the generation of useful test data for project-specific data types and the emulation of certain operations used by the chunks such as file I/O or GUI events is challenging.", "Finally, even after overcoming these limitations, there can still be problems that prevent the compilation of the extracted chunk.", "We investigate how many chunks need to be disregarded during detection because of these challenges.", "\\newline RQ 3: How effective is our approach in detecting simions? \\newline We ask if the approach is able to detect significant amounts of functionally similar code.", "Moreover, as we investigated the technical challenges in RQ 2, we are interested in the share of the code of real-world systems that we are able to analyze.", "Finally, we want to know how many simions we can find in realistic systems.", "\\newline </subsection> <subsection> <title> IV-B Data Collection Procedure </title> In Section [@ref:LABEL:sec:detection] , we described three different chunking strategies.", "The general idea is to run a complete simion detection with all strategies on a set of software systems to collect the needed data for answering the research questions.", "For practical reasons, however, we cannot perform a complete detection using the sliding window chunking strategy, because it creates far more chunks than are feasible to analyze.", "Therefore, we collect data using that strategy only for RQ1.", "\\newline For RQ 1 and RQ 2, we employ no filters, since we are only interested in determining the difficulty of the problem and technical challenges regardless of the precision of the results.", "For RQ 3, where we are interested in the amount of detected simions, we use all filters.", "\\newline <subsubsection> <title> RQ 1 </title> ConQAT writes the total number of extracted chunks into its log file, which answers RQ 1.", "We use separate configurations for the different chunking strategies.", "\\newline </subsubsection> <subsubsection> <title> RQ 2 </title> To answer RQ 2, we use two different configurations.", "The first one is similar to RQ 1, using a specific statistics processor for collecting the required data.", "Our input generator logs the number of chunks for which no input could be generated.", "Additionally, our configuration determines and counts the types of the input parameters and aggregates these values.", "\\newline Another part of the configuration counts the number of chunks that contain calls to I/O, networking, SQL, or UI code.", "We cannot execute chunks containing such calls successfully, as the expected files, network peers, or databases are not available, or the required UI initialization was not performed.", "We identify calls to these groups by the package the corresponding class resides in, {e.\u2009g.} , a call to {java.io.", "File.canRead} would be counted as I/O. These packages also contain methods that can be safely called even without the correct environment being set up (such as methods from {java.io.", "StringReader} )", ", so we expect to slightly overestimate these numbers.", "On the other hand, we only count methods that are directly called from the chunk.", "Methods that are called indirectly (from other methods called from the chunk) are not included in these numbers.", "However, as we are only interested in the magnitude of this problem, we consider this heuristic sufficient.", "\\newline The second configuration is a slightly simplified detection pipeline, that uses the approach described in Section [@ref:LABEL:sec:detection] to generate code and tries to compile the chunks.", "Statistics on the number of chunks that could not be compiled are reported.", "For both configurations, we disabled the type-1.5 clone filter and the permutation step, as these distort the statistics slightly.", "\\newline </subsubsection> <subsubsection> <title> RQ 3 </title> For the last research question we utilize the full simion detection pipeline to count the number of simions detected by our implementation.", "Our code is instrumented to report the number of chunks lost at the steps of the pipeline.", "\\newline </subsubsection> </subsection> <subsection> <title> IV-C Analysis Procedure </title> <subsubsection> <title> RQ 1 </title> For the size of the problem, we report the total number of chunks per chunking strategy to show the order of magnitude.", "To make the numbers more comparable and to allow an estimate for the simion detection in systems of other sizes, we give the number relative to the lines of code (SLOC) and calculate the mean value.", "\\newline </subsubsection> <subsubsection> <title> RQ 2 </title> We show the relative distribution and calculate the mean per strategy for each of the following metrics, which characterize different technical challenges.", "We give the values for all relative metrics rounded to full percentages.", "First, we analyze the difficulty of generating inputs by two metrics.", "One is the number of chunks for whose input parameters we cannot generate values.", "The other is the number of inputs of project-specific data types, because it is especially hard to generate meaningful input for them.", "Second, for the execution there are certain types of methods that are hard to emulate during random testing.", "We analyze the number and share of calls to I/O, network, SQL, and UI.", "Third, the chunks need to be compiled to be executed.", "Hence, we investigate the fraction of chunks that cannot be compiled.", "For these challenges, we add qualitative, manual analysis of the chunks that cannot be further used in the detection approach to get more insights into the reasons.", "\\newline </subsubsection> <subsubsection> <title> RQ 3 </title> We analyze two different types of study objects.", "The first type is a set of programs of which we know that they have to exhibit similar functionality, because they were produced according to the same specification.", "These study objects show whether the detection approach works in principle.", "We expect to get at least as many simions reported as there are implementations of the specification.", "The second type of study objects are real-world, large systems of which we do not know beforehand of any simions.", "We show the change in chunks during the execution of the detection pipeline in absolute and relative terms.", "We round the relative values to percentages with two positions after the decimal point to be able to differentiate small results.", "\\newline </subsubsection> </subsection> <subsection> <title> IV-D Study Objects </title> We chose two different types of study objects: (1) a large number of Java programs that are functionally similar and (2) a set of real-world Java systems.", "The study objects of type 1 are small programs, so that they are easy to analyze and they are built according to the same specification so that we can be sure that they exhibit largely the same functionality.", "We selected a set of programs, we used in an earlier study [@bib:juergens2010code] .", "We will refer to it as \u201cInfo1\u201d.", "They are implementations of a specification about an e-mail address validator by computer science undergraduate students.", "We only include programs passing a defined test suite to ensure a certain similarity in functionality.", "This results in 109 programs with a size ranging from 8 to 55 statements.", "\\newline The second type of study objects represents real Java systems to show realistic measurements for chunks and simions.", "As selection criteria, we chose systems that cover a broad range of application domains, sizes, and functionalities.", "Furthermore, we chose systems we are already familiar with to support the interpretation of the results.", "The selection resulted in the five open source Java systems that represent libraries, GUI applications, and servers, shown in Table [@ref:LABEL:tab:projects] together with their size in SLOC ( {source lines of code} , {i.\u2009e.} , the number of non-blank and non-comment lines).", "\\newline </subsection>  </section>"], ["<section> <title> V Results </title>  <subsection> <title> V-A RQ 1: Problem Size </title> Table [@ref:LABEL:tab:number_of_chunks] shows the absolute and relative numbers of chunks extracted for each study object and the different chunking strategies.", "The sliding window strategy extracts the highest number of chunks with up to 2.68 chunks per SLOC and 1.44 chunks/SLOC on average.", "The intent-based strategy creates less chunks with at most 0.40 chunks/SLOC and 0.25 chunks/SLOC on average.", "The smallest number of chunks is extracted using the method-based strategy.", "It creates at most 0.09 chunks/SLOC and 0.05 chunks/SLOC on average.", "\\newline Overall, the number of chunks is large, especially for the sliding window strategy.", "The detection approach needs to be able to cope with several thousand chunks.", "\\newline </subsection> <subsection> <title> V-B RQ 2: Technical Challenges </title> The first pair of columns in Table [@ref:LABEL:tab:rq2] shows the fraction of chunks for which the approach cannot construct input values.", "The two main cases where no input can be generated are chunk parameters that refer to (1) an interface or abstract class and (2) a collection with an unknown component type .", "In the first case it is unclear which implementation should be chosen to obtain an object that implements the interface.", "In the second case we do not know what type of objects to put in the collection.", "For all systems the fraction for which no input can be generated is higher for the intent-based chunking strategy compared to the method-based strategy.", "In case of {Freemind} and the intent-based chunking strategy for as much as 94% of the chunks no input could be generated.", "An investigation revealed that {Freemind} uses untyped Collections.", "Except for {Commons Lang} , where primitive types are dominant, input generation failed for more than 30% of the chunks with both chunking strategies.", "\\newline We determined how many chunks have parameters (either input or output) of project-specific data types.", "The results are shown in the second pair of columns in Table [@ref:LABEL:tab:rq2] .", "A considerable fraction of the chunks (60\u201387%) refer to project-specific data types.", "With our approach, these would not qualify as candidates for cross-project simions, since the other project would not have the same data types.", "\\newline To estimate the use of methods that require a specific environment, we determined for each chunk whether it contains direct calls to methods from one of the groups I/O, networking, SQL, or UI.", "The relative numbers of chunks containing calls to a category are shown in Figure [@ref:LABEL:fig:chunkCalls] for each of the categories.", "It is not surprising, that the numbers depend on the application.", "The library {Commons Lang} has only a couple of calls to I/O code, the HTTP server {Jetty} has the highest number of calls to networking code, and the drawing tool {JHotDraw} dominates the UI group.", "The most common groups are UI (if the application has a user interface at all), followed by I/O. \\newline In the execution step, a chunk containing methods from at least one of these groups is likely to fail as the expected environment is not provided.", "The last pair of columns in Table [@ref:LABEL:tab:rq2] lists how many chunks are affected by such methods for both of the chunking strategies.", "Overall, as many as 24% of the methods can be affected ( {JHotDraw} ), or for the intent-based chunking strategy more than 60% of the chunks, thus having a significant impact on the number of chunks we can process by our dynamic approach.", "Interestingly, the relative numbers for the intent-based strategy are higher than for the method-based strategy in all cases.", "This suggests, that the methods containing I/O or UI code are typically longer than the remaining methods and thus produce more chunks.", "\\newline Finally, we checked how many of the chunks we were able to make compilable by providing a suitable context."]], "target": "The relative number of chunks we could make compilable of those for which at least one input could be generated is shown in Table . These numbers do not indicate a principal limitation, as each of the chunks we extracted is a valid subtree of the AST and thus can be executed in a suitable context. They rather document limitations in our tool. An inspection of the problematic chunks revealed weaknesses in chunks dealing with generic data types, anonymous inner classes, method local classes, and combinations thereof. Still, we are able to automatically generate a context for at least 82% in all cases and up to 99% for {Freemind} . Clearly, these numbers could be improved by using more advanced algorithms for generating the context for a chunk. The other results from RQ 2 and those of RQ 3 presented next suggest, however, that the chunks we lost as we cannot make them compile is not the main bottleneck of the detection pipeline."}, {"tabular": ["  Training costs (batchsize=256)  &  Struct-18-A  &  Struct-MV2-A ", " Mem  &  seconds / iter  &  Mem  &  seconds / iter ", " With SR loss  &  $ 9.9 $ GB  &  $ 0.46 $ s  &  $ 18.8 $ GB  &  $ 0.38 $ s ", " Without SR loss  &  $ 9.2 $ GB  &  $ 0.44 $ s  &  $ 17.9 $ GB  &  $ 0.37 $ s  "], "ref_sec": [["<section> <title> 1 Introduction </title>  Deep neural networks deliver outstanding performance across a variety of use-cases but quite often fail to meet the computational budget requirements of mainstream devices.", "Hence, model efficiency plays a key role in bridging deep learning research into practice.", "Various model compression techniques rely on a key assumption that the deep networks are over-parameterized, meaning that a significant proportion of the parameters are redundant.", "This redundancy can appear either explicitly or implicitly.", "In the former case, several structured [@bib:he2017,lipruning] , as well as unstructured [@bib:han2015deep,han2015pruning,automatedpruningmanessi,admm] , pruning methods have been proposed to systematically remove redundant components in the network and improve run-time efficiency.", "On the other hand, tensor-decomposition methods based on singular values of the weight tensors, such as spatial SVD or weight SVD, remove somewhat implicit elements of the weight tensor to construct low-rank decompositions for efficient inference [@bib:denton2014,jaderberg2014speeding,kuzmin2019taxonomy] .", "\\newline Redundancy in deep networks can also be seen as network weights possessing an unnecessarily high degrees of freedom (DOF).", "Alongside various regularization methods [@bib:weightdecay,dropout] that impose constraints to avoid overfitting, another approach for reducing the DOF is by decreasing the number of learnable parameters.", "To this end, [@bib:jaderberg2014speeding,dcfnet,basisconv] propose using certain basis representations for weight tensors.", "In these methods, the basis vectors are fixed and only their coefficients are learnable.", "Thus, by using a smaller number of coefficients than the size of weight tensors, the DOF can be effectively restricted. But, note that, this is useful only during training since the original higher number of parameters are used during inference.", "[@bib:dcfnet] shows that systematically choosing the basis (e.g. the Fourier-Bessel basis) can lead to model size shrinkage and flops reduction even during inference.", "\\newline In this work, we explore restricting the degrees of freedom of convolutional kernels by imposing a structure on them.", "This structure can be thought of as constructing the convolutional kernel by super-imposing several constant-height kernels.", "A few examples are shown in Fig. [@ref:LABEL:fig:OS_illustration] , where a kernel is constructed via superimposition of $ M $ linearly independent masks with associated constant scalars $ \\alpha_{m} $ , hence leading to $ M $ degrees of freedom for the kernel.", "The very nature of the basis elements as binary masks enables efficient execution of the convolution operation as explained in Sec. [@ref:LABEL:sec:conv_composite] .", "\\newline In Sec. [@ref:LABEL:sec:structuredconv] , we introduce Structured Convolutions as a special case of this superimposition and show that it leads to a decomposition of the convolution operation into a sum-pooling operation and a significantly smaller convolution operation.", "We show how this decomposition can be applied to convolutional layers as well as fully connected layers.", "We further propose a regularization method named Structural Regularization that promotes the normal convolution weights to have the desired structure that facilitates our proposed decomposition.", "Overall, our key contributions in this work are: \\newline <list> \\ We introduce Composite Kernel structure, which accepts an arbitrary basis in the kernel formation, leading to an efficient convolution operation.", "Sec. [@ref:LABEL:sec:composite_definition] provides the definition.", "\\newline \\ \\ We propose Structured Convolutions, a realization of the composite kernel structure.", "We show that a structured convolution can be decomposed into a sum-pooling operation followed by a much smaller convolution operation.", "A detailed analysis is provided in Sec. [@ref:LABEL:sec:decomposition] .", "\\newline \\ \\ Finally, we design Structural Regularization, an effective training method to enable the structural decomposition with minimal loss of accuracy.", "Our process is described in Sec. [@ref:LABEL:sec:regularization] .", "\\newline \\ </list> \\newline  </section>"], ["<section> <title> 2 Related Work </title>  The existing literature on exploiting redundancy in deep networks can be broadly studied as follows.", "\\newline Tensor Decomposition Methods.", "The work in [@bib:zhang2015] proposed a Generalized SVD approach to decompose a $ B\\!\\times\\!C\\!\\times\\!k\\!\\times\\!k $ convolution (where $ B $ and $ C $ are output and input channels, and $ k $ is the spatial size) into a $ B^{\\prime}\\!\\times\\!C\\!\\times\\!k\\!\\times\\!k $ convolution followed by a $ B\\!\\times\\!B^{\\prime}\\!\\times\\!1\\!\\times\\!1 $ convolution.", "Likewise, [@bib:jaderberg2014speeding] introduced Spatial SVD to decompose a $ k\\!\\times\\!k $ kernel into $ k\\!\\times\\!p $ and $ p\\!\\times\\!k $ kernels.", "[@bib:tai2015convolutional] further developed a non-iterative method for such low-rank decomposition.", "CP-decomposition [@bib:kolda2009tensor,lebedev2014speeding] and tensor-train decomposition [@bib:oseledets2011tensor,su2018tensorized,yang2017tensor] have been proposed to decompose high dimensional tensors.", "In our method, we too aim to decompose regular convolution into computationally lightweight units.", "\\newline Structured Pruning. [@bib:amc,he2017,Li2016] presented channel pruning methods where redundant channels in every layer are removed.", "The selection process of the redundant channels is unique to every method, for instance, [@bib:he2017] addressed the channel selection problem using lasso regression.", "Similarly, [@bib:wen2016learning] used group lasso regularization to penalize and prune unimportant groups on different levels of granularity.", "We refer readers [@bib:kuzmin2019taxonomy] for a survey of structured pruning and tensor decomposition methods.", "To our advantage, the proposed method in this paper does not explicitly prune, instead, our structural regularization loss imposes a form on the convolution kernels.", "\\newline Semi-structured and Unstructured Pruning.", "Other works [@bib:lebedev2016fast,liu2018rethinking,elsen2019fast] employed block-wise sparsity (also called semi-structured pruning) which operates on a finer level than channels.", "Unstructured pruning methods [@bib:azarian2020learned,han2015deep,kusupati2020soft,admm] prune on the parameter-level yielding higher compression rates.", "However, their unstructured nature makes it difficult to deploy them on most hardware platforms.", "\\newline Using Prefixed Basis.", "Several works [@bib:dcfnet,basisconv] applied basis representations in deep networks.", "Seminal works [@bib:mallat2012group,sifre2013rotation] used wavelet bases as feature extractors.", "Choice of the basis is important, for example, [@bib:dcfnet] used Fourier-Bessel basis that led to a reduction in computation complexity.", "In general, tensor decomposition can be seen as basis representation learning.", "We propose using structured binary masks as our basis, which leads to an immediate reduction in the number of multiplications.", "\\newline Orthogonal to structured compression, [@bib:lin2019tsm,wu2018shift,zhong2018shift] utilized shift-based operations to reduce the overall computational load.", "Given the high computational cost of multiplications compared to additions [@bib:horowitz20141] , [@bib:chen2019addernet] proposed networks where the majority of the multiplications are replaced by additions.", "\\newline  </section>"], ["<section> <title> 3 Composite Kernels </title>  We first give a definition that encompasses a wide range of structures for convolution kernels.", "\\newline <theorem> Definition 1 .", "For $ \\mathcal{R}^{C\\!\\times\\!N\\!\\times\\!N} $ , a Composite Basis $ \\mathcal{B}=\\{\\beta_{1},\\beta_{2},...,\\beta_{M}\\} $ is a linearly independent set of binary tensors of dimension $ C\\!\\times\\!N\\!\\times\\!N $ as its basis elements.", "That is, $ \\beta_{m}\\in\\mathcal{R}^{C\\!\\times\\!N\\!\\times\\!N} $ , each element $ \\beta_{mijk} $ of $ \\beta_{m}\\in\\{0,1\\} $ , and $ \\sum_{m=1}^{M}\\alpha_{m}\\beta_{m}=0\\;\\;\\text{iff}\\;\\;\\alpha_{m}=0\\;\\forall m $ .", "\\newline </theorem> The linear independence condition implies that $ M\\leq CN^{2} $ .", "Hence, the basis spans a subspace of $ \\mathcal{R}^{C\\!\\times\\!N\\!\\times\\!N} $ .", "The speciality of the Composite Basis is that the basis elements are binary, which leads to an immediate reduction in the number of multiplications involved in the convolution operation.", "\\newline <theorem> Definition 2 .", "A kernel $ W\\in\\mathcal{R}^{C\\!\\times\\!N\\!\\times\\!N} $ is a Composite Kernel", "if it is in the subspace of the Composite Basis.", "That is, it can be constructed as a linear combination of the elements of $ \\mathcal{B} $ : $ \\exists $ $ \\boldsymbol{\\alpha}=[\\alpha_{1},..,\\alpha_{M}] $ such that $ W\\coloneqq\\sum_{m=1}^{M}\\alpha_{m}\\beta_{m} $ .", "\\newline </theorem> Note that, the binary structure of the underlying Composite Basis elements defines the structure of the Composite Kernel.", "Fig. [@ref:LABEL:fig:OS_illustration] shows a $ 3\\!\\times\\!3 $ Composite Kernel (with $ C=1,N=3 $ ) constructed using different examples of a Composite Basis.", "In general, the underlying basis elements could have a more random structure than what is demonstrated in those examples shown in Fig. [@ref:LABEL:fig:OS_illustration] .", "\\newline Conventional kernels (with no restrictions on DOF) are just special cases of Composite Kernels, where $ M=CN^{2} $ and each basis element has only one nonzero element in its $ C\\!\\times\\!N\\!\\times\\!N $ grid.", "\\newline <subsection> <title> 3.1 Convolution with Composite Kernels </title> Consider a convolution with a Composite Kernel of size $ C\\!\\times\\!N\\!\\times\\!N $ , where $ N $ is the spatial size and $ C $ is the number of input channels.", "To compute an output, this kernel is convolved with a $ C\\!\\times\\!N\\!\\times\\!N $ volume of the input feature map.", "Let\u2019s call this volume $ X $ .", "Therefore, the output at this point will be: \\newline <equation> $ X*W=X*\\sum_{m=1}^{M}\\alpha_{m}\\beta_{m}=\\sum_{m=1}^{M}\\alpha_{m}(X*\\beta_{m})=% \\sum_{m=1}^{M}\\alpha_{m}\\,sum(X\\bullet\\beta_{m})=\\sum_{m=1}^{M}\\alpha_{m}E_{m} $ </equation> \\newline where \u2018 $ * $ \u2019 denotes convolution, \u2018 $ \\bullet $ \u2019 denotes element-wise multiplication.", "Since $ \\beta_{m} $ is a binary tensor, $ sum(X\\bullet\\beta_{m}) $ is same as adding the elements of $ X $ wherever $ \\beta_{m}=1 $ , thus no multiplications are needed.", "Ordinarily, the convolution $ X*W $ would involve $ CN^{2} $ multiplications and $ CN^{2}-1 $ additions.", "In our method, we can trade multiplications with additions.", "From ( [@ref:LABEL:eq:conv_OS] ), we can see that we only need $ M $ multiplications and the total number of additions becomes: \\newline <equationgroup> <equation> $ \\text{Num additions}=\\sum_{m=1}^{M}\\underbrace{(sum(\\beta_{m})-1)% }_{\\text{from }sum(X\\bullet\\beta_{m})}\\;\\;+\\underbrace{(M-1)}_{\\text{from }% \\sum\\alpha_{m}E_{m}}=\\sum_{m=1}^{M}sum(\\beta_{m})-1 $ $ \\text{Num additions}=\\sum_{m=1}^{M}\\underbrace{(sum(\\beta_{m})-1)% }_{\\text{from }sum(X\\bullet\\beta_{m})}\\;\\;+\\underbrace{(M-1)}_{\\text{from }% \\sum\\alpha_{m}E_{m}}=\\sum_{m=1}^{M}sum(\\beta_{m})-1 $ </equation> </equationgroup> \\newline Depending on the structure, number of the additions per output can be larger than $ CN^{2}-1 $ .", "For example, in Fig. [@ref:LABEL:fig:OS_illustration] (b) where $ C=1,N=3,M=4 $ , we have $ \\sum_{m}sum(\\beta_{m})-1=15>CN^{2}-1=8 $ ).", "In Sec. [@ref:LABEL:sec:num_ops] , we show that the number of additions can be amortized to as low as $ M-1 $ .", "\\newline </subsection>  </section>"], ["<section> <title> 4 Structured Convolutions </title>  <theorem> Definition 3 .", "A kernel in $ \\mathcal{R}^{C\\!\\times\\!N\\!\\times\\!N} $ is a Structured Kernel if it is a Composite Kernel with $ M=cn^{2} $ for some $ 1\\leq n\\leq N,1\\leq c\\leq C $ , and if each basis tensor $ \\beta_{m} $ is made of a $ (C-c+1)\\times(N-n+1)\\times(N-n+1) $ cuboid of $ 1 $ \u2019s, while rest of its coefficients being $ 0 $ .", "\\newline </theorem> A Structured Kernel is characterized by its dimensions $ C,N $ and its underlying parameters $ c,n $ .", "Convolutions performed using Structured Kernels are called Structured Convolutions.", "\\newline Fig. [@ref:LABEL:fig:OS_illustration] (b) depicts a 2D case of a $ 3\\!\\times\\!3 $ structured kernel where $ C=1,N=3,c=1,n=2 $ .", "As shown, there are $ M=cn^{2}=4 $ basis elements and each element has a $ 2\\!\\times\\!2 $ sized patch of $ 1 $ \u2019s. \\newline Fig. [@ref:LABEL:fig:structured_kernel] shows a 3D case where $ C=4,N=3,c=2,n=2 $ .", "Here, there are $ M=cn^{2}=8 $ basis elements and each element has a $ 3\\!\\times\\!2\\!\\times\\!2 $ cuboid of $ 1 $ \u2019s. Note how these cuboids of $ 1 $ \u2019s (shown in colors) cover the entire $ 4\\!\\times\\!3\\!\\times\\!3 $ grid.", "\\newline <subsection> <title> 4.1 Decomposition of Structured Convolutions </title> A major advantage of defining Structured Kernels this way is that all the basis elements are just shifted versions of each other (see Fig. [@ref:LABEL:fig:structured_kernel] and Fig. [@ref:LABEL:fig:OS_illustration] (b)).", "This means, in Eq. ( [@ref:LABEL:eq:conv_OS] ), if we consider the convolution $ X*W $ for the entire feature map $ X $ , the summed outputs $ X*\\beta_{m} $ for all $ \\beta_{m} $ \u2019s are actually the same (except on the edges of $ X $ ).", "As a result, the outputs $ \\{X*\\beta_{1},...,X*\\beta_{cn^{2}}\\} $ can be computed using a single sum-pooling operation on $ X $ with a kernel size of $ (C-c+1)\\times(N-n+1)\\times(N-n+1) $ .", "Fig. [@ref:LABEL:fig:conv_3x3] shows a simple example of how a convolution with a $ 3\\!\\times\\!3 $ structured kernel can be broken into a $ 2\\!\\times\\!2 $ sum-pooling followed by a $ 2\\!\\times\\!2 $ convolution with a kernel made of $ \\alpha_{i} $ \u2019s. \\newline Furthermore, consider a convolutional layer of size $ C_{out}\\!\\times\\!C\\!\\times\\!N\\!\\times\\!N $ that has $ C_{out} $ kernels of size $ C{\\mkern-1.0mu \\times\\mkern-1.0mu }N{\\mkern-1.0mu \\times\\mkern-1.0mu }N $ .", "In our design, the same underlying basis $ \\mathcal{B}=\\{\\beta_{1},...,\\beta_{cn^{2}}\\} $ is used for the construction of all $ C_{out} $ kernels in the layer.", "Suppose any two structured kernels in this layer with coefficients $ \\boldsymbol{\\alpha}^{(1)} $ and $ \\boldsymbol{\\alpha}^{(2)} $ , i.e. $ W_{1}=\\sum_{m}\\alpha_{m}^{(1)}\\beta_{m} $ and $ W_{2}=\\sum_{m}\\alpha_{m}^{(2)}\\beta_{m} $ .", "The convolution output with these two kernels is respectively, $ X*W_{1}=\\sum_{m}\\alpha_{m}^{(1)}(X*\\beta_{m}) $ and $ X*W_{2}=\\sum_{m}\\alpha_{m}^{(2)}(X*\\beta_{m}) $ .", "We can see that the computation $ X*\\beta_{m} $ is common to all the kernels of this layer.", "Hence, the sum-pooling operation only needs to be computed once and then reused across all the $ C_{out} $ kernels.", "\\newline A Structured Convolution can thus be decomposed into a sum-pooling operation and a smaller convolution operation with a kernel composed of $ \\alpha_{i} $ \u2019s. Fig. [@ref:LABEL:fig:structural_decomposition] shows the decomposition of a general structured convolution layer of size $ C_{out}\\!\\times\\!C\\!\\times\\!N\\!\\times\\!N $ .", "\\newline Notably, standard convolution ( $ C{\\mkern-1.0mu \\times\\mkern-1.0mu }N{\\mkern-1.0mu \\times\\mkern-1.0mu }N $ ), depthwise convolution ( $ 1{\\mkern-1.0mu \\times\\mkern-1.0mu }N{\\mkern-1.0mu \\times\\mkern-1.0mu }N $ ), and pointwise convolution ( $ C{\\mkern-1.0mu \\times\\mkern-1.0mu }1{\\mkern-1.0mu \\times\\mkern-1.0mu }1 $ ) kernels can all be constructed as 3D structured kernels, which means that this decomposition can be widely applied to existing architectures.", "See supplementary material for more details on applying the decomposition to convolutions with arbitrary stride, padding, dilation .", "\\newline </subsection> <subsection> <title> 4.2 Reduction in Number of Parameters and Multiplications/Additions </title> The sum-pooling component after decomposition requires no parameters.", "Thus, the total number of parameters in a convolution layer get reduced from $ C_{out}CN^{2} $ (before decomposition) to $ C_{out}cn^{2} $ (after decomposition).", "The sum-pooling component is also free of multiplications.", "Hence, only the smaller $ c{\\mkern-1.0mu \\times\\mkern-1.0mu }n{\\mkern-1.0mu \\times\\mkern-1.0mu }n $ convolution contributes to multiplications after decomposition.", "\\newline Before decomposition, computing every output element in feature map $ Y\\in\\mathcal{R}^{C_{out}{\\mkern-1.0mu \\times\\mkern-1.0mu }H^{\\prime}{\\mkern-1.% 0mu \\times\\mkern-1.0mu }W^{\\prime}} $ involves $ CN^{2} $ multiplications and $ CN^{2}-1 $ additions.", "Hence, total multiplications involved are $ CN^{2}C_{out}H^{\\prime}W^{\\prime} $ and total additions involved are $ (CN^{2}-1)C_{out}H^{\\prime}W^{\\prime} $ .", "\\newline After decomposition, computing every output element in feature map $ Y $ involves $ cn^{2} $ multiplications and $ cn^{2}-1 $ additions.", "Hence, total multiplications and additions involved in computing $ Y $ are $ cn^{2}C_{out}H^{\\prime}W^{\\prime} $ and $ (cn^{2}-1)C_{out}H^{\\prime}W^{\\prime} $ respectively.", "Now, computing every element of the intermediate sum-pooled output $ E\\in\\mathcal{R}^{c{\\mkern-1.0mu \\times\\mkern-1.0mu }H_{1}{\\mkern-1.0mu \\times% \\mkern-1.0mu }W_{1}} $ involves $ ((C-c+1)(N-n+1)^{2}-1) $ additions.", "Hence, the overall total additions involved can be written as: \\newline <equation> $ C_{out}\\left(\\frac{((C-c+1)(N-n+1)^{2}-1)cH_{1}W_{1}}{C_{out}}+(cn^{2}-1)H^{% \\prime}W^{\\prime}\\right) $ </equation> \\newline We can see that the number of parameters and number of multiplications have both reduced by a factor of $ \\mathbf{{cn^{2}}/{CN^{2}}} $ . And in the expression above, if $ C_{out} $ is large enough, the first term inside the parentheses gets amortized and the number of additions $ \\approx(cn^{2}-1)C_{out}H^{\\prime}W^{\\prime} $ .", "As a result, the number of additions also reduce by approximately the same proportion $ \\approx cn^{2}/CN^{2} $ .", "We will refer to $ CN^{2}/cn^{2} $ as the compression ratio from now on.", "\\newline Due to amortization, the additions per output are $ \\approx cn^{2}-1 $ , which is basically $ M-1 $ since $ M=cn^{2} $ .", "\\newline </subsection> <subsection> <title> 4.3 Extension to Fully Connected layers </title> For image classification networks, the last fully connected layer (sometimes called linear layer) dominates w.r.t.", "the number of parameters, especially if the number of classes is high.", "The structural decomposition can be easily extended to the linear layers by noting that a matrix multiplication is the same as performing a number of $ 1{\\mkern-1.0mu \\times\\mkern-1.0mu }1 $ convolutions on the input.", "Consider a kernel $ W\\in\\mathcal{R}^{P\\times Q} $ and input vector $ X\\in\\mathcal{R}^{Q\\times 1} $ .", "The linear operation $ WX $ is mathematically equivalent to the $ 1\\times 1 $ convolution $ unsqueezed(X)*unsqueezed(W) $ , where $ unsqueezed(X) $ is the same as $ X $ but with dimensions $ Q\\times 1\\times 1 $ and $ unsqueezed(W) $ is the same as $ W $ but with dimensions $ P\\times Q\\times 1\\times 1 $ .", "In other words, each row of $ W $ can be considered a $ 1\\times 1 $ convolution kernel of size $ Q\\times 1\\times 1 $ .", "\\newline Now, if each of these kernels (of size $ Q\\times 1\\times 1 $ ) is structured with underlying parameter $ R $ (where $ R\\leq Q $ ), then the matrix multiplication operation can be structurally decomposed as shown in Fig. [@ref:", "LABEL:fig:structural_decomposition_matrix] .", "\\newline Same as before, we get a reduction in both the number of parameters and the number of multiplications by a factor of $ \\mathbf{R/Q} $ , as well as the number of additions by a factor of $ \\frac{R(Q-R)+(PR-1)P}{(PQ-1)P} $ .", "\\newline </subsection>  </section>"], ["<section> <title> 5 Imposing Structure on Convolution Kernels </title>  To apply the structural decomposition, we need the weight tensors to be structured.", "In this section, we propose a method to impose the desired structure on the convolution kernels via training.", "\\newline From the definition, $ W=\\sum_{m}\\alpha_{m}\\beta_{m} $ , we can simply define matrix $ A\\in\\mathcal{R}^{CN^{2}{\\mkern-1.0mu \\times\\mkern-1.0mu }cn^{2}} $ such that its $ i^{th} $ column is the vectorized form of $ \\beta_{i} $ .", "Hence, $ \\text{vectorized}(W)=A\\cdot\\boldsymbol{\\alpha} $ , where $ \\boldsymbol{\\alpha}=[\\alpha_{1},...,\\alpha_{cn^{2}}] $ .", "\\newline Another way to see this is from structural decomposition.", "We may note that the $ (C-c+1){\\mkern-1.0mu \\times\\mkern-1.0mu }(N-n+1){\\mkern-1.0mu \\times\\mkern-1.0% mu }(N-n+1) $ sum-pooling can also be seen as a convolution with a kernel of all $ 1 $ \u2019s; we refer to this kernel as $ \\mathbf{1}_{(C-c+1){\\mkern-1.0mu \\times\\mkern-1.0mu }(N-n+1){\\mkern-1.0mu % \\times\\mkern-1.0mu }(N-n+1)} $ .", "Hence, the structural decomposition is: \\newline <equation> $ X*W=X*\\mathbf{1}_{(C-c+1){\\mkern-1.0mu \\times\\mkern-1.0mu }(N-n+1){\\mkern-1.0% mu \\times\\mkern-1.0mu }(N-n+1)}*\\boldsymbol{\\alpha}_{c{\\mkern-1.0mu \\times% \\mkern-1.0mu }n{\\mkern-1.0mu \\times\\mkern-1.0mu }n} $ </equation> \\newline That implies, $ W=\\mathbf{1}_{(C-c+1){\\mkern-1.0mu \\times\\mkern-1.0mu }(N-n+1){\\mkern-1.0mu % \\times\\mkern-1.0mu }(N-n+1)}*\\boldsymbol{\\alpha}_{c{\\mkern-1.0mu \\times\\mkern-% 1.0mu }n{\\mkern-1.0mu \\times\\mkern-1.0mu }n} $ .", "Since the stride of the sum-pooling involved is $ 1 $ , this can be written in terms of a matrix multiplication with a Topelitz matrix [@bib:strang1986proposal] : \\newline <equation> $ \\text{vectorized}(W)=\\text{Topelitz}(\\mathbf{1}_{(C-c+1){\\mkern-1.0mu \\times% \\mkern-1.0mu }(N-n+1){\\mkern-1.0mu \\times\\mkern-1.0mu }(N-n+1)})\\cdot\\text{% vectorized}(\\boldsymbol{\\alpha}_{c{\\mkern-1.0mu \\times\\mkern-1.0mu }n{\\mkern-1% .0mu \\times\\mkern-1.0mu }n}) $ </equation> \\newline Hence, the structure matrix $ A $ referred above is basically $ \\text{Topelitz}(\\mathbf{1}_{(C-c+1){\\mkern-1.0mu \\times\\mkern-1.0mu }(N-n+1){% \\mkern-1.0mu \\times\\mkern-1.0mu }(N-n+1)}) $ .", "\\newline <subsection> <title> 5.1 Training with Structural Regularization </title> Now, for a structured kernel $ W $ characterized by $ \\{C,N,c,n\\} $ , there exists a $ cn^{2} $ length $ \\boldsymbol{\\alpha} $ such that $ W=A\\boldsymbol{\\alpha} $ .", "Hence, a structured kernel $ W $ satisfies the property: $ W=AA^{+}W $ , where $ A^{+} $ is the Moore-Penrose inverse [@bib:ben2003generalized] of $ A $ .", "Based on this, we propose training a deep network with a Structural Regularization loss that can gradually push the deep network\u2019s kernels to be structured via training : \\newline <equation> $ \\mathcal{L}_{total}=\\mathcal{L}_{task}+\\lambda\\sum_{l=1}^{L}\\frac{\\left\\lVert(% I-A_{l}A_{l}^{+})W_{l}\\right\\rVert_{F}}{\\left\\lVert W_{l}\\right\\rVert_{F}} $ </equation> where $ \\left\\lVert\\cdot\\right\\rVert_{F} $ denotes Frobenius norm and $ l $ is the layer index.", "To ensure that regularization is applied uniformly to all layers, we use $ \\left\\lVert W\\right\\rVert_{F} $ normalization in the denominator.", "It also stabilizes the performance of the decomposition w.r.t $ \\lambda $ .", "The overall proposed training recipe is as follows: \\newline Proposed Training Scheme: \\newline <list> \\ {Step 1} : Train the original architecture with the Structural Regularization loss.", "\\newline <list> \\ After Step 1, all weight tensors in the deep network will be almost structured.", "\\newline \\ </list> \\newline \\ \\ {Step 2} : Apply the decomposition on every layer and compute $ \\alpha_{l}=A_{l}^{+}W_{l} $ .", "\\newline <list> \\ This results in a smaller and more efficient decomposed architecture with $ \\alpha_{l} $ \u2019s as the weights.", "Note that, every convolution / linear layer from the original architecture is now replaced with a sum-pooling layer and a smaller convolution / linear layer.", "\\newline \\ </list> \\newline \\ </list> \\newline The proposed scheme trains the architecture with the original $ C{\\mkern-1.0mu \\times\\mkern-1.0mu }N{\\mkern-1.0mu \\times\\mkern-1.0mu }N $ kernels in place but with a structural regularization loss.", "The structural regularization loss imposes a restrictive $ cn^{2} $ degrees of freedom while training but in a soft or gradual manner (depending on $ \\lambda $ ): \\newline <list> \\ If $ \\lambda=0 $ , it is the same as normal training with no structure imposed.", "\\newline \\ \\ If $ \\lambda $ is very high, the regularization loss will be heavily minimized in early training iterations.", "Thus, the weights will be optimized in a restricted $ cn^{2} $ dimensional subspace of $ \\mathcal{R}^{C{\\mkern-1.0mu \\times\\mkern-1.0mu }N{\\mkern-1.0mu \\times\\mkern-1.% 0mu }N} $ .", "\\newline \\ \\ Choosing a moderate $ \\lambda $ gives the best tradeoff between structure and model performance.", "\\newline \\ </list> \\newline We talk about training implementation details for reproduction, such as hyperparameters and training schedules, in Supplementary material, where we also show our method is robust to the choice of $ \\lambda $ .", "\\newline </subsection>  </section>"], ["<section> <title> 6 Experiments </title>  We apply structured convolutions to a wide range of architectures and analyze the performance and complexity of the decomposed architectures.", "We evaluate our method on ImageNet [@bib:ILSVRC15] and CIFAR-10 [@bib:cifar] benchmarks for image classification and Cityscapes [@bib:cordts2016cityscapes] for semantic segmentation.", "\\newline <subsection> <title> 6.1 Image Classification </title> We present results for ResNets [@bib:resnet] in Tables [@ref:LABEL:table:cifar_resnet] and [@ref:LABEL:table:imagenet_resnet] .", "To demonstrate the efficacy of our method on modern networks, we also show results on MobileNetV2 [@bib:mobilenetv2] and EfficientNet [@bib:efficientnet] in Table [@ref:LABEL:table:imagenet_mv2] and [@ref:LABEL:table:imagenet_efficientnet] .", "\\newline To provide a comprehensive analysis, for each baseline architecture, we present structured counterparts, with version \"A\" designed to deliver similar accuracies and version \"B\" for extreme compression ratios.", "Using different $ \\{c,n\\} $ configurations per-layer, we obtain structured versions with varying levels of reduction in model size and multiplications/additions (please see Supplementary material for details).", "For the \"A\" versions of ResNet, we set the compression ratio ( $ CN^{2}/cn^{2} $ ) to be 2 $ \\times $ for all layers.", "For the \"B\" versions of ResNets, we use nonuniform compression ratios per layer.", "Specifically, we compress stages 3 and 4 drastically (4 $ \\times $ ) and stages 1 and 2 by $ 2\\times $ .", "Since MobileNet is already a compact model, we design its \"A\" version to be $ 1.33\\times $ smaller and \"B\" version to be $ 2\\times $ smaller.", "\\newline We note that, on low-level hardware, additions are much power-efficient and faster than multiplications [@bib:chen2019addernet,horowitz20141] .", "Since the actual inference time depends on how software optimizations and scheduling are implemented, for most objective conclusions, we provide the number of additions / multiplications and model sizes.", "Considering observations for sum-pooling on dedicated hardware units [@bib:young2018performing] , our structured convolutions can be easily adapted for memory and compute limited devices.", "\\newline Compared to the baseline models, the Struct-A versions of ResNets are $ 2\\times $ smaller, while maintaining less than $ 0.65\\% $ loss in accuracy.", "The more aggressive Struct-B ResNets achieve $ 60 $ - $ 70\\% $ model size reduction with about $ 2 $ - $ 3\\% $ accuracy drop.", "Compared to other methods, Struct-56-A is $ 0.75\\% $ better than AMC-Res56 [@bib:amc] of similar complexity and Struct-20-A exceeds ShiftResNet20-6 [@bib:wu2018shift] by $ 0.45\\% $ while being significantly smaller.", "Similar trends are observed with Struct-Res18 and Struct-Res50 on ImageNet.", "Struct-56-A and Struct-50-A achieve competitive performance as compared to the recent GhostNets [@bib:ghostnet] .", "For MobileNetV2 which is already designed to be efficient, Struct-MV2-A achieves further reduction in multiplications and model size with SOTA performance compared to other methods, see Table [@ref:LABEL:table:imagenet_mv2] .", "Applying structured convolutions to EfficientNet-B1 results in Struct-EffNet that has comparable performance to EfficientNet-B0, as can be seen in Table [@ref:LABEL:table:imagenet_efficientnet] .", "\\newline The ResNet Struct-A versions have similar number of adds and multiplies (except ResNet50) because, as noted in Sec. [@ref:LABEL:sec:num_ops] , the sum-pooling contribution is amortized. But sum-pooling starts dominating as the compression gets more aggressive, as can be seen in the number of adds for Struct-B versions.", "Notably, both \"A\" and \"B\" versions of MobileNetV2 observe a dominance of the sum-pooling component.", "This is because the number of output channels are not enough to amortize the sum-pooling component resulting from the decomposition of the pointwise ( $ 1\\!\\times\\!1 $ conv) layers.", "\\newline Fig. [@ref:LABEL:fig:structured_comparisons] compares our method with state-of-the-art structured compression methods - WeightSVD [@bib:zhang2016accelerating] , Channel Pruning [@bib:he2017] , and Tensor-train [@bib:su2018tensorized] .", "Note, the results were obtained from [@bib:kuzmin2019taxonomy] .", "Our proposed method achieves approximately $ 1\\% $ improvement over the second best method for ResNet18 ( $ 2\\times $ ) and MobileNetV2 ( $ 1.3\\times $ ).", "Especially for MobileNetV2, this improvement is valuable since it significantly outperforms all the other methods (see Struct-V2-A in Table [@ref:LABEL:table:imagenet_mv2] ).", "\\newline </subsection> <subsection> <title> 6.2 Semantic Segmentation </title> After demonstrating the superiority of our method on image classification, we evaluate it for semantic segmentation that requires reproducing fine details around object boundaries.", "We apply our method to a recently developed state-of-the-art HRNet [@bib:WangSCJDZLMTWLX19] .", "Table [@ref:LABEL:table:cityscapes] shows that the structured convolutions can significantly improve our segmentation model efficiency: HRNet model is reduced by 50% in size, and 30% in number of additions and multiplications, while having only 1.5% drop in mIoU. More results for semantic segmentation can be found in the supplementary material.", "\\newline </subsection> <subsection> <title> 6.3 Computational overhead of Structural Regularization term </title> The proposed method involves computing the Structural Regularization term ( [@ref:LABEL:eq:structural_regularization_loss] ) during training.", "Although the focus of this work is more on inference efficiency, we measure the memory and time-per-iteration for training with and w/o the Structural Regularization loss on an NVIDIA V100 GPU.", "We report these numbers for Struct-18-A and Struct-MV2-A architectures below.", "As observed, the additional computational cost of the regularization term is negligible for a batchsize of $ 256 $ .", "This is because, mathematically, the regularization term, $ \\sum_{l=1}^{L}\\frac{\\left\\lVert(I-A_{l}A_{l}^{+})W_{l}\\right\\rVert_{F}}{\\left% \\lVert W_{l}\\right\\rVert_{F}} $ , is independent of the input size.", "Hence, when using a large batchsize for training, the regluarization term\u2019s memory and runtime overhead is relatively small (less than $ 5\\% $ for Struct-MV2-A and $ 10\\% $ for Struct-18-A).", "\\newline </subsection> <subsection> <title> 6.4 Directly training Structured Convolutions as an architectural feature </title> In our proposed method, we train the architecture with original $ C\\!\\times\\!N\\!\\times\\!N $ kernels in place and the regularization loss imposes desired structure on these kernels.", "At the end of training, we decompose the $ C\\!\\times\\!N\\!\\times\\!N $ kernels and replace each with a sum-pooling layer and smaller layer of $ c\\!\\times\\!n\\!\\times\\!n $ kernels.", "\\newline A more direct approach could be to train the decomposed architecture (with the sum-pooling + $ c\\!\\times\\!n\\!\\times\\!n $ layers in place) directly."]], "target": "The regularization term is not required in this direct approach, as there is no decomposition step, hence eliminating the computation overhead shown in Table . We experimented with this direct training method and observed that the regularization based approach always outperformed the direct approach (by $ 1.5 $ % for Struct-18-A and by $ 0.9 $ % for Struct-MV2-A)."}, {"tabular": ["  Case  &  Assignment ", " \\ $ 1\\leq m_{s}\\leq 2 $ <ln> $ m_{1}+m_{2}\\geq 1 $ <ln> $ m_{1}+m_{3}\\geq 1 $ <ln> $ m_{2}+m_{3}\\geq 1 $  &  $ \\alpha_{1}=2-m_{s} $ <ln> $ \\alpha_{2}=\\alpha_{3}=0 $ <ln> $ \\alpha_{12}=m_{1}+m_{2}-1 $ <ln> $ \\alpha_{13}=m_{1}+m_{3}-1 $ <ln> $ \\alpha_{23}=1-m_{1} $ <ln> $ \\alpha_{123}=0 $ <ln> ", " $ 1\\leq m_{s}\\leq 2 $ <ln> $ m_{1}+m_{2}\\geq 1 $ <ln> $ m_{1}+m_{3}\\geq 1 $ <ln> $ m_{2}+m_{3}\\leq 1 $  &  $ \\alpha_{1}=2-m_{s} $ <ln> $ \\alpha_{2}=\\alpha_{3}=0 $ <ln> $ \\alpha_{12}=m_{1}+m_{2}-1 $ <ln> $ \\alpha_{13}=m_{1}+m_{3}-1 $ <ln> $ \\alpha_{23}=1-m_{1} $ <ln> $ \\alpha_{123}=0 $ <ln> ", " $ 1\\leq m_{s}\\leq 2 $ <ln> $ m_{1}+m_{2}\\geq 1 $ <ln> $ m_{1}+m_{3}\\leq 1 $ <ln> $ m_{2}+m_{3}\\leq 1 $  &  $ \\alpha_{1}=1-(m_{2}+m_{3}) $ <ln> $ \\alpha_{2}=1-(m_{1}+m_{3}) $ <ln> $ \\alpha_{3}=m_{3} $ <ln> $ \\alpha_{12}=m_{s}-1 $ <ln> $ \\alpha_{13}=\\alpha_{23}=0 $ <ln> $ \\alpha_{123}=0 $ <ln> ", " $ 1\\leq m_{s}\\leq 2 $ <ln> $ m_{1}+m_{2}\\leq 1 $ <ln> $ m_{1}+m_{3}\\leq 1 $ <ln> $ m_{2}+m_{3}\\leq 1 $  &  $ \\alpha_{1}=1-(m_{2}+m_{3}) $ <ln> $ \\alpha_{2}=1-(m_{1}+m_{3}) $ <ln> $ \\alpha_{3}=m_{3} $ <ln> $ \\alpha_{12}=m_{s}-1 $ <ln> $ \\alpha_{13}=\\alpha_{23}=0 $ <ln> $ \\alpha_{123}=0 $ <ln> ", " $ 2\\leq m_{s}\\leq 3 $  &  $ \\alpha_{1}=\\alpha_{2}=\\alpha_{3}=0 $ <ln> $ \\alpha_{12}=1-m_{3} $ <ln> $ \\alpha_{13}=1-m_{2} $ <ln> $ \\alpha_{23}=1-m_{1} $ <ln> $ \\alpha_{123}=m_{s}-2 $  "], "ref_sec": [["<section> <title> 1 Introduction </title>  The problem of private information retrieval (PIR), introduced in [@bib:ChorPIR] , has attracted much interest in the information theory community with leading efforts [@bib:RamchandranPIR,YamamotoPIR,VardyConf2015,RazanPIR,JafarPIRBlind] .", "In the classical setting of PIR, a user wants to retrieve a file out of $ K $ files from $ N $ databases, each storing the same content of entire $ K $ files, such that no individual database can identify the identity of the desired file.", "Sun and Jafar [@bib:JafarPIR] characterized the optimal normalized download cost of the classical setting to be $ D^{*}=1+\\frac{1}{N}+\\cdots+\\frac{1}{N^{K-1}} $ .", "Fundamental limits of many interesting variants of the PIR problem have been investigated in [@bib:JafarColluding,arbitraryCollusion,RobustPIR_Razane,symmetricPIR,KarimCoded,arbmsgPIR,MultiroundPIR,codedsymmetric,wang2017linear,codedcolluded,MPIRjournal,BPIRjournal,symmetricByzantine,codedcolludingZhang,MPIRcodedcolludingZhang,CodeColludeByzantinePIR,tandon2017capacity,KimCache,wei2017fundamental,kadhe2017private,chen2017capacity,wei2017capacity,wei2017fundamental_partial,StorageConstrainedPIR_Wei,SI_Gastpar,mirmohseni2017private,PrivateSearch,abdul2017private,StorageConstrainedPIR,KarimAsymmetricPIR,PIR_WTC_II,noisyPIR,SecurePIR,securePIRcapacity,securestoragePIR,XSTPIR,Tian_upload,Staircase_PIR,PIR_cache_edge,Kumar_PIRarbCoded,PIR_decentralized,TamoISIT,LiConverse,PIR_lifting,PIR_networks,Karim_nonreplicated] .", "\\newline A common assumption in most of these works is that the databases have sufficiently large storage space that can accommodate all $ K $ files in a {replicated} manner.", "This may not be the case for peer-to-peer (P2P) and device-to-device (D2D) networks, where information retrieval takes place directly between the users.", "Here, the user devices (databases) will have {limited} and {heterogeneous} sizes.", "This motivates the investigation of PIR from databases with {heterogeneous storage constraints} .", "In this work, we aim to jointly design the storage mechanism (content placement) and the information retrieval scheme such that the normalized PIR download cost is minimized in the retrieval phase.", "\\newline Reference [@bib:StorageConstrainedPIR] studies PIR from {homogeneous storage-limited} databases.", "In [@bib:StorageConstrainedPIR] , each database has the {same} limited storage space of $ \\mu KL $ bits with $ 0\\leq\\mu\\leq 1 $ , where $ L $ is the message size (note, perfect replication would have required $ \\mu=1 $ ).", "The goal of [@bib:StorageConstrainedPIR] is to find the optimal centralized uncoded caching scheme (content placement) that minimizes the PIR download cost.", "[@bib:StorageConstrainedPIR] shows that symmetric batch caching scheme of [@bib:Caching_Maddah_Ali] for content placement together with Sun-Jafar scheme in [@bib:JafarPIR] for information retrieval result in the lowest normalized download cost.", "[@bib:StorageConstrainedPIR] characterizes the optimal storage-download cost trade-off as the lower convex hull of $ N $ pairs $ (\\frac{t}{N},1+\\frac{1}{t}+\\cdots+\\frac{1}{t^{K-1}}) $ , $ t=1,\\cdots,N $ .", "\\newline Meanwhile, the content assignment problem for {heterogeneous} databases (caches) is investigated in the context of coded caching in [@bib:Yener_heterogeneous] .", "In the coded caching problem [@bib:Caching_Maddah_Ali] , the aim is to jointly design the placement and delivery phases in order to minimize the traffic load in the delivery phase during peak hours.", "Reference [@bib:Yener_heterogeneous] proposes an optimization framework where placement and delivery schemes are optimized by solving a linear program.", "Using this optimization framework, [@bib:Yener_heterogeneous] investigates the effects of heterogeneity in cache sizes on the delivery load memory trade-off with uncoded placement.", "\\newline In this paper, we investigate PIR from databases with heterogeneous storage sizes (see Fig. [@ref:LABEL:hetero] ).", "The $ n $ th database can accommodate $ m_{n}KL $ bits, i.e., the storage system is constrained by the storage size vector $ \\bm{m}=(m_{1},\\cdots,m_{N}) $ .", "We aim to characterize the optimal normalized PIR download cost of this problem, and the corresponding optimal placement and optimal retrieval schemes.", "We focus on uncoded placement as in [@bib:StorageConstrainedPIR] and [@bib:Yener_heterogeneous] .", "\\newline Motivated by [@bib:Yener_heterogeneous] , we first show that the optimal normalized download cost is characterized by a linear program.", "For the achievability, each message is partitioned into $ 2^{N}-1 $ partitions (the size of the power set of $ [N] $ , denoted $ \\mathcal{P}([N]) $ ).", "For every partition, we apply the Sun-Jafar scheme [@bib:JafarPIR] .", "The linear program arises as a consequence of optimizing the achievable download cost with respect to the partition sizes subject to the storage constraints.", "For the converse, we slightly modify the converse in [@bib:StorageConstrainedPIR] to be valid for the heterogeneous case.", "These achievability and converse proofs result in exactly the same linear program, yielding the {exact capacity} for this PIR problem for all $ K $ , $ N $ , $ \\bm{m} $ .", "Interestingly, this is unlike the caching problem in [@bib:Yener_heterogeneous] with no privacy requirements, where the linear program is only an achievability, and is shown to be the exact capacity only in special cases.", "\\newline By studying the properties of the solution of the linear program, we show that, surprisingly, the optimal normalized download cost for the heterogeneous problem is identical to the optimal normalized download cost for the corresponding homogeneous problem, where the homogeneous storage constraint is $ \\mu=\\frac{1}{N}\\sum_{n=1}^{N}m_{n} $ for all databases.", "This implies that there is no loss in the PIR capacity due to heterogeneity of storage spaces of the databases.", "In fact, the PIR capacity depends only on the sum of the storage spaces and does not depend on how the storage spaces are distributed among the databases.", "The general proof for this intriguing result is a consequence of an existence proof for a positive linear combination using the theory of positive linear dependence in [@bib:Davis_PosLin] (and using Farkas\u2019 lemma [@bib:boyd2004convex] as a special case) for the constraint set of the linear program.", "As a byproduct of the structural results, we show that, for the optimal content assignment, at most two consecutive types of message partitioning exist, i.e., message $ W_{k} $ should be partitioned such that there are repeated partitions over $ i $ databases and at most one more repeated partitions over $ i+1 $ databases for some $ i $ , where $ i\\in\\{1,\\cdots,N\\} $ .", "While for general $ N $ we show the existence of an optimal content placement that attains the homogeneous PIR capacity, for $ N=3 $ , we provide an explicit (parametric in $ \\bm{m} $ ) optimal content placement.", "\\newline  </section>"], ["<section> <title> 2 System Model </title>  We consider PIR from databases with heterogeneous sizes; see Fig. [@ref:LABEL:hetero] .", "We consider a storage system with $ K $ i.i.d.", "messages (files).", "The $ k $ th message is of length $ L $ bits, i.e., \\newline <equationgroup> <equation> $  H(W_{1},\\cdots,W_{K})=KL,\\quad H(W_{k})=L,\\quad k\\in[K] $ $  H(W_{1},\\cdots,W_{K})=KL,\\quad H(W_{k})=L,\\quad k\\in[K] $ </equation> </equationgroup> \\newline The storage system consists of $ N $ non-colluding databases.", "The storage size of the $ n $ th database is limited to $ m_{n}KL $ bits, for some $ 0\\leq m_{n}\\leq 1 $ .", "Specifically, we denote the contents of the $ n $ th database by $ Z_{n} $ , such that, \\newline <equationgroup> <equation> $  H(Z_{n})\\leq m_{n}KL,\\quad n\\in[N] $ $  H(Z_{n})\\leq m_{n}KL,\\quad n\\in[N] $ </equation> </equationgroup> \\newline The system operates in two phases: In the placement phase, the data center (content generator) stores the message set in the $ N $ databases, in such a way to minimize the download cost in the retrieval phase subject to the heterogeneous storage constraints.", "The placement is done in a {centralized} fashion [@bib:Caching_Maddah_Ali] .", "The user (retriever) has no access to the data center.", "Here, we focus on uncoded placement as in [@bib:StorageConstrainedPIR,Yener_heterogeneous] , i.e., file $ W_{k} $ can be partitioned as, \\newline <equationgroup> <equation> $  W_{k}=\\bigcup_{{\\mathcal{S}}\\subseteq[N]}W_{k,{\\mathcal{S}}} $ $  W_{k}=\\bigcup_{{\\mathcal{S}}\\subseteq[N]}W_{k,{\\mathcal{S}}} $ </equation> </equationgroup> where $ W_{k,{\\mathcal{S}}} $ is the set of $ W_{k} $ bits that appear in the database set $ {\\mathcal{S}}\\subseteq\\mathcal{P}([N]) $ , where $ \\mathcal{P}(\\cdot) $ is the power set.", "$ H(W_{k,{\\mathcal{S}}})=|W_{k,{\\mathcal{S}}}|L $ , where $ 0\\leq|W_{k,{\\mathcal{S}}}|\\leq 1 $ .", "Under an uncoded placement, we have the following message size constraint, \\newline <equationgroup> <equation> $  1=\\frac{1}{KL}\\sum_{k=1}^{K}H(W_{k})=\\frac{1}{KL}\\sum_{k=1}^{K}% \\sum_{{\\mathcal{S}}\\subseteq[N]}H(W_{k,{\\mathcal{S}}})=\\sum_{{\\mathcal{S}}% \\subseteq[N]}\\alpha_{\\mathcal{S}} $ $  1=\\frac{1}{KL}\\sum_{k=1}^{K}H(W_{k})=\\frac{1}{KL}\\sum_{k=1}^{K}% \\sum_{{\\mathcal{S}}\\subseteq[N]}H(W_{k,{\\mathcal{S}}})=\\sum_{{\\mathcal{S}}% \\subseteq[N]}\\alpha_{\\mathcal{S}} $ </equation> </equationgroup> where $ \\alpha_{\\mathcal{S}}=\\frac{1}{K}\\sum_{k=1}^{K}|W_{k,{\\mathcal{S}}}| $ .", "In addition, we have the individual database storage constraints, \\newline <equationgroup> <equation> $  m_{n}\\geq\\frac{1}{KL}H(Z_{n})=\\sum_{\\begin{subarray}{c}{\\mathcal% {S}}\\subseteq[N],n\\in{\\mathcal{S}}\\end{subarray}}\\alpha_{\\mathcal{S}},\\quad n% \\in[N] $ $  m_{n}\\geq\\frac{1}{KL}H(Z_{n})=\\sum_{\\begin{subarray}{c}{\\mathcal% {S}}\\subseteq[N],n\\in{\\mathcal{S}}\\end{subarray}}\\alpha_{\\mathcal{S}},\\quad n% \\in[N] $ </equation> </equationgroup> \\newline In the retrieval phase, the user is interested in retrieving $ W_{\\theta} $ , $ \\theta\\in[K] $ privately.", "The user submits a query $ Q_{n}^{[\\theta]} $ to the $ n $ th database.", "Since the user has no information about the files, the messages and queries are statistically independent, i.e., \\newline <equationgroup> <equation> $  I(W_{1:K};Q_{1:N}^{[\\theta]})=0 $ $  I(W_{1:K};Q_{1:N}^{[\\theta]})=0 $ </equation> </equationgroup> The $ n $ th database responds with an answer string, which is a function of the received query and the stored content, i.e., \\newline <equationgroup> <equation> $  H(A_{n}^{[\\theta]}|Q_{n}^{[\\theta]},Z_{n})=0,\\quad n\\in[N] $ $  H(A_{n}^{[\\theta]}|Q_{n}^{[\\theta]},Z_{n})=0,\\quad n\\in[N] $ </equation> </equationgroup> \\newline To ensure privacy, the query submitted to the $ n $ th database when intended to retrieve $ W_{\\theta} $ should be statistically indistinguishable from the one when intended to retrieve $ W_{\\theta^{\\prime}} $ , i.e., \\newline <equationgroup> <equation> $ (Q_{n}^{[\\theta]},A_{n}^{[\\theta]},W_{1:K})\\sim(Q_{n}^{[\\theta^{% \\prime}]},A_{n}^{[\\theta^{\\prime}]},W_{1:K}),\\quad\\theta,\\theta^{\\prime}\\in[K] $ $ (Q_{n}^{[\\theta]},A_{n}^{[\\theta]},W_{1:K})\\sim(Q_{n}^{[\\theta^{% \\prime}]},A_{n}^{[\\theta^{\\prime}]},W_{1:K}),\\quad\\theta,\\theta^{\\prime}\\in[K] $ </equation> </equationgroup> where $ \\sim $ denotes statistical equivalence.", "\\newline The user needs to decode the desired message $ W_{\\theta} $ reliably from the received answer strings, consequently, \\newline <equationgroup> <equation> $  H(W_{\\theta}|Q_{1:N}^{[\\theta]},A_{1:N}^{[\\theta]})=o(L) $ $  H(W_{\\theta}|Q_{1:N}^{[\\theta]},A_{1:N}^{[\\theta]})=o(L) $ </equation> </equationgroup> where $ \\frac{o(L)}{L}\\rightarrow 0 $ as $ L\\rightarrow\\infty $ .", "\\newline An achievable PIR scheme satisfies constraints ( [@ref:LABEL:privacy] ) and ( [@ref:LABEL:reliability] ) for some file size $ L $ .", "The download cost $ D $ is the size of the total downloaded bits from all databases, \\newline <equationgroup> <equation> $  D=\\sum_{n=1}^{N}H(A_{n}^{[\\theta]}) $ $  D=\\sum_{n=1}^{N}H(A_{n}^{[\\theta]}) $ </equation> </equationgroup> \\newline For a given storage constraint vector $ \\bm{m} $ , we aim to jointly design the placement phase (i.e., $ Z_{n} $ , $ n\\in[N] $ ) and the retrieval scheme to minimize the normalized download cost $ D^{*}=\\frac{D}{L} $ in the retrieval phase.", "\\newline  </section>"], ["<section> <title> 3 Main Results </title>  Theorem [@ref:LABEL:Thm1] characterizes the optimal download cost under heterogeneous storage constraints in terms of a linear program.", "The main ingredients of the proof of Theorem [@ref:LABEL:Thm1] are introduced in Section [@ref:LABEL:capacity] for $ N=3 $ , and the complete proof is given in Section [@ref:LABEL:general-capacity] for general $ N $ .", "\\newline <theorem> Theorem 1 For PIR from databases with heterogeneous storage sizes $ \\bm{m}=(m_{1},\\cdots,m_{N}) $ , the optimal normalized download cost is the solution of the following linear program, \\newline <equationgroup> <equation> $ \\min_{\\alpha_{\\mathcal{S}}\\geq 0}\\sum_{\\ell=1}^{N}\\sum_{{\\mathcal% {S}}:|{\\mathcal{S}}|=\\ell}\\alpha_{\\mathcal{S}}\\left(1+\\frac{1}{\\ell}+\\cdots+% \\frac{1}{\\ell^{K-1}}\\right) $ $ \\min_{\\alpha_{\\mathcal{S}}\\geq 0} $ $ \\sum_{\\ell=1}^{N}\\sum_{{\\mathcal{S}}:|{\\mathcal{S}}|=\\ell}\\alpha_% {\\mathcal{S}}\\left(1+\\frac{1}{\\ell}+\\cdots+\\frac{1}{\\ell^{K-1}}\\right) $ </equation> <equation> $ \\text{s.t.}\\sum_{{\\mathcal{S}}:|{\\mathcal{S}}|\\geq 1}\\alpha_{% \\mathcal{S}}=1 $ $ \\sum_{{\\mathcal{S}}:|{\\mathcal{S}}|\\geq 1}\\alpha_{\\mathcal{S}}=1 $ </equation> <equation> $ \\sum_{{\\mathcal{S}}:n\\in{\\mathcal{S}}}\\alpha_{\\mathcal{S}}\\leq m_% {n},\\quad n\\in[N] $ $ \\sum_{{\\mathcal{S}}:n\\in{\\mathcal{S}}}\\alpha_{\\mathcal{S}}\\leq m_% {n},\\quad n\\in[N] $ </equation> </equationgroup> where $ {\\mathcal{S}}\\in\\mathcal{P}([N]) $ .", "\\newline </theorem> Theorem [@ref:LABEL:Thm2] shows the equivalence between the optimum download costs of the heterogeneous and homogeneous problems.", "The proof of Theorem [@ref:LABEL:Thm2] is given in Section [@ref:LABEL:equivalence] .", "\\newline <theorem> Theorem 2 The normalized download cost of the PIR problem with heterogeneous storage sizes $ \\bm{m}=(m_{1},\\cdots,m_{N}) $ is equal to the normalized download cost of the PIR problem with homogeneous storage sizes $ \\mu=\\frac{1}{N}\\sum_{n=1}^{N}m_{n} $ for all databases, i.e., $ D^{*}(\\bm{m})=D^{*}(\\bar{\\bm{m}}) $ , where $ \\bar{\\bm{m}} $ is such that $ \\bar{m}_{n}=\\mu $ , for $ n=1,\\cdots,N $ .", "\\newline </theorem> <theorem> Remark 1 Theorem [@ref:LABEL:Thm2] implies that the storage size asymmetry does not hurt the PIR capacity, so long as the placement phase is optimized.", "This is unlike, for instance, access asymmetry in the case of replicated databases [@bib:KarimAsymmetricPIR] .", "This is also unlike, as another instance, non-optimized content placement even for symmetric database sizes [@bib:Karim_nonreplicated] .", "\\newline </theorem> <theorem> Remark 2 Stronger than what is stated, i.e., the equivalence between heterogeneous and homogeneous storage cases, Theorem [@ref:LABEL:Thm2] in fact implies that the optimal download cost in ( [@ref:LABEL:general_achievable] ) depends only on the sum storage space $ \\sum_{n=1}^{N}m_{n} $ .", "Thus, any distribution of storage space within the given sum storage space yields the same PIR capacity.", "In particular, a uniform distribution (the corresponding homogeneous case) has the same PIR capacity.", "Hence, there is no loss in the PIR capacity due to heterogeneity of storage spaces of the databases.", "\\newline </theorem>  </section>"], ["<section> <title> 4 Representative Example: = N 3  </title>  We introduce the main ingredients of the achievability and converse proofs using the example of $ N=3 $ databases.", "Without loss of generality, we take $ K=3 $ in this section.", "\\newline <subsection> <title> 4.1 Converse Proof </title> We note that [@bib:StorageConstrainedPIR] can be applied to any storage constrained PIR problem with arbitrary storage $ Z_{1:N} $ .", "Hence, specializing to the case of $ N=3 $ (and $ K=3 $ ) with i.i.d.", "messages and uncoded content leads to [@bib:StorageConstrainedPIR] , \\newline <equationgroup> <equation> $  D\\geq L+\\frac{4}{27}\\sum_{k=1}^{3}H(W_{k})+\\frac{11}{108}\\sum_{i% =1}^{3}\\sum_{k=1}^{3}H(W_{k}|Z_{i})+\\frac{17}{54}\\sum_{i=1}^{3}\\sum_{k=1}^{3}H% (W_{k}|\\mathbf{Z}_{[3]\\setminus i})+o(L) $ $  D\\geq $ $  L+\\frac{4}{27}\\sum_{k=1}^{3}H(W_{k})+\\frac{11}{108}\\sum_{i=1}^{3% }\\sum_{k=1}^{3}H(W_{k}|Z_{i})+\\frac{17}{54}\\sum_{i=1}^{3}\\sum_{k=1}^{3}H(W_{k}% |\\mathbf{Z}_{[3]\\setminus i})+o(L) $ </equation> </equationgroup> Using the uncoded storage assumption in ( [@ref:LABEL:uncoded] ), we can further lower bound ( [@ref:LABEL:lb0] ) as, \\newline <equationgroup> <equation> $  D\\geq L+\\frac{4}{27}\\sum_{\\begin{subarray}{c}{\\mathcal{S}}% \\subseteq[1:3]\\\\ |{\\mathcal{S}}|\\geq 1\\end{subarray}}\\sum_{k=1}^{3}|W_{k,{\\mathcal{S}}}|L+\\frac% {11}{108}\\sum_{i=1}^{3}\\sum_{\\begin{subarray}{c}{\\mathcal{S}}\\subseteq[1:3]% \\setminus i\\\\ |{\\mathcal{S}}|\\geq 1\\end{subarray}}\\sum_{k=1}^{3}|W_{k,{\\mathcal{S}}}|L $ $  D\\geq $ $  L+\\frac{4}{27}\\sum_{\\begin{subarray}{c}{\\mathcal{S}}\\subseteq[1:% 3]\\\\ |{\\mathcal{S}}|\\geq 1\\end{subarray}}\\sum_{k=1}^{3}|W_{k,{\\mathcal{S}}}|L+\\frac% {11}{108}\\sum_{i=1}^{3}\\sum_{\\begin{subarray}{c}{\\mathcal{S}}\\subseteq[1:3]% \\setminus i\\\\ |{\\mathcal{S}}|\\geq 1\\end{subarray}}\\sum_{k=1}^{3}|W_{k,{\\mathcal{S}}}|L $ </equation> <equation> $ +\\frac{17}{54}\\sum_{i=1}^{3}\\sum_{k=1}^{3}|W_{k,\\{i\\}}|L+o(L) $ $ +\\frac{17}{54}\\sum_{i=1}^{3}\\sum_{k=1}^{3}|W_{k,\\{i\\}}|L+o(L) $ </equation> <equation> $ =L+\\frac{2}{3}\\sum_{\\begin{subarray}{c}{\\mathcal{S}}\\subseteq[1:3% ]\\\\ |{\\mathcal{S}}|=1\\end{subarray}}\\sum_{k=1}^{3}|W_{k,{\\mathcal{S}}}|L+\\frac{1}{% 4}\\sum_{\\begin{subarray}{c}{\\mathcal{S}}\\subseteq[1:3]\\\\ |{\\mathcal{S}}|=2\\end{subarray}}\\sum_{k=1}^{3}|W_{k,{\\mathcal{S}}}|L $ $ = $ $  L+\\frac{2}{3}\\sum_{\\begin{subarray}{c}{\\mathcal{S}}\\subseteq[1:3% ]\\\\ |{\\mathcal{S}}|=1\\end{subarray}}\\sum_{k=1}^{3}|W_{k,{\\mathcal{S}}}|L+\\frac{1}{% 4}\\sum_{\\begin{subarray}{c}{\\mathcal{S}}\\subseteq[1:3]\\\\ |{\\mathcal{S}}|=2\\end{subarray}}\\sum_{k=1}^{3}|W_{k,{\\mathcal{S}}}|L $ </equation> <equation> $ +\\frac{4}{27}\\sum_{\\begin{subarray}{c}{\\mathcal{S}}\\subseteq[1:3]% \\\\ |{\\mathcal{S}}|=3\\end{subarray}}\\sum_{k=1}^{3}|W_{k,{\\mathcal{S}}}|L+o(L) $ $ +\\frac{4}{27}\\sum_{\\begin{subarray}{c}{\\mathcal{S}}\\subseteq[1:3]% \\\\ |{\\mathcal{S}}|=3\\end{subarray}}\\sum_{k=1}^{3}|W_{k,{\\mathcal{S}}}|L+o(L) $ </equation> </equationgroup> Normalizing with $ L $ , taking the limit $ L\\rightarrow\\infty $ , and using the definition $ \\alpha_{\\mathcal{S}}=\\frac{1}{K}\\sum_{k=1}^{K}|W_{k,{\\mathcal{S}}}| $ lead to the following lower bound on the normalized download cost $ D^{*} $ , \\newline <equationgroup> <equation> $  D^{*}\\geq 1+2\\sum_{\\begin{subarray}{c}{\\mathcal{S}}\\subseteq[3]% \\\\ |{\\mathcal{S}}|=1\\end{subarray}}\\alpha_{\\mathcal{S}}+\\frac{3}{4}\\sum_{\\begin{% subarray}{c}{\\mathcal{S}}\\subseteq[3]\\\\ |{\\mathcal{S}}|=2\\end{subarray}}\\alpha_{\\mathcal{S}}+\\frac{4}{9}\\sum_{\\begin{% subarray}{c}{\\mathcal{S}}\\subseteq[3]\\\\ |{\\mathcal{S}}|=3\\end{subarray}}\\alpha_{\\mathcal{S}} $ $  D^{*}\\geq $ $  1+2\\sum_{\\begin{subarray}{c}{\\mathcal{S}}\\subseteq[3]\\\\ |{\\mathcal{S}}|=1\\end{subarray}}\\alpha_{\\mathcal{S}}+\\frac{3}{4}\\sum_{\\begin{% subarray}{c}{\\mathcal{S}}\\subseteq[3]\\\\ |{\\mathcal{S}}|=2\\end{subarray}}\\alpha_{\\mathcal{S}}+\\frac{4}{9}\\sum_{\\begin{% subarray}{c}{\\mathcal{S}}\\subseteq[3]\\\\ |{\\mathcal{S}}|=3\\end{subarray}}\\alpha_{\\mathcal{S}} $ </equation> <equation> $ =3\\sum_{\\begin{subarray}{c}{\\mathcal{S}}\\subseteq[3]\\\\ |{\\mathcal{S}}|=1\\end{subarray}}\\alpha_{\\mathcal{S}}+\\frac{7}{4}\\sum_{\\begin{% subarray}{c}{\\mathcal{S}}\\subseteq[3]\\\\ |{\\mathcal{S}}|=2\\end{subarray}}\\alpha_{\\mathcal{S}}+\\frac{13}{9}\\sum_{\\begin{% subarray}{c}{\\mathcal{S}}\\subseteq[3]\\\\ |{\\mathcal{S}}|=3\\end{subarray}}\\alpha_{\\mathcal{S}} $ $ = $ $  3\\sum_{\\begin{subarray}{c}{\\mathcal{S}}\\subseteq[3]\\\\ |{\\mathcal{S}}|=1\\end{subarray}}\\alpha_{\\mathcal{S}}+\\frac{7}{4}\\sum_{\\begin{% subarray}{c}{\\mathcal{S}}\\subseteq[3]\\\\ |{\\mathcal{S}}|=2\\end{subarray}}\\alpha_{\\mathcal{S}}+\\frac{13}{9}\\sum_{\\begin{% subarray}{c}{\\mathcal{S}}\\subseteq[3]\\\\ |{\\mathcal{S}}|=3\\end{subarray}}\\alpha_{\\mathcal{S}} $ </equation> </equationgroup> where ( [@ref:LABEL:lb1] ) follows from the message size constraint ( [@ref:LABEL:msg_size] ).", "\\newline We further lower bound ( [@ref:LABEL:lb1] ) by minimizing the right hand side with respect to $ \\{\\alpha_{\\mathcal{S}}\\}_{{\\mathcal{S}}\\subseteq[3]} $ under storage constraints.", "Thus, the solution of the following linear program serves as a lower bound (converse) for the normalized download cost, \\newline <equationgroup> <equation> $ \\min_{\\alpha_{\\mathcal{S}}\\geq 0}3(\\alpha_{1}+\\alpha_{2}+\\alpha_{% 3})+\\frac{7}{4}(\\alpha_{12}+\\alpha_{13}+\\alpha_{23})+\\frac{13}{9}\\alpha_{123} $ $ \\min_{\\alpha_{\\mathcal{S}}\\geq", "0} $ $  3(\\alpha_{1}+\\alpha_{2}+\\alpha_{3})+\\frac{7}{4}(\\alpha_{12}+% \\alpha_{13}+\\alpha_{23})+\\frac{13}{9}\\alpha_{123} $ </equation> <equation> $ \\text{s.t.}\\alpha_{1}+\\alpha_{2}+\\alpha_{3}+\\alpha_{12}+\\alpha_{1% 3}+\\alpha_{23}+\\alpha_{123}=1 $ $ \\alpha_{1}+\\alpha_{2}+\\alpha_{3}+\\alpha_{12}+\\alpha_{13}+\\alpha_{% 23}+\\alpha_{123}=1 $ </equation> <equation> $ \\alpha_{1}+\\alpha_{12}+\\alpha_{13}+\\alpha_{123}\\leq m_{1} $ $ \\alpha_{1}+\\alpha_{12}+\\alpha_{13}+\\alpha_{123}\\leq m_{1} $ </equation> <equation> $ \\alpha_{2}+\\alpha_{12}+\\alpha_{23}+\\alpha_{123}\\leq m_{2} $ $ \\alpha_{2}+\\alpha_{12}+\\alpha_{23}+\\alpha_{123}\\leq m_{2} $ </equation> <equation> $ \\alpha_{3}+\\alpha_{13}+\\alpha_{23}+\\alpha_{123}\\leq m_{3} $ $ \\alpha_{3}+\\alpha_{13}+\\alpha_{23}+\\alpha_{123}\\leq m_{3} $ </equation> </equationgroup> where variables $ \\{\\alpha_{\\mathcal{S}}\\}_{|{\\mathcal{S}}|=1} $ are $ \\{\\alpha_{1},\\alpha_{2},\\alpha_{3}\\} $ , which represent the content stored in databases 1, 2 and 3 exclusively; variables $ \\{\\alpha_{\\mathcal{S}}\\}_{|{\\mathcal{S}}|=2} $ are $ \\{\\alpha_{12},\\alpha_{13},\\alpha_{23}\\} $ , which represent the content stored in databases 1 and 2, 1 and 3, and 2 and 3, respectively; and variable $ \\{\\alpha_{\\mathcal{S}}\\}_{|{\\mathcal{S}}|=3} $ is $ \\{\\alpha_{123}\\} $ , which represents the content stored in all three databases simultaneously.", "\\newline Next, we show that the lower bound expressed as a linear program in ( [@ref:LABEL:final_lb] ) can be achieved.", "\\newline </subsection> <subsection> <title> 4.2 Achievability Proof </title> In the placement phase, let $ |W_{k,{\\mathcal{S}}}|=\\alpha_{\\mathcal{S}} $ for all $ k\\in[K] $ .", "Assign the partition $ W_{k,{\\mathcal{S}}} $ to the set $ {\\mathcal{S}} $ of the databases for all $ k\\in[K] $ .", "To retrieve $ W_{\\theta} $ privately, $ \\theta\\in[K] $ , the user applies the Sun-Jafar scheme [@bib:JafarPIR] over the partitions of the files.", "\\newline The partitions $ W_{k,1} $ , $ W_{k,2} $ , $ W_{k,3} $ are placed in a single database each.", "Thus, we apply [@bib:JafarPIR] with $ N=1 $ , and download \\newline <equationgroup> <equation> $  K(|W_{k,1}|+|W_{k,2}|+|W_{k,3}|)L=3(\\alpha_{1}+\\alpha_{2}+\\alpha% _{3})L $ $  K(|W_{k,1}|+|W_{k,2}|+|W_{k,3}|)L=3(\\alpha_{1}+\\alpha_{2}+\\alpha% _{3})L $ </equation> </equationgroup> The partitions $ W_{k,12} $ , $ W_{k,13} $ , $ W_{k,23} $ are placed in two databases each.", "Thus, we apply [@bib:JafarPIR] with $ N=2 $ , and download \\newline <equationgroup> <equation> $ \\left(1+\\frac{1}{2}+\\frac{1}{2^{2}}\\right)(|W_{k,12}|+|W_{k,13}|+% |W_{k,23}|)L=\\frac{7}{4}(\\alpha_{12}+\\alpha_{13}+\\alpha_{23})L $ $ \\left(1+\\frac{1}{2}+\\frac{1}{2^{2}}\\right)(|W_{k,12}| $ $ +|W_{k,13}|+|W_{k,23}|)L=\\frac{7}{4}(\\alpha_{12}+\\alpha_{13}+% \\alpha_{23})L $ </equation> </equationgroup> Finally, the partition $ W_{k,123} $ is placed in all three databases.", "Thus, we apply [@bib:JafarPIR] with $ N=3 $ , and download \\newline <equationgroup> <equation> $ \\left(1+\\frac{1}{3}+\\frac{1}{3^{2}}\\right)|W_{k,123}|L=\\frac{13}{% 9}\\alpha_{123}L $ $ \\left(1+\\frac{1}{3}+\\frac{1}{3^{2}}\\right)|W_{k,123}|L=\\frac{13}{% 9}\\alpha_{123}L $ </equation> </equationgroup> \\newline Concatenating the downloads, file $ W_{\\theta} $ is reliably decodable.", "Hence, by summing up the download costs in ( [@ref:LABEL:ach-singles] ), ( [@ref:LABEL:ach-doubles] ) and ( [@ref:LABEL:ach-triples] ), we have the following normalized download cost, \\newline <equationgroup> <equation> $ \\frac{D}{L}=3(\\alpha_{1}+\\alpha_{2}+\\alpha_{3})+\\frac{7}{4}(% \\alpha_{12}+\\alpha_{13}+\\alpha_{23})+\\frac{13}{9}\\alpha_{123} $ $ \\frac{D}{L}=3(\\alpha_{1}+\\alpha_{2}+\\alpha_{3})+\\frac{7}{4}(% \\alpha_{12}+\\alpha_{13}+\\alpha_{23})+\\frac{13}{9}\\alpha_{123} $ </equation> </equationgroup> which matches the lower bound in ( [@ref:LABEL:final_lb] ) and is subject to the same constraints.", "Hence, the solution to the linear program in ( [@ref:LABEL:final_lb] ) is achievable, and gives the {exact PIR capacity} .", "\\newline </subsection> <subsection> <title> 4.3 Explicit Storage Assignment </title> In this section, we solve the linear program in ( [@ref:LABEL:final_lb] ) to find the optimal storage assignment explicitly for $ N=3 $ .", "To that end, we denote $ \\beta_{\\ell}=\\sum_{{\\mathcal{S}}:|{\\mathcal{S}}|=\\ell}\\alpha_{\\mathcal{S}} $ , i.e., \\newline <equationgroup> <equation> $ \\beta_{1}=\\alpha_{1}+\\alpha_{2}+\\alpha_{3} $ $ \\beta_{1} $ $ =\\alpha_{1}+\\alpha_{2}+\\alpha_{3} $ </equation> <equation> $ \\beta_{2}=\\alpha_{12}+\\alpha_{13}+\\alpha_{23} $ $ \\beta_{2} $ $ =\\alpha_{12}+\\alpha_{13}+\\alpha_{23} $ </equation> <equation> $ \\beta_{3}=\\alpha_{123} $ $ \\beta_{3} $ $ =\\alpha_{123} $ </equation> </equationgroup> \\newline We first construct a {relaxed} optimization problem by summing up the three individual storage constraints in ( [@ref:LABEL:final_lb] ) into a single constraint.", "The relaxed problem is, \\newline <equationgroup> <equation> $ \\min_{\\beta_{i}\\geq 0}3\\beta_{1}+\\frac{7}{4}\\beta_{2}+\\frac{13}{9% }\\beta_{3} $ $ \\min_{\\beta_{i}\\geq 0} $ $  3\\beta_{1}+\\frac{7}{4}\\beta_{2}+\\frac{13}{9}\\beta_{3} $ </equation> <equation> $ \\text{s.t.}\\beta_{1}+\\beta_{2}+\\beta_{3}=1 $ $ \\beta_{1}+\\beta_{2}+\\beta_{3}=1 $ </equation> <equation> $ \\beta_{1}+2\\beta_{2}+3\\beta_{3}\\leq m_{s} $ $ \\beta_{1}+2\\beta_{2}+3\\beta_{3}\\leq m_{s} $ </equation> </equationgroup> where we define the sum storage space $ m_{s}=m_{1}+m_{2}+m_{3} $ .", "Plugging $ \\beta_{1}=1-\\beta_{2}-\\beta_{3} $ , \\newline <equationgroup> <equation> $ \\min_{\\beta_{2},\\beta_{3}\\geq 0}3-\\frac{5}{4}\\beta_{2}-\\frac{14}{% 9}\\beta_{3} $ $ \\min_{\\beta_{2},\\beta_{3}\\geq 0} $ $  3-\\frac{5}{4}\\beta_{2}-\\frac{14}{9}\\beta_{3} $ </equation> <equation> $ \\text{s.t.}\\beta_{2}+\\beta_{3}\\leq 1 $ $ \\beta_{2}+\\beta_{3}\\leq 1 $ </equation> <equation> $ \\beta_{2}+2\\beta_{3}\\leq m_{s}-1 $ $ \\beta_{2}+2\\beta_{3}\\leq m_{s}-1 $ </equation> </equationgroup> \\newline Since ( [@ref:LABEL:relaxed] ) is a linear program, the solution lies at the boundary of the feasible set.", "We have three cases depending on the sum storage space $ m_{s} $ .", "\\newline <paragraph> <title> Regime 1: </title> When $ m_{s}<1 $ : In this case, the second constraint in ( [@ref:LABEL:relaxed] ) requires $ \\beta_{2}+2\\beta_{3}<0 $ , while we must have $ \\beta_{2},\\beta_{3}\\geq 0 $ .", "Hence, there is no feasible solution for the relaxed problem and thus the original problem ( [@ref:LABEL:final_lb] ) is infeasible as well.", "\\newline </paragraph> <paragraph> <title> Regime 2: </title> When $ 1\\leq m_{s}\\leq 2 $ : In this case, the constraint $ \\beta_{2}+\\beta_{3}\\leq 1 $ is not binding.", "Hence, the solution satisfies the second constraint with equality, $ \\beta_{2}+2\\beta_{3}=m_{s}-1 $ , which is non-negative in this regime.", "Thus, ( [@ref:LABEL:relaxed] ) can be written in an unconstrained manner as, \\newline <equationgroup> <equation> $ \\min_{\\beta_{3}\\geq 0}\\>3-\\frac{5}{4}(m_{s}-1-2\\beta_{3})+\\frac{1% 4}{9}\\beta_{3}=\\min_{\\beta_{3}\\geq 0}\\>\\frac{17}{4}-\\frac{5}{4}m_{s}+\\frac{17}% {18}\\beta_{3} $ $ \\min_{\\beta_{3}\\geq 0}\\>3-\\frac{5}{4}(m_{s}-1-2\\beta_{3})+\\frac{1% 4}{9}\\beta_{3}=\\min_{\\beta_{3}\\geq 0}\\>\\frac{17}{4}-\\frac{5}{4}m_{s}+\\frac{17}% {18}\\beta_{3} $ </equation> </equationgroup> The optimal solution for ( [@ref:LABEL:unconstrained] ) is $ \\beta_{3}^{*}=0 $ and therefore $ \\beta_{2}^{*}=m_{s}-1 $ .", "From the equality constraint $ \\beta_{1}+\\beta_{2}+\\beta_{3}=1 $ , we have $ \\beta_{1}^{*}=2-m_{s} $ .", "Next, we map the solution of the relaxed problem in ( [@ref:LABEL:relaxed] ) to a feasible solution in the original problem in ( [@ref:LABEL:final_lb] ).", "From ( [@ref:LABEL:beta3-def] ), $ a_{123}^{*}=\\beta_{3}^{*}=0 $ .", "Thus, at the boundary of the inequality set of ( [@ref:LABEL:final_lb] ), we have, \\newline <equationgroup> <equation> $ \\alpha_{1}+\\beta_{2}-\\alpha_{23}=m_{1}\\Rightarrow% \\alpha_{1}+m_{s}-1-\\alpha_{23}=m_{1}\\Rightarrow\\alpha_{1}-\\alpha_{% 23}=1-(m_{2}+m_{3}) $ $ \\alpha_{1}+\\beta_{2}-\\alpha_{23}=m_{1}\\Rightarrow% \\alpha_{1}+m_{s}-1-\\alpha_{23}=m_{1}\\Rightarrow\\alpha_{1}-\\alpha_{% 23}=1-(m_{2}+m_{3}) $ </equation> <equation> $ \\alpha_{2}+\\beta_{2}-\\alpha_{13}=m_{2}\\Rightarrow% \\alpha_{2}+m_{s}-1-\\alpha_{13}=m_{2}\\Rightarrow\\alpha_{2}-\\alpha_{% 13}=1-(m_{1}+m_{3}) $ $ \\alpha_{2}+\\beta_{2}-\\alpha_{13}=m_{2}\\Rightarrow% \\alpha_{2}+m_{s}-1-\\alpha_{13}=m_{2}\\Rightarrow\\alpha_{2}-\\alpha_{% 13}=1-(m_{1}+m_{3}) $ </equation> <equation> $ \\alpha_{3}+\\beta_{2}-\\alpha_{12}=m_{3}\\Rightarrow% \\alpha_{3}+m_{s}-1-\\alpha_{12}=m_{3}\\Rightarrow\\alpha_{3}-\\alpha_{% 12}=1-(m_{1}+m_{2}) $ $ \\alpha_{3}+\\beta_{2}-\\alpha_{12}=m_{3}\\Rightarrow% \\alpha_{3}+m_{s}-1-\\alpha_{12}=m_{3}\\Rightarrow\\alpha_{3}-\\alpha_{% 12}=1-(m_{1}+m_{2}) $ </equation> </equationgroup> Depending on the sign of $ 1-(m_{j}+m_{k}) $ , where $ j,k\\in\\{1,2,3\\} $ , we have different content assignments.", "The common structure of ( [@ref:LABEL:assign1] )-( [@ref:LABEL:assign3] ) is $ \\alpha_{i}-\\alpha_{jk}=1-(m_{j}+m_{k}) $ .", "We assign $ \\alpha_{i}=\\alpha_{jk}+1-(m_{j}+m_{k}) $ if $ m_{j}+m_{k}\\leq 1 $ and $ \\alpha_{jk}=\\alpha_{i}-1+(m_{j}+m_{k}) $ otherwise.", "This ensures that $ \\alpha_{\\mathcal{S}}\\geq 0 $ for all $ {\\mathcal{S}}\\subseteq[1:3] $ .", "Using these assignments, we have sub-cases depending on the sign of $ 1-(m_{j}+m_{k}) $ ."]], "target": "We summarize explicit content assignment for these cases in Table , where we take $ m_{1}\\geq m_{2}\\geq m_{3} $ without loss of generality, to reduce the number of cases to enumerate. With these solutions, the optimal normalized download cost in this regime is,"}, {"tabular": ["  Pipeline/Task  &  $ T $  &  $ D_{e2e} $  &  Tasks  &  $ C $  &  $ \\pi $  &  $ p $ ", " $ \\tau_{1} $  &  20  &  20  &  -  &  free  &  9  &  1 ", " $ P^{1} $  &  150  &  150  &  $ \\tau_{1}^{1} $  &  free  &  3  &  1 ", " $ \\tau_{2}^{1} $  &  10  &  9  &  2 ", " $ \\tau_{3}^{1} $  &  8  &  5  &  3 ", " $ \\tau_{4}^{1} $  &  15  &  2  &  2 ", " $ \\tau_{5}^{1} $  &  25  &  2  &  1 ", " $ \\tau_{2} $  &  30  &  30  &  -  &  6  &  9  &  3 ", " $ \\tau_{3} $  &  200  &  200  &  -  &  40  &  2  &  3  "], "ref_sec": [["<section> <title> I Introduction and motivation </title>  Designing and analysing a distributed real-time system is a very challenging task.", "The main source of complexity arises from the large number of parameters to consider: task priority, computation times and deadlines, synchronisation, precedence and communication constraints, etc.", "Finding the \u201coptimal\u201d values for the parameters is not easy, and often the robustness of the solution strongly depends on the exact values: a small change in one parameter may completely change the behaviour of the system and even compromise the correctness.", "For these reasons, designers are looking for analysis methodologies that enable incremental design and exploration of the space of parameters.", "\\newline Task computation times are particularly important parameters.", "In modern processor architectures, it is very difficult to precisely compute worst-case computation times of tasks, and estimations derived by previous executions are often used in the analysis.", "However, estimations may turn out to be optimistic, hence an error in the estimation of a worst-case execution time may compromise the schedulability of the system.", "\\newline The goal of this research is to characterise the space of the parameters of a real-time system for which the system is schedulable, i.e. all tasks meet their deadlines.", "Parametric analyses for real-time systems have been proposed in the past, especially on single processors [@bib:Bini-thesis,Bin07b,Palopoli-rtss08,AFKS12] .", "\\newline In this paper, we investigate the problem of doing parametric analysis of real-time distributed systems scheduled by fixed priority.", "We consider an application modelled by a set of pipelines of tasks (also called {transactions} in [@bib:Tindell:1994:HSA:195612.195618] ), where each pipeline is a sequence of tasks that can be periodic or sporadic, and all tasks in a pipeline must complete before an end-to-end deadline.", "We consider that all nodes in the distributed system are connected by one or more CAN bus [@bib:Davis07] .", "\\newline We propose: \\newline <list> \\ a new method for doing parametric analysis of distributed real-time systems scheduled by fixed priority scheduling.", "The method extends the sensitivity analysis proposed by Bini et al. [@bib:Bin07b,Bini-thesis] by considering distributed systems and non-preemptive scheduling.", "\\newline \\ \\ a model of a distributed real-time system using parametric timed Automata, and a model checking methodology using the Inverse Method [@bib:AS13,AFKS12,FLMS-time12] ; \\newline \\ \\ comparison of these two approaches with classical holistic analysis using the MAST tool [@bib:MAST-2001,MAST-web-page] , in terms of complexity and precision of the analysis.", "\\newline \\ </list> \\newline  </section>"], ["<section> <title> II Related Work </title>  There has been a lot of research work on parametric schedulability analysis, especially on single processor systems.", "Bini and Buttazzo [@bib:Bin04b] proposed an analysis of fixed priority single processor systems based on Lehoczky test [@bib:Leh89] .", "Later, Bini, Di Natale and Buttazzo [@bib:Bin07b] proposed a more complex analysis, which considers also the task periods as parameters.", "Such results are summarised and extended in Bini\u2019s PhD thesis [@bib:Bini-thesis] .", "\\newline Parameter sensitivity can be also be carried out by repeatedly applying classical schedulability tests, like the holistic analysis [@bib:Palencia1998,Tindell:1994:HSA:195612.195618] .", "One example of this approach is used in the MAST tool [@bib:MAST-2001,MAST-web-page] , in which it is possible to compute the {slack} (i.e. the percentage of variation) with respect to one parameter for single processor and for distributed systems by applying binary search in that parameter space [@bib:Palencia1998] .", "\\newline A similar approach is followed by the SymTA/S tool [@bib:Symtas-05] , which is based on the {event-stream} model [@bib:Richter:2002:EMI:882452.874327] .", "Another interesting approach is the Modular Performance Analysis (MPA) [@bib:Wandeler:2006:SAE:1177177.1177184] which is based on Real-Time Calculus [@bib:thiele2000real] .", "In both cases, the analysis is compositional, therefore less complex than the holistic analysis; nevertheless, these approaches are not fully parametric, in the sense that it is necessary to repeat the analysis for every combination of parameters values in order to obtain the schedulability region.", "\\newline Model checking on {Parametric Timed Automata} (PTA) can be used for parametric schedulability analysis, as proposed by Cimatti, Palopoli and Ramadian [@bib:Palopoli-rtss08] .", "In particular, thanks to generality of the PTA modelling language, it is possible to model a larger class of constraints, and perform parametric analysis on many different variables, for example task offsets.", "Their approach has been recently extended to distributed real-time systems [@bib:Pal13] .", "\\newline Also based on PTA is the approach proposed by Andr\u00e9 et al. [@bib:AFKS12] .", "Their work is based on the Inverse Method [@bib:AS13] and it is very general because it permits to perform analysis on any system parameter.", "However, this generality can be paid in terms of complexity.", "\\newline In this paper, we first propose an extensions of the methods in [@bib:Bini-thesis] for distributed real-time systems.", "We also propose a model of a distributed real-time systems in PTA, and compare the two approaches against classical holistic analysis.", "\\newline  </section>"], ["<section> <title> III System model </title>  We consider distributed real-time systems consisting of several computational nodes, each one hosting one single processor, connected by one or more shared networks.", "We consider preemptive fixed priority scheduling for processors, as this is the most popular scheduling algorithm used in industry today, and non-preemptive fixed priority scheduling for networks.", "In particular, the CAN bus protocol is a very popular network protocol that can be analysed using non-preemptive fixed priority scheduling analysis [@bib:Davis07] .", "We will consider extensions to our methodology to other scheduling algorithms and protocols in future works.", "\\newline For the sake of simplicity and uniformity of notation, in this paper we use the same terminology to denote processors and communication networks, and tasks and messages.", "Therefore, without loss of generality, from now on we will use the term {task} to denote both tasks and messages, and the term {processor} to denote both processors and networks \\newline A distributed real-time system consists of a set of task pipelines $ \\{\\mathcal{P}^{1},\\ldots,\\mathcal{P}^{n}\\} $ to be executed on a set of $ m $ processors $ \\{p_{1},p_{2},\\ldots,p_{m}\\} $ .", "In order to simplify the notation, in the following we sometime drop the pipeline index when there is no possibility of misinterpretation.", "\\newline A {pipeline} is a chain of tasks $ \\mathcal{P}=\\{\\tau_{1},\\ldots,\\tau_{n}\\} $ , and each task is allocated on one possibly different processor.", "A pipeline is assigned two fixed parameters: $ T $ is the pipeline period, and $ D_{e2e} $ is the end-to-end deadline.", "This means that the first task of the pipeline is activated every $ T $ units of time, and every activation is an {instance} (or {job} ) of the task.", "We denote the $ k $ -th instance of task $ \\tau_{i} $ as $ \\tau_{i,k} $ .", "Every successive task in the pipeline is activated when the corresponding instance of the previous task has completed; finally, the last task must complete before $ D_{e2e} $ units of time from the activation of the first task.", "Therefore, tasks must be executed in a sequence: job $ \\tau_{i,k} $ cannot start executing before job $ \\tau_{i-1,k} $ has completed.", "\\newline A task can be a piece of code to be executed on a CPU, or a message to be sent on a network.", "More precisely, a real-time periodic task $ \\tau_{i}=(C_{i},T_{i},D_{i},\\overline{D}_{i},\\pi_{i},J_{i}) $ is modelled by the following fixed parameters: \\newline <list> \\ $ T_{i} $ is the task period.", "All tasks in the same pipeline have period equal to the pipeline period $ T $ ; \\newline \\ \\ $ \\pi_{i} $ is the task priority; the higher $ \\pi_{i} $ , the larger the priority; \\newline \\ \\ $ \\overline{D}_{i} $ is the task {fixed deadline} ; all jobs of $ \\tau_{i} $ must complete within $ \\overline{D}_{i} $ from their activation.", "\\newline \\ </list> Also, a task has the following free parameters: \\newline <list> \\ $ C_{i} $ is the worst-case computation time (or worst-case transmission time, in case it models a message).", "In this paper we want to characterise the schedulability of the system in the space of the computation times, so $ C_{i} $ is a free parameter.", "\\newline \\ \\ $ D_{i} $ is a variable denoting an upper bound on the task worst-case completion time.", "We will call this variable {actual task deadline} or simply {task deadline} .", "Of course, we require that $ D_{i}\\leq\\overline{D}_{i} $ .", "Remember that fixed priority does not use the task deadline for scheduling, but just for schedulability analysis.", "As we will see later, we will use this variable for imposing precedence constraints on pipelines.", "We say that a task has constrained deadline when $ D_{i}\\leq T_{i} $ , and unconstrained deadline when $ D_{i}>T_{i} $ .", "\\newline \\ \\ $ J_{i} $ is the task start time jitter (see below).", "\\newline \\ </list> \\newline As anticipated, a task consists of an infinite number of jobs $ \\tau_{i,k},k=1,\\ldots $ .", "Each job is activated at time $ a_{i,k}=kT_{i} $ , can start executing (or can be sent on the network) no earlier than time $ s_{i,k} $ , with $ a_{i,k}\\leq s_{i,k}\\leq a_{i,k}+J_{i} $ , executes (or is transmitted over the network) for $ c_{i,k}\\leq C_{i} $ units of time, and completes (or is received) at $ f_{i,k} $ .", "For the task to be schedulable, it must be $ \\forall k,f_{i,k}\\leq d_{i,k}=a_{i,k}+D_{i} $ .", "A sporadic task has the same parameters as a periodic task, but parameter $ T_{i} $ denotes the {minimum inter-arrival time} between two consecutive instances.", "We also define the $ i $ -th level hyperperiod as $ H_{i}=\\mathsf{lcm}(T_{1},\\ldots,T_{i}) $ .", "\\newline In this paper, we use the following convention.", "All tasks belonging to a pipeline $ \\mathcal{P}=\\{\\tau_{1},\\ldots,\\tau_{n}\\} $ are activated at the same time $ a_{i,k}=a_{1,k} $ .", "However, only the first task can start executing immediately: $ s_{1,k}=a_{1,k} $ .", "The following tasks can only start executing when the previous task has completed: $ \\forall i=2,\\ldots,n\\;\\;s_{i,k}=f_{i-1,k} $ .", "The task jitter is the worst case start time of a task: $ J_{i}\\geq\\max_{k}\\{s_{i,k}-a_{i,k}\\} $ .", "\\newline A scheduling algorithm is {fully preemptive} if the execution of a lower priority job can be suspended at any instant by the arrival of a higher priority job, which is then executed in its place.", "A scheduling algorithm is {non-preemptive} if a lower priority job can complete its execution regardless of the arrival of higher priority jobs.", "In this paper, we consider preemptive fixed priority scheduling for CPUs, and non-preemptive fixed priority scheduling for networks.", "\\newline  </section>"], ["<section> <title> IV Analytic method </title>  In this section we describe a novel method for parametric analysis of distributed system.", "The method is based on the sensitivity analysis by Bini et al. [@bib:Bin07b,Bini-thesis] , and extends it to include jitter and deadline parameters.", "\\newline <subsection> <title> IV-A Single processor preemptive fixed priority scheduling </title> There are many ways to test the schedulability of a set of real-time periodic tasks scheduled by fixed priority on a single processor.", "In the following, we will use the test proposed by Seto et al. [@bib:seto98] because it is amenable to parametric analysis of computation times, jitters and deadlines.", "\\newline With respect to the original formulation, we now consider tasks with constrained deadlines (i.e. $ D_{i} $ can be less than or equal to $ T_{i} $ ).", "\\newline <theorem> Theorem 1 .", "Consider a system of sporadic tasks $ \\{\\tau_{1},\\ldots,\\tau_{n}\\} $ with constrained deadlines and zero jitter, executed on a single processor by a fixed priority scheduler.", "Assume all tasks are ordered in decreasing order of priorities, with $ \\tau_{1} $ being the highest priority task.", "\\newline Task $ \\tau_{i} $ is schedulable if and only if: \\newline <equation> $ \\exists\\mathbf{n}\\in\\mathbb{N}^{i-1}\\left\\{\\begin{array}[]{ll}C_{i}+\\sum_{j=1}% ^{i-1}n_{j}C_{j}\\leq n_{k}T_{k}&\\forall k=1,\\ldots,i-1\\\\ C_{i}+\\sum_{j=1}^{i-1}n_{j}C_{j}\\leq D_{i}\\end{array}\\right.", "$ </equation> where $ \\mathbb{N}^{i-1} $ is the set of all possible vectors of $ (i-1) $ positive integers.", "\\newline </theorem> <proof> Proof.", "See [@bib:Bini-thesis] and [@bib:seto98] .", "\u220e \\newline </proof> Notice that, with respect to the original formulation, we have separated the case of $ k=i $ from the rest of the inequalities.", "\\newline The theorem allows us to only consider sets of linear inequalities, because the non-linearity has been encoded in the variables $ n_{j} $ .", "The resulting system is a set of inequalities in disjunctive and conjunctive form.", "Geometrically, this corresponds to a non-convex polyhedron in the space of the variables $ C_{i},D_{i} $ .", "\\newline How many vectors $ \\mathbf{n} $ do we have to consider? If the deadline $ D_{i} $ is known, the answer is to simply consider all vectors corresponding to the minimal set of scheduling points by Bini and Buttazzo [@bib:Bini-2004-ID20] .", "If $ D_{i} $ is unknown, we have to consider many more vectors: more specifically, we must select all multiples of the period of any task $ \\tau_{j} $ with priority higher than $ \\tau_{i} $ , until the maximum possible value of the deadline.", "All vectors until time $ t $ can be computed as: \\newline <equation> $ \\mathcal{B}^{i-1}(t)=\\left\\{\\mathbf{n}\\;|\\;\\exists k,h,kT_{h}\\leq t:\\forall j,% \\;n_{j}=\\left\\lceil\\frac{kT_{h}}{T_{j}}\\right\\rceil\\right\\}. $ </equation> If a task is part of a pipeline with end-to-end deadline equal to $ D_{e2e} $ , then $ D_{i}\\leq D_{e2e} $ (keep in mind that, by now, the deadline is supposed to not exceed the task period).", "Therefore, we have to check all $ \\mathbf{n}\\in\\mathcal{B}^{i-1}(D_{e2e}) $ .", "\\newline The number of vectors (and correspondingly, the number of inequalities) depends on the relationship between the task periods.", "In real applications, we expect the periods to have \u201cnice\u201d relationships: for example, in many cases engineers choose periods that are multiples of each others.", "Therefore, we expect the set of inequalities to have manageable size for realistic problems.", "\\newline We have one such non-convex region for every task $ \\tau_{i} $ .", "Since we have to check the schedulability of all tasks on a CPU, we must {intersect} all such regions to obtain the final region of schedulable parameters.", "\\newline </subsection> <subsection> <title> IV-B Unconstrained deadlines and jitters </title> We now extend Seto\u2019s test to unconstrained deadlines and variable jitters.", "When considering a task with deadline greater than period, the worst-case response time may be found in any instance, not necessarily in the first one (as with the classical model of constrained deadline tasks).", "Therefore, we have to check the workload not only of the first job, but also of the following jobs of $ \\tau_{i} $ .", "Let use define $ h_{i}=\\frac{H_{i}}{T_{i}} $ , i.e. the number of jobs of $ \\tau_{i} $ contained in the $ i $ -level hyperperiod.", "Then, task $ \\tau_{i} $ is schedulable if and only if the following system of inequalities is verified: \\newline <equationgroup> <equation> $ \\forall h=1,\\ldots,h_{i},\\exists\\mathbf{n}\\in\\mathbb{B}^{i-1}(hT_% {i}+D_{e2e}) $ $ \\forall $ $  h=1,\\ldots,h_{i},\\exists\\mathbf{n}\\in\\mathbb{B}^{i-1}(hT_{i}+D_{% e2e}) $ </equation> <equation> $ \\left\\{\\begin{aligned} hC_{i}&+\\sum_{j=1}^{i-1}n_{j}C_{j}\\leq n_{% k}T_{k},\\forall k=1,\\ldots,i-1\\\\ hC_{i}&+\\sum_{j=1}^{i-1}n_{j}C_{j}\\leq(h-1)T_{i}+D_{i}\\end{aligned}\\right.", "$ $ \\left\\{\\begin{aligned} hC_{i}&+\\sum_{j=1}^{i-1}n_{j}C_{j}\\leq n_{% k}T_{k},\\forall k=1,\\ldots,i-1\\\\ hC_{i}&+\\sum_{j=1}^{i-1}n_{j}C_{j}\\leq(h-1)T_{i}+D_{i}\\end{aligned}\\right.", "$ </equation> </equationgroup> \\newline The correctness of the test is proved by the following Lemma.", "\\newline <theorem> Lemma 1 .", "Consider a system $ \\mathcal{T}=\\{\\tau_{1},\\ldots,\\tau_{i-1},\\tau_{i}\\} $ . Let $ \\mathcal{T}^{(h)} $ be a task set obtained from $ \\mathcal{T} $ by substituting $ \\tau_{i} $ with $ \\tau_{i}^{(h)} $ having computation time $ C_{i}^{(h)}=hC_{i} $ , deadline $ D_{i}^{(h)}=(h-1)T_{i}+D_{i} $ and the same priority $ \\pi_{i}^{(h)}=\\pi_{i} $ .", "\\newline If for every $ h=1,\\ldots,h_{i} $ , task $ \\tau_{i}^{(h)} $ completes before its deadline, then the first $ h_{i} $ jobs of $ \\tau_{i} $ will also complete before their deadlines.", "\\newline </theorem> <proof> Proof.", "By induction.", "Base of induction: the response time of job $ h=1 $ corresponds to the response time of the first job of $ \\tau_{i}^{(1)} $ (trivially true).", "Therefore, if $ \\tau_{i}^{(1)} $ is schedulable, also the first job of $ \\tau_{i} $ is schedulable.", "\\newline Now, the induction step.", "Suppose the Lemma is valid for $ h=1,\\ldots,k $ , we are now going to prove that is also valid for $ h=1,\\ldots,k+1 $ .", "By assumption, the first job of $ \\tau_{i}^{(h)} $ is schedulable for $ h=1,\\ldots,k $ .", "As a consequence of the validity of the Lemma, also the first $ k $ instances of $ \\tau_{i} $ are schedulable.", "Let $ f_{i,k} $ be the finishing time of the first job of $ \\tau_{i}^{(k)} $ .", "We have two cases: either $ f_{i,k}\\leq kT_{i} $ , or $ kT_{i}<f_{i,k}\\leq(k-1)T_{i}+D_{i} $ .", "\\newline In the first case, job $ k+1 $ is only subject to the interference of higher priority tasks.", "Therefore, its worst case response time correspond to the situation in which all higher priority tasks arrive at the same time $ kT_{i} $ (critical instant), and it is therefore equal to the response time of the first job $ h=1 $ , hence also schedulable.", "We can conclude that the Lemma is true without further induction steps.", "\\newline In the second case, the $ k+1 $ job has to wait for the previous job $ k $ to finish before it can start executing.", "In particular, there is no idle time in interval $ [0,f_{i,k+1}] $ .", "Therefore, the response time of job $ k+1 $ coincides with the response time of task $ \\tau_{i}^{(k+1} $ , and if the second one is schedulable, also job $ k+1 $ is schedulable.", "\\newline Finally, since the first instance of $ \\tau_{i}^{(h)} $ is schedulable for all $ h=1,\\ldots,h_{i} $ , and given that $ C_{i}^{(h)}=hC_{i} $ , then from Theorem [@ref:LABEL:th:setos-test] follows that the system of Inequalities in ( [@ref:LABEL:eq:h-jobs] ) is verified.", "\u220e \\newline </proof> To take into account the task jitter, we can appropriately adjust the last term that accounts for the task deadline, and the set $ \\mathcal{B}^{i-1}(t) $ .", "\\newline <theorem> Theorem 2 .", "Task $ \\tau_{i} $ is schedulable if: \\newline <equationgroup> <equation> $ \\forall h=1,\\ldots,\\frac{H_{i}}{T_{i}},\\;\\;\\exists\\mathbf{n}\\in% \\mathbb{B}^{i-1}(D_{e2e})\\;\\; $ $ \\forall $ $  h=1,\\ldots,\\frac{H_{i}}{T_{i}},\\;\\;\\exists\\mathbf{n}\\in\\mathbb{B% }^{i-1}(D_{e2e})\\;\\; $ </equation> <equation> $ \\left\\{\\begin{aligned} hC_{i}&+\\sum_{j=1}^{i-1}n_{j}C_{j}\\leq n_{% k}T_{k}-J_{k}\\;\\forall k=1,\\ldots,i-1\\\\ hC_{i}&+\\sum_{j=1}^{i-1}n_{j}C_{j}\\leq(h-1)T_{i}+D_{i}-J_{i}\\end{aligned}\\right.", "$ $ \\left\\{\\begin{aligned} hC_{i}&+\\sum_{j=1}^{i-1}n_{j}C_{j}\\leq n_{% k}T_{k}-J_{k}\\;\\forall k=1,\\ldots,i-1\\\\ hC_{i}&+\\sum_{j=1}^{i-1}n_{j}C_{j}\\leq(h-1)T_{i}+D_{i}-J_{i}\\end{aligned}\\right.", "$ </equation> </equationgroup> where \\newline <equation> $ \\mathcal{B}^{i-1}(t)=\\left\\{\\mathbf{n}\\;|\\;\\exists k,h,kT_{h}-\\overline{D}_{h}% \\leq t:\\forall jn_{j}=\\left\\lceil\\frac{kT_{h}+\\overline{D_{h}}}{T_{j}}\\right% \\rceil\\right\\}. $ </equation> \\newline </theorem> <proof> Proof.", "We report here a sketch of the complete proof.", "For every higher priority interfering task $ \\tau_{k} $ , the worst case situation is when the first instance arrives at $ J_{k} $ , whereas the following instances arrive as soon as it is possible.", "Therefore, the scheduling points must be modified from $ n_{k}T_{k} $ to $ n_{k}T_{k}-J_{k} $ .", "For what concerns task $ \\tau_{i} $ , the critical instant corresponds to the situation in which the first instance can only start at $ J_{i} $ , hence the available interval is $ (h-1)T_{i}+D_{i}-J_{i} $ .", "\u220e \\newline </proof> Notice that the introduction of unconstrained deadline adds a great amount of complexity to the problem.", "In particular, the number of non-convex regions to intersect is now $ \\mathcal{O}(\\sum_{i=1}^{n}\\frac{H_{i}}{T_{i}}) $ , which is dominated by $ \\mathcal{O}(nH_{n}) $ .", "So, the proposed problem representation does not scale with increasing hyperperiods; however, as we will show in Section [@ref:LABEL:sec:experiments] , the problem is tractable when periods are harmonic or quasi-harmonic, as it often happens in real applications.", "\\newline </subsection> <subsection> <title> IV-C Non preemptive scheduling </title> In this paper we model the network as a non-preemptive fixed priority scheduled resource.", "In non-preemptive fixed priority scheduling, the worst-case response time for a task $ \\tau_{i} $ can be found in its longest $ i $ -level active period [@bib:Bril:07] .", "A $ i $ -level active period $ L_{i} $ is an interval $ [a,b) $ such that the amount of processing that needs to be performed due to jobs with priority higher than or equal to $ \\tau_{i} $ (including $ \\tau_{i} $ itself) is larger than 0 $ \\forall t\\in(a,b) $ , and equal to 0 at instants $ a $ and $ b $ .", "The longest $ L_{i} $ can be found by computing the lowest fixed point of the following recursive function [@bib:George:96] : \\newline <equationgroup> <equation> $ \\left\\{\\begin{aligned} L_{i}^{0}&=B_{i}+C_{i}\\\\ L_{i}^{(s)}&=B_{i}+\\sum_{j<=i}\\lceil\\frac{L_{i}^{(s-1)}}{T_{j}}\\rceil C_{j}% \\end{aligned}\\right.", "$ $ \\left\\{\\begin{aligned} L_{i}^{0}&=B_{i}+C_{i}\\\\ L_{i}^{(s)}&=B_{i}+\\sum_{j<=i}\\lceil\\frac{L_{i}^{(s-1)}}{T_{j}}\\rceil C_{j}% \\end{aligned}\\right.", "$ </equation> </equationgroup> where $ B_{i}=\\max_{i<j}(C_{i}-1) $ .", "\\newline In order to find the worst-case response time of task $ \\tau_{i} $ , all jobs $ \\tau_{i,k} $ that appear in the longest $ L_{i} $ need to be checked, with $ k\\in[1,\\lceil\\frac{L_{i}}{T_{i}}\\rceil] $ .", "\\newline To obtain the worst-case response time, we compute first its worst-case start time.", "When there is no jitter, George et al. [@bib:George:96] give the following formula to compute the worst-case start time of a job $ \\tau_{i,k} $ : \\newline <equationgroup> <equation> $ \\left\\{\\begin{aligned} s_{i,k}^{(0)}&=B_{i}+\\sum_{j<i}C_{j}\\\\ s_{i,k}^{(l+1)}&=B_{i}+(k-1)C_{i}+\\sum_{j<i}(\\left\\lfloor\\frac{s_{i,k}^{l}}{T_% {j}}\\right\\rfloor+1)C_{j}\\\\ \\end{aligned}\\right.", "$ $ \\left\\{\\begin{aligned} s_{i,k}^{(0)}&=B_{i}+\\sum_{j<i}C_{j}\\\\ s_{i,k}^{(l+1)}&=B_{i}+(k-1)C_{i}+\\sum_{j<i}(\\left\\lfloor\\frac{s_{i,k}^{l}}{T_% {j}}\\right\\rfloor+1)C_{j}\\\\ \\end{aligned}\\right.", "$ </equation> </equationgroup> Note that $ (k-1)C_{i} $ is the computation time of the preceding $ (k-1) $ jobs.", "Since a lower priority task\u2019s execution cannot be preempted, this could \u201cpush\u201d one job of a higher priority task to interfere with its future jobs.", "\\newline Observe that the iterating computation of $ L_{i} $ in Equation ( [@ref:LABEL:eq:level-i-active-period] ) is non decreasing and (when the system utilisation is no larger than 1) $ B_{i}+\\sum_{j<=i}\\lceil\\frac{H_{i}}{T_{i}}\\rceil C_{i}<=B_{i}+H_{i} $ , so the length of $ L_{i} $ will not exceed $ B_{i}+H_{i} $ .", "\\newline In this paper, the worst-case execution time of the tasks are considered free parameters.", "However, $ L_{i} $ can still be upper bounded by $ \\overline{L_{i}}=\\max_{i<j}(T_{j})+H_{i} $ .", "Now, we can derive a similar feasibility test for non preemptive scheduling as in Theorem [@ref:LABEL:thm:jitter] .", "\\newline <theorem> Theorem 3 .", "A non preemptive task $ \\tau_{i} $ is schedulable if : \\newline <equationgroup> <equation> $ \\forall h=1,\\ldots,\\lceil\\frac{\\overline{L_{i}}}{T_{i}}\\rceil,\\;% \\;\\exists\\mathbf{n}\\in\\mathbb{B}^{i-1}(D_{e2e})\\;\\; $ $ \\forall h=1,\\ldots,\\lceil\\frac{\\overline{L_{i}}}{T_{i}}\\rceil,\\;% \\;\\exists\\mathbf{n}\\in\\mathbb{B}^{i-1}(D_{e2e})\\;\\; $ </equation> <equation> $ \\left\\{\\begin{aligned} B_{i}&+(h-1)C_{i}+\\sum\\limits_{j=1}^{i-1}n% _{j}C_{j}\\leq n_{l}T_{l}-J_{l}\\;\\;\\forall l=1,\\ldots,i-1\\\\ B_{i}&+(h-1)C_{i}+\\sum\\limits_{j=1}^{i-1}n_{j}C_{j}\\leq(h-1)T_{i}+D_{i}-C_{i}-% J_{i}\\\\ \\end{aligned}\\right.", "$ $ \\left\\{\\begin{aligned} B_{i}&+(h-1)C_{i}+\\sum\\limits_{j=1}^{i-1}n% _{j}C_{j}\\leq n_{l}T_{l}-J_{l}\\;\\;\\forall l=1,\\ldots,i-1\\\\ B_{i}&+(h-1)C_{i}+\\sum\\limits_{j=1}^{i-1}n_{j}C_{j}\\leq(h-1)T_{i}+D_{i}-C_{i}-% J_{i}\\\\ \\end{aligned}\\right.", "$ </equation> </equationgroup> where $ \\mathbb{B}^{i-1}(D_{e2e}) $ is defined as in Theorem [@ref:LABEL:thm:jitter] , and $ B_{i} $ is the blocking time that task $ \\tau_{i} $ suffers from lower priority tasks: \\newline <equation> $ \\forall i,\\forall j>i\\;\\;B_{i}\\leq C_{j}-1 $ </equation> \\newline </theorem> <proof>", "Proof. See the sufficient part of proof in [@bib:seto98] and Theorem [@ref:LABEL:thm:jitter] .", "\u220e \\newline </proof> Term $ B_{i} $ is an additional free variable used to model the blocking time that a task suffers from lower priority tasks.", "It is possible to avoid the introduction of this additional variable by substituting it in the inequalities with a simple Fourier-Motzkin elimination.", "\\newline Like in the preemptive case, for every non preemptive task, this theorem builds a set of inequalities.", "The system schedulability region is the intersection of all the sets.", "The complexity of this procedure is the same as for the preemptive case.", "\\newline </subsection> <subsection> <title> IV-D Distributed systems </title> Until now, we have considered the parametric analysis of independent tasks on single processor systems, with computation times, deadlines and jitter as free parameters.", "In particular, the equations in Theorem [@ref:LABEL:thm:jitter] and Theorem [@ref:LABEL:thm:jitter-np] give us a way to express the constraints on the system in a fully parametric way: all solutions to the system of Inequalities ( [@ref:LABEL:eq:final-single-proc] ) and ( [@ref:LABEL:eq:final-single-proc-np] ) are all the combinations of computations times, deadlines and jitters that make the single processor system schedulable.", "\\newline It is important to make one key observation.", "If we fix the computation times and the jitters of all tasks, and we leave the deadlines as the only free variables, the worst-case response time of each task can be found by minimising the deadline variables.", "As an example, consider the following task set (the same as in [@bib:Bin04b] ) to be scheduled by preemptive fixed priority scheduling on a single processor: \\newline hline \\ Task & $ C_{i} $ & $ T_{i} $ & $ D_{i} $ & $ p_{i} $ \\ hline \\ $ \\tau_{1} $ & 1 & 3 & 3 & 3 \\ $ \\tau_{2} $ & 2 & 8 & 7 & 2 \\ $ \\tau_{3} $ & 4 & 20 & ? & 1 \\ hline \\newline We consider $ D_{3} $ as a parameter and set up the system of inequalities according to Equation ( [@ref:LABEL:eq:final-single-proc] ).", "After reduction of the non-useful constraints, we obtain \\newline <equation> $ 12\\leq D_{3}\\leq 20 $ </equation> Notice that $ 12 $ is actually the worst-case response time of $ \\tau_{3} $ .", "\\newline The second key observation is that a precedence constraint between two consecutive tasks $ \\tau_{i} $ and $ \\tau_{i+1} $ in the same pipeline can be expressed as $ D_{i}\\leq J_{i+1} $ .", "This basically means that the worst-case response time of task $ \\tau_{i} $ should never exceed the jitter (i.e. worst-case start time) of task $ \\tau_{i+1} $ .", "Therefore, we have a way to relate tasks allocated on different processors that belong to the same pipeline.", "\\newline Finally, the last task in every pipeline, let us call it $ \\tau_{n} $ must complete before the end-to-end deadline: $ D_{n}\\leq D_{e2e} $ .", "\\newline We are now ready use inequalities in ( [@ref:LABEL:eq:final-single-proc] ) as building blocks for the parametric analysis of distributed systems.", "The procedure to build the final system of inequalities is as follows: \\newline <list> \\ For each processor, we build the system of inequalities ( [@ref:LABEL:eq:final-single-proc] ), and for every network the system of inequalities in ( [@ref:LABEL:eq:final-single-proc-np] ).", "All these systems are independent of each other, because they are constraints on different tasks, so they use different variables.", "The combined system contains $ 3*N $ variables, where $ N $ is the total number of tasks.", "\\newline \\ \\ For every pipeline, we add the following precedence constraints: \\newline <list> \\ For the first task in the pipeline, let us denote it as $ \\tau_{1} $ , we set its jitter to 0: $ J_{1}=0 $ .", "\\newline \\ \\ For every pair of consecutive tasks, let us denote them as $ \\tau_{i} $ and $ \\tau_{i+1} $ , we impose the precedence constraint: $ D_{i}\\leq J_{i+1} $ ; \\newline \\ \\ For the last task in the pipeline, let us denote it as $ \\tau_{n} $ , we impose that it must complete before its end-to-end deadline $ D_{n}\\leq D_{e2e} $ .", "\\newline \\ </list> Such constraints must intersect the combined system to produce the final system of constraints.", "\\newline \\ </list> \\newline To give readers an idea how the parameter space of a distributed system would look like, here is a very simple example, built with the goal of showing the general methodology without taking too much space.", "We consider a system with two processors (and no network), two tasks $ \\tau_{1} $ and $ \\tau_{3} $ , and one pipeline consisting of two tasks, $ \\tau_{21} $ and $ \\tau_{22} $ .", "\\newline hline \\ Pipeline & Task & $ \\pi_{i} $ & Resource & $ T_{i} $ & $ \\overline{D}_{i}(D_{e2e}) $ \\ - & $ \\tau_{1} $ & 2 & CPU1 & 10 & 4 \\ hline \\ $ P^{2} $ & $ \\tau_{1}^{2} $ & 1 & CPU1 & 20 & 6 \\ $ \\tau_{2}^{2} $ & 2 & CPU2 \\ - & $ \\tau_{3} $ & 1 & CPU2 & 16 & 16 \\ hline To make sure that for each task we have one single inequality (see Equation ( [@ref:LABEL:eq:final-single-proc] )) we set up the deadlines short enough so that one schedulability point for each task needs to be considered, thus avoiding complex disjoints.", "\\newline Based on the analysis in this section, we derive a set of constraints, where $ J $ , $ C $ and $ D $ are the free variables for the tasks.", "\\newline <equation> $ \\begin{cases}J_{1}\\geq 0,C_{1}\\geq 0,C_{1}^{2}\\geq 0,C_{2}^{2}\\geq 0,J_{3}\\geq 0% ,C_{3}\\geq 0\\\\ D_{1}\\leq 4,D_{3}\\leq 16\\\\ C_{1}+J_{1}\\leq D_{1}\\\\ C_{21}+C_{1}\\leq D_{21}\\\\ C_{2}^{2}+J_{2}^{2}\\leq D_{2}^{2}\\\\ C_{3}+C_{2}^{2}+J_{2}^{2}\\leq 20\\\\ C_{3}+C_{2}^{2}+J_{3}\\leq D_{3}\\\\ J_{1}^{2}=0,D_{1}^{2}\\leq J_{2}^{2},D_{2}^{2}\\leq 6\\\\ \\end{cases} $ </equation> In the first two lines, we show the \u201ctrivial\u201d inequalities: all values must be non-negative, and every deadline must not exceed the corresponding maximum deadline specified in the table.", "The inequalities at line 3 and 4 and the inequalities at line 5, 6 and 7 are (reduced) constraints (according to Theorem [@ref:LABEL:thm:jitter] ) on the schedulability of tasks on processor 1 and 2, respectively.", "Finally, the inequalities in the last line are the ones imposed by the precedence constraints between $ \\tau_{1}^{2} $ and $ \\tau_{2}^{2} $ .", "\\newline A real system will produce a much more complex set of constraints.", "For each task we will need to prepare a set of disjoint inequalities, that must be intersect with each other: this may greatly increment the number of inequalities to be considered.", "Also, often we need to model the network.", "Therefore, we prepared a software tool to automatically build and analyse the set of inequalities for a distributed system.", "\\newline </subsection> <subsection> <title> IV-E Implementation </title> The analytic method proposed in this section has been implemented in RTSCAN [@bib:RTSCAN-web-page] , a C/C++ library publicly available as open source code that collects different types of schedulability tests.", "The code for the parametric schedulability analysis uses the PPL (Parma Polyhedra Library) [@bib:BagnaraHZ06TR] , a library specifically designed and optimised to represent and operate on polyhedra.", "The library efficiently operates on rational numbers with arbitrary precision: therefore, in this work we make the assumption that all variables (computations times, deadlines and jitter) are defined in the domain of integers.", "This does not represent a great problem, since in practice every value is multiple of a real-time clock expressed as number of ticks.", "\\newline An evaluation of this tool, and of the complexity of the analysis presented here, will be presented in Section [@ref:LABEL:sec:experiments] .", "\\newline </subsection>  </section>"], ["<section> <title> V The Inverse Method approach </title>  <subsection> <title> V-A Parametric Timed Automata </title> Timed Automata are finite-state automata augmented with clocks, i.e., real-valued variables increasing uniformly, that are compared within guards and invariants with timing delays [@bib:AD94] .", "Parametric timed automata (PTAs) [@bib:AHV93] extend timed automata with parameters, i.e., unknown constants, that can be used in guards and invariants.", "\\newline Formally, given a set $ X $ of clocks and a set $ P $ of parameters, a constraint $ C $ over $ X $ and $ P $ is a conjunction of linear inequalities on $ X $ and $ P $ .", "Given a parameter valuation (or point) $ \\pi $ , we write $ \\pi\\models C $ when the constraint where all parameters within $ C $ have been replaced by their value as in $ \\pi $ is satisfied by a non-empty set of clock valuations.", "\\newline <theorem> Definition 1 .", "A PTA $ \\mathcal{A} $ is $ (\\Sigma,Q,q_{0},X,P,K,I,{\\rightarrow}) $ with $ \\Sigma $ a finite set of actions, $ Q $ a finite set of locations, $ q_{0}\\in Q $ the initial location, $ X $ a set of clocks, $ P $ a set of parameters, $ K $ a constraint over $ P $ , $ I $ the invariant assigning to every $ q\\in Q $ a constraint over $ X $ and $ P $ , and $ {\\rightarrow} $ a step relation consisting of elements $ (q,g,a,\\rho,q^{\\prime}) $ , where $ q,q^{\\prime}\\in Q $ , $ a\\in\\Sigma $ , $ \\rho\\subseteq X $ is the set of clocks to be reset, and the guard $ g $ is a constraint over $ X $ and $ P $ .", "\\newline </theorem> The semantics of a PTA $ \\mathcal{A} $ is defined in terms of states, i.e., couples $ (q,C) $ where $ q\\in Q $ and $ C $ is a constraint over $ X $ and $ P $ .", "Given a point $ \\pi $ , we say that a state $ (q,C) $ is $ \\pi $ -compatible if $ \\pi\\models C $ .", "Runs are alternating sequences of states and actions, and traces are time-abstract runs, i.e., alternating sequences of {locations} and actions.", "The trace set of $ \\mathcal{A} $ corresponds to the traces associated with all the runs of $ \\mathcal{A} $ .", "Given $ \\mathcal{A} $ and $ \\pi $ , we denote by $ \\mathcal{A}[\\pi] $ the (non-parametric) timed automaton where each occurrence of a parameter has been replaced by its constant value as in $ \\pi $ .", "One defines $ \\mathit{Post}_{\\mathcal{A}(K)}^{i}(S) $ as the set of states reachable from a set $ S $ of states in exactly $ i $ steps under $ K $ , and $ \\mathit{Post}_{\\mathcal{A}(K)}^{*}(S)=\\bigcup_{i\\geq 0}\\mathit{Post}_{\\mathcal% {A}(K)}^{i}(S) $ .", "\\newline Detailed definitions on parametric timed automata can be found in, e.g., [@bib:AS13] .", "\\newline The Inverse Method exploits the model of Timed Automata and the knowledge of a reference point of timing values for which the good behaviour of the system is known.", "The method synthesises automatically a dense zone of points around the reference point, for which the discrete behaviour of the system, that is the set of all the admissible sequences of interleaving events, is guaranteed to be the same.", "Although the principle of the inverse method shares similarities with sensitivity analysis, its algorithm proceeds by iterative state space exploration.", "Furthermore, its result comes under the form of a fully parametric constraint, in contrast to sensitivity analysis.", "By repeatedly applying the method, we are able to decompose the parameter space into a covering set of \u201ctiles\u201d, which ensure a uniform behaviour of the system: it is sufficient to test only one point of the tile in order to know whether or not the system behaves correctly on the whole tile.", "\\newline </subsection> <subsection> <title> V-B System model with PTAs </title> In this section, we show how we modelled a schedulability problem as defined in [@ref:LABEL:sec:system-model] , similarly to what has been done in [@bib:FLMS-time12] .", "In the current implementation, we only model pipelines with end-to-end deadlines no larger than their periods.", "Moreover, all pipelines are strictly periodic, and have 0 offset.", "This means that the results of the parametric analysis produced by this model are only valid for periodic synchronous pipelines.", "\\newline We illustrate our model with the help of an example of two pipelines $ \\mathcal{P}^{1},\\mathcal{P}^{2} $ with $ \\mathcal{P}^{1}=\\{\\tau_{1},\\tau_{2}\\} $ , $ \\mathcal{P}^{2}=\\{\\tau_{3},\\tau_{4}\\} $ , $ p(\\tau_{1})=p(\\tau_{4})=p_{1} $ , $ p(\\tau_{2})=p(\\tau_{3})=p_{2} $ , $ p_{1} $ being a preemptive processor and $ p_{2} $ being non-preemptive.", "We have that $ \\pi_{1}>\\pi_{4} $ and $ \\pi_{3}>\\pi_{2} $ .", "\\newline In Figure [@ref:LABEL:fig:pipeline]", ", we show the model of a pipeline.", "A pipeline is a sequence of tasks that are to be executed in order: when a task completes its instance, it instantly activates the next one in the pipeline.", "Once every task in the pipeline has completed, the pipeline waits for the next period to start.", "\\newline In Figure [@ref:LABEL:fig:preemptive] , we present how we model a preemptive processor.", "The processor can be {idle} , waiting for a task activations.", "As soon as a request has been received, it moves to one of the states where the corresponding higher priority task is running.", "If it receives another activation request, it moves to the state corresponding to the highest priority task running.", "Moreover, while a task executes, the scheduler automaton checks if the corresponding pipeline misses its deadline.", "In the case of a deadline miss, the processor moves to a special failure state and stops any further computation.", "\\newline In Figure [@ref:LABEL:fig:nonpreemptive] , we present the model a non-preemptive processor.", "Similarly to the previous case, the processor can be idle, waiting for an activation request.", "As soon as a request as been received it moves to a corresponding state, setting a token corresponding to the activated task to $ 1 $ .", "If another request is sent at the same time, it sets a corresponding token to $ 1 $ , and moves to the state where the highest priority task will be running.", "Once a task is completed, the processor set the corresponding token to $ 0 $ , and according to the token set to $ 1 $ , moves to the state where the highest priority task will be running.", "Similarly to the previous case, while a task executes, the automaton checks for deadline misses, and in that case it stops any further computation by moving to a special failure state.", "\\newline Since we model periodic pipelines, and the model explores all possible traces, we expect that schedulability region will be larger that the one obtained with other techniques which only consider sporadic pipelines (like the analysis proposed in Section [@ref:LABEL:sec:analytic] ).", "An assessment of this difference is provided in the next section.", "\\newline </subsection>  </section>"], ["<section> <title> VI Evaluation </title>  We evaluated the effectiveness and the running time of three different tools for parametric schedulability analysis: the RTSCAN tool, which implements the analytic method described in Section [@ref:LABEL:sec:analytic] ; the IMITATOR tool [@bib:FLMS-time12] , described in Section [@ref:LABEL:sec:timed-automata] ; and the MAST tool [@bib:MAST-web-page] .", "\\newline We highlight that the three tools are implemented in different languages, and use different ways to optimise the analysis; RTSCAN is implemented in C/C++ and uses on the PPL library; IMITATOR is implemented in OCaml, but it also used the PPL libraries for building regions; finally, MAST is implemented in Ada.", "\\newline For MAST, we have selected the \u201cOffset Based analysis\u201d, proposed in [@bib:Palencia1998] .", "For IMITATOR, we consider all pipelines as strictly periodic, and only deadlines less than periods.", "We evaluated the tools on two different test cases, in increasing order of complexity.", "We will first present the results, in terms of schedulability regions, for two different test cases.", "In order to simplify the visualisation of the results, for each test case, we will present the 2D region of two parameters only: however, all three methods are general and can be applied to any number of parameters.", "\\newline In Section [@ref:LABEL:sec:execution-times] , after discussing some important implementation details, we will and present the execution times of the three tools.", "\\newline <subsection> <title> VI-A Test case 1 </title> The first test case has been adapted from [@bib:Palencia1998] (we reduced the computation times of some tasks to position the system in a interesting schedulability region).", "It consists of 2 processors, connected by a CAN bus, three simple periodic tasks and one pipeline."]], "target": "The parameters are listed in Table . Processor 1 and 3 model two different computational nodes that are scheduled by preemptive fixed priority, and Processor 2 models a CAN bus with non-preemptive fixed priority policy. The only pipeline models a remote procedure call from CPU 1 to CPU 3. All tasks have deadlines equal to periods, and also the pipeline has end-to-end deadline equal to its period. Only two messages are sent on the network, and if the pipeline is schedulable, they cannot interfere with each other. We wish to perform parametric schedulability analysis with respect to $ C_{1} $ and $ C_{1}^{1} $ ."}, {"tabular": ["  Quartile:  &  1 ^(st)  &  2 ^(nd)  &  3 ^(rd)  &  4 ^(th) ", " Wiki  &  62%  &  8%  &  3%  &  51% ", " SE  &  37%  &  4%  &  6%  &  46% ", "  &    &    &    &    "], "ref_sec": [["<section> <title> 1 Introduction </title>  Politeness is a central force in communication, arguably as basic as the pressure to be truthful, informative, relevant, and clear [@bib:Grice75,Leech83,Brown:Levinson:1978] .", "Natural languages provide numerous and diverse means for encoding politeness and, in conversation, we constantly make choices about where and how to use these devices.", "\\newcite Kaplan99 observes that \u201cpeople desire to be {paid} respect\u201d and identifies honorifics and other politeness markers, like {please} , as \u201cthe coin of that payment\u201d.", "In turn, politeness markers are intimately related to the power dynamics of social interactions and are often a decisive factor in whether those interactions go well or poorly [@bib:GyasiObeng:1997,Chilton:1999,Andersson:Pearson:1999,Rogers:Lee-Wong:2003,Holmes:Stubbe:2005] .", "\\newline The present paper develops a computational framework for identifying and characterizing politeness marking in requests.", "We focus on requests because they involve the speaker imposing on the addressee, making them ideal for exploring the social value of politeness strategies [@bib:Clark:Schunk:1980,Francik:Clark:1985] .", "Requests also stimulate extensive use of what \\newcite Brown:PolitenessSomeUniversalsInLanguageUsage:1987 call {negative politeness} : speaker strategies for minimizing (or appearing to minimize) the imposition on the addressee, for example, by being indirect ( {Would you mind} ) or apologizing for the imposition ( {I\u2019m terribly sorry, but} ) [@bib:Lakoff73,RLakoff:1977,Brown:Levinson:1978] .", "\\newline Our investigation is guided by a new corpus of requests annotated for politeness.", "The data come from two large online communities in which members frequently make requests of other members: Wikipedia, where the requests involve editing and other administrative functions, and Stack Exchange, where the requests center around a diverse range of topics (e.g., programming, gardening, cycling).", "The corpus confirms the broad outlines of linguistic theories of politeness pioneered by \\newcite Brown:PolitenessSomeUniversalsInLanguageUsage:1987, but it also reveals new interactions between politeness markings and the morphosyntactic context.", "For example, the politeness of {please} depends on its syntactic position and the politeness markers it co-occurs with.", "\\newline Using this corpus, we construct a politeness classifier with a wide range of domain-independent lexical, sentiment, and dependency features operationalizing key components of politeness theory, including not only the negative politeness markers mentioned above but also elements of {positive politeness} (gratitude, positive and optimistic sentiment, solidarity, and inclusiveness).", "The classifier achieves near human-level accuracy across domains, which highlights the consistent nature of politeness strategies and paves the way to using the classifier to study new data.", "\\newline Politeness theory predicts a negative correlation between politeness and the power of the requester, where power is broadly construed to include social status, authority, and autonomy [@bib:Brown:PolitenessSomeUniversalsInLanguageUsage:1987] .", "The greater the speaker\u2019s power relative to her addressee, the less polite her requests are expected to be: there is no need for her to incur the expense of paying respect, and failing to make such payments can invoke, and hence reinforce, her power.", "We support this prediction by applying our politeness framework to Wikipedia and Stack Exchange, both of which provide independent measures of social status.", "We show that polite Wikipedia editors are more likely to achieve high status through elections; however, once elected, they become less polite.", "Similarly, on Stack Exchange, we find that users at the top of the reputation scale are less polite than those at the bottom.", "\\newline Finally, we briefly address the question of how politeness norms vary across communities and social groups.", "Our findings confirm established results about the relationship between politeness and gender, and they identify substantial variation in politeness across different programming language subcommunities on Stack Exchange.", "\\newline  </section>"], ["<section> <title> 2 Politeness data </title>  Requests involve an imposition on the addressee, making them a natural domain for studying the inter-connections between linguistic aspects of politeness and social variables.", "\\newline <paragraph> <title> Requests in online communities </title> We base our analysis on two online communities where requests have an important role: the Wikipedia community of editors and the Stack Exchange question-answer community.", "On Wikipedia, to coordinate on the creation and maintenance of the collaborative encyclopedia, editors can interact with each other on user talk-pages; requests posted on a user talk-page, although public, are generally directed to the owner of the talk-page.", "On Stack Exchange, users often comment on existing posts requesting further information or proposing edits; these requests are generally directed to the authors of the original posts.", "\\newline Both communities are not only rich in user-to-user requests, but these requests are also part of consequential conversations, not empty social banter; they solicit specific information or concrete actions, and they expect a response.", "\\newline </paragraph> <paragraph> <title> Politeness annotation </title> Computational studies of politeness, or indeed any aspect of linguistic pragmatics, demand richly labeled data.", "We therefore label a large portion of our request data (over 10,000 utterances) using Amazon Mechanical Turk (AMT), creating the largest corpus with politeness annotations (see Table [@ref:LABEL:tab:data] for details).", "\\newline We choose to annotate requests containing exactly two sentences, where the second sentence is the actual request (and ends with a question mark).", "This provides enough context to the annotators while also controlling for length effects.", "Each annotator was instructed to read a batch of 13 requests and consider them as originating from a co-worker by email.", "For each request, the annotator had to indicate how polite she perceived the request to be by using a slider with values ranging from \u2018\u2018very impolite\u2019\u2019 to \u2018\u2018very polite\u2019\u2019.", "Each request was labeled by five different annotators.", "\\newline We vetted annotators by restricting their residence to be in the U.S. and by conducting a linguistic background questionnaire.", "We also gave them a paraphrasing task shown to be effective for verifying and eliciting linguistic attentiveness [@bib:munro2010crowdsourcing] , and we monitored the annotation job and manually filtered out annotators who submitted uniform or seemingly random annotations.", "\\newline Because politeness is highly subjective and annotators may have inconsistent scales, we applied the standard z-score normalization to each worker\u2019s scores.", "Finally, we define the politeness score (henceforth {politeness} ) of a request as the average of the five scores assigned by the annotators.", "The distribution of resulting request scores (shown in Figure [@ref:LABEL:fig:annotation] ) has an average of 0 and standard deviation of 0.7 for both domains; positive values correspond to polite requests (i.e., requests with normalized annotations towards the \u201cvery polite\u201d extreme) and negative values to impolite requests.", "A summary of all our request data is shown in Table [@ref:LABEL:tab:data] .", "\\newline </paragraph> <paragraph> <title> Inter-annotator agreement </title> To evaluate the reliability of the annotations we measure the inter-annotator agreement by computing, for each batch of 13 documents that were annotated by the same set of 5 users, the mean pairwise correlation of the respective scores.", "For reference, we compute the same quantities after randomizing the scores by sampling from the observed distribution of politeness scores.", "As shown in Figure [@ref:LABEL:fig:interannotator] , the labels are coherent and significantly different from the randomized procedure ( $ p<0.0001 $ according to a Wilcoxon signed rank test).", "\\newline </paragraph> <paragraph> <title> Binary perception </title> Although we did not impose a discrete categorization of politeness, we acknowledge an implicit binary perception of the phenomenon: whenever an annotator moved a slider in one direction or the other, she made a binary politeness judgment.", "However, the boundary between somewhat polite and somewhat impolite requests can be blurry.", "To test this intuition, we break the set of annotated requests into four groups, each corresponding to a politeness score quartile.", "For each quartile, we compute the percentage of requests for which all five annotators made the same binary politeness judgment."]], "target": "As shown in Table , full agreement is much more common in the 1 ^(st) (bottom) and 4 ^(th) (top) quartiles than in the middle quartiles. This suggests that the politeness scores assigned to requests that are only somewhat polite or somewhat impolite are less reliable and less tied to an intuitive notion of binary politeness."}, {"tabular": ["  Corpus  &  #Doc.  &  #Doc. (unique)  &  #Relation ", " SciDTB  &  1355  &  798  &  18978 ", " RST-DT  &  438  &  385  &  23611 ", " BioDRB  &  24  &  24  &  5859  "], "ref_sec": [["<section> <title> 1 Introduction </title>  Discourse relation depicts how the text spans in a text relate to each other.", "These relations can be categorized into different types according to semantics, logic or writer\u2019s intention.", "Annotations of such discourse relations can benefit many down-stream NLP tasks including machine translation [@bib:guzman2014using,joty2014discotk] and automatic summarization [@bib:gerani2014abstractive] .", "\\newline Several discourse corpora have been proposed in previous work, grounded with various discourse theories.", "Among them Rhetorical Structure Theory TreeBank (RST-DT) [@bib:carlson2003building] and Penn Discourse TreeBank (PDTB) [@bib:prasad2007penn] are the most widely-used resources.", "PDTB focuses on shallow discourse relations between two arguments and ignores the whole organization.", "RST-DT based on Rhetorical Structure Theory (RST) [@bib:mann1988rhetorical] represents a text into a hierarchical discourse tree.", "Though RST-DT provides more comprehensive discourse information, its limitations including the introduction of intermediate nodes and absence of non-projective structures bring the annotation and parsing complexity.", "\\newline \\newcite li2014text and \\newcite yoshida2014dependency both realized the problems of RST-DT and introduced dependency structures into discourse representation.", "\\newcite stede2016parallel adopted dependency tree format to compare RST structure and Segmented Discourse Representation Theory(SDRT) [@bib:lascarides2008segmented] structure for a corpus of short texts.", "Their discourse dependency framework is adapted from syntactic dependency structure [@bib:hudson1984word,bohmova2003prague] , with words replaced by elementary discourse units (EDUs).", "Binary discourse relations are represented from dominant EDU (called \u201chead\u201d) to subordinate EDU (called \u201cdependent\u201d), which makes non-projective structure possible.", "However, \\newcite li2014text and \\newcite yoshida2014dependency mainly focused on the definition of discourse dependency structure and directly transformed constituency trees in RST-DT into dependency trees.", "On the one hand, they only simply treated the transformation ambiguity, while constituency structures and dependency structures did not correspond one-to-one.", "On the other hand, the transformed corpus still did not contain non-projective dependency trees, though \u201ccrossed dependencies\u201d actually exist in the real flexible discourse structures [@bib:wolf2005representing] .", "In such case, it is essential to construct a discourse dependency treebank from scratch instead of through automatically converting from the constituency structures.", "\\newline In this paper, we construct the discourse dependency corpus SciDTB .", "based on scientific abstracts, with the reference to the discourse dependency representation in \\newcite li2014text.", "We choose scientific abstracts as the corpus for two reasons.", "First, we observe that when long news articles in RST-DT have several paragraphs, the discourse relations between paragraphs are very loose and their annotations are not so meaningful.", "Thus, short texts with obvious logics become our preference.", "Here, we choose scientific abstracts from ACL Anthology which are usually composed of one passage and have strong logics.", "Second, we prefer to conduct domain-specific discourse annotation.", "RST-DT and PDTB are both constructed on news articles, which are unspecific in domain coverage.", "We choose the scientific domain that is more specific and can benefit further academic applications such as automatic summarization and translation.", "Furthermore, our treebank SciDTB can be made as a benchmark for evaluating discourse parsers.", "Three baselines are provided as fundamental work.", "\\newline  </section>"], ["<section> <title> 2 Annotation Framework </title>  In this section, we describe two key aspects of our annotation framework, including elementary discourse units (EDU) and discourse relations.", "\\newline <subsection> <title> 2.1 Elementary Discourse Units </title> We first need to divide a passage into non-overlapping text spans, which are named elementary discourse units (EDUs).", "We follow the criterion of [@bib:polanyi1988formal] [@bib:polanyi1988formal] and [@bib:irmer2011bridging] [@bib:irmer2011bridging] which treats clauses as EDUs.", "\\newline However, since a discourse unit is a semantic concept but a clause is defined syntactically, in some cases segmentation by clauses is still not the most proper strategy.", "In practice, we refer to the guidelines defined by [@bib:carlson2001discourse] .", "For example, subjective clauses, objective clauses of non-attributive verbs and verb complement clauses are not segmented.", "Nominal postmodifiers with predicates are treated as EDUs.", "Strong discourse cues such as \u201c despite \u201d and \u201c because of \u201d starts a new EDU no matter they are followed by a clause or a phrase.", "We give an EDU segmentation example as follows.", "\\newline <list> \\ [Despite bilingual embedding\u2019s success,][ the contextual information ][which is of critical importance to translation quality,][ was ignored in previous work. ] \\newline \\ </list> \\newline It is noted, as in Example 1, there are EDUs which are broken into two parts (in bold) by relative clauses or nominal postmodifiers.", "Like RST, we connect the two parts by a pseudo-relation type Same-unit to represent their integrity.", "\\newline </subsection> <subsection> <title> 2.2 Discourse Relations </title> A discourse relation is defined as tri-tuple $ (h,d,r) $ , where $ h $ means the head EDU, $ d $ is the dependent EDU, and $ r $ defines the relation category between $ h $ and $ d $ .", "For a discourse relation, head EDU is defined as the unit with essential information and dependent EDU with supportive content.", "Here, we follow \\newcite carlson2001discourse to adopt deletion test in the determination of head and dependent.", "If one unit in a binary relation pair is deleted but the whole meaning can still be almost understood from the other unit, the deleted unit is treated as dependent and the other one as the head.", "\\newline For the relation categories, we mainly refer to the work of [@bib:carlson2001discourse] and [@bib:bunt2016iso] .", "Table [@ref:LABEL:category] presents the discourse relation set of SciDTB, which are not explained detailedly one by one due to space limitation.", "Through investigation of scientific abstracts, we define 17 coarse-grained relation types and 26 fine-grained relations for SciDTB.", "\\newline It is noted that we make some modifications to adapt to the scientific domain.", "For example, In SciDTB, Background relation is divided into three subtypes: Related , Goal and General , because the background description in scientific abstracts usually has more different intents.", "Meanwhile, for attribution relation we treat the attributive content rather than act as head, which is contrary to that defined in [@bib:carlson2001discourse] , because scientific facts or research arguments mentioned in attributive content are more important in abstracts.", "For some symmetric discourse relations such as joint and comparison , where two connected EDUs are equally important and have interchangeable semantic roles, we follow the strategy as [@bib:li2014text] and treat the preceding EDU as the head.", "\\newline Another issue on coherence relations is about polynary relations which involve more than two EDUs.", "The first scenario is that one EDU dominates a set of posterior EDUs as its member.", "In this case, we annotate binary relations from head EDU to each member EDU with the same relation.", "The second scenario is that several EDUs are of equal importance in a polynary relation.", "For this case, we link each former EDU to its neighboring EDU with the same relation, forming a relation chain similar to \u201cright-heavy\u201d binarization transformation in [@bib:morey2017much] .", "\\newline By assuring that each EDU has one and only one head EDU, we can obtain a dependency tree for each scientific abstract.", "An example of dependency annotation is shown in Figure [@ref:LABEL:anno_example] .", "\\newline </subsection>  </section>"], ["<section> <title> 3 Corpus Construction </title>  Following the annotation framework, we collected 798 abstracts from ACL anthology and constructed the SciDTB corpus.", "The construction details are introduced as follows.", "\\newline <paragraph> <title> Annotator Recruitment </title> To select annotators, we put forward two requirements to ensure the annotation quality.", "First, we required the candidates to have linguistic knowledge.", "Second, each candidate was asked to join a test annotation of 20 abstracts, whose quality was evaluated by experts.", "After the judgement, 5 annotators were qualified to participate in our work.", "\\newline </paragraph> <paragraph> <title> EDU Segmentation </title> We performed EDU segmentation in a semi-automatic way.", "First, we did sentence tokenization on raw texts using NLTK 3.2 [@bib:bird2004nltk] .", "Then we used SPADE [@bib:soricut2003sentence] , a pre-trained EDU segmenter relying on Charniak\u2019s syntactic parser [@bib:charniak2000maximum] , to automatically cut sentences into EDUs.", "Then, we manually checked each segmented abstract to ensure the segmentation quality.", "Two annotators conducted the checking task, with one proofreading the output of SPADE, and the other reviewing the proofreading.", "The checking process was recorded for statistical analysis.", "\\newline </paragraph> <paragraph> <title> Tree Annotation </title> Labeling dependency trees was the most labor-intensive work in the corpus construction.", "798 segmented abstracts were labeled by 5 annotators in 6 months.", "506 abstracts were annotated more than twice separately by different annotators, with the purpose of analysing annotation consistency and providing human performance as an upper bound.", "The annotated trees were stored in JSON format.", "For convenience, we developed an online tool for annotating and visualising discourse dependency trees.", "\\newline </paragraph>  </section>"], ["<section> <title> 4 Corpus Statistics </title>  SciDTB contains 798 unique abstracts with 63% labeled more than once and 18,978 discourse relations in total."]], "target": "Table compares the size of SciDTB with RST-DT and another PDTB-style domain-specific corpus BioDRB , we can see SciDTB has a comparable size with RST-DT. Moreover, it is relatively easy for SciDTB to augment its size since the dependency structure simplifies the annotation to some extent. Compared with BioDRB, SciDTB has larger size and passage-level representations."}, {"tabular": ["  SBB  &  MSSA  &  $ \\tau $  &  Dice(%) ", " $ \\texttimes $  &  $ \\texttimes $  &  $ \\textunderscore $  &  72.50 ", " $ \\checkmark $  &  $ \\texttimes $  &  $ \\textunderscore $  &  73.43 ", " $ \\texttimes $  &  $ \\checkmark $  &  0.75  &  73.65 ", " $ \\checkmark $  &  $ \\checkmark $  &  0.60  &  73.90 ", " $ \\checkmark $  &  $ \\checkmark $  &  0.75  &  74.57 ", " $ \\checkmark $  &  $ \\checkmark $  &  0.90  &  74.35  "], "ref_sec": [["<section> <title> 1 Introduction </title>  Volumetric medical image segmentation, which provides the detailed pixel-wise categorization of organ regions, is critical to a series of medical analysis, e.g., lung tumour detection [@bib:chlebus2018automatic,li2018h] , gland disease classification [@bib:kirschner2012automatic,yu2017volumetric] .", "Recently, a family of deep models, including fully convolutional networks (FCNs) [@bib:long2015fully] and 3D convolutions [@bib:tran2015learning] , has been proposed to improve the segmentation accuracy, by extracting the powerful feature representation of organ regions.", "However, a lot of the progress of deep models come from the design of neural network architectures, which heavily relies on expert domain knowledge.", "\\newline Inspired by the AutoML [@bib:cai2018proxylessnas,liu2018darts] , there has been significant interest in automatically searching the neural network architecture (NAS) through the given searching space.", "The goal of NAS is to discover better neural network architectures with the higher performance, the fewer parameters, and even lower computation cost [@bib:cai2018proxylessnas,liu2018darts] .", "For medical image segmentation, Weng et al. [@bib:weng2019unet] , Kim et al. [@bib:kim2019scalable] , Bae et al. [@bib:bae2019resource] , Zhu et al. [@bib:zhu2019v] explore to search the building blocks to construct UNet [@bib:ronneberger2015u] structure in a gradient-based manner or reinforce learning methods.", "Yu et al. [@bib:yu2019c2fnas] further develop a more effective search strategy to alliterative the huge memory-cost problems caused by the 3D task.", "In the field of computer vision, Liu et al. [@bib:liu2019auto] also search resolution sampling strategy, which is a operator configuration.", "Despite the successes of these methods, the searching schemes mainly focus on {searching the effective operators in different layers} .", "However, the huge variations in abnormalities\u2019 size, shape, location in 3D medical images usually require information from multi-level feature representations for the robust and dense prediction ( {i.e.} multi-level feature aggregations).", "\\newline In literature, many previous studies have demonstrated that aggregation of multi-level features could address the issues of the huge variations in abnormalities for more accurate segmentation [@bib:ibtehaz2020multiresunet,li2018h,lin2018scn,ronneberger2015u,zhou2018unet++] .", "Intuitively, merging the high-level and low-level features extracted from different layers helps to enrich semantic representation and capture detailed information.", "For example, the latest studies [@bib:ibtehaz2020multiresunet,isensee2018nnu,li2018h,zhang2019progressively] designed to propagate coarse semantic context information back to the shallow layers through top-down and lateral skip-connections, where different layers have various size of 3D receptive field capturing the multi-scale context.", "More recently, Zhou et al. [@bib:zhou2018unet++] exploit more effectively connected architectures via deeper aggregation strategy, which iteratively and hierarchically merges features across adjacent layers, yielding better performance.", "However, all of the existing aggregation strategies under the preset are {designed manually} .", "Specifically, they perform feature fusion among adjacent or all layers based on a fixed pattern, which may miss useful or involve useless information.", "\\newline To address the above drawbacks, we advocate the idea that searching block-wise operators as well as scale-wise aggregations strategies are equally important for medical segmentation task, and investigate a novel searching method, UXNet, for more general aggregation search space as shown in Fig. [@ref:LABEL:fig:_diff_arch] .", "Concretely, during the searching process, UXNet allows each layer of the network to select optimum operation ( {e.g.} traditional convolution or dilated convolution) with a {proper receptive field} to generate better feature representations.", "As shown in Fig. [@ref:LABEL:fig:_diff_arch_multiscale] , based on the extracted multi-level feature representations, a searching strategy for multi-level feature aggregation is further conducted to discover a more efficient fusion method (i.e. which levels of the feature representations are selected to be aggregated in a specific node) for precise segmentation.", "Besides, the block-wise operators and scale-wise aggregations can be jointly searched in a differentiable manner through a continuous relaxation.", "Thus, the overall optimization process of NAS can be automatically driven by the segmentation accuracy, surpassing the methods of using a pre-fixed set of 3D receptive fields to construct the multi-scale context modelling.", "\\newline The main contributions of this paper are three folds.", "(1) We present a novel architecture searching setting: searching for the optimal multi-level feature aggregation strategy to fuse the feature maps in UNet-like architecture for 3D medical image analysis.", "(2) A novel UXNet searching scheme is proposed by leveraging block-wise operation searching, as well as scale-wise aggregation searching in a uniform framework.", "(3) Extensive experiments demonstrate that UXNet outperforms existing state-of-the-art results on most challenging semantic segmentation benchmarks on the 3D Medical Segmentation Decathlon (MSD) challenge [@bib:simpson2019large] .", "UXNet\u2019s computational complexity is cheap, such that the best-performing network can be searched less than 1.5 days on two TitanXP GPUs.", "\\newline  </section>"], ["<section> <title> 2 Methods </title>  We illustrate the proposed UNXet in Fig. [@ref:LABEL:fig:overview] .", "The network has encoder and decoder architectures, which is the same as classical UNet, along with the {Searchable Building Block} (SBB) and {Multi-Scale Searchable Aggregation} (MSSA) architecture in-between.", "The former is applied to search the optimum operations in each layer, while the later is used to determine whether or not aggregate the feature maps from various levels in each node.", "\\newline In practice, we input the volumetric image into the encoder network, producing convolution feature maps (i.e., $ N_{0,0},N_{0,1},N_{0,2},N_{0,3},N_{0,4} $ ) at different levels.", "For each levels, input feature maps are fed into a SBB, which enables a flexible combination of various convolution and pooling operators (i.e. the yellow ellipse in Fig. [@ref:LABEL:fig:overview] ), to do the transformation.", "\\newline MSSA aggregates multi-scale information for assisting the segmentation of organ regions having various sizes.", "As illustrated in Fig. [@ref:LABEL:fig:overview] , MSSA includes several stages.", "At each stage, the feature maps from all of the levels are firstly regarded as the candidates for aggregation to generate feature maps of the next stage (e.g., at the $ 0^{th} $ stage, $ N_{0,0},N_{0,1},N_{0,2},N_{0,3},N_{0,4} $ is connected to $ N_{1,1} $ at the $ 1^{st} $ stage).", "It is worthy note that the encoder network also involves candidate dense connections for feature aggregation.", "Compared with the existing approaches that adjust the weight of the connection, MSSA further enables/disables the connection based on its importance to the final recondition task.", "It facilitates a more straightforward way to guide the search of a connection between feature maps at different stages.", "Besides, thanks to SBBs that preserve useful information, MSSA can simplify the search process by eliminating the unnecessary lowest-resolution feature maps at each stage (e.g., $ N_{1,4} $ and $ N_{2,3} $ ), while yielding better segmentation result.", "\\newline <subsection> <title> 2.1 Searchable Building Block </title> As illustrated in Fig. [@ref:LABEL:fig:overview] (a), SBB takes input as a conventional feature map.", "The block has two layers of convolution/pooling operations.", "In each layer, we search for an appropriate operation from all options (e.g. traditional convolution or dilated convolution), using the appropriate receptive field to capture useful image content at different levels.", "\\newline In each SBB, there are three types of layers: (1) normal layer (i.e., 3 $ \\times $ 3 $ \\times 3 $ conv, 3 $ \\times $ 3 $ \\times $ 1 conv, 5 $ \\times $ 5 $ \\times $ 5 conv, pseudo-3d conv, 2*3 $ \\times $ 3 $ \\times $ 3 conv, 3 $ \\times $ 3 $ \\times $ 3 conv with rate 2 or 5 $ \\times $ 5 $ \\times $ 5 conv with rate 2; (2) reduction layer (i.e., max pooling, average pooling or 3 $ \\times $ 3 $ \\times $ 3 conv with stride 2; (3) expansion layer (i.e., transpose conv or trilinear interpolation).", "We define the SBB in encoder to be the combination of normal and reduction layers for yielding higher level feature maps.", "The SBB in the decoder has normal and expansion layers to recover spatial resolution of feature maps.", "\\newline Specifically, in each layer of SBB, we select from all candidate operations.", "We formulate this selection as a search process: $ y=\\sum_{i=1}^{n}\\alpha_{i}O_{i}(x) $ , where $ \\alpha_{i} $ is a learnable weight.", "Given a layer, we denote $ O $ as a set of candidate operations.", "$ x $ is the input feature map to each candidate.", "Here, the feature map $ y $ plays as a weighted summation of outputs of all candidates.", "During the network training, we optimize learnable weights by using the continuous relaxation [@bib:cai2018proxylessnas] , where the operation having the maximum weight is selected.", "\\newline </subsection> <subsection> <title> 2.2 Multi-Scale Search Aggregation </title> We propose MSSA to learn to connect/disconnect the information propagation pathways.", "As illustrated in Fig. [@ref:LABEL:fig:overview] , we search aggregations over multiple stages.", "At each stage, all levels of feature maps have searchable connections (see the feature aggregation module in Fig. [@ref:LABEL:fig:overview] (b)) with feature maps at the next stage.", "To compute the feature map $ N_{i,j} $ at the $ i^{th} $ stage of the $ j^{th} $ level, we use searchable connections to combine the multi-scale information as: \\newline <equation> $ N_{i,j}=\\begin{cases}\\sum_{k=0}^{j-1}(\\sigma(\\beta_{i,k\\rightarrow i,j})T(N_{i% ,k}))&i=0\\\\ \\sum_{k=0}^{L-1}(\\sigma(\\beta_{i-1,k\\rightarrow i,j})T(N_{i-1,k}))&i>0\\\\ \\end{cases} $ </equation> where the weight $ \\beta $ is learnable weight of each searchable connection, we use $ \\sigma $ function to map it to $ [0,1] $ , which is used to measure the importance of the propagated feature, and more larger score means the corresponding connection is more important.", "The $ i=0 $ indicates the aggregations are occurred in the encoder network, and $ L $ represents the number of candidate features in last stage.", "The $ T $ means a series of transformation to align feature maps with different level, we use a sum operator to aggregate all input features.", "Fig. [@ref:LABEL:fig:_diff_arch] shows that our multi-scale search space is able to cover existing human-designed networks for medical segmentation.", "\\newline Furthermore, we minimize the discretization gap with a simple regulation to improve the sparsity, pushing the connection weight to the extreme (i.e., 0 or 1).", "That is, we aim to either enable or disable a specific pathway.", "The regulation is formulated as: \\newline <equation> $ \\pazocal{J}=-(\\sigma(\\beta)-0.5)^{2} $ </equation> \\newline After searching the connections for aggregating multi-scale information, we prune the searchable pathway to satisfy the rules as follows (as illustrated in Figure [@ref:LABEL:fig:_diff_arch] ).", "(a) A threshold $ \\tau $ is pre-defined, when $ \\sigma(\\beta)\\geq\\tau $ , we keep the corresponding connections.", "(b) Once the aggregation node has no connection with features in the next stage, we delete the relevant connections.", "\\newline </subsection> <subsection> <title> 2.3 Network Training </title> All of the learnable weights (i.e., $ \\alpha $ in SBB and $ \\beta $ in MSSA) can be optimized efficiently using gradient descent.", "To further avoid overfitting the training data, we follow the bi-level optimization policy in [@bib:cai2018proxylessnas,liu2018darts] .", "That is, we divide the training data into two sets $ trainA $ and $ trainB $ , which are used to train the set of operation weights $ w $ (e.g., convolutional layers) and the set of connection weights $ \\alpha,\\beta $ , respectively.", "We solve the objective $ \\pazocal{L}_{trainA}(w,\\alpha,\\beta) $ for $ w $ , by fixing the connection weights.", "Then we fix the operation weights and solve the objective $ \\pazocal{L}_{trainB}(w,\\alpha,\\beta) $ and $ \\pazocal{J}_{trainB}(\\beta) $ for the connection weights.", "Here, $ \\pazocal{L}_{trainA}(w,\\alpha,\\beta) $ represents the cross-entropy loss.", "$ \\pazocal{L}_{trainB}(w,\\alpha,\\beta) $ and $ \\pazocal{J}_{trainB}(\\beta) $ denotes the L2-regulation (see Eqn. [@ref:LABEL:eq3] ).", "\\newline </subsection>  </section>"], ["<section> <title> 3 Experiments </title>  <subsection> <title> 3.1 Datasets and Settings </title> Following previous NAS-based methods, we evaluate UXNet on three subset of 3D Medical Segmentation Decathlon (MSD) challeng [@bib:simpson2019large] (i.e. the brain, heart, and prostate), which contains 484, 20, 32 cases for training respectively.", "We adopt the same image pre-processing strategy in [@bib:isensee2018nnu] .", "Since the annotation of test datasets are not publicly available, we report the 5-fold cross-validation results as in [@bib:bae2019resource,isensee2018nnu,kim2019scalable] .", "We also report the validation results on the 2D lesion segmentation dataset released by the Skin Lesion Segmentation and Classification 2018 challenge [@bib:hardie2018skin] , which provides 2594 training images.", "We publish all results in terms of the dice coefficient and the higher score indicates the better result.", "\\newline When learning network weights $ w $ in Sec. [@ref:LABEL:sec:network_training] , we use Adam optimizer with an initial learning rate of 0.0003, and the betas range from 0.9 to 0.99.", "The initial values of $ \\alpha $ and $ \\beta $ are set as 1 and 0.", "They are optimized using Adam optimizer with a learning rate of 0.003 and weight.", "When we finish searching and pruning the network, we retrain the derived network from scratch.", "The computation of UXNet is cheap by training 1.5 days on two TitanXP GPUs for brain task, which is cheaper than RONASMIS [@bib:bae2019resource] that trained 3.1 days on one RTX 2080Ti GPU and SCNAS [@bib:kim2019scalable] that trained one day on four V100 GPUs.", "Please refer to appendix for more training details of each dataset.", "\\newline </subsection> <subsection> <title> 3.2 Ablation Studies </title> We first conduct the ablation studies on the brain dataset of MSD challenge by removing the critical component of UXNet, i,e., SBB and MSSA.", "In such case, the model degrades to the original UNnet and achieves the score of $ 72.5 $ %.", "It lags far behind our full model in terms of segmentation performance.", "As shown in Table [@ref:LABEL:tab:ablation_study] , by adding the SBBs, the performance is promoted to $ 73.43 $ %.", "It demonstrates that the SBB enables each layer of the network to produce rich presentation via optimum operation.", "Note that we adopt the UNet-Style connections to fuse the features in encoder and decoder network in this case.", "When the MSSA is enabled, the discovered optimal feature aggregation strategy could further improve the score to $ 74.57 $ %.", "\\newline We further evaluate the influence of hyper-parameter $ \\tau $ in Sec. [@ref:LABEL:sec:MSSA] .", "The higher value indicates the more sparse aggregations."]], "target": "Table. shows the results of using different $ \\tau $ to prune the over-connections. we find that using a too low or high threshold to clip the connections will decrease the model\u2019s performance. It indirectly illustrates that involving useless or missing useful information will lead to the poor performance."}, {"tabular": ["  Delay Bound  &  5  &  10  &  15  &  20  &  25  &  30  &  35 ", " No. Sensors  &    &    &    &    &    &    &  ", " 5  &  0  &  1  &  1  &  1  &  1  &  1  &  1 ", " 10  &  0  &  3  &  2  &  1  &  1  &  1  &  1 ", " 15  &  0  &  4  &  2  &  2  &  1  &  1  &  1 ", " 20  &  0  &  6  &  3  &  2  &  1  &  1  &  1 ", " 25  &  0  &  8  &  4  &  3  &  2  &  2  &  2 ", " 30  &  0  &  9  &  4  &  3  &  2  &  2  &  2 ", " 35  &  0  &  11  &  5  &  4  &  3  &  2  &  2 ", " 40  &  0  &  13  &  6  &  4  &  3  &  3  &  2  "], "ref_sec": [["<section> <title> I Introduction </title>  In the last ten years, the yearly productivity in U.S. grew $ 42\\% $ percent without any change in total hours worked in a year [@bib:ford2015rise] .", "This has been enabled through education and automation.", "Automation is based on sensors for gathering information which needs a communication infrastructure.", "Machine to machine (M2M) communications is becoming a possible solution through cheap hardware.", "However, the scale of the deployed sensors is beyond the capacity of the current wireless networks [@bib:hasan2013random] .", "\\newline The scaling problem is dubbed in the current research as massive random access problem.", "The reliability aspect of the sensor communication which is reflected in the 5G research as the Ultra Reliable Low Latency Communications (URLLC).", "Reliability is defined as the percentage of the devices that obtain the requested service.", "Part of the research is focusing on solving the random access challenge reliably [@bib:lien2012cooperative] within a bounded delay, resulting in a massive reliable random access problem.", "Both problems combined can be reformulated in resource management terms as delay guarantees for massive number of users.", "\\newline The performance analysis for random access is built on certain assumptions about arrival traffic.", "This constrains the analytical guarantees such that they are only valid with the assumed traffic distribution.", "Exposed to a different traffic distribution, the system maybe considered instable in terms of a performance metric e.g., delay.", "\\newline In order to provide traffic agnostic guarantees the protocols should be able to react to different arrival distribution.", "We can explore the reactivity of a protocol against a chosen metric such as stability.", "Tree algorithms for instance have a stable throughput with respect to various arrival traffic.", "However, the same cannot be said for delay such that it scales linearly with the number of users added to the system.", "High delay is not tolerable for some applications, and should be bounded.", "Protocols that solve such a bounded delay problem are required.", "\\newline In this work we present Admission Control based Traffic Agnostic Delay Constrained Random Access (AC/DC-RA) protocol, that provides stochastic delay bounds.", "The Traffic Agnosticism is achieved via separation of the backlog and initial access.", "Stochastic Delay Constraint is enabled through an Admission Control decision that is based on a novel collision multiplicity estimation algorithm.", "Our contributions are four-fold: \\newline <list> \\ We use a novel admission control decision, that takes place before the contention resolution (Sec. [@ref:LABEL:sec:admission] ).", "This enables guarantees for traffic agnostic stochastic delay constraints for random access.", "\\newline \\ \\ A novel scalable collision multiplicity estimator is provided that is based on the famous Coupon Collector\u2019s Problem (Sec. [@ref:LABEL:sec:esti] ).", "\\newline \\ \\ We make use of a Parallel Multi-Channel Tree Resolution that re-arranges exploration of the contention slots in order to achieve stochastic delay bounds (Sec. [@ref:LABEL:sec:mpcta] ).", "\\newline \\ \\ We provide a dimensioning model for the suggested AC/DC-RA protocol which provides optimized use of system resources (Sec. [@ref:LABEL:sec:analysis] ).", "\\newline \\ </list> \\newline <subsection> <title> I-A Notation </title> The sets are denoted with calligraphic capital letters $ \\mathcal{A} $ .", "Sequences are denoted with bold lower-case letters $ \\mathbf{a} $ .", "Sequences of sequences are denoted with bold upper-case letters $ \\mathbf{A} $ .", "$ \\text{E}[.] $ is used for expectation and $ e $ denotes the natural exponent.", "$ \\hat{(.)} $ is used for the estimated quantities.", "\\newline </subsection>  </section>"], ["<section> <title> II System Model </title>  The topology is star with a central station.", "The traffic is assumed to be uplink only.", "There are $ N_{max} $ total users and $ N_{t} $ active users at a time-instance $ t $ .", "The traffic of the uplink communication is sporadic, thus the set of active users is unknown to the receiver.", "We base our resource model on a two dimensional grid like in an OFDMA system where one dimension is frequency and the other is time.", "We define, each one of the cells, as a resource.", "The resources are used on a contention basis.", "Due to the broadcast nature of the wireless communication each sensor can access the same resource at the same time and interfere with each other.", "This phenomena is called a collision.", "This behaviour can be abstracted with a model.", "We use the collision channel model i.e., resources have 3 distinct states, idle (0, no request), singleton (1, 1 request) or a collision ( $ e $ , $ > $ 1 requests).", "Unless physical layer enhancements are assumed the central entity cannot differentiate two or more users and treat them equally.", "We assume no capture or interference cancellation capability.", "We also assume an instant and costless feedback.", "Implementation of such feedback channels is discussed in previous work [@bib:gursuslotted] .", "We will use the term backlogged user for the collided users and initial arrival for the first attempt.", "\\newline We define Quality of Service (QoS) as the reliability ( $ R $ ) that a packet is received at the destination within a certain delay constraint ( $ L $ ) after it is generated.", "We denote a set of sensors that have the same QoS requirement as class $ j $ and its reliability requirement as $ R_{j} $ and delay constraint as $ L_{j} $ .", "The delay $ L $ incorporates delay stemming from re-transmissions due to collisions and reflect the performance of the random access channel.", "Any delay stemming from channel fading is not considered in this paper and only a radio resource perspective is evaluated.", "\\newline We will use the term outer protocol for the traffic shaping part of the protocol that is achieved via the admission control and the term inner protocol for the contention resolution part.", "\\newline <subsection> <title> II-A Proposal </title> We propose an outer protocol that separates the initial arrivals from backlogged users. And an inner protocol that resolves each set of backlogged users in an isolated manner.", "The outer protocol is used for initial arrivals only.", "Users may collide through the use of the outer protocol.", "An admission decision is given for the collided users through the outer protocol.", "If admitted, the collided users access the inner protocol.", "The outer protocol uses an Admission Channel (AC) and the inner protocol uses a Resolution Channel (RC).", "\\newline The admission is based on the stochastic delay constraint of the user, the collision multiplicity and the available capacity of the resolution channel.", "In the following, we explain in detail how this decision is taken.", "The admission decision is only for the resolution resources i.e., contention resources.", "We do not consider a contention free resource admission scenario and it is left for future work.", "\\newline The set of resources $ \\mathcal{M}_{AC} $ and $ \\mathcal{M}_{RC} $ form Admission Channel (AC) and Resolution Channel (RC) respectively where $ {M}_{AC}+{M}_{RC}={M} $ being the total number of resources with $ {M}_{x}=|\\mathcal{M}_{x}| $ denoting the cardinality of the set.", "There may be multiple admission channels with respect to each QoS class $ j $ denoted as $ \\mathcal{M}_{AC_{j}} $ and $ \\sum{M}_{AC_{j}}={M}_{AC} $ .", "\\newline This protocol can be summarized with a flow diagram as given in Fig. [@ref:LABEL:fig:flow_diagram] .", "When an event notification is received, the user is activated and starts using the outer protocol.", "It selects the admission channel that is appropriate for the QoS class.", "There are more than one admission channel so that the system can infer the QoS from the channel.", "Then it selects one of the resources in that channel.", "This selection is done with pre-set probabilities known to the user.", "It transmits the packet using that resource.", "This terminates the outer protocol.", "The outcome of the transmission can be a success or a collision.", "The central entity observes the outcome for that resource.", "If it is a success, the user is informed via a broadcast and it goes back to sleep mode.", "If a collision occurred, then an admission control decision is taken for that resource by the central entity.", "All users that have used that resource are either rejected or admitted and informed via a broadcast feedback.", "In case of a rejection, a user may have another radio interface. Or the sensor can report the failure to higher layers and trigger higher layer solutions e.g. switch to local control.", "In case of an admission, the inner protocol is initiated.", "The inner protocol used is a binary tree algorithm such that after each collision users have to re-select one of two new resources.", "The users are informed about the resources via a broadcast feedback.", "The feedback and the allocation method of these resources guarantee that all admitted users are successfully resolved by the inner protocol before the delay constraint.", "\\newline An example for the resource allocation is illustrated in Fig. [@ref:LABEL:fig:srawg] .", "The illustration shows the allocation of Admission Channel and Resolution Channel resources on the resource grid, where the horizontal axis represents time and the vertical axis represents frequencies.", "Resource use is colored in pink for initial access and in gray for backlogged access.", "The boxes depict the resources allocated to the respective resolution in RC.", "The rejection of an initial access is depicted with a red cross.", "For clarification of the example ternary outcome $ (0,1,e) $ of the resource use is illustrated with different symbols.", "A user that requires to report a fire within $ 100 $ ms with $ 0.99 $ reliability selects the admission channel $ 2 $ that represents its QoS requirement in this case.", "It transmits the data packet on frequency two at time-slot 1 which is a resource part of that Admission Channel.", "The outcome is a collision as other users have selected the same resource.", "Then the number of users that accessed this resource is estimated.", "The admission control decides that the resolution is possible within the delay constraint ( $ 100 $ ms) with the given reliability.", "The delay constraint is represented with 6 time-slots in the example.", "The admission control calculates the number of frequencies as $ 2 $ needed for parallelization.", "Then it checks if it is possible to allocate $ 2 $ frequencies in the resolution channel for that resolution.", "As there is available capacity in RC, the user in our example and all the other users that have collided with it are admitted to RC and resolved with a tree resolution.", "The allocated resource grid for the resolution is illustrated as a gray box limited with the lines.", "Within this gray box the collided users make a random selection on each time-slot bound to frequency 4 and 5.", "On time-slot 2, three of the users have selected the frequency 4 while one user have selected the frequency 5.", "In this case outcome on frequency 4 is a collision and on frequency 5 is a success.", "The collided users re-select one of the frequencies randomly again on the time-slot 3.", "This time one user has selected the frequency 4 and two users have selected the frequency 5.", "This results in another success.", "Two of the users still need to be resolved.", "Thus, process continues until time-slot $ 6 $ where both of the users have selected their own resource.", "The resolution is completed before the delay constraint as guaranteed by the admission decision.", "In the meanwhile another sensor is rejected at time-slot $ 3 $ since required capacity is not available in RC.", "\\newline </subsection>  </section>"], ["<section> <title> III AC/DC-RA - Outer Protocol </title>  The outer protocol is used for the initial access of the devices.", "We do not use any collision avoidance mechanism to avoid delay before any user can reach the system.", "\\newline Our proposal is based on two design choices.", "First, there are multiple Admission Channels and the user should select the one that is appropriate for the Quality of Service class .", "Second, we customize the resource selection probabilities within any of the Admission Channels to enable collision multiplicity estimation for arbitrary number of active sensors.", "Lastly, the Outer Protocol is terminated when this information is transferred to the admission control which is the gateway between two sub-protocols.", "\\newline <subsection> <title> III-A Separate Admission Channels - QoS Information </title> We assume that all the devices have gone through an initial connection establishment or have overheard a broadcast.", "Through this information exchange, each device is aware of the appropriate admission channel for the required QoS. \\newline There are multiple ACs for the initial access for different QoS classes, such that all the users in the same AC require the same delay bound and reliability.", "The AC is a set of resources $ \\mathcal{M}_{{AC}_{j}} $ e.g. for QoS class $ j $ .", "Sum of all the resources orthogonal to time in the admission channels results in cardinality of the admission channels $ {M}_{AC} $ .", "As detailed in Section [@ref:LABEL:Sec:sys] a resource represents a single cell in the resource grid.", "\\newline We assume that a slot size is fixed and the bandwidth of a slot matches the payload size for each specific class.", "As each class is using a fixed AC, it can be expected that each AC has a unique bandwidth matching the payload size.", "Different classes can co-exist as different slices in the same resource grid for heterogeneous bandwidths.", "The possibility of this approach is investigated in 5G standardization under the bandwidth parts topic [@bib:jeon2018nr] .", "Bandwidth parts enable co-existence of different payloads through adjusting the bandwidth of a slot.", "For the rest of the paper we assume homogeneous payload size among different classes and the effects of heterogeneous bandwidths are not investigated.", "\\newline </subsection> <subsection> <title> III-B Resource Selection Probabilities - Collision Size Estimator </title> We assume that a set of sensors of size $ N_{t} $ at time instant $ t $ , selects randomly one resource from a set of resources in the admission channel at the same time.", "Depending on this selection a sensor may collide or be successful.", "Also some resources maybe unoccupied.", "The central entity can only observe the ternary outcome (0, 1, e) of these resources.", "From this outcome it has to make a collision size guess.", "\\newline A similar estimation problem has already been investigated in the state of the art for RFID tag readings [@bib:kodialam2006fast] for throughput optimization.", "However, the estimation time scales at best linearly with the number of sensors $ N_{t} $ .", "However, the work relies on Poisson approximation that is valid only with high number of resources.", "Usually, such resources are scarce and costly in terms of delay.", "To solve this problem, another work has considered the resource selection probabilities as a design parameter trading off precision for estimation speed [@bib:stefanovic2013joint] .", "Here, we aim at generalizing such an estimation to any number of active sensors and map it to the well-known Coupon Collector\u2019s Problem (CCP).", "\\newline <subsubsection> <title> III-B1 Coupon Collector\u2019s Problem </title> There are $ M $ unique coupons that are obtained through independent draws from an urn with replacement.", "The problem is to find the expected number of draws until all $ M $ coupons are collected.", "Coupons may have equal or unequal selection probabilities.", "We will refer to selection probability of the $ i^{\\text{th}} $ coupon as $ p_{i} $ such that, \\newline <equation> $ 1=\\sum_{i=1}^{M}p_{i}. $ </equation> \\newline This problem is solved for equal and unequal coupon selection probabilities [@bib:adler2003coupon] .", "We do not focus on expected number of draws until all $ M $ coupons are collected, but we will focus on the expected number of draws given a certain set of uniquely drawn coupons $ \\mathcal{M}_{s+c} $ .", "Thus, we are guessing the expected number of draws that have been made given that a certain set of unique collected coupons.", "\\newline </subsubsection> <subsubsection> <title> III-B2 Analogy to Collision Size Estimation </title> We define a contention in a single time-slot $ t $ as an experiment.", "Suppose there are $ N_{t} $ sensors selecting $ M $ resources randomly on a contention basis at time-slot instance $ t $ .", "We observe the outcome of the contention on these $ M $ resources.", "We define the outcome on a contention resource $ i $ as $ o_{i} $ where a sequence of outcome is $ \\mathbf{o}=(o_{1},o_{2},o_{3},o_{4})=(1,0,e,1) $ for an example with $ M=4 $ .", "The ternary outcome $ o_{i}\\in\\{0,1,e\\} $ of the contention for resource $ i $ is converted to the set of coupons collected.", "We consider idle resources as not-selected coupons, i.e., the $ \\mathcal{M}_{s+c} $ can be defined as, \\newline <equation> $ i\\begin{cases}\\in\\mathcal{M}_{s+c}\\text{if }o_{i}\\neq 0\\\\ \\notin\\mathcal{M}_{s+c}\\text{if }o_{i}=0.\\end{cases} $ </equation> Using this set we calculate the expected number of draws $ E[Z] $ , corresponding to the estimated number of sensors at time-slot $ t $ $ \\hat{N}_{t} $ .", "Then, the set of selected coupons can be written as $ \\mathcal{M}_{s+c}=\\{1,3,4\\} $ since resource $ 2 $ is idle.", "Using the probability of selecting any of the $ M $ resources.", "\\newline The estimated number of active sensors $ \\hat{N}_{t} $ is given with expected number of draws given a set of uniquely drawn coupons with unequal probabilities \\newline <equation> $ \\hat{N}_{t}=\\operatorname*{E}\\left[Z|\\mathcal{M}_{s+c}\\right]=\\sum\\limits_{z=0% }^{\\infty}\\left(1-\\prod\\limits_{i\\in\\mathcal{M}_{s+c}}(1-e^{-p_{i}z})\\right), $ </equation> where the probability that a sensor did not select a resource $ i $ is multiplied for each resource for $ z $ sensors.", "Then this is subtracted from one to calculate the probability that all of these resources are selected at least once.", "Then the expectation is taken over $ z $ .", "The sum is up to infinity to calculate the probability of an outcome given there are up to infinite sensors.", "For large enough $ z $ , probability that a resource is not selected converges to $ 0 $ .", "So Eq. ( [@ref:LABEL:eqn:ccp_ue] ) gives us the expected number of sensors given the outcome.", "Further explanation for Eq. ( [@ref:LABEL:eqn:ccp_ue] ) is given in App.", "[@ref:LABEL:app:proof1] .", "\\newline It is clear that each different selection $ \\mathcal{M}_{s+c} $ may give a different result in terms of number of sensors.", "We define the highest expected number of sensors as $ \\operatorname*{E}\\left[Z|\\mathcal{M}_{s+c}=\\mathcal{M}\\right]=\\hat{N}_{max} $ for, $ \\mathcal{M} $ , the outcome of the complete set, i.e., $ \\forall\\,\\,i\\,\\,o_{i}\\neq 0 $ , where we have a collision or success on all resources.", "The estimation range for the number of active sensors $ N_{t} $ is up to $ \\hat{N}_{max} $ .", "Therefore, the resource selection probabilities $ p_{i} $ should be adjusted, such that $ \\hat{N}_{max} $ is larger than the worst case number of sensors.", "On the other hand it is intuitively clear that adjusting $ p_{i} $ to increase $ \\hat{N}_{max} $ results in further decrease in precision of the estimation.", "Otherwise we can decrease $ p_{i} $ to increase $ N_{max} $ to infinity.", "\\newline In Table.", "[@ref:LABEL:fig:mean_plot_ccp] we have summarized $ \\hat{N}_{max} $ with different distributions of $ p_{i} $ .", "We have used the constraint in Eq. ( [@ref:LABEL:eq:ccp_const] ) in order to calculate $ p_{i} $ for various distributions.", "The $ p_{i} $ for each distribution is as follows: (1) for geometric distribution with a fixed $ p $ we set the selection probability as $ p_{i}=(1-p)^{i}\\cdot p $ , (2) for Poisson distribution with a mean $ \\lambda $ we set the selection probability as $ p_{i}=\\frac{\\lambda^{i}e^{-\\lambda}}{i!} $ , (3) for power series, defined the selection probability as $ p_{i}=p^{0}\\cdot\\alpha^{i} $ .", "We have to set $ p^{0} $ and adjust $ \\alpha $ accordingly.", "We then used the Eq. ( [@ref:LABEL:eqn:ccp_ue] ) to calculate $ \\hat{N}_{max} $ .", "In Table. [@ref:LABEL:fig:mean_plot_ccp] we see that $ p^{0}\\approx\\frac{1}{N_{max}} $ .", "Thus, using the power series we can easily adjust the estimation.", "\\newline </subsubsection> <subsubsection> <title> III-B3 Collision Size Estimation </title> After we have the estimated number of active devices $ \\hat{N}_{t} $ , we will use the maximum likelihood to partition these devices into each resource.", "In the following parts we will use $ N_{t} $ instead of $ \\hat{N}_{t} $ for ease of reading.", "\\newline The problem is now to partition $ N_{t} $ devices to $ M $ bins.", "The partitioning is constrained with the outcome $ \\mathbf{o} $ , i.e., collision on resource $ 2 $ and success on resource $ 5 $ translates in to $ o_{2}=e,o_{5}=1 $ .", "Possible guesses $ \\mathbf{g} $ will be sequences that fulfills the outcome constraints.", "The guess of resource $ i $ in the $ x^{\\text{th}} $ sequence is $ g_{i}^{x} $ .", "We also use $ g_{i} $ for a guess for resource $ i $ , and $ \\mathbf{g^{x}} $ as the guess sequence $ x $ .", "Now we can write the constraints \\newline <equationgroup> <equation> $  g_{i}\\begin{cases}\\hidden@noalign\\textstyle=0&\\text{if }o_{i}=% 0\\\\ \\hidden@noalign\\textstyle=1&\\text{if }o_{i}=1\\\\ \\hidden@noalign\\textstyle\\geq 2\\text{, }\\leq\\left(N_{t}-\\sum_{j=1}^{i-1}g_{% j}\\right)&\\text{if }o_{i}=e.\\end{cases} $ $  g_{i}\\begin{cases}\\hidden@noalign\\textstyle=0&\\text{if }o_{i}=% 0\\\\ \\hidden@noalign\\textstyle=1&\\text{if }o_{i}=1\\\\ \\hidden@noalign\\textstyle\\geq 2\\text{, }\\leq\\left(N_{t}-\\sum_{j=1}^{i-1}g_{% j}\\right)&\\text{if }o_{i}=e.\\end{cases} $ </equation> </equationgroup> \\newline We define the guess set $ \\mathcal{G} $ such that it involves all guess sequences fulfilling a given outcome sequence $ \\mathbf{o} $ and the number of active devices $ N_{t} $ .", "For example, with $ M=3 $ and a outcome sequence of $ \\mathbf{o}=(o_{1}=1,o_{2}=e,o_{3}=e) $ where we have $ N_{t}=7 $ we will have $ \\mathcal{G}=\\left((1,2,4),(1,3,3),(1,4,2)\\right)=\\{\\mathbf{g^{1}},\\mathbf{g^{2% }},\\mathbf{g^{3}}\\} $ , such that $ g^{2}_{3}=3 $ and $ \\mathbf{g^{2}}=\\{1,3,3\\} $ .", "\\newline We can calculate the probability of a guess as in \\newline <equation> $ P_{\\mathbf{g}}=\\prod_{i\\in\\mathcal{M}}\\left({{N_{t}-\\sum_{j=1}^{i-1}g_{j}}% \\choose{g_{i}}}\\left(p_{i}\\right)^{g_{i}}\\right).", "$ </equation> \\newline This will enable calculation of the most likely partition, to have an estimate on how many sensors are on each resource as \\newline <equation> $ \\hat{\\mathbf{u}}=\\arg\\max_{\\mathbf{g}}P_{\\mathbf{g}},\\forall\\,\\,\\mathbf{g}\\in% \\mathcal{G}, $ </equation> where $ \\hat{\\mathbf{u}} $ is the sequence for the collisions size estimation for all resources.", "The equation is complex to calculate with increasing dimensions of $ \\mathbf{g} $ as it is a combinatorial maximum likelihood calculation.", "It depends on $ N_{max} $ and cardinality $ |\\mathbf{g}| $ such that $ {N_{max}}^{|\\mathbf{g}|} $ cases may be evaluated depending on the feedback.", "For practical implementations a heuristic estimator can be used an example is as such \\newline <equation> $ \\hat{u_{i}}=\\begin{cases}\\lceil p_{i}\\cdot\\hat{N}\\rceil&o_{i}=e\\\\ o_{i}&o_{i}\\neq e\\end{cases}, $ </equation> where $ \\hat{N} $ is the total backlog estimation given by Eq. (3) that uses the outcome sequence $ \\mathbf{o} $ and $ N_{max} $ as input \\newline </subsubsection> <subsubsection> <title> III-B4 Comparison </title> As a comparison for our estimation technique, we choose two maximum-likelihood estimators (MLE).", "First one is based on the observation of non-idle resources only $ M_{s+c}\\triangleq\\sum_{i=1}^{M}\\mathbbm{1}_{o_{i}\\geq 1} $ (that is, without knowledge of the number of idle resources), where $ \\mathbbm{1} $ is the indicator function .", "The MLE operates on the following exact probability of observing $ M_{s+c} $ non-idle resources, given a total of $ M $ resources and a total of $ N_{t} $ sensors: \\newline <equationgroup> <equation> $  P_{\\text{MLE}}[M_{s+c}|M,N_{t}]=\\frac{\\genfrac{\\{\\}}{0.0pt}{% N_{t}}{M_{s+c}}M!}{M^{N_{t}}(M-M_{s+c})!}, $ $  P_{\\text{MLE}}[M_{s+c}|M,N_{t}] $ $ = $ $ \\frac{\\genfrac{\\{\\}}{0.0pt}{N_{t}}{M_{s+c}}M!}{M^{N_{t}}(M-M_% {s+c})!}, $ </equation> <equation> $ \\hat{N}_{t}=\\arg\\max_{N_{t}}P_{\\text{MLE}}[M_{s+c}|M,N_{t}] $ $ \\hat{N}_{t} $ $ = $ $ \\arg\\max_{N_{t}}P_{\\text{MLE}}[M_{s+c}|M,N_{t}] $ </equation> </equationgroup> where $ \\genfrac{\\{\\}}{0.0pt}{N_{t}}{M_{x}} $ are the Stirling number of the second kind.", "\\newline Second comparative technique is adaptation of the work from Zanella [@bib:6134704] on the RFID collision set estimation.", "The work is based on observing the number of collided $ M_{c} $ and successful $ M_{s} $ resources, and, using the approximation of the exact expression, computes the maximum-likelihood $ N_{t} $ by finding the roots of the expression, i.e. finding the number of resources that maximizes the idle likelihood while minimizing the collision likelihood as in: \\newline <equation> $ \\frac{N_{t}-M_{s}}{M_{c}}=\\frac{\\frac{N_{t}}{M}(e^{\\frac{N_{t}}{M}}-1)}{e^{% \\frac{N_{t}}{M}}-1-\\frac{N_{t}}{M}}. $ </equation> \\newline The average collision size is then computed from $ \\hat{N_{t}} $ as in $ \\frac{\\hat{N_{t}}-M_{s}}{M_{c}} $ .", "It has to be noted that, since neither of MLE approaches vary the resource selection probabilities (i.e., both use uniform probabilities), none of them can give a reliable estimate above a certain total number of active devices $ N_{max} $ , i.e., whenever $ M_{c}=M $ is observed.", "\\newline We have conducted Monte Carlo simulations in MATLAB for comparing the estimators.", "The resource selection probabilities are set with respect to power distribution calculated in section [@ref:LABEL:sec:esti] for CCP and $ N_{max} $ values are set as $ 500 $ , $ 1000 $ and $ 2000 $ .", "The resource selection probabilities are set uniformly for the baseline case.", "The reason for this selection is that the state of the art uses the Poissonization of the outcomes which is a valid approach only with equal resource selection probabilities.", "In Fig. [@ref:LABEL:fig:collisionsize] collision size estimation error is plotted with 18 resources $ M $ .", "The absolute estimation error is calculated as $ |\\hat{u_{i}}-u_{i}| $ taking the difference between estimated and actual number of users per resource $ i $ , which is then averaged as in $ \\operatorname*{E}[|\\hat{u_{i}}-u_{i}|] $ over multiple runs and multiple resources.", "CCP is compared against the state of the art with varying the number of active devices from $ 1 $ to $ 1000 $ .", "Each number of active users are simulated for 1000 runs.", "The limitation of uniform resource selection is observed from the results.", "The MLE estimator saturates with $ M=18 $ after 100 users since the observation is always a set of collisions when the resource blocks have equal probability to be accessed.", "Thus, the MLE estimates 100 users with full collision set and the error linearly grows with the number of active users.", "In CCP, with increasing number of users an idle occurs and this enables scalability up to $ N_{max} $ active users.", "\\newline We have also evaluated an error in the setting of $ N_{max} $ and how such a wrong setting will affect the system in Fig. [@ref:LABEL:fig:collisionsize] .", "The $ N_{max} $ set to $ 500 $ represents the case where we may have more users accessing the medium than the allowed maximum.", "We see that the absolute estimation errors are almost the same up to $ 500 $ active users.", "After this point the estimation error grows linearly with increasing number of users similar to the state of the art.", "On the other hand the case where $ N_{max} $ is set to $ 2000 $ represents that we always have a higher limit for maximum number of users compared to active number of users.", "This has a less critical effect compared to setting a lower maximum limit.", "This can be observed in Fig. [@ref:LABEL:fig:collisionsize] where the absolute error has increased slightly but is in general lower compared to the previous case.", "Thus, it can be concluded that a relatively high $ N_{max} $ can be selected to avoid the saturation effect.", "\\newline The scalability comes with the cost of precision loss with low number of active users.", "Even though, the mean error difference is approximately 1 user up to 200 active users, the state of the art is better than the CCP.", "This is due to setting the unequal access probabilities for scalability that is enforced due to limited amount of estimation resources.", "\\newline The precision of the estimation is evaluated on average.", "Thus, the strictness of the stochastic delay constraints provided through the use of the estimator is valid on a set of realizations, but not for each realization of the random process.", "Also, the stochastic delay constraint would be valid if the number of arriving users is upper-bound so the exact estimation can be converted to an upper-bound for reliability.", "We enable this via adding the mean estimation error $ \\operatorname*{E}[|\\hat{u_{i}}-u_{i}|] $ from the analysis as a pessimism factor on top of the collision multiplicity estimate.", "This makes sure that the stochastic delay constraint is not violated due to estimation error.", "We have evaluated the results for the guarantees where the estimator is integrated in the system in Sec. [@ref:LABEL:sec:eval] .", "\\newline The outcome of the estimation and the QoS requirement is obtained from the initial access of the sensors to the admission channel.", "Given these information the delay of the contention tree resolution can be obtained through stochastic analysis.", "This information enables the admission control decision.", "In the following section we investigate the stochastic delay analysis of the inner protocol.", "\\newline </subsubsection> </subsection>  </section>"], ["<section> <title> IV AC/DC-RA - Inner Protocol </title>  In this section, we first introduce the inner protocol and quickly move on to the investigation of the stochastic analysis for delay constraints.", "\\newline We deploy a version of binary tree resolution algorithm for isolated resolution of each contention.", "Instead of a distributed decision as usually the case for tree resolution we assume a centralized decision.", "In a distributed version, users select the contention resource with respect to the outcome of other contentions, i.e., with respect to the feedback.", "A central decision can allocate the respective resolution slot such that the user does not have to monitor the feedback continuously.", "The contention goes on until all users are resolved.", "Such a central decision requires breadth-first exploration of the tree.", "The number of required resources for a depth-first exploration is unbounded while for breadth-first it is deterministic and number of resources are exponential $ 2^{m} $ with tree level $ m $ .", "Another advantage of breadth-first is a possible exploration of multiple contention slots simultaneously if parallel resources exist orthogonal to time, i.e., multiple frequencies.", "We call this parallelization of the resolution and $ M_{P} $ denotes the number of parallel allocated resource for a resolution.", "An example with two possible tree algorithm parallelizations is given in Fig. [@ref:LABEL:fig:parallelization] .", "The resolution starts with $ 8 $ users and with the first split $ 3 $ users select one resolution slot while the remaining $ 5 $ select the other resolution slot.", "The users are resolved with a parallelization of 2 and 4.", "In the case of parallelization of $ M_{P}=2 $ the resolution needs a capacity of $ 2 $ frequencies for a duration of 4 time-slots to schedule all resolution slots.", "However, with a parallelization of $ M_{P}=4 $ the resolution needs a capacity of $ 4 $ frequencies for a duration of 3 time-slots.", "Thus, required capacity increases since higher amount of parallel resources are blocked for faster resolution.", "\\newline Stochastic delay analysis for tree algorithms that use no parallelization can be found in [@bib:molle1993conflict] .", "A parallelization of $ Q $ , the branch size, is investigated in [@bib:janssen2000analysis] .", "A parallelization of $ M_{P} $ , an arbitrary factor, is investigated in [@bib:gursu_mpcta] .", "Multichannel Parallel - Contention Tree Algorithm (MP-CTA) [@bib:gursu_mpcta] provides analytic results for breadth-first parallelized explorations of the tree.", "The advantage of MP-CTA protocol compared to [@bib:janssen2000analysis] is the ability to keep the throughput constant while increasing the parallelization as [@bib:janssen2000analysis] sacrifices throughput for parallelization.", "The delay analysis is based on parallelization of $ M_{P} $ and it enables an efficient resolution mode selection for the required delay constraint.", "In our analysis the $ M_{P} $ will map to parallel resources in the same time-slot i.e., with $ M_{RC} $ resource in resolution channel we have a maximum possible parallelization of $ M_{P}=M_{RC} $ .", "\\newline <subsection> <title> IV-A Delay Constrained Resolution </title> In this section, we investigate how the analysis for the inner protocol can be used for the admission control.", "\\newline For a stochastic delay constraint $ L $ and reliability $ R $ , e.g., $ R=0.95 $ , means that the delay constraint $ L $ should be achieved $ 95 $ percent of the time.", "\\newline Stochastic delay bounds for MP-CTA are given in [@bib:gursu_mpcta] for different number of sensors.", "These values can be placed in a look up table (LUT) for varying $ N $ number of sensors, $ L $ the delay, for a specific reliability $ R $ as in Tab."]], "target": "The LUT then outputs the minimum number of parallelization $ M_{P} $ required to fulfill the stochastic delay constraint of all the devices in the contention resolution. If it is infeasible then it returns zero. For example, given 10 devices and a delay of 5 slots, it is infeasible to achieve a resolution where all devices are resolved with $ 0.95 $ reliability. This is denoted as $ M_{P}=0 $ . However, a delay of 10 slots is achievable with a parallelization of $ M_{P}=3 $ ."}, {"tabular": ["  Generative Model  &  IS  &  SSIM ", " SAGE (ours)  &  1.71  &  0.68 ", " WGAN-GP  &  1.35  &  0.57 ", " DC-GAN  &  1.12  &  0.32  "], "ref_sec": [["<section> <title> I Introduction </title>  With recent attempts towards Assistive AI and Computer-Aided Diagnosis in the medical world, a common problem encountered is the lack of curated data to train the networks on.", "Generative Adversarial Networks (GANs) have been shown to learn the data distribution and generate variety of data from it, which can serve as additional data for training.", "However, GANs also need data to train on and the need for data to create more data poses a classic causality dilemma.", "In the proposed research, we demonstrate a technique to generate tumor images using limited dataset, by expanding the latent representation of the tumor features.", "Such an approach would prove significant in studying rare mutations such as 19/20 co-gain \u2013 an indicator of prognosis in brain tumors.", "\\newline Clinical Background: Glioblastoma multiforme (GBM) is the most common and aggressive form of malignant tumor, comprising of 54% of all primary brain tumors [@bib:aans] , reporting a 5-year survival rate of 5% [@bib:tamimi2017epidemiology] .", "A significant statistic is that the survival rate has not improved in the last three decades [@bib:tamimi2017epidemiology] , which therefore, brings to attention the need for accurate evaluation of prognosis and efficacy of chemotherapy.", "Assessment of overall clinical outcomes typically requires a combination of clinical, molecular and multi-modal imaging data.", "This process is time consuming, invasive, cumbersome and overloads the clinical workforce.", "Some of the many factors that contribute to this are increasing incidence of GBM, high resolution imaging, paucity of resources for molecular testing, lack of follow-up, inconsistent data recording across modalities, etc.", "As an answer to this problem, researchers have started looking into optimizing the radiology pipeline by estimating prognostic markers in imaging, in an attempt to bypass the overhead caused by molecular data collection and analysis.", "Among the many prognostic markers [@bib:hegi2005mgmt,hartmann2013long,geisenberger2015molecular] , the 19/20 co-gain is yet to be explored.", "In this paper, we show that there are discriminatory imaging biomarkers indicating mutation and that we can recreate them using SAGE.", "\\newline Motivation for SAGE: One of the major challenges in the medical analysis is lack of data.", "19/20 co-gain is a rare mutation, and is therefore, a limited dataset.", "Furthermore, it also has a high inter-class similarity and intra-class diversity (Fig [@ref:LABEL:fig:var] ), which makes the visual assessment of biomarkers very unapparent to the naked eye.", "Any analysis using GANs will be successful only if the GAN is capable of generating synthetic images that capture the nuance and diversity of features in the mutation.", "This implies the existence of a data distribution that GANs can learn from.", "Additionally, since 19/20 co-gain has never before been analyzed for visual manifestation in MR Imaging (MRI), we begin by demonstrating that there is, indeed, a presence of consistent imaging biomarkers that have a positive correlation with presence or absence of 19/20 co-gain.", "Following this, we will proceed to learn and recreate these biomarkers using SAGE.", "The motivation behind developing SAGE as opposed to using DC-GAN or WGAN is that SAGE is designed to increase the apparent size of the dataset.", "In other words, to mitigate the issue of lack of data samples to use in GAN, SAGE increases the size of dataset in latent space by deconstructing features and offers a wider search space of feature combinations (i.e., data distributions).", "We show that SAGE learns useful features from the dataset to generates good quality and diverse synthetic images.", "\\newline The rest of the paper is organized as follows: Section 2 contains related work and contributions.", "Section 3 details the technical approach.", "Section 4 contains the description of dataset and evaluation setup.", "Section 5 presents the results and Section 6 is conclusions.", "\\newline  </section>"], ["<section> <title> II RELATED WORK AND CONTRIBUTIONS </title>  <subsection> <title> II-A Related Work </title> Deep learning has recently achieved remarkable success in various aspects of automated workflow involving MR Image Analysis such as diagnosis [@bib:litjens2016deep] , grading [@bib:citak2018machine] , segmentation [@bib:pereira2016brain] and other clinical outcomes [@bib:chang2018deep] .", "Generative Adversarial Networks (GANs) have recently gained attention in the medical field due to their efficacy in modeling and recreating data distributions to tackle common issues with medical datasets [@bib:han2018gan,livergan1,livergan2] , the primary goal being data augmentation.", "For MRI, GANs have been used for data augmentation [@bib:han2018gan] , segmentation [@bib:zhang2018ms] , data anonymization [@bib:shin2018medical] , etc.", "Most of these GANs generate the whole image at once.", "In our case, this approach does not necessarily generate the most detailed images.", "Since we are analyzing unapparent visual manifestations, it is critical that the tumor features be generated with sufficient detail.", "For High- and Low-Grade Gliomas, there has been some research in generating images for data augmentation [@bib:han2019learning,han2018gan] .", "Some of the research has attempted to use manifold learning [@bib:khayatkhoei2018disconnected] , attention-based learning [@bib:zhang2018self] , VAEs [@bib:higgins2017beta] etc.", "for improving learning quality.", "Real-world medical data, however, resides in a higher dimension with many complex attributes operating together.", "Adding to this complexity, medical data is often fragmented and scarce.", "With SAGE, we establish the impact of separating features and latent space re-sampling to increase the apparent sample space of the dataset to match the complexity and dimensionality of the data.", "We propose that this can result in significantly better-quality images.", "\\newline </subsection> <subsection> <title> II-B Contributions </title> <list> \\ A novel approach that can generate diverse synthetic images from very limited datasets using feature recasting, \\newline \\ \\ Feature disentanglement and sequential generation for higher-resolution images and added control over generated tumor properties, \\newline \\ \\ Quantitative analysis of efficacy of proposed method in learning and recreating visually unapparent data distribution compared to naive GANs \\newline \\ </list> \\newline </subsection>  </section>"], ["<section> <title> III TECHNICAL APPROACH </title>  <subsection> <title> III-A Overview of Approach </title> The Sequential Attribute GEnerator (SAGE) framework has three modules: (a) Shape Generation module, (b) Texture Generation module and (c) Merge module.", "Essentially, SAGE generates tumor crops using (a) and (b) and then merges the tumors with tumor-less brain slices in (c).", "The detailed explanation of each of these modules is given in Section [@ref:LABEL:ssec:framework] .", "From the overall dataset, we separate the slices of MR Images that contain tumor vs the ones that don\u2019t.", "Tumors are segmented manually from the slices that contain them to create Tumor Crops (TC).", "Developing deep learning approaches for brain tumor segmentation is an actively growing field.", "Some of the best performing networks [@bib:myronenko20183d] will also have some errors.", "If a tumor is wrongly segmented, the error will be propagated through the other modules of SAGE, generating incorrect tumors.", "We, therefore, opted for manual segmentation to obtain TCs.", "The TCs are then subjected to simple binary thresholding to obtain the shape masks.", "These will be referred to as Binary Tumor Crops (BTCs) henceforth.", "The slices that do not contain tumors are used in the merge block input (Tumor-less Images in Figure 2) and are called Pseudo-healthy Images (PHI).", "When a tumor grows in the brain, it pushes through the healthy brain tissue.", "Since there is yet no mathematical model that emulates this impact, we have used PHIs.", "PHIs were a part of the brain that contained a tumor and contain residual impact: thereby, the closest model we can use for merging with the tumors realistically.", "The properties of tumors and PHIs correspond to where the tumor can be located in the brain.", "BTCs are a representation of shape of tumors whereas TCs have the texture properties.", "So, we now separate and re-sample from three macro-features: shape, texture and location.", "Recasting and re-sampling of these features gives us an apparent increase in sample space since we now have three data distributions instead of one.", "Figure [@ref:LABEL:fig:sage] outlines the overall SAGE approach.", "The following sections provide details on the SAGE workflow.", "\\newline </subsection> <subsection> <title> III-B Description of Framework </title> In this section, we will detail each of the blocks of SAGE as described in Section [@ref:LABEL:ssec:overview] .", "The shape generation module of SAGE consists of a Latent-space Multiplexed Sampling GAN (LMS-GAN).", "The texture generation module consists of a Texture Alignment Network (TAN).", "The overall approach is shown in Figure [@ref:LABEL:fig:sage] .", "\\newline LMS-GAN: The Latent-space Multiplexed Sampling GAN (LMS-GAN) is the shape generator module of SAGE.", "It is a densely connected GAN framework that multiplexes between latent space vectors of data and noise to generate a binary mask of the tumor.", "The input of LMS-GAN are the BTCs.", "LMS-GAN performs a non-linear mapping from the input image or noise to the latent space.", "From the latent space, the network randomly samples data points and gives it as an input to the generator, periodically switching the latent vector between noise ( $ Z_{noise} $ ) and real data ( $ Z_{real} $ ).", "By randomly sampling points from the latent space, it can generate as many variations of an input vector to the generator as needed.", "However, since the latent vector input to the generator is no longer a feature vector from a coherent image, the generator block is designed as a densely connected network and not a convolutional network.", "To train the generator, the loss function switches with the input data type.", "If the sampling is done from $ Z_{real} $ , the network is trained on Mean Squared Error (MSE) loss (Eq [@ref:LABEL:eqn:L_real] ) and if the samples are $ Z_{noise} $ , loss is Binary Cross-Entropy (BCE) loss (Eq [@ref:LABEL:eqn:L_noise] ).", "The overall loss function is shown in (Eq [@ref:LABEL:eqn:sage-loss] ).", "\\newline <equationgroup> <equation> $ \\mathcal{L}_{X\\sim p(real)}:\\mathbb{E}_{x\\sim p_{real}(x)}||x_{real}-y||_{2}% \\vspace{-5mm} $ </equation> </equationgroup> \\newline <equation> $ \\vspace{-3mm}\\mathcal{L}_{X\\sim p(noise)}:\\mathbb{E}_{x\\sim p_{noise}(x)}\\Big{% [}\\log\\Big{(}1-\\mathbf{D}\\big{(}\\ \\mathbf{G}(z_{noise})\\ \\big{)}\\Big{)}\\Big{]} $ </equation> \\newline <equation> $ \\mathcal{L}_{LMS}={\\mathbb{I}_{x}}\\times\\mathcal{L}_{X\\sim p(noise)}+({1-% \\mathbb{I}_{x}})\\times\\mathcal{L}_{X\\sim p(real)} $ </equation> \\newline where $ \\mathbb{I}_{x} $ is the indicator function which is 0 when $ X $ is data and 1 when $ X $ is noise.", "The noise is sampled from random normal distribution.", "$ \\mathbf{G} $ and $ \\mathbf{D} $ are Generator and Discriminator, respectively.", "$ y $ is the generated image and $ x_{real} $ is the real image.", "$ Z_{real/noise} $ are the latent space representations of real and noisy inputs.", "This is depicted in Fig [@ref:LABEL:fig:lms-gan] .", "\\newline Texture Alignment Network (TAN): This module generates texture.", "TAN takes the Synthetic-BTCs (Syn-BTCs) generated by LMS-GAN and \u201cassigns\u201d a texture.", "TAN randomly samples a texture from the pool of TCs, learns its feature representation and aligns the distribution of Syn-TC with that of the sampled TC distribution.", "The output of this block is a synthetic tumor crop (Syn-TC).", "Some researchers have performed feature reassignment [@bib:gatys2015neural,jing2019neural] in the past.", "In our approach, we use features from Conv3, 4, 5 blocks of VGG19 and train on perceptual loss (Eq [@ref:LABEL:eqn:TAN-loss] ).", "We only include the deeper layers because texture of an image is learnt in the deeper layers of a CNN.", "\\newline <equation> $ \\mathcal{L}_{TAN}:\\sum_{i}\\omega_{i}\\times||\\mathrm{g}_{i}(T)-\\mathrm{g}_{i}(I% )||_{2}+||\\mathbb{F}_{i}(B)-\\mathbb{F}_{i}(I)||_{2}\\vspace{-1mm} $ </equation> \\newline where: $ \\mathcal{L}_{TAN} $ : overall loss function, $ \\omega_{i} $ : weight of $ i^{th} $ layer, T: sampled TC, B: input Syn-BTC, I: output Syn-TC, $ \\mathbb{F}_{i} $ output of $ i^{th} $ layer and $ \\mathrm{g}_{i} $ : Gram matrix of $ i^{th} $ layer output.", "\\newline Merge Block: When we merge the Syn-TCs with PHIs, we must ensure maximum diversity and uniformity to generate realistic images.", "For diversity, we randomly select location, relative size of tumor (with respect to whole image) and rotation angle for the Syn-TC.", "The values of these attributes are chosen from a pre-defined allowed range of values.", "This range is obtained from the mean and standard deviation of the original dataset attributes so we can choose attribute properties with a certain confidence.", "Once the tumor is merged, we apply edge-tapering Gaussian filters for smoothness.", "The overall framework of SAGE allows us to address the issues of image quality because of data scarcity commonly experienced by GAN approaches.", "In the following section, the results of using SAGE framework to generate GBM images are detailed.", "\\newline </subsection>  </section>"], ["<section> <title> IV EXPERIMENTAL SETUP </title>  <subsection> <title> IV-A Description of Dataset </title> We use a dataset containing FLAIR MR images for a cohort of 25 patients with known 19/20 co-gain status.", "The data is divided into two classes: control class that has 19/20 co-gain absent and the mutated class that has co-gain present.", "This dataset was provided by our collaborators at Emory University.", "The cohort is divided into 14 control and 11 mutated patients.", "Each patient has an average of only 9 FLAIR images with tumor, making it an average of 110 images per class (without train-test split), this makes for an even smaller training set.", "Thus, this is a very limited dataset to train GANs.", "\\newline </subsection> <subsection> <title> IV-B Evaluation Protocol </title> 1.", "Qualitative Evaluation: For qualitative evaluation, we present three types of results, each of which demonstrates a specific strength of SAGE.", "One result evaluates how well SAGE can learn the inherent data distribution of the data and translate it into generating new tumors.", "This is shown by comparing SAGE generated images with real images as well as DC-GAN and WGAN-GP results.", "We chose four real tumor crops with very different visual properties for each class and picked corresponding generated images from all three GANs that most closely resembled these tumors.", "This provides evidence of how well SAGE can learn the properties of highly diverse and limited data.", "For the second type of results, we demonstrated how SAGE can generate a diverse range of synthetic tumors compared to other GANs.", "This is an evaluation of the robustness of SAGE towards mode-collapse and loss of detail in generated images.", "For the third type of results, we show how well SAGE can capture the quality and detail of tumors.", "Thus, we zoom in into an example set of images for each class and compare resolution and detail between real, SAGE, DC-GAN and WGAN-GP.", "\\newline 2.", "Quantitative Evaluation: Inception Score and Structural Similarity: We report Inception Score (IS) and Structural Similarity Index (SSIM) to demonstrate the quality of generated images.", "The Inception Score evaluates two properties: (a) Image quality and (b) Image diversity.", "To assess image quality, meaningful generated objects should have a conditional label distribution with low entropy [@bib:salimans2016improved] .", "For ensuring diversity in generated images, the marginal probability should have high entropy [@bib:salimans2016improved] .", "Inception score is a combination of these two constraints.", "It achieves this by calculating the KL Divergence between the conditional and marginal distributions.", "The average KL divergence value of all generated images is the score.", "The lowest value of IS is 1 and the highest value is the number of classes.", "The SSIM is a comprehensive measure of similarity between images.", "The value of SSIM lies between 0 and 1, where 0 means the images aren\u2019t similar at all.", "Ideally, we want our images to follow the same distribution as real data but also be different enough to ensure diversity.", "Therefore, we can identify the range of 0.35-0.75 as the preferred range of SSIM to evaluate our synthetic image quality.", "\\newline 3.", "Blind Test by Radiologists: We conducted tests for assessing the quality of generated tumor crops via a blind prediction test with radiologists.", "We asked radiologists to manually distinguish between real and synthetic tumors.", "The evaluation was done with 3 radiologists.", "They were provided with control and mutated folders, each containing random number of real and synthetic images.", "These images were randomly chosen and resized to 100x100 for consistency.", "Furthermore, the names of these images were replaced by random numbers to hide any indicators that may point towards the nature of the image.", "For a completely blind assessment, the statistics of data split were not revealed to the radiologists.", "The control group had a total of 92 images: 45 real and 47 synthetic while the mutated group had 82 images: 35 real and 47 synthetic images.", "The radiologists were then asked to mark the images R/F (Real or Fake).", "These results are summarized in Table [@ref:LABEL:tab:blind_test] .", "Here, accuracy (ACC) is the proportion of correct predictions made, False Positive Rate (FPR) is the proportion of synthetic images that were mistaken as real, False Negative Rate (FNR) is the proportion of real images mistaken as synthetic and Precision (PR) is the proportion of true positives over all positive calls.", "During our design of experiment for this test, there were two key questions which we answered.", "These questions and answers are given below: \\newline Question 1: What is your hypothesis that you are trying to show by having us classify them? Do you predict that a human will or will not be able to tell the difference?", "\\newline Answer 1: We want to have an expert evaluation of the quality of generated tumor images.", "If the generated images are of high quality, one will not be able to distinguish them from real images.", "Based on this we can have a metric of the quality of generated images.", "This will also help us to improve the quality of generated images in the future.", "\\newline Question 2: What is the reasoning for the tumor being so tightly cropped? For us, it is very unusual to look at an image that is only the segmented tumor.", "In this way, all the images seem very strange to us.", "It is difficult to reliably tell the difference because there is not much context.", "\\newline Answer 2: The images are tightly cropped since only the tumor is (synthetically) generated.", "Cropped images are used for data augmentation for the training of deep learning algorithms (to distinguish mutated/control groups) assuming the detection/segmentation has been done perfectly.", "We aim to evaluate how well the tumor features are learnt when isolated.", "Yes, the context is very important for image interpretation and currently we take that into account in the classification step where we learn the discriminatory features.", "\\newline 4.", "Learning Discriminatory Features: We focus our evaluation on our primary goal: to evaluate whether SAGE can consistently and precisely learn the desired feature distribution.", "To evaluate this, we use the generated images to classify real images.", "The rationale is: if SAGE-generated images can detect discriminatory features in a real test set, then SAGE has captured the desired data distribution unique to each class.", "We compare these results using synthetic images generated using DC-GAN and WGAN-GP to demonstrate the effectiveness of SAGE in learning from limited data.", "We report accuracy, sensitivity, specificity and dice score for a complete evaluation of performance.", "Here accuracy is the proportion of total correct predictions, sensitivity is the true positive rate, specificity is the true negative rate and dice score is the harmonic mean of precision (proportion of true positives over all positive calls) and recall (sensitivity).", "\\newline </subsection>  </section>"], ["<section> <title> V Results and Discussion </title>  We perform evaluations for qualitative and quantitative analysis of generated tumor crops and the results of blind test by radiologists.", "The results are summarized below: \\newline 1.", "Qualitative Evaluation of Generated Images: We evaluate the ability of SAGE to learn from the data distribution accurately.", "To show that SAGE can learn the features compared to other GANs, the tumor crops and whole slide images generated by SAGE, DC-GAN [@bib:radford2015unsupervised] and WGAN-GP [@bib:gulrajani2017improved] are shown in Figure [@ref:LABEL:fig:sage-res] .", "We use WGAN-GP over standard WGAN because the gradient penalty has been shown to provide superior results [@bib:gulrajani2017improved] .", "SAGE generates attributes sequentially, giving us complete control over how and what kind of images we generate.", "It can be seen from Figure [@ref:LABEL:fig:sage-res] that we are able to recreate specific tumor attributes.", "For comparison, we cherry-picked the DC-GAN and WGAN-GP images that looked visually similar to the corresponding real and SAGE image.", "It can be seen that SAGE is able to recreate the detailed attributes more faithfully.", "\\newline Our goal, however, is not limited to recreating features exactly.", "We also want to generate good quality images and a diverse set of synthetic images that learn from the real distribution.", "Figure [@ref:LABEL:fig:diversity-res] shows the diversity of images generated using each of the GANs.", "It can be seen that DC-GAN suffers from mode-collapse due to lack of data.", "WGAN-GP is relatively better at handling it, however, given the limited data available, it cannot combat mode-collapse and generate detailed attributes at the same time.", "Therefore, we can see that while WGAN-GP generates diverse images compared to DC-GAN, it is unable to achieve the same quality of tumors as SAGE.", "SAGE not only generates better quality images, but also generates more diverse set of synthetic images.", "Additionally, Figure [@ref:LABEL:fig:zoomed-res] , zooms in on some examples of generated images where DC-GAN and WGAN-GP fail to generate an acceptable detail and quality of tumor image.", "\\newline 2.", "Quantitative Evaluation: As a quantitative measure, we calculated the Inception Score of generated images.", "For two classes, the score can be between 1 and 2.", "The closer the score is to 2, the better our GAN performs.", "For SAGE, we calculated Inception score for a total of 1000 images (500 each of control and mutated).", "We calculated SSIM over the same set.", "We used similar settings to evaluate Inception score and SSIM for DC-GAN and WGAN-GP."]], "target": "The results are shown in Table . We note that SAGE gives an inception score value of 1.71 which shows that it is effective in learning both quality and diversity of data. The SSIM value above 0.5 indicates that visual properties of generated images are similar to real images but since the SSIM is not too close to 1, we can infer that there is a certain level of diversity associated with data."}, {"tabular": ["  Layers  &  Type  &  Input size  &  Kernel  &  Options  &  Trainable parameters ", " 1  &  Input  &  256 $ \\times $ 256 $ \\times $ 3  &  -  &  Normalization  &  0 ", " 2  &  Pooling  &  128 $ \\times $ 128 $ \\times $ 3  &  -  &  Max Pool 2 $ \\times $ 2  &  0 ", " 3  &  Convolutional  &  64 $ \\times $ 64 $ \\times $ 6  &  8 $ \\times $ 8  &  Max Pool 2 $ \\times $ 2  &  1158 ", " 4  &  Convolutional  &  32 $ \\times $ 32 $ \\times $ 12  &  4 $ \\times $ 4  &  Max Pool 2 $ \\times $ 2  &  1164 ", " 5  &  Convolutional  &  16 $ \\times $ 16 $ \\times $ 21  &  2 $ \\times $ 2  &  Max Pool 2 $ \\times $ 2  &  882 ", " 6  &  Fully-connected  &  1 $ \\times $ 1 $ \\times $ 4608  &  -  &  ReLU Activation  &  4609 $ \\times $ 1515 ", " 7  &  Fully-connected  &  1 $ \\times $ 1 $ \\times $ 1515  &  -  &  Sigmoid Activation  &  1516 $ \\times $ 1515 ", " 8  &  Output  &  1 $ \\times $ 1 $ \\times $ 1515  &  -  &  Denormalization  &  0 ", " Total  &  -  &  -  &  -  &  -  &  9,282,579  "], "ref_sec": [["<section> <title> 1 Introduction </title>  <subsection> <title> 1.1 Classic porous material characterization </title> Data science is becoming an essential tool to analyze the structural features of porous materials based on the tomography images [@bib:liu2015machine,kalidindi2015application,steinmetz2016analytics] .", "The behavior and performance of porous material are strongly related to the characteristics of its internal micro\u2013structure.", "In order to discover the descriptive features and the process\u2013structure\u2013property relationships in a porous material, we need to achieve a rich representation of the internal structure of the porous materials [@bib:liu2015machine,niezgoda2011understanding,niezgoda2013novel,fernando2020inter] .", "Spatial description of such micro\u2013structures have created added\u2013value in diverse fields of studies, from composite material engineering [@bib:fast2011new,knezevic2007fast,landi2010multi] and food processing [@bib:derossi2017characterizing,derossi2013statistical] , to the petroleum and petrochemical industries [@bib:blunt2013pore,andra2013digital] .", "For instance, during the past two decades, the field of digital rock physics grew rapidly and showed outstanding advances owing to the power of imaging and analysis techniques [@bib:blunt2013pore,blunt2017multiphase] .", "Based on the captured images, we are able to build realistic simulation models and run many digital measurements and experiments on porous material such as pore and throat sizes, hydraulic and electric conductance, two\u2013phase displacement, and mechanical deformations [@bib:huang2016multi,rabbani2020triple] .", "Direct calculation of the aforementioned physical properties based on the tomographic data could be a complicated and computationally expensive task especially in the case of large\u2013size images [@bib:mohammadmoradi2016petrophysical,rokhforouz2016numerical,rabbani2019pore] .", "In this regards, machine learning approaches can be utilized to make hybrid [@bib:rabbani2019hybrid] or full artificially intelligent models [@bib:rabbani2017estimation] which are able to reduce the computational costs significantly while maintaining the level of accuracy.", "\\newline </subsection> <subsection> <title> 1.2 Deep learning for porous material studies </title> Shallow neural networks are powerful tools for modeling moderately complex problems in a timely and efficient manner [@bib:gholami2012prediction,russell2016artificial,van2016machine] while they are not very suitable to predict high orders of non\u2013linearity [@bib:bianchini2014complexity] .", "In contrary, deep learning models are capable of estimating a highly non\u2013linear behaviour if they are trained on an adequately diversified and large set of input and output data [@bib:schmidhuber2015deep,lecun2015deep] .", "Convolutional neural networks (CNNs) as a particular type of deep neural networks can be used for analyzing data with a recognized grid-like topology, similar to image data [@bib:lecun1995convolutional,lecun1989generalization,krizhevsky2012imagenet] .", "A typical CNN uses several filters to extract higher level features from the input data or images and gradually narrows it down to the specified output features [@bib:mnih2015human] .", "CNNs have been mostly used for image segmentation, recognition, classification, and regression [@bib:lecun1995convolutional,krizhevsky2012imagenet,he2016deep] .", "In mathematical terms, convolution is a spatial operation to transform an original function or data into a secondary realization using an operating kernel [@bib:keys1981cubic] .", "Convolution on an input image could lead to generating negative values which are not usually favorable considering the physical meaning of the output layer in that specific problem.", "As a post\u2013processing technique, Rectified Linear Units (ReLU) layer can be used to change the negative values into zero [@bib:jarrett2009best,glorot2011deep,nair2010rectified] .", "At each level of convolution, we can use a down\u2013sampling method such as maximum or average pooling to condense the volume of data without loosing noticeable amount of information [@bib:zhou1988computation] .", "In many cases CNNs can be followed by some fully\u2013connected dense layers of nodes to give more flexibility to the model [@bib:girshick2015fast] .", "CNNs have been used in many recent porous material studies for different purposes including segmentation of porous media images [@bib:niu2020digital,karimpouli2019segmentation] , image quality improvement [@bib:kamrava2019enhancing] , super resolution, reconstruction [@bib:li2018transfer,wang2018super] , classification [@bib:baraboshkin2019deep] , and regression [@bib:kreyenberg2019velocity,alqahtani2019machine] .", "Here we briefly describe a background of these applications and narrow the topic down to the specific approach of the present study.", "\\newline <subsubsection> <title> 1.2.1 Image improvement and reconstruction </title> Considering the multi\u2013scale nature of many of the porous micro\u2013structures, it is necessary to have plenty of details in images while covering a large volume of the object at the same time.", "In this regards, super resolution techniques powered by CNNs are valuable tools to be trained on pairs of low and high resolution images.", "There are plenty of recent studies that have presented quantitative methods to obtain a high resolution tomography image of porous material using images with lower spatial resolution [@bib:wang2018super,wang2018porous,wang2018ct,wang2019ct,da2019enhancing,da2019super] .", "As a recent example, Kamrava et al. [@bib:kamrava2019enhancing] have used a cross\u2013correlation\u2013based simulation to generate an augmented dataset of porous shale images and make a CNN that is able to improve the image quality of similar porous textures.", "\\newline The resolution enhancement can go further to a level that we are able to generate a detailed realization of the porous material based on the simple input of noise maps through a specific type of CNNs known as Generative Adversarial Networks (GAN) [@bib:mosser2018stochastic,mosser2017reconstruction,feng2019accurate,shams2020coupled] .", "As an example, Mosser et al. [@bib:mosser2018stochastic] presented a workflow to train a GAN based on the available 3\u2013D tomography images and to reconstruct similar realizations of the original images, while not making an exact copy of them.", "Then by looking at the hydrodynamic properties of the constructed porous material, they have evaluated the similarity of the realizations.", "\\newline </subsubsection> <subsubsection> <title> 1.2.2 Classification of porous materials </title> CNNs are good tools to classify images based on texture, visible elements, or objects [@bib:krizhevsky2012imagenet] .", "This texture recognition has several applications in material and geological sciences to classify or cluster a dataset of porous material images.", "Additionally, some other researchers utilized the CNN framework for recognition of the materials texture [@bib:decost2017exploring,baraboshkin2019deep] .", "For instance, in geoscience, classifying different types of rocks in terms of mineralogy and micro\u2013structure could be a time consuming and biased task if done by hand, while CNNs have widely been used in the past three years to automate these processes in a timely and efficient manner [@bib:floriello2019automatic] .", "\\newline </subsubsection> <subsubsection> <title> 1.2.3 Image\u2013based regression models </title> Many diverse physical properties of porous materials have been estimated using CNNs in recent years; from thermal to hydraulic and mechanical features.", "Wei et al. [@bib:wei2018predicting] proposed a CNN to predict the effective thermal conductivities of composite materials and porous media with more than 0.98 accuracy ( $ R^{2} $ ) on 100 testing image samples while training on 1400 samples.", "Additionally, permeability and porosity have been heavily investigated through CNNs [@bib:wu2018seeing,srisutthiyakorn2016deep,alqahtani2018deep,araya2018deep] .", "CNNs are able to take both binary or gray\u2013scale images of porous materials to estimate porosity and permeability with an acceptable error.", "Alqahtani et al. [@bib:alqahtani2019machine] used CNNs to estimate porosity, average pore size and specific surface of the porous rocks based on both types of 2\u2013D tomography images and found that binary images could give a more accurate estimation of porous material characteristics compared to the gray\u2013scale ones.", "However, the morphology of the binarized images is highly dependent on the thresholding technique and it suffers from an inherent uncertainty [@bib:zhang2019challenges] .", "In another attempt, Cang et al. [@bib:cang2018improving] designed a CNN for prediction of physical properties of heterogeneous materials and successfully predicted the Young modulus, diffusion and permeability of the porous material with more than 90% of certainty on their testing data.", "Recently, Karimpouli and Tahmasebi [@bib:karimpouli2019image] developed a CNN model to estimate P-wave/S-wave velocities based on the cross-sectional images of porous material.", "They were able to estimate these parameters with coefficients of determination around 0.65, and 0.74, respectively.", "\\newline </subsubsection> </subsection> <subsection> <title> 1.3 Highlights of the present study </title> Considering the above-mentioned categories of CNN applications in porous material research, the present study can be considered as an image\u2013based regression model.", "Compared to the previous efforts and studies to estimate porous material properties via deep learning, the present work is more comprehensive and generalized considering the following aspects: \\newline <list> \\ Input and output data are obtained from 3\u2013D micro\u2013structures while most of the recent studies have analyzed 2\u2013D image data [@bib:alqahtani2019machine,karimpouli2019image,wei2018predicting] .", "\\newline \\ \\ The number of semi\u2013real samples of the input data in this study is relatively large compared to the previous studies (Karimpouli et al . [@bib:karimpouli2019image] used 256 samples, Alqahtani et al . [@bib:alqahtani2019machine] used 7262 samples, while the present study uses 17700 samples).", "\\newline \\ \\ The number of extracted features is substantially higher than recent comparable studies (Karimpouli et al . [@bib:karimpouli2019image] 2 features, Alqahtani et al . [@bib:alqahtani2019machine] 3 features, the present study 30 features including 15 curves each of which with 100 data points).", "\\newline \\ \\ A dimensionless and size\u2013independent approach is introduced to calculate porous material features that enables us to analyze images with any spatial resolution, while in many of past studies input images have a fixed spatial resolution such as [@bib:cang2018improving,alqahtani2018deep,alqahtani2019machine,karimpouli2019image] .", "\\newline \\ </list> \\newline </subsection>  </section>"], ["<section> <title> 2 Methodology </title>  In this study, we use an augmented set of semi\u2013realistic tomography images of geological porous material to train a convolutional neural network (CNN).", "The aim of this artificial intelligence model is to predict multiple physical properties of a porous material based on pore scale images of it.", "We call this deep learning model of porous material characterization as DeePore .", "To the best of our knowledge, the generated dataset of this study is unprecedented in terms of comprehensiveness and a wide range of variables.", "\\newline <subsection> <title> 2.1 Input data augmentation </title> The original core of the image dataset is composed of 60 real micro-tomography images which their detailed information and corresponding references are available in Appendix A. Considering the fact that it is critical for CNNs to be trained on a large\u2013size dataset of images, and due to the limited availability of the diverse and realistic tomography data of porous material, we have adopted a previously developed algorithm to generate more realizations of such data by transforming the existing ones [@bib:cohen1998three] .", "To do this, we select two different images out of the 60 samples and interpolate a hybrid texture among them by weighted averaging of the normalized distance maps.", "A simplified example of the interpolation technique is illustrated in Fig. [@ref:LABEL:fig:interp] .", "In this example, two initial grayscale images with different textures (Fig. [@ref:LABEL:fig:interp] -a and h) are binarized using a locally adaptive Otsu algorithm [@bib:yan2005multistage] (Fig. [@ref:LABEL:fig:interp] -b and i).", "Then, normalized maps of the Euclidean distances are calculated (Fig. [@ref:LABEL:fig:interp] -c and j) and combined by weighted averaging to mimic an interpolated texture (Fig. [@ref:LABEL:fig:interp] -e).", "Finally, we can set the threshold level on the obtained hybrid map to reach any desired amount of porosity ((Fig. [@ref:LABEL:fig:interp] -d, g and k).", "\\newline As a more realistic example, Fig. [@ref:LABEL:fig:augment] illustrates the data augmentation technique over only two real tomography images (Fig. [@ref:LABEL:fig:augment] -c and w).", "In this figure, by going from top to the bottom rows, texture is gradually changing from sample #1 to #2.", "Meanwhile, by moving from left to right side of the matrix, porosity is increasing by manipulating the threshold level mentioned above.", "\\newline It is noteworthy to highlight that the distance maps should be normalized prior to averaging in order to avoid large elements of one image from cloaking the smaller ones in the other image.", "In order to generate the hybrid realizations of each pair of the real images, we have assumed 10 uniform random numbers between 0 to 1 as interpolation weights, as well as 10 uniform random numbers between 0.1 to 0.45 as final porosity fractions.", "Consequently, the total number of the images in the dataset would be $ {60\\choose 2}\\times 10 $ that yields 17700.", "However, considering that we aim to calculate several physical properties of these materials, it is expected to filter out outlayer geometries with non\u2013physical or null properties that cannot be modelled through the regression techniques.", "For example, in the case that there is no percolating pathway from one side to the other side of the sample, hydraulic permeability, will be zero and formation factor which indicates electrical resistivity of the void space approaches to infinity.", "Also, tortuosity will have a null value in such cases.", "\\newline </subsection> <subsection> <title> 2.2 Building the ground truth data </title> After construction of the augmented set of image data, we use a series of in\u2013house codes developed based on the available literature to analyze the micro\u2013structures of the porous material.", "In this regards, pore network modeling techniques [@bib:xiong2016review] are used to simulate different physics and processes on the 3\u2013D porous samples and the results obtained are assumed to be the ground truth for training the DeePore CNN.", "In addition to pore network modeling, solid network models are also used to characterize the mechanical behaviour of the studied material similar to networks presented by Herman in 2013 [@bib:herman2013shear] but in 3\u2013D. A sample realization of the solid and pore networks of a porous material are visualized in Figure [@ref:LABEL:fig:data_workflow] -d and g. In order to construct these networks, initially we need a 3\u2013D binarized image (Fig. [@ref:LABEL:fig:data_workflow] -a) that is segmented to the void and solid spaces which are respectively shown in Figure [@ref:LABEL:fig:data_workflow] -b and e. Then using watershed segmentation algorithm we break down an interconnected micro\u2013structure into a separately labelled map of nodes as can be seen in Figure [@ref:LABEL:fig:data_workflow] -c and f. The color gradient in these illustrations indicate the relative equivalent radius of the nodes extracted for both void (pore) and solid networks.", "When we have detected location and boundaries of each node, then by analyzing the node map connectivities, two networks can be extracted for both void and solid spaces (Fig. [@ref:LABEL:fig:data_workflow] -d and g).", "Watershed segmentation algorithm which is used to break down the micro-structures into a mathematically describable 3\u2013D network, has been widely employed for porous material characterization from tomography images [@bib:wildenschild2013x,gostick2017versatile,Rabbani2014] .", "This algorithm uses the Euclidean distance transform of a binary object to detect the narrowest parts of the connections between different nodes.", "More details on the methodology and validation of watershed segmentation algorithm can be found in [@bib:gostick2017versatile,Rabbani2014] .", "\\newline Now in order to build the ground truth data for training DeePore CNN, we investigate the constructed pore and solid networks by measuring several morphological features and running physical simulations (Fig. [@ref:LABEL:fig:data_workflow] -h).", "In this section, we briefly describe the simulation techniques employed and some assumptions made to generalize the analysis of outcomes.", "As an example, we have illustrated 3 simulation results on a sample pore network model in Fig. [@ref:LABEL:fig:data_workflow] -g.", "These simulation results are fluid saturation in a 2\u2013phase drainage process, electricity flow through the saturated pore\u2013space, and pore pressure of the single\u2013phase fluid flow, respectively depicted in Fig. [@ref:LABEL:fig:data_workflow] -h1 to h3 to give some insight on the ground truth generation.", "\\newline The list of the physical properties and features that have been obtained for each of the samples within the dataset is provided in Table [@ref:LABEL:tab:outputs] .", "As can be seen, we have reported 15 single\u2013value features that comprehensively describe the morphological, hydraulic, mechanical and electrical properties of porous material.", "Additionally, 4 functions and 11 distribution curves are extracted for each porous sample to describe its characteristics (Fig. [@ref:LABEL:fig:data_workflow] -i).", "Here is a brief introduction to the calculated set of properties and more details regarding the methodology of extracting each of the features is available in the corresponding references in Table [@ref:LABEL:tab:outputs] .", "\\newline <subsubsection> <title> 2.2.1 Morphological properties </title> Based on the extracted network models, probability distribution in addition to the average values are reported for pore body radius, pore throat radius, throat length, grain radius, and pore connectivity [@bib:Rabbani2014] which is also known as the network coordination number [@bib:blunt2013pore] .", "In addition, pore density that indicates the number of pores per unit volume of the geometry, pore sphericity [@bib:kong2018pore] , grain sphericity [@bib:beard1973influence] , and specific surface are calculated for each of the 3\u2013D images in the dataset.", "Furthermore, we have used Dijkstra\u2019s algorithm [@bib:shanti2014x,dijkstra1959note] to find the shortest path from one face to the other face of the pore networks and calculate tortuosity.", "In this regards, the shortest path between each two random pairs of the pores from inlet to the outlet of the pore network is calculated for several times and average value of all shortest paths is reported as tortuosity.", "Finally, as a morphological property of porous material we have calculated the two\u2013point correlation function of the binarized images which shows how well\u2013correlated are the porosity of two random points selected with a specified distance between them [@bib:blair1996using] .", "For more details regarding the calculation methodology for each of the properties please refer to references provided in Table [@ref:LABEL:tab:outputs] .", "\\newline </subsubsection> <subsubsection> <title> 2.2.2 Hydraulic properties </title> Absolute and relative permeabilities are calculated based on the extracted pore networks.", "For calculating the fluid conductance in each pore throat, realistic cross-sectional shapes of the throats are used to provide better results [@bib:rabbani2019hybrid] .", "Additionally, for calculation of the two\u2013phase flow functions, we have assumed zero contact angle in the case that two immiscible fluids are present within the porous media.", "The procedure we use to model two\u2013phase displacement in a pore network is a quasi\u2013static approach with stepwise increment of the non\u2013wetting phase pressure and domination of the capillary forces over the viscous forces.", "This approach is fully described by Valvatne and Blunt [@bib:valvatne2004predictive] .", "The capillary pressure curve is another important hydraulic property of the porous material and as discussed, we require a technique to remove the pressure unit of this parameter.", "To this end, we have used the concept of Leverett J-function curve [@bib:chaouche1994capillary] which is a dimensionless version of the capillary pressure normalized for different porosities, permeabilities, contact angle and interfacial tension between the two displacing fluids in porous media.", "\\newline </subsubsection> <subsubsection> <title> 2.2.3 Electrical properties </title> Formation and cementation factors are two electrical properties of porous material that have been calculated using the extracted pore network models [@bib:oren1998extending,man2000pore] .", "These parameters are critical in Archie\u2019s equation [@bib:archie1942electrical] and helps to describe the electrical behaviour of a porous medium saturated with a conductive fluid.", "Formation factor is the ratio of the electrical resistance of the fully saturated porous media to the electrical resistance of the pure fluid [@bib:adler1992formation] .", "Also, with a similar approach, this value remains the same when we are measuring the ratio of the mass diffusivity of a component in a bulk fluid relative to its diffusivity through the fully\u2013saturated porous media [@bib:sevostianov2017connection,aminnaji2019effects] .", "\\newline </subsubsection> <subsubsection> <title> 2.2.4 Mechanical property </title> In addition to many pore\u2013dependent properties, we have modelled relative Young modulus of the material which is a solid phase feature.", "For this purpose, we assume that the extracted solid network is a truss\u2013like structure and by applying normal compressional force on each side of the geometry, the directional Young modulus is calculated by dividing the normal stress over the strain ratio [@bib:buxton2007bending,wallach2001mechanical] .", "Then arithmetic average of the directional values is calculated and divided by the Young modulus of the pure non\u2013porous material to obtain the relative Young modulus which is a dimensionless number [@bib:gor2015elastic] .", "\\newline </subsubsection> <subsubsection> <title> 2.2.5 Dimensionless approach </title> It is noteworthy that we have removed the original spatial resolution of the data and defined a unified unit of length which is equal to the physical size of each voxel.", "For example, the unit of absolute permeability which is area has become $ px^{2} $ which means that we need to multiply the resulted permeability by the spatial resolution to the power of two in order to retrieve the re\u2013scaled permeability value.", "In the same manner, all other reported features are dimensionless or described only in length unit which is convertible to voxel size.", "The list of all alternative units is presented in Table [@ref:LABEL:tab:outputs] .", "For features that can be calculated directionally such as permeability, we have assumed an isotropic structure and reported the arithmetic average of the values in x , y , and z directions.", "Although in some cases this averaging does not have explicit physical meaning, but it is used to cover directional non\u2013conformities in the porous structures that can affect the extensibility of the model.", "\\newline </subsubsection> </subsection> <subsection> <title> 2.3 Deep learning method </title> In the previous sections, we have generated a large dataset of semi\u2013realistic micro\u2013structures of porous material and a wide range of 30 physical properties are calculated for each of the samples.", "Now we aim to build a machine learning model that is able to estimate these properties purely by analyzing input images and with no explicit knowledge of the underlying physics.", "As discussed, CNNs have proved to be efficient in image classification, segmentation, and regression.", "So, we have designed a CNN structure combined with two dense layers of neurons to make a regression model that is able to estimate all physical properties of porous material mentioned above in a supervised manner.", "Data workflow and CNN structure are presented in Fig. [@ref:LABEL:fig:data_workflow] and Table [@ref:LABEL:tab:CNN] .", "Here, we are providing more details regarding the structure of the network and the training process.", "\\newline <subsubsection> <title> 2.3.1 Network input layer </title> Initially, we take a 3\u2013D image from the dataset with the size of $ 256^{3} $ voxels and extract three perpendicular mid\u2013planes of the volumetric data (Fig. [@ref:LABEL:fig:data_workflow] -k to l).", "Then, the distance transform of the solid space is calculated for each of the images and they are normalized based on the maximum values of each of them (Fig. [@ref:LABEL:fig:data_workflow] -m).", "Distance maps are not only able to deliver information about the original binary map, but also, describe the Euclidean distances between each point of that binary map to the nearest boundary.", "This additional information enriches the model input layer with more data compared to passing a simple binary array.", "\\newline Now, based on the three normalized distance maps, we generate a fictitious RGB image by stacking them into each other and generate an initial feature map to be used as the input for the CNN (Fig. [@ref:LABEL:fig:data_workflow] -n).", "The reason to mimic an RGB image is the common use of these image formats as input of a CNN.", "In addition, RGB images are easy to store and read from hard disk and there are plenty of lossless compression methods invented to minimize their size when stored on disk [@bib:gormish1997lossless] .", "Usage of the whole 3\u2013D data as the input of the CNN instead of the perpendicular mid\u2013planes could increase the accuracy of the results, while it can significantly increase the computational expenses which is not desirable due to the general purpose of this research to propose an efficient and adequately accurate model.", "\\newline </subsubsection> <subsubsection> <title> 2.3.2 Network hidden layers </title> At the first layer of CNN, we initially run a 2 by 2 max\u2013pooling filter to reduce the size of the input data without loosing too much information (Fig. [@ref:LABEL:fig:data_workflow] -o2).", "Then, 3 convolutional layers are designed to gradually decrease the size of the information while maintaining the main geometrical features by applying different sequential filters on the input images (Fig. [@ref:LABEL:fig:data_workflow] -o3 to o5).", "Each convoloutional layer is followed by a $ 2\\times 2 $ max\u2013pooling filter to finally make data small enough to be fitted into a fully\u2013connected dense layer.", "The first dense layer is activated by ReLU, while the second one uses sigmoid (Fig. [@ref:LABEL:fig:data_workflow] -o6 and o7).", "This network architecture is designed by manually testing a range of different structures and monitoring the performance of the model in terms of the learning speed and accuracy.", "It is noted that the sizes of the convolution kernels are $ 8\\times 8 $ , $ 4\\times 4 $ , and $ 2\\times 2 $ , respectively, for the aforementioned three convolutional layers and stride is equal to $ 1\\times 1 $ for all three kernels.", "More details on the structure of the proposed CNN is provided in Table [@ref:LABEL:tab:CNN] .", "\\newline </subsubsection> <subsubsection> <title> 2.3.3 Network output layer </title> As it can be seen in Table [@ref:LABEL:tab:CNN] , the output layer of the network is a one dimensional array of 1515 elements (Fig. [@ref:LABEL:fig:data_workflow] -o8).", "The first 15 elements of the array are 15 single\u2013value features calculated for each of the porous samples as described in Table [@ref:LABEL:tab:outputs] , rows 1 to 15.", "The next 1500 elements of the output array describe 4 functions and 11 distribution curves each of which occupies 100 elements of the array.", "The range of the array indices for each of the output parameters is described in Table [@ref:LABEL:tab:outputs] .", "In order to fit the wide range of variables and functions into an array of 1515 elements, certain reshaping and interpolation operations are required for the raw results of pore scale modelling.", "The four functions that occupy indices from 16 to 415, are Leverett J-function, wetting relative permeability, non\u2013wetting relative permeability and two\u2013point correlation function.", "The first three are functions of wetting phase saturation which is a fraction between 0 to 1.", "So, in order to summarize each of these three curves into 100 elements, we have divided the whole curve into 100 pieces each of which with 0.01 distance from each other in terms of wetting phase saturation.", "Similarly, for two\u2013point correlation function, we have split the curve into 100 segments each of which with 0.5 voxel distance to the next one, in order to cover a total lag distance of 50 voxels in 100 elements.", "For more details regarding this function please refer to [@bib:blair1996using] .", "Next, in order to fit each of the 11 distribution curves into 100 elements, we are using the cumulative format of the probability distributions (CDF) that pack a full range of variable changes into a sigmoid\u2013like curve between 0 and 1.", "Consequently, we have divided the CDF curve into 100 pieces with 0.01 distance between every two consecutive points in Y axis and embedded the corresponding values of X axis into the output array.", "For more clarification, a sample set of the described functions and distribution curves will be presented in the Results and Discussion section (Table [@ref:LABEL:tab:sample_output] and Fig. [@ref:LABEL:fig:sample_output] ).", "\\newline </subsubsection> <subsubsection> <title> 2.3.4 Model training </title> Development and training of the DeePore CNN is implemented in Python using Keras with TensorFlow backend [@bib:abadi2016tensorflow] .", "Additionally SciPy, Numpy and Matplotlib [@bib:hunter2007matplotlib] as open\u2013source packages of Python are used for data pre\u2013 and post\u2013processing.", "Back\u2013Propagation RMSprop algorithm [@bib:riedmiller1993direct] with the learning rate of $ 10^{-5} $ is used for training the CNN by minimizing the prediction loss in terms of mean squared error.", "We have used 64 % of the data samples for training the network, 16 % for evaluation and 20 % are kept outside of the workflow for independent and un\u2013biased testing of the results obtained.", "We have trained the model for 200 epochs with batch size of 100 samples per each updating of the model gradient.", "The input and output data are fed into the model using large size Hierarchical Data Format (HDF) files.", "A Python Generator method, reads the data batch by batch from the HDF file to avoid occupying a large amount of machine memory.", "\\newline </subsubsection> </subsection>  </section>"], ["<section> <title> 3 Results and Discussions </title>  In this section, three main outcomes of this study are discussed.", "Initially we describe the significance and applications of the present dataset of the porous material and then we focus on the statistical lessons learned by examining cross\u2013correlations of the dataset features.", "Finally, the accuracy of the features estimated by the model will be checked on the testing samples to demonstrate the capability of DeePore workflow for rapid characterization of the porous material.", "\\newline <subsection> <title> 3.1 Comprehensive Porous Material Dataset </title> In this research we have generated a comprehensive dataset of semi--real micro--porous structures with 17700 samples and a wide range of morphological, hydraulic, electrical and mechanical features are calculated for each of the samples.", "The presented dataset is unprecedented in terms of the number of the samples and variety of the extracted features.", "The main application of this dataset is to be used as the raw material for more advanced machine learning studies on porous material.", "In addition to the raw 3-D geometries, Python codes, extracted pore networks and all calculated characteristics are available in the public domain for replication and improvements in future studies .", "\\newline </subsection> <subsection> <title> 3.2 Statistical Lessons Learned </title> Considering the large number of analyzed samples of porous material, we have created a rich dataset to investigate the existing trends and relationships among the calculated features.", "Binary correlation coefficients of 15 single-value features are visualized in Fig. [@ref:LABEL:fig:heatmap] as a heat map to summarize the statistical significance of cross-parameter relationships.", "Pure blue color at the intersection of two parameters indicates strong correlation and pure red color shows a strong inverse correlation between them.", "As an example, absolute permeability of porous media is well correlated with average pore\u2013throat radius which is expected based on the available literature [@bib:rezaee2006relationships] , while it does not have a significant relationship with average grain radius and finally it has an inverse relationship with relative Young module of the porous material.", "This relationship is physically justifiable considering that large values of relative Young module indicate a tight and consolidated structure of porous material [@bib:yeheskel2000new] which leads to lower permeability.", "Although, many of these relationships have been widely investigated in the literature [@bib:mortensen1998relation,timur1968investigation,pittman1992relationship] , having a diverse range of them in a single map (Fig. [@ref:LABEL:fig:heatmap] ), could provide a concise but broad insight about porous material characteristics.", "\\newline </subsection> <subsection> <title> 3.3 Model performance </title> Model training is performed both in CPU and GPU architectures.", "CPU\u2013based computations are done on a machine with four 3.2 GHz.", "Intel Xenon processors and 32 GB of memory.", "This arrangement enables the model to be trained in 3 to 4 minutes per epoch.", "In addition, prediction of the porous media features based on the trained model takes 9.23 ms per sample on average which is 4 to 6 orders of magnitude faster if the features are to be extracted by physical simulations (Fig. [@ref:LABEL:fig:loss] -a) [@bib:rabbani2019hybrid] .", "\\newline Using GPU to accelerate the training and prediction stages can make this workflow even faster.", "Using an Nvidia GeForce GTX 1660 Ti with Max-Q Design Graphic Card with Compute Capability of 7.5 and 6 GB of memory, we are able to train the model around 4 times faster, in which each epoch would take around 45 s to finish.", "Also prediction of test dataset can be performed with the speed of 0.379 ms per sample on average which is 24 times faster than the CPU\u2013based instance (Fig. [@ref:LABEL:fig:loss] -a) and it is a considerable improvement.", "\\newline At each epoch of training the validation and training losses are measured in terms of mean square error to ensure that over\u2013fitting is not occurring and training has reached an optimum point (Fig. [@ref:LABEL:fig:loss] -b).", "In our case, increasing the number of epochs to more than 100, it hardly reduces the validation loss less than $ 10^{-3} $ .", "So, we have stopped the training at 100 epochs as visualized in Fig. [@ref:LABEL:fig:loss] ."]], "target": "As presented in Table , the total number of parameters to be trained is 9,282,579. On the other hand, considering the filtered outlier data, the total training data points would be $ 10153\\times 1515 $ which is 65% more than the trainable parameters and this makes it hard for the network to memorize data points instead of learning the trends. Thus, based on the above\u2013mentioned numbers and decreasing trend of the validation loss, it can be stated that over\u2013fitting has not occurred before 100 epochs."}, {"tabular": ["    &  Basis ", " Texture  &  asphalt  &  carpet  &  coarse  &  concrete  &  granite  &  tiles  &  wood  &  union ", " asphalt  &  95.24  &  95.39  &  95.24  &  94.32  &  95.08  &  95.70  &  94.32  &  94.47 ", " carpet  &  100  &  99.92  &  100  &  100  &  100  &  99.92  &  100  &  100 ", " coarse  &  99.09  &  99.35  &  99.09  &  99.35  &  99.09  &  99.35  &  98.83  &  98.83 ", " concrete  &  91.84  &  93.73  &  92.72  &  93.35  &  93.22  &  93.22  &  91.72  &  92.85 ", " granite  &  94.43  &  93.85  &  94.08  &  93.62  &  94.43  &  94.08  &  93.16  &  93.97 ", " tiles  &  98.27  &  98.40  &  97.90  &  98.27  &  98.09  &  98.40  &  97.59  &  97.78 ", " wood  &  76.85  &  78.78  &  78.14  &  77.81  &  77.81  &  78.78  &  77.49  &  77.49  "], "ref_sec": [["<section> <title> I Introduction </title>  The Global Positioning System (GPS) receiver has become an essential component of both hand-held mobile devices and vehicles of all types.", "Applications of GPS, however, are constrained by a number of known limitations.", "A GPS receiver must have access to unobstructed lines of sight to a minimum of four satellites, and obscured satellites significantly jeopardize localization quality.", "Indoors, a GPS receiver either is slow to obtain a fix, or more likely does not work at all.", "Even outdoors, under optimal circumstances, accuracy is limited to a few meters (or perhaps a meter with modern SBAS systems).", "These limitations make GPS insufficient for fine-positioning applications such as guiding a car to a precise location in a parking lot, or guiding a robot within an indoor room or warehouse.", "To overcome the robustness and accuracy limitations of GPS, alternative localization technologies have been proposed, which are either less accurate than GPS (e.g., triangulation of cellphone towers and WiFi hotspots), or expensive and cumbersome to deploy (e.g., RFID localization or special-purpose sensors embedded in the environment).", "Inertial navigation and odometry, which are often used in robotics for fine-positioning tasks, require a known initial position, drift over time, and lose track (requiring manual re-initialization) when the device is powered off.", "\\newline This paper proposes a system that provides millimeter-scale localization, both indoors and outside on land.", "The key observation behind our approach is that seemingly-random ground textures exhibit distinctive features that, in combination, provide a means for unique identification.", "Even apparently homogeneous surfaces contain small imperfections \u2013 cracks, scratches, or even a particular arrangement of carpet fibers \u2013 that are persistently identifiable as local features.", "While a single feature is not likely to be unique over a large area, the spatial relationship among a group of such features in a small region is likely to be distinctive, at least up to the uncertainty achievable with coarse localization methods such as GPS or WiFi triangulation.", "Inspired by this observation, we construct a system called Micro-GPS that includes a downward-facing camera to capture fine-scale ground textures, and an image processing unit capable of locating that texture patch in a pre-constructed compact database within a few hundred milliseconds.", "\\newline The use of image features for precise localization has a rich history, including works such as Photo Tourism [@bib:Snavely2006] and Computational Re-Photography [@bib:Bae2010] .", "Thus, a main contribution of our work is determining how some of the algorithms used for feature detection and matching in \u201cnatural\u201d images, as used by previous work, can be adapted for \u201ctexture-like\u201d images of the ground.", "In searching for a robust combination of such methods, we exploit two key advantages of ground-texture images.", "First, the ground can be photographed from much closer range than typical features in the environment, leading to an order-of-magnitude improvement in precision.", "Second, the statistics of texture-like images lead to a greater density of features, leading to greater robustness over time.", "\\newline Our system consists of two phases: an offline database construction phase, and an online localization phase (Figure [@ref:LABEL:fig:teaser] ).", "We begin by collecting ground texture images and aligning them using global pose optimization.", "We extract local features (keypoints) and store them in a database, which is subsequently compressed to a manageable size.", "For localization, we find keypoints in a query image and search the database for candidate matches using approximate nearest neighbor matching.", "Because it is common for more than 90% of the matches to be spurious, we use voting to reject outliers, based on the observation that inlier matches will vote for a consistent location whereas outliers distribute their votes randomly.", "Finally, we use the remaining inlier matches to precisely calculate the location of the query image.", "\\newline The major contributions of this paper are: \\newline <list> \\ Describing a low-cost global localization system based on ground textures and making relevant code and instructions available for reproduction.", "\\newline \\ \\ Capturing and making available datasets of seven indoor and outdoor ground textures.", "\\newline \\ \\ Investigating the design decisions necessary for practical matching in texture-like images, as opposed to natural images.", "This includes the choice of descriptor, strategies for reducing storage costs, and a robust voting procedure that can find inliers with high reliability.", "\\newline \\ \\ Demonstrating a real-world application of precise localization: a robot that uses Micro-GPS to record a path and then follow it with sub-centimeter accuracy.", "\\newline \\ </list> \\newline The ability to localize a vehicle or robot precisely has the potential for far-reaching applications.", "A car could accurately park (or guide the driver to do so) in any location it recognizes from before, avoiding obstacles mere centimeters away.", "A continuously-updated map of potholes could be used to guide drivers to turn slightly to avoid them.", "The technology applies equally well to vehicles smaller than cars, such as Segways, electric wheelchairs, and mobility scooters for the elderly or disabled, any of which could be guided to precise locations or around hard-to-see obstacles.", "Indoor applications include guidance of warehouse robots and precise control over assistive robotics in the home.", "\\newline  </section>"], ["<section> <title> II Related Work </title>  Textures for Tracking and Localization: Textures such carpet, wood grain, concrete or asphalt all have bumps, grooves, and variations in color from location to location, and we typically use the overall pattern or {statistics} of this variation to recognize a particular material.", "Indeed computer-based modeling and recognition of textures traditionally proceeded along statistical lines [@bib:Dana:1999,Leung:2001] .", "Moreover, researchers have successfully synthesized new texture by example using parametric [@bib:heeger1995pyramid] and non-parametric [@bib:efros1999texture] models.", "However, when we study the {particular} arrangement of bumps and variations present at any location in real-world textures, we find that it is unlikely to be repeated elsewhere.", "\\newline [@bib:kelly2007field] introduce a warehouse automation system in which a downward facing camera installed on each robot is used to help track the robot.", "They observe that ground surfaces usually exhibit cracks and scratches, and it is possible to track the motion of the camera over a pre-constructed visual map.", "This work, however, still assumes a known initial location and surface textures are leveraged only for pairwise ( {local} ) frame matching, much as is done in an optical mouse.", "Other similar systems [@bib:fang2009ground,kozak2016ranger] align the test frame with a small set of map frames determined either by an odometry or the most recent successful estimation.", "In contrast, our approach performs {global} localization, which could be used to initialize tracking systems such as these.", "\\newline [@bib:clarkson2009fingerprinting] demonstrate that seemingly-random textures can provide a means for unique identification.", "The authors observe that the fine-scale variations in the fibers of a piece of paper can be used to compute a \u201cfingerprint\u201d that uniquely identifies the piece of paper.", "Our work demonstrates that ground textures, including man-made ones such as carpet, share similar properties at sufficiently fine scales, and thus may be used for localization.", "\\newline Relocalization: Structure from motion allows reconstruction of a large scale 3D point cloud offline, but relocating a newly captured image in the reconstructed point cloud without any initial guess about the camera position is challenging.", "Previous works explore direct 2D-to-3D matching [@bib:sattler2011fast] to estimate the 6 DoF pose of a photo with respect to a reconstructed point cloud.", "[@bib:li2012worldwide] propose a method to leverage a co-occurrence prior for RANSAC and achieve relocalization on a larger georegistered 3D point cloud within a few seconds.", "Relocalization is an essential module of modern SLAM systems, such as ORB-SLAM [@bib:mur2015orb] , which uses a bag-of-words model for matching.", "[@bib:kendall2015posenet] train a convolutional neural network (PoseNet) to regress the input RGB image to the 6-DoF camera pose.", "Researchers have also explored using skylines from omni-images to perform relocalization [@bib:ramalingam2009geolocalization] .", "\\newline All the above approaches, except PoseNet, involve large-scale feature matching, which quickly becomes a bottleneck because of the number of false matches.", "To speed up feature matching, more compact models can be constructed by selecting a subset of stable 3D points from the original models [@bib:li2010location,cao2014minimal] .", "An effective approach to handle a high outlier ratio is voting [@bib:zeisl2015camera] .", "This has also proven successful in the field of image retrieval, where spatial verification is commonly applied to rerank the retrieved list of images, and variants of Hough voting have been proposed to improve efficiency and robustness [@bib:avrithis2014hough,wu2015adaptive,schonbergervote] .", "With more sensors available, one can utilize the gravity direction [@bib:svarm2016city] as an additional constraint.", "[@bib:baatz2010handling] leverage both gravity direction and a 3D scene model to rectify images, transforming the 6-DOF pose estimation problem into a 3-DOF problem.", "\\newline Mobile devices are ideal deployment platforms for a relocalization system.", "[@bib:lim2015real] achieve localization on a micro aerial vehicle at interactive framerates by distributing feature extraction over multiple frames.", "[@bib:middelberg2014scalable] achieve real-time performance by combining online camera tracking and an external localization server.", "[@bib:irschara2009structure] and [@bib:wendel2011natural] demonstrate that GPUs, which are now widely available on mobile processors, can be used to accelerate localization.", "\\newline Almost all of the above relocalization techniques rely on landmarks, such as buildings, that are normally positioned at least a few meters from the camera.", "This distance, combined with finite image resolution and inherent uncertainty in camera intrinsics, means that even a small error in feature localization results in a large uncertainty in estimated camera pose.", "This inaccuracy can be ameliorated by increasing the field of view of the camera [@bib:schonbein2014omnidirectional,arth2011real] , because as more features are detected, more constraints can be introduced to improve pose estimation.", "Further uncertainty comes from the ambiguity in landmark identification, since it is not unusual to find buildings or parts of buildings (such as windows) that appear the same from a significant distance.", "Moreover, natural images used by the above systems suffer from perspective foreshortening, which brings difficulties to feature matching.", "Many features are not \u201ctime-invariant\u201d in highly dynamic scenes.", "Thus, it is necessary to update the database frequently.", "Finally, these systems can be affected by changes in lighting.", "In contrast with these systems, our work positions the camera close to the texture being imaged and uses controlled lighting, leading to higher precision and robustness.", "\\newline  </section>"], ["<section> <title> III System </title>  <subsection> <title> III-A Mapping </title> Hardware Setup and Data Collection: Our imaging system consists of a Point Grey CM3 grayscale camera pointed downwards at the ground (Figure [@ref:LABEL:fig:teaser] , left).", "A shield blocks ambient light around the portion of the ground imaged by the camera, and a set of LED lights arranged symmetrically around the lens provides rotation-invariant illumination.", "The distance from the camera to the ground is set to 260 mm for most types of textures we have experimented with.", "Our system is insensitive to this distance, as long as a sufficient number of features can be detected.", "The camera output is processed by an NVIDIA Jetson TX1 development board.", "Our prototype has the camera and development board mounted on a mobile cart, which may be moved manually or can be driven with a pair of computer-controlled motorized wheels.", "The latter capability is used for the \u201cautomatic path following\u201d demonstration described in Section [@ref:LABEL:sec:follow] .", "For initial data capture, however, we manually move the cart in a zig-zag path to ensure that an area can be fully covered.", "This process, while is easily mastered by non-experts, could be automated by putting more engineering effort or even through crowd-sourcing when there are more users.", "\\newline Image Stitching: To construct a global map, we assume the that surface is locally planar, which is true even for most outdoor surfaces.", "Our image stitching pipeline consists of frame-to-frame matching followed by global optimization, leveraging extensive loop closures provided by the zig-zag path.", "Since the computation becomes significantly more expensive as the area grows, we split a large area into several regions (which we reconstruct separately) and link the already-reconstructed regions.", "This allows us to quickly map larger areas with decent quality.", "Figure [@ref:LABEL:fig:teaser] , right shows the \u201casphalt\u201d dataset, which covers 19.76 $ \\text{m}^{2} $ in high detail.", "\\newline Datasets: We have experimented with a variety of both indoor and outdoor datasets, covering ground types ranging from ordered (carpet) to highly stochastic (granite), and including both the presence (concrete) and absence (asphalt) of visible large-scale variation.", "We have also captured test images for the datasets on a different day (to allow perturbations to the ground surfaces) to evaluate our system.", "Figure [@ref:LABEL:fig:example_textures] shows example patches from our dataset.", "We will make these datasets, together with databases of SIFT features and test-image sequences, available to the research community.", "\\newline Database Construction: The final stage in building a map is extracting a set of features from the images and constructing a data structure for efficiently locating them.", "This step involves some key decisions, which we evaluate in Section [@ref:LABEL:sec:evaluation] .", "Here we only describe our actual implementation.", "We use the SIFT scale-space DoG detector and gradient orientation histogram descriptor [@bib:lowe2004distinctive] , since we have found it to have high robustness and (with its GPU implementation [@bib:wu2007siftgpu] ) reasonable computational time.", "For each image in the map, we typically find 1000 to 2000 SIFT keypoints, and randomly select 50 of them to be stored in the database.", "This limits the size of the database itself, as well as the data structures used for accelerating nearest-neighbor queries.", "We choose random selection after observing that features with higher DoG response are not necessarily highly repeatable features: they are just as likely to be due to noise, dust, etc.", "To further speed up computation and reduce the size of the database, we apply PCA [@bib:pearson1901liii] to the set of SIFT descriptors and project each descriptor onto the top $ k $ principal components.", "As described in Section [@ref:LABEL:sec:evaluation] , for good accuracy we typically use $ k=8 $ or $ k=16 $ in our implementation, and there is minimal cost to using a \u201cuniversal\u201d PCA basis constructed from a variety of textures, rather than a per-texture basis.", "\\newline One of the major advantages of our system is that the height of the camera is fixed, so that the scale of a particular feature is also fixed.", "This means that when searching for a feature with scale $ s $ in the database, we only need to check features with scale $ s $ as well.", "In practice, to allow some inconsistency, we quantize scale into 10 buckets and divide the database into 10 groups based on scale.", "Then we build a search index for each group using FLANN [@bib:muja2009fast] .", "During testing, given a feature with scale $ s $ , we only need to search for the nearest neighbor in the group to which $ s $ belongs.", "\\newline </subsection> <subsection> <title> III-B Localization </title> The input to our online localization phase is a single image.", "We assume that the height of the camera above the ground is the same as during mapping (or that the ratio of heights is known), so that the image scale is consistent with the images in the database.", "\\newline Feature Computation and Matching: We first extract SIFT features from the test image and project onto $ k $ principal components, as in database construction.", "For each descriptor, we search for the nearest neighbor using the pre-build search index for the appropriate scale.", "\\newline Precise Voting: Recall that we only keep 50 features per database image, so only a small subset of features will have a correct match in the database.", "Finding this small set of inliers is challenging, since methods such as RANSAC work poorly if outliers greatly outnumber inliers.", "\\newline We instead adopt a voting approach based on the observation that, due to the randomness of ground textures, false matches are usually evenly distributed in the map.", "Fortunately, since true matches usually come from one or two images, they are concentrated in a small cluster.", "Figure [@ref:LABEL:fig:precise_voting] , left, shows a heat map of feature matches in a database, with red indicating high density, green intermediate, and blue indicating low density.", "While we are able to build a system based on this principle, the correct features are distributed throughout the entire area corresponding to the test image.", "This leads to poor robustness, because there is only a moderately-high density of votes in the map near the location of the test image.", "The solution is to concentrate the votes: we want all of the true features to vote for the {same} point in the map, leading to a much greater difference between the peak corresponding to the true location and the background density of outliers.", "\\newline In particular, each feature casts a vote for {the origin of the test image} by assuming that nearest neighbors are true matches.", "Denote a feature extracted from the test image as $ f_{t} $ and its nearest neighbor in the database as $ f_{d} $ .", "If the feature pair $ \\{f_{t},f_{d}\\} $ is a true match, we can compute the pose of the test image $ T $ in world coordinates, denoted $ [R|t]_{T}^{W} $ , by composing the pose of $ f_{d} $ in world coordinates and the pose of $ f_{t} $ in the test image: \\newline <equation> $ [R|t]_{T}^{W}=[R|t]_{f_{d}}^{W}\\;[R|t]_{f_{t}}^{f_{d}}\\;[R|t]_{T}^{f_{t}}, $ </equation> where $ [R|t]_{f_{t}}^{f_{d}} $ is assumed to be the identity.", "We then vote for the location of the origin of the test image, which is the translational component of $ [R|t]_{T}^{W} $ .", "\\newline Using this strategy, implemented via voting on a relatively fine spatial grid with each cell set to 50 $ \\times $ 50 pixels, we find a much tighter peak of votes relative to the uniform background of outliers, as shown in Figure [@ref:LABEL:fig:precise_voting] , right.", "After voting, the cell with the highest score is very likely to contain the true origin of the test image.", "We select all of the features in that peak as likely inliers, and perform RANSAC just on them to obtain a final estimate of the pose of the image.", "\\newline </subsection>  </section>"], ["<section> <title> IV Evaluation </title>  In order to evaluate the accuracy and robustness of a localization system, a typical approach would be to obtain ground-truth location and pose using a precise external measurement setup.", "However, this is impractical in our case due to the large areas mapped and the precision with which we are attempting to localize.", "Moreover, we are more interested in {repeatability} , rather than absolute accuracy, given that most of the applications we envision will involve going to (or avoiding) previously-mapped locations.", "\\newline We therefore adopt an evaluation methodology based on comparing the query image against an image captured during mapping.", "Using the pose predicted by Micro-GPS, we find the closest image in the database, and compute feature correspondences (using all SIFT features in the image, not just the features stored in the database).", "If there are insufficiently many correspondences, we mark the localization result as a failure.", "We then compute a best-fit relative pose using those features.", "If the pose differs by more than 30 pixels (4.8 mm) in translation or 1.5 $ ^{\\circ} $ in rotation from the pose output by Micro-GPS, we again mark the result as a failure.", "Finally, given a sequence of consecutive poses that should be temporally coherent, we detect whether the poses of any frames differ significantly from their neighbors.", "\\newline The performance of our system, implementing the pipeline described in Section [@ref:LABEL:sec:online_localization] , is shown in Table [@ref:LABEL:tab:microgps] .", "The second column shows the elapsed time between capture of database and test sequence, which demonstrates that our system is robust against changes in the scene.", "The two columns at right show performance with 8-dimensional and 16-dimensional descriptors, respectively.", "In the following, unless otherwise specified, we use 16-dimensional descriptors due to their good performance across all textures.", "\\newline The correct pose is recovered over 90% of the time for most datasets (with independent per-frame localization and no use of temporal coherence), with the exception of the wood floor.", "This is because relatively few SIFT features are available in this dataset.", "(We have recently demonstrated that a highly repeatable feature detector can be learned in a texture-specific manner [@bib:zhangdetector] .", "Unfortunately, such a pipeline is currently not efficient enough for a mobile platform.) \\newline <subsection> <title> IV-A Impact of Design Decisions </title> Selection of Feature: We first evaluate the impact on accuracy of using different combinations of feature detector and descriptor.", "While SIFT [@bib:lowe2004distinctive] has been popular since its introduction more than a decade ago, more recent alternatives such as SURF [@bib:bay2006surf] , ORB [@bib:rublee2011orb] have been shown to achieve similar performance at lower computational cost.", "Even more recently, convolutional neural networks have been used in learned descriptors [@bib:tian2017l2,mishchuk2017working,he2018local,zhang2017learning,luo2018geodesc,keller2018learning] to achieve much better matching performance than SIFT.", "In all of these cases, however, the performance has been optimized for {natural} , rather than texture-like, images.", "To evaluate the effectiveness of a learned descriptor on texture images, we select the recent well-performing HardNet [@bib:mishchuk2017working] as our backbone network and learn a texture descriptor (HardNet-texture) using patches cropped from our dataset.", "During training, we also perform non-uniform intensity scaling to account for possible changes in exposure.", "Figure [@ref:LABEL:fig:compare_descriptors] compares the accuracy of different combinations of feature detector and descriptor.", "The SIFT {detector} outperforms both SURF and ORB, while both SIFT and HardNet-texture perform better than alternative {descriptors} .", "Because the SIFT descriptor can withstand more aggressive dimension reduction, it is the best choice for our current deployment.", "However, we observe that HardNet-texture shows significant improvement compared to the original HardNet optimized for natural images [@bib:winder2007learning] .", "This suggests that domain-specific training may hold the promise for future improvements in the quality of learned descriptors.", "\\newline Choice of PCA Basis: We next investigate whether the PCA basis used for dimensionality reduction should be specific to each dataset, or whether a \u201cuniversal\u201d basis computed from the union of different textures achieves similar performance."]], "target": "Table shows localization performance for each combination of texture and PCA basis, including a basis computed from the union of all datasets. The difference caused by switching PCA basis is negligible, and we conclude that there is no drawback of using a single PCA basis computed from all of the datasets."}, {"tabular": ["    &  OOV rate ", " in-domain  &  ", " medical  &  2.42% ", " out-of-domain  &  ", " IT  &  20.09% ", " koran  &  18.63% ", " law  &  9.39% ", " subtitles  &  18.16%  "], "ref_sec": [["<section> <title> 1 Introduction </title>  Even though neural models have improved the state-of-the-art in machine translation considerably in recent years, they still underperform in specific conditions.", "One such condition is out-of-domain translation.", "[@bib:Koehn2017] found that neural machine translation (NMT) systems perform poorly in such settings and that their poor performance cannot be explained solely by the fact that out-of-domain translation is difficult: non-neural, statistical machine translation (SMT) systems were superior at this task.", "For this reason, [@bib:Koehn2017] identified translation of out-of-domain text as a key challenge for NMT.", "\\newline Catastrophic failure to translate out-of-domain text can be viewed as overfitting to the training domain, i.e. systems learn idiosyncrasies of the domain rather than more general features.", "Our goal is to learn models that generalize well to unseen data distributions, including data from other domains.", "We will refer to this property of showing good generalization to unseen domains as domain robustness .", "\\newline We consider domain robustness a desirable property of NLP systems, along with other types of robustness, such as robustness against adversarial examples ( [@bib:Goodfellow2015] ) or typos in the input [@bib:Belinkov2018] .", "While domain adaptation with small amounts of parallel or monolingual in-domain data has proven very effective for NMT [@bib:luong2015,sennrich-haddow-birch:2016:P16-11,R17-1049,li-EtAl:2019:WMT1] , the target domain(s) may be unknown when a system is built, and there are language pairs for which training data is only available for limited domains.", "Hence, domain robustness of systems without any domain adaptation is not only of theoretical interest, but also relevant in practice.", "\\newline Model architectures and training techniques have evolved since [@bib:Koehn2017] \u2019s study, and it is unclear to what extent this problem still persists.", "We therefore revisit the hypothesis that NMT systems exhibit low domain robustness.", "In preliminary experiments, we demonstrate that current models still fail at out-of-domain translation: BLEU scores drop drastically for test domains other than the training domain.", "However, the overall out-of-domain translation quality of NMT systems is now on par with SMT systems.", "\\newline An analysis of our baseline systems reveals that hallucinated content occurs frequently in out-of-domain translations (see Figure [@ref:LABEL:fig:hallucinated-example] for an example).", "Several authors present anecdotal evidence for NMT systems occasionally falling into a hallucination mode where translations are grammatically correct but unrelated to the source sentence [@bib:Arthur2016,Koehn2017,Nguyen2018] .", "Our manual evaluation shows that hallucination is more pronounced in out-of-domain translation.", "We therefore expect methods that alleviate the hallucination problem to indirectly improve domain robustness.", "\\newline As a means to reduce hallucination, we experiment with several techniques and assess their effectiveness in improving domain robustness: reconstruction [@bib:Tu2017,niu-etal-2019-bi] , subword regularization [@bib:Kudo2018] , neural noisy channel models [@bib:li2016mutual,yee2019simple] , and defensive distillation [@bib:papernot2016distillation] , as well as combinations of these techniques.", "The main contributions of this paper are: \\newline <list> \\ we perform an analysis of SMT and NMT systems that confirms that while in-domain BLEU increased, domain robustness remains a major problem even with state-of-the-art Transformer architectures.", "Our comparison of SMT and NMT shows differences in how performance degrades in unseen domains: SMT mostly suffers in terms of fluency, while NMT tends to produce more fluent, but less adequate translations (hallucinations).", "\\newline \\ \\ we test several techniques related to adequacy, robustness, or out-of-domain translation in regard to their effectiveness in improving domain robustness in NMT.", "We find that several techniques are moderately successful, most notably reconstruction, which reduces the average percentage of hallucinations in out-of-domain test sets from 35% to 29%.", "\\newline \\ \\ we show that despite moderate improvements, domain robustness remains a challenge in NMT, and provide code and data sets to serve as baselines for future work.", "\\newline \\ </list> \\newline  </section>"], ["<section> <title> 2 Data Sets </title>  We report experiments on two different translation directions: German $ \\to $ English (DE $ \\to $ EN) and German $ \\to $ Romansh (DE $ \\to $ RM).", "\\newline <subsection> <title> 2.1 German \u2192 English </title> For all DE $ \\to $ EN experiments, we use the same corpora as [@bib:Koehn2017] , available from OPUS [@bib:Lison2016] .", "\\newline We use corpora from OPUS to define five domains: medical , IT , koran , law and subtitles .", "See Table [@ref:LABEL:tab:corpora] for an overview of sizes per domain.", "The domains are quite distant, and we therefore expect that systems trained on a single domain will have low domain robustness if tested on other domains.", "\\newline For each domain, we select 2000 consecutive sentence pairs each for development and testing.", "Our test sets are different from [@bib:Koehn2017] , so results are not directly comparable.", "In all experiments, the medical domain serves as the training domain, while the remaining four domains are used for testing.", "\\newline </subsection> <subsection> <title> 2.2 German \u2192 Romansh </title> To complement our DE $ \\to $ EN experiments, we also train systems for DE $ \\to $ RM.", "Romansh is a Romance language that, with an estimated $ 40\\,000 $ native speakers, is low-resource, but has some parallel resources thanks to its status as an official Swiss language.", "Domain robustness is of particular relevance in low-resource settings since training data is typically only available for few domains.", "Our training data consists of $ 100\\,000 $ sentence pairs, specifically the Allegra corpus by [@bib:scherrer-cartoni-2012-trilingual] which contains mostly law text, and an in-house collection of government press releases.", "As test domain (unseen during training), we use blog posts from Convivenza .", "From both data sets we randomly select 2000 consecutive sentence pairs as test sets.", "\\newline </subsection>  </section>"], ["<section> <title> 3 State-of-the-art Models Exhibit Low Domain Robustness </title>  In this section, we establish that current NMT systems exhibit low domain robustness by analyzing our baseline systems automatically and manually.", "\\newline <subsection> <title> 3.1 Experimental Setup for Baseline Models </title> We use Moses scripts for punctuation normalization and tokenization.", "We apply truecasing trained on in-domain training data.", "Similarly, we apply BPE [@bib:Sennrich2016] with 32k (DE $ \\to $ EN) and 16k (DE $ \\to $ RM) merge operations learned from in-domain data.", "We train two baselines: \\newline NMT Baseline A standard Transformer base model trained with Sockeye [@bib:DBLP:journals/corr/VaswaniSPUJGKP17,Hieber2018] .", "\\newline SMT Baseline A standard, phrase-based statistical model trained with Moses [@bib:koehnmoses] , using mtrain [@bib:laubli2018mtrain] as frontend with standard settings.", "\\newline We always test on several domains, including the training domain.", "We use a beam size of 10 to translate test data.", "We report case-sensitive BLEU [@bib:Papineni2002] scores on detokenized text, computed with SacreBLEU [@bib:post:2018:WMT] .", "\\newline </subsection> <subsection> <title> 3.2 Analysis of Baseline Systems </title> Tables [@ref:LABEL:tab:baselines-de-en] and [@ref:LABEL:tab:baselines-de-rm] show automatic evaluation results for all our baseline models.", "Neural models achieve good performance on the respective in-domain test sets (61.5 BLEU on medical for DE $ \\to $ EN; 52.5 BLEU on law for DE $ \\to $ RM), but on out-of-domain text, translation quality is clearly diminished, with an average BLEU of roughly 12 (DE $ \\to $ EN) and 19 (DE $ \\to $ RM).", "The following analysis will focus on our DE $ \\to $ EN baseline systems.", "\\newline Compared to results reported by [@bib:Koehn2017] , NMT has improved markedly since their study was conducted, and is now on par with SMT in out-of-domain settings ( $ 11.8 $ BLEU versus $ 11.7 $ ).", "However, on the in-domain test set, our NMT baseline outperforms the SMT baseline by 3 BLEU.", "This result suggests that higher in-domain performance does not guarantee better out-of-domain translations.", "\\newline Unknown words constitute one possible reason for failing to translate out-of-domain texts."]], "target": "As shown in Table , the percentage of words that are not seen during training is much higher in all out-of-domain test sets. However, unknown words cannot be the only reason for low translation quality: The test sets with the lowest BLEU scores ( koran and subtitles ) actually have an out-of-vocabulary (OOV) rate similar to the IT test set, where BLEU scores are much higher for both baseline models."}, {"tabular": ["  Property  &  Value ", " Average shortest path length  &  6.23 ", " Average clustering  &  0.03 ", " Degree assortativity  &  -0.2  "], "ref_sec": [["<section> <title> 1 Introduction </title>  During the last years we have experienced an explosion of applications and solutions based on Blockchain technologies [@bib:nakamoto2008bitcoin,Swan:2015:BBN:3006358] .", "Bitcoin and other platforms such as Ethereum have implemented and established decentralized ledgers to perform transactions using digital currencies and tokens worldwide.", "On these platforms, transactions occur through a distributed system that validates operations by making collective decisions rather than querying centralized authorities.", "While many researchers and innovators are currently working on creating new applications for this type of technologies [@bib:WEF2018] , it is a current challenge for product developers to understand how people behave, interact and self-organize in such systems.", "Human behavioral patterns reveal choices and preferences [@bib:Pentland:2008:HST:1450928] which can inform product development models [@bib:clack2016smart] for solving critical issues such as token adoption and valuation [@bib:tokcatalini2018initial,tokyoder1983price,tokcong2018tokenomics] .", "\\newline Blockchain based solutions can be understood in the framework of multi-layered systems [@bib:DBLP:journals/corr/abs-1807-00955,PhysRevE.89.032804] .", "A web of computers at the bottom layer compete with each other for validating transactions and storing them on the chain.", "This process is called mining and comprises the technical side of the platform.", "Mining is the mechanism that ensures secure decentralized operations [@bib:8029379] .", "Miners can be rewarded for validating transactions.", "Transactions are first validated locally against a copy of the ledger and later contrasted with results from other miners before final inclusion on the chain.", "The security [@bib:Eyal:2018:MEB:3234519.3212998] , privacy [@bib:rahulamathavan2017privacy] and scalability [@bib:eyal2016bitcoin] of these tasks are major technological challenges for the future development of tokenized systems.", "\\newline Another network runs on top of miners which is a social network [@bib:yaniv2018,7796940,fi8010007] .", "In this network, users (and robots) perform transactions with each other.", "These transactions can consist of money transfer or other applications depending on the token and associated service.", "Understanding the structure and dynamics of social networks [@bib:Krafft2016,Yaniv2014,Pentland2012] , as well as user behaviors more generally, is crucial for designing new products and evaluating the performance of current ones [@bib:UserBeh2006] .", "The complexity of possible behaviors is measured in terms of information [@bib:BarYam2005ComplexityR] and capturing it is fundamental for building strategies that deal with changing environments [@bib:Ashby1991] .", "\\newline ERC20 is a protocol to implement tokens on Ethereum\u2013the platform of the ETH crypto-currency.", "Tokens are created with smart contracts [@bib:Pompianu2017,Anderson2016] , which consist in software that runs when transactions are performed.", "Transactions are based on tokens, which are assigned to specific contracts.", "Users can either buy ETH or tokens of other contracts.", "Not all contracts are open to the public.", "Some of them are closed and private.", "Analogously, not all tokens have an economic purpose [@bib:Christidis2016] .", "Tokens can symbolize company shares, property documents, or supply chain steps, among others.", "\\newline Tokens may revolutionize economic activities.", "Current developments include new ways to raise funds for projects and entrepreneurs, as well as new ways to deploy and monetize digital applications for innovators [@bib:CHEN2018567] .", "Different economic activities may tune the currency they operate with in order to satisfy their particular needs [@bib:DBLP:journals/corr/abs-1807-00955] .", "Some activities require tokens to be adopted in order to interact with the service, while other tokens can be simply acquired and held in the wallet.", "Understanding the reasons for token adoption and use is a major challenge for Blockchain product developers.", "The large variety of use purposes and services makes it non-trivial to guess the value and potential adoption of tokens.", "Access to data provides the opportunity to measure and observe the current use of Blockchain services and learn from their statistical properties.", "\\newline In this work we study the social network that arises during transactions on the ERC20 platform during an arbitrary day.", "In a previous work [@bib:yaniv20182] , we showed that the transaction network reached a stable, complex topology over time [@bib:Pentland2014] .", "That study validated the application of network analysis for understanding the structure of tokenized networks.", "Here we further investigate the complex structure of the transaction network and characterize token adoption and transaction activity.", "We show heterogeneous user roles and behaviors in terms of transaction diversity and activity.", "We also characterize tokens in terms of emergent adoption and properties in the transaction network.", "\\newline In section [@ref:LABEL:methods] we present our data set and describe how we construct the networks.", "In section [@ref:LABEL:results] we show visualizations and properties of the transaction network, including those of users and tokens.", "In section [@ref:LABEL:discussion] we discuss our results and provide conclusions.", "\\newline  </section>"], ["<section> <title> 2 Methods and Data </title>  We use transaction data from the ERC20 platform on Nov 4th, 2018.", "Each record represents a transaction.", "It includes a time stamp and identifiers for the seller, buyer and token.", "Identifiers are formatted by hashcodes.", "The dataset has a total of 150,506 records, regarding 2,718 tokens and 86,810 users.", "Out of the total, 52,446 users are exclusive buyers, 15,398 users are exclusive sellers and 18,966 users do both activities.", "\\newline In order to analyze transaction activity on ERC20, we create a social network.", "Users are represented as nodes.", "Edges represent token transactions.", "Edges are directed from user $ i $ (seller) to $ j $ (buyer) and weighted by the total number of transactions between $ i $ and $ j $ .", "Each transaction is associated with a token.", "Therefore, edges also represent the set of tokens exchanged between $ i $ and $ j $ .", "\\newline  </section>"], ["<section> <title> 3 Results </title>  Visualizations of the transaction network are presented in Figures [@ref:LABEL:fig:visualization] (full network) and [@ref:LABEL:fig:visualization_sub] (giant component).", "Users are represented by dots and connections indicate transactions.", "Distinct edge colors represent transacted tokens.", "If a couple users exchange more than one token, the edge is colored according to the mostly transacted one.", "The structure of this network is comprised by a set of disconnected components (Figure [@ref:LABEL:fig:visualization] bottom) and one giant component (Figure [@ref:LABEL:fig:visualization] top left and Figure [@ref:LABEL:fig:visualization_sub] ).", "The disconnected components are usually associated to a single token.", "Their adoption size ranges widely, from a couple or few users (located at the bottom of Figure [@ref:LABEL:fig:visualization] ) up to several hundreds (located at the top of Figure [@ref:LABEL:fig:visualization] ).", "These colored modules are respectively paired to smart contracts and tokenized services.", "Network representations inform about structural patterns of token adoption beyond the total number of adopters.", "\\newline The larger disconnected components in Figure [@ref:LABEL:fig:visualization] present a hub-spoke structure.", "They respectively consist of a few sellers per tokens (two in a few cases) transacting with multiple buyers exclusively.", "By sorting components by number of users, the figure shows the progression of token adoption from a single transaction up to the hub-spoke structures.", "Some tokens are aggregated with each other in the giant component as intermediary users trade and bridge across communities."]], "target": "The properties of the giant component network are presented in Table . The small world effect is present, and clustering is low (the network is very sparse). The degree assortativity is negative, indicating that hubs are usually connected to lowly connected users rather than to other hubs."}, {"tabular": ["    &  Multi-Ref  &  Single-Ref ", "  &  M  &  R  &  GM  &  M  &  R  &  GM ", " baseline  &  12.8  &  34.0  &  76.9  &  6.9  &  20.9  &  71.2 ", " SMRT  &  13.8  &  36.1  &  77.7  &  8.1  &  24.0  &  72.5 ", " PT + baseline  &  13.6  &  35.8  &  77.5  &  7.1  &  21.7  &  71.5 ", " PT + SMRT  &  13.9  &  36.6  &  77.6  &  7.9  &  23.7  &  72.3  "], "ref_sec": [["<section> <title> 1 Introduction </title>  Non-task-oriented dialog is a low-resource NLP task.", "While large and noisy related corpora exist [@bib:serban2018survey] , the publicly-released curated corpora are small.", "[@bib:serban2018survey] note that smaller corpora have lower lexical diversity and topic coverage, leading to models with poor quality non-diverse responses.", "Pretraining on larger data may improve performance, but requires a large dialog corpus in the right language and related domain.", "\\newline We leverage Simulated Multiple Reference Training [@bib:khayrallah-etal-2020-simulated] to overcome sparse dialog data.", "SMRT uses a word-level knowledge distillation-inspired objective and a paraphraser to simulate multiple references per training example.", "[@bib:khayrallah-etal-2020-simulated] introduce SMRT for machine translation (MT) and simulate training on {all} translations for a source sentence, assuming: (1) all paraphrases of a target are translations of the source; and (2) all translations of the source are paraphrases of the target.", "(1) is true for dialog, but (2) is not\u2014valid chatbot responses vary in meaning.", "SMRT captures {syntactic} diversity though it cannot represent all {semantic} variations.", "\\newline We apply SMRT to chatbots and find that it: (1) improves human and automatic quality scores; (2) improves lexical diversity; (3) performs as well as pretraining in human evaluation with better performance on automatic measures of diversity and quality.", "\\newline  </section>"], ["<section> <title> 2 Method </title>  We model the non-task-oriented dialog system (chatbot) task as conditional language modeling.", "These models are typically trained using Negative Log Likelihood (NLL) with respect to a single reference.", "An alternative approach is Knowledge Distillation [@bib:hinton2015distilling,kim-rush-2016-sequence] which assumes access to a teacher distribution ( $ q(y\\,|\\,x) $ ) and minimizes the cross entropy with the teacher\u2019s probability distribution.", "\\newline <paragraph> <title> Simulated Multiple Reference Training </title> SMRT is structured similarly to word-level Knowledge Distialltion, but uses a paraphraser as the teacher distribution ( $ q(y^{\\prime}\\,|\\,y) $ ).", "The paraphraser conditions on the reference $ y $ (rather than the source $ x $ ) and generates a paraphrase $ y^{\\prime} $ .", "Additionally, SMRT {samples} a new paraphrase of the reference every epoch.", "The SMRT training objective for the $ i^{th} $ target word in the reference $ y $ , given the prompt $ x $ , with a target vocabulary $ \\mathcal{V} $ is: \\newline <equationgroup> <equation> $ \\mathcal{L}_{\\text{SMRT}}=-\\;\\sum_{v\\in\\mathcal{V}}\\Big{[}\\;p_{% \\textsc{paraphraser}}(y^{\\prime}_{i}=v\\,|\\,y,y^{\\prime}_{j<i}) $ $ \\mathcal{L}_{\\text{SMRT}}=-\\;\\sum_{v\\in\\mathcal{V}}\\Big{[}\\;p_{% \\textsc{paraphraser}} $ $ (y^{\\prime}_{i}=v\\,|\\,y,y^{\\prime}_{j<i}) $ </equation> <equation> $ \\times\\;\\log\\big{(}p_{\\textsc{chatbot}}(y^{\\prime}_{i}=v\\,|\\,x,y^% {\\prime}_{j<i})\\big{)}\\Big{]} $ $ \\times\\;\\log\\big{(}p_{\\textsc{chatbot}} $ $ (y^{\\prime}_{i}=v\\,|\\,x,y^{\\prime}_{j<i})\\big{)}\\Big{]} $ </equation> </equationgroup> The paraphraser and chatbot each condition on the previously sampled paraphrase tokens ( $ y^{\\prime}_{j<i} $ ).", "\\newline </paragraph>  </section>"], ["<section> <title> 3 Experimental Setup </title>  <subsection> <title> 3.1 Dialog models </title> We train Transformer [@bib:transformer] chatbots in fairseq using parameters from the flores benchmark for low-resource MT [@bib:guzman-etal-2019-flores] for both a standard NLL baseline and SMRT.", "Following [@bib:khayrallah-etal-2020-simulated] , we sample from the 100 highest probability tokens from the paraphraser distribution at each time-step [@bib:fan-etal-2018-hierarchical] .", "\\newline We train and evaluate on DailyDialog [@bib:li-etal-2017-dailydialog] , a high quality corpus with multiple references for evaluation.", "We train on the $ \\sim\\, $ 80,000 turns of English-learners practicing \u2018daily dialogues\u2019 in various contexts, e.g., chatting about vacation or food.", "\\newline See [@ref:LABEL:app:expt] for full details for replication.", "\\newline </subsection> <subsection> <title> 3.2 Paraphraser </title> We use the state-of-the-art Prism multilingual paraphraser [@bib:thompson-post-2020-automatic,thompson-post-2020-paraphrase] .", "It is trained as a multilingual MT model on $ \\sim\\, $ 100 million sentence pairs in 39 languages.", "Paraphrasing is treated as zero-shot translation (e.g., English to English).", "\\newline </subsection> <subsection> <title> 3.3 Evaluation Protocols </title> Human Evaluation We use Amazon Mechanical Turk to collect human judgments.", "For every HIT we display a prompt and two responses; the worker indicates their preferred response (or tie).", "Following [@bib:baheti-etal-2018-generating] , we employ the pairwise bootstrap test [@bib:efron1994introduction] and report statistical significance at the 95% confidence level.", "\\newline <paragraph> <title> Automatic Quality Evaluation </title> We use multirefeval for DailyDialog [@bib:gupta-etal-2019-investigating] .", "In [@ref:LABEL:sec:results] we report METEOR , ROUGE-L , and Greedy Match for the original and multiple references.", "See [@ref:LABEL:app:results] for all 14 metrics.", "For reading ease we report metrics scaled 0 to 100.", "\\newline </paragraph> <paragraph> <title> Automatic Diversity Evaluation </title> To measure lexical diversity, we use the type/token ratio of unigrams, bigrams, and trigrams [@bib:li-etal-2016-diversity] .", "\\newline </paragraph> </subsection>  </section>"], ["<section> <title> 4 Results </title>  SMRT is preferred over the baseline system in human evaluation, as shown in [@ref:LABEL:tab:main_human] .", "It outperforms the baseline in automatic quality too: see [@ref:LABEL:tab:main_automatic_results] .", "Our {baseline} outperforms nearly all systems in [@bib:gupta-etal-2019-investigating] for these metrics, suggesting it is a strong baseline.", "SMRT has higher lexical diversity than the baseline, though not as high as the human reference response ( [@ref:LABEL:tab:main_lexical_diversity] ).", "\\newline  </section>"], ["<section> <title> 5 Analysis </title>  SMRT outperforms a strong baseline; here we analyze it in additional settings: pretraining and MMI.", "\\newline <subsection> <title> 5.1 Pretraining </title> Pretraining is another way of incorporating auxiliary data in the model.", "We pretrain on the OpenSubtitles corpus [@bib:lison-tiedemann-2016-opensubtitles2016] , which consists of $ \\sim\\, $ 200 million turns from movie subtitles.", "Similar to DailyDialog, it consists of conversational data on a variety of topics.", "After pretraining on OS, we fine-tune on DailyDialog.", "\\newline <paragraph> <title> Results </title> In the human evaluation ( [@ref:LABEL:tab:pretrain_human] ), SMRT performs comparably to baseline pretraining."]], "target": "In automatic evaluation , SMRT outperforms pretraining. We combine SMRT with pretraining and find that this again performs comparably to baseline pretraining in human evaluation, and pretraining with SMRT performs better in the automatic evaluation. Finally, we compare SMRT with and without pretraining, and find with pretraining is preferred in human evaluation, while they perform similarly on the automatic metrics."}, {"tabular": ["  Model  &  @1  &  @MRR ", " Freeze pre-trained agent  &  41.9  &  45.5 ", " No pre-training  &  41.8  &  45.7 ", " Single agent  &  42.4  &  46.4 ", " Ours  &  42.9  &  46.5  "], "ref_sec": [["<section> <title> 1 Introduction </title>  While knowledge graphs (KGs) are widely adopted in natural language processing applications, a major bottleneck hindering its usage is the sparsity of manually curated facts [@bib:min2013distant] , leading to extensive studies on KG completion (or reasoning).", "[@bib:Trouillon2016ComplexEF,dettmers2018convolutional,das2017go,xiong2017deeppath,lin-etal-2018-multi-hop,Meilicke2019AnytimeBR] Traditional approaches on the KG reasoning task are mainly based on logic rules [@bib:landwehr2007integrating,landwehr2010fast,galarraga2013amie,galarraga2015fast] .", "They represent relations as predicates and ontological constraints as first-order logic rules.", "These methods are referred to as symbolic-based methods .", "Despite their good performance in recent work [@bib:Meilicke2019AnytimeBR,meilicke2020reinforced] , the symbolic-based methods are inherently limited by the symbolic representation and rely on whether the associated relations of the given rules can be generalized well, shown in Section [@ref:LABEL:Symbolic] .", "\\newline To resolve the limitation of symbolic representations, embedding-based methods were proposed.", "They learn low-dimensional distributed representations for entities and relations in order to capture semantic meanings.", "These methods [@bib:Bordes2013TranslatingEF,Socher2013ReasoningWN,wang2014knowledge,Yang2014EmbeddingEA,Trouillon2016ComplexEF,dettmers2018convolutional,Dettmers2017Convolutional2K,Sun2019RotatEKG,Zhang_2019] have shown superior performance on various benchmark datasets.", "However, embedding-based methods apply \u201cone-hop\u201d reasoning directly, thus fail to provide human-friendly interpretations for both the learned embeddings and the reasoning modules.", "\\newline To make the decision process more interpretable, many recent efforts formulate KG reasoning as a multi-hop reasoning process [@bib:xiong2017deeppath,das2017go,shen2018reinforcewalk,chen2018variational,lin-etal-2018-multi-hop] .", "A major issue of walk-based methods is that, during the training phase, the agent only receives sparse \u201chit or not\u201d reward signals after a long sequence of decisions.", "[@bib:lin-etal-2018-multi-hop] tries to alleviate this issue by shaping the reward with an embedding-based distance measurement.", "However, the path with the highest probability does not always have the highest shaped reward.", "For this reason, walk-based methods can still be improved.", "Fortunately, we observe that walk-based and symbolic-based methods are complementary to each other: On one hand, symbolic rules can be {fetched even without learning} .", "However, due to the limitation of symbols, they are not easy to be generalized.", "On the other hand, walk-based methods benefit from the embeddings which encapsulate rich semantic information, thus have better generalizability but are {hard to train} only with a sparse signal, i.e. the \u201chit or not\u201d reward.", "\\newline In this work, we aim to tackle the lack of reward in walk-based methods via symbolic-based methods.", "Given a KG, a symbolic-based model is first applied to fetch a set of symbolic rules.", "A set of high-confidence rules are then leveraged to guide the training process of a walk-based model by providing additional rewards when its agent uncovers a path that is within the rule set.", "We note that the agent can have efficiency issues with a huge action space, as the graph can be extremely dense with various relation-entity combinations.", "Fortunately, symbolic rules represented by relations and variables instead of concrete entities allow us to separate the agent into a relation agent that focuses on selecting a path of relations (to form a symbolic rule), and an entity agent that selects the concrete entity given the current relation, so to scale to large and dense graphs.", "The process of selecting a relation-entity combination can be separated into two steps: we first select a relation by a {relation agent} and then select an entity by an {entity agent} .", "Besides, they can interpret the reward from a rule miner in different fashions.", "\\newline  </section>"], ["<section> <title> 2 Related Work </title>  Symbolic-based methods attempt to reveal symbolic patterns of relation paths in the form of first-order logic rules.", "Early works include FOIL and its follow-up works [@bib:quinlan1990learning,landwehr2007integrating,landwehr2010fast] .", "Later, AMIE and AMIE+ [@bib:galarraga2013amie,galarraga2015fast] is developed for efficient mining without counterexamples.", "Recently, AnyBURL [@bib:Meilicke2019AnytimeBR,meilicke2020reinforced] have shown comparable performance to state-of-the-art neural models with significant advantages in efficiency.", "\\newline Embedding-based methods learn a matching score for a target triple with distributed representation, and thus can capture rich semantic information of entities and relations [@bib:Bordes2013TranslatingEF,Socher2013ReasoningWN,Yang2014EmbeddingEA,Trouillon2016ComplexEF,Dettmers2017Convolutional2K,Zhang2018DiscriminativePK,Sun2019RotatEKG] .", "They have achieved superior performance in various datasets.", "However, they are one-hop prediction models that ignore complex patterns for complete reasoning paths.", "They are also short of interpretability for their embeddings and matching functions.", "\\newline Reasoning-based methods capture advantages of both symbolic and embedding representations to generate reasoning paths [@bib:xiong2017deeppath,das2017go,shen2018reinforcewalk,chen2018variational] .", "Recently, Multi-Hop model [@bib:lin-etal-2018-multi-hop] is proposed to leverage pretrained embedding models to compensate for false-negative rewards given by the incomplete graph.", "\\newline  </section>"], ["<section> <title> 3 Problem and Preliminaries </title>  In this section, we review the KG reasoning task.", "We also describe the current symbolic-based and walk-based methods, which are leveraged in the proposed method.", "\\newline <subsection> <title> 3.1 Problem Formulation </title> A KG consisting of fact triples is represented as $ \\mathcal{G}=\\{(e_{i},r,e_{j})\\}\\subseteq\\mathcal{E}\\times\\mathcal{R}\\times% \\mathcal{E} $ , where $ \\mathcal{E} $ and $ \\mathcal{R} $ are the set of entities and relations, respectively.", "Given a query $ (e_{s},r_{q},?) $ where $ e_{s} $ is a subject entity and $ r_{q} $ is a query relation, the task of KG reasoning is to find a set of object entities $ E_{o} $ such that $ (e_{s},r_{q},e_{o}) $ , where $ e_{o}\\in E_{o} $ , is a fact triple missing in $ \\mathcal{G} $ .", "We denote the queries $ (e_{s},r_{q},?) $ as tail queries .", "We note that we can also perform the reversed version $ (?,r_{q},e_{o}) $ , i.e. head queries .", "However, in this paper, we only evaluate our method on tail queries to keep consistent with the majority of existing works.", "\\newline </subsection> <subsection> <title> 3.2 Symbolic-based Methods </title> Some previous work used symbolic Horn rules to perform KG reasoning.", "They mine rules directly from the KG and predict missing facts by grounding these rules.", "In this task, a Horn rule can be represented by binary predicates, i.e. relations, and variables which can be grounded by constants, i.e. entities.", "\\newline Horn rules can be categorized into C rules and AC rules [@bib:Meilicke2019AnytimeBR] , which are generalized from cyclic and acyclic paths in $ \\mathcal{G} $ , respectively.", "\\newline $ \\begin{array}[]{ll}\\textbf{C}:&r(X,Y)\\leftarrow b_{1}(X,A_{2})\\land...\\land b_% {n}(A_{n},Y)\\\\ \\textbf{AC}:&r(X,c_{0})\\leftarrow b_{1}(X,A_{2})\\land...\\land b_{n}(A_{n},c_{n% })\\\\ \\end{array} $ \\newline In these two logical formula, lower-case arguments (i.e. $ c_{0} $ and $ c_{n} $ ) represent constants, and upper-case ones (i.e. $ X $ , $ Y $ and $ A_{n} $ ) stand for variables.", "We use $ r(\\cdots) $ to denote a {rule head} and the conjunction of atoms $ b_{1}(\\cdots),\\dots,b_{n}(\\cdots) $ to denote a {rule body} .", "We note that $ r(c_{i},c_{j}) $ is equivalent to the fact triple $ (c_{i},r,c_{j}) $ .", "\\newline For query $ r(X,Y) $ , if there are other paths starting from entity $ X $ and ending at entity $ Y $ without using relation $ r $ , these cyclic paths can be treated as reasoning paths to find target entity $ Y $ and forms C rules.", "For AC rules, the paths\u2019 ending entities do not need to be Y. If there is a strong correlation between entity $ c_{0} $ and $ c_{n} $ , and an acyclic path\u2019s ending entity is $ c_{n} $ , then we can predict the ending entity $ Y $ as $ c_{0} $ based on this AC rule.", "AC rules are less generalizable compared to C rules especially on large KGs with huge entity space as they require certain entity constants.", "Thus, in this work, we only use C rules as symbolic guidance.", "\\newline One recent symbolic-based method, AnyBURL [@bib:Meilicke2019AnytimeBR] , proves to achieve comparable performance with state-of-the-art neural models.", "It first mines symbolic rules by sampling paths from the $ \\mathcal{G} $ , and then predict object entities by matching subject entities and query relations in the given queries with the rules.", "\\newline However, such methods have limitations.", "The Upper part of figure [@ref:LABEL:fig:percentage] shows a counterexample.", "A rule which has only one instance on the KG cannot be generalized well.", "In addition, as a symbolic reasoner commonly relies on high quality rules, it can behave inconsistently on different datasets.", "The lower part of figure [@ref:LABEL:fig:percentage] shows the average quality of top 15 rules mined from two datasets.", "The y-axis indicates the average percentage of rules that can successfully hit the target entities in the KG.", "For WN18RR, the average percentage of the top 1 rule is much larger than those of other rules.", "Therefore, a symbolic method can easily differentiate these top rules to reason.", "\\newline </subsection> <subsection> <title> 3.3 Neural Reasoning Methods </title> To capture semantic meanings, given a query $ (e_{s},r_{q},?) $ , embedding-based approaches use one-hop reasoning to learn confidence scores of all triples via a matching function $ f(e_{x},r_{q},\\cdot) $ and return $ e_{o} $ such that $ f(e_{s},r_{q},e_{o}) $ has the highest score.", "To make the neural approach more interpretable, given $ e_{s} $ and $ r_{q} $ , walk-based methods learn an agent to reach the target $ e_{o} $ by finding a path from $ e_{s} $ to $ e_{o} $ that implies the query relation $ r_{q} $ .", "At step $ t $ , the current state is represented by a tuple $ s_{t}=(e_{t},(e_{s},r_{q})) $ , where $ e_{t} $ is the entity being visited at step $ t $ .", "The agent then samples the next relation-entity pair to visit from possible actions $ A_{t}=\\{(r^{\\prime},e^{\\prime})|(e_{t},r^{\\prime},e^{\\prime})\\in\\mathcal{G}\\} $ .", "A pseudo self-loop relation $ r_{s} $ that forms action $ (e_{t},r_{s},e_{t}) $ is also included and indicates termination.", "The number of hops is fixed for each query\u2019s reasoning process.", "\\newline </subsection>  </section>"], ["<section> <title> 4 Proposed Method </title>  In this study, we try to leverage both symbolic and neural reasoning approaches.", "For the symbolic method, C rules\u2019 confidence scores only depend on associated relations.", "However, the entities inside the reasoning paths can also contribute to the confidence of these paths.", "We first tried to rerank the pre-mined rules based on the probabilities of generating them in neural reasoning approaches.", "In this way, we have to mask all reasoning paths that are not in the ruleset.", "However, due to the large searching space, the signal is too sparse to train such models.", "Then, instead of strictly mask reasoning paths, we give an extra reward to the reasoning agent if the selected path is a rule.", "We show this result in the [@ref:LABEL:ablation] ablation study.", "As C rules are represented by relation constants and entity variables, we can significantly prune the action space by separate the agent into a relation and entity agent.", "They work alternatively and communicate to perform KG reasoning.", "Extensive experiments show the effectiveness of this joint framework.", "Specifically, for the joint framework, at each reasoning step $ t $ , from entity $ e_{t} $ , the relation agent first chooses a relation $ r_{t} $ and then the entity agent selects an entity $ e_{t+1} $ such that $ (e_{t},r_{t},e_{t+1})\\in\\mathcal{G} $ .", "In this way, the overall action space is greatly reduced.", "Our framework is depicted in Figure [@ref:LABEL:fig:my_label] .", "\\newline <subsection> <title> 4.1 Relation agent </title> At each step $ t $ , the relation agent selects a single relation $ r_{t} $ which is incident to the current entity $ e_{t} $ .", "Given a query and a set of pre-mined rules, the agent first filter out rules whose heads are not same as the query relation, and then it selects $ r_{t} $ from the $ t^{th} $ atoms of the remaining rule bodies, i.e. $ b_{t}(\\cdots) $ in the C rule patterns.", "However, if no rule can significantly outperform others, it will be difficult to select a good $ r_{t} $ .", "\\newline We alleviate this issue by using reinforcement learning techniques to train the relation agent.", "During the pretraining phase which will be described in later parts, it learns the confidence score distribution of all rules.", "During the training phase, it then applies the pretrained symbolic strategy and keeps tuning the relation distribution by utilizing embeddings semantic information to increase performance.", "In another word, the relation agent leverages both confidence scores of pre-mined rules as well as embedding shaped hit rewards.", "\\newline </subsection> <subsection> <title> 4.2 Entity agent </title> At each step $ t $ , the entity agent first calculates probabilities of all candidate entities in the current step entity space based on $ e_{s} $ , $ r_{q} $ , and the entity history $ {\\bm{h}}_{t}^{E} $ .", "After the relation agent selects the current step relation $ r_{t} $ , the entity space will be pruned and only contains entity incident on that relation.", "The entity agent then selects an entity based on pruned entity space.", "In this way, the entity and relation agent can reason independently.", "In experiments, we have also tried to let the entity agent generate distribution based on relation agent pruned entity space.", "In this way, the entity agent takes in the selected relation and can leverage the information from the relation agent.", "However, the entity space may be extremely small and thus the unlikely candidates may become relatively confident.", "In inference time, the entities in a small pruned space may be more likely to be chosen.", "This relation pruning bias is caused by the information loss of the pruned entity space.", "It makes the entity agent less effective, especially on large and dense KG.", "\\newline </subsection> <subsection> <title> 4.3 Policy Network </title> Relation agent\u2019s search policy is parameterized by the embedding of query relation $ {\\bm{r}}_{q} $ and relation history $ {\\bm{h}}_{t}^{R} $ .", "The relation history is encoded using an LSTM [@bib:hochreiter1997long] : $ {\\bm{h}}_{t}^{R}=\\text{LSTM}({\\bm{h}}_{t-1}^{R},{\\bm{r}}_{t-1}) $ , where $ {\\bm{r}}_{t-1}\\in\\mathbb{R}^{d} $ is the embedding of the last relation.", "We initialize $ {\\bm{h}}_{0}^{R}=\\text{LSTM}(\\bm{0},{\\bm{r}}_{s}) $ , where $ {\\bm{r}}_{s} $ is a special start relation embedding to form a initial relation-entity pair with source entity embedding $ {\\bm{e}}_{s} $ .", "Relation space embeddings $ {\\bm{R}}_{t}\\in\\mathbb{R}^{|R_{t}|\\times d} $ consist embeddings of all the relations in relation space $ R_{t} $ at step t. Finally, relation agent outputs a probability distribution $ {\\bm{d}}^{R}_{t} $ and samples a relation from it.", "\\newline <equation> $ {\\bm{d}}^{R}_{t}=\\sigma({\\bm{R}}_{t}\\times{\\bm{W}}_{1}\\ \\text{ReLU}({\\bm{W}}_{% 2}[{\\bm{h}}_{t}^{R};{\\bm{r}}_{q}])) $ </equation> where $ \\sigma $ is the softmax operator, $ {\\bm{W}}_{1} $ and $ {\\bm{W}}_{2} $ is trainable parameters.", "We design relation agent\u2019s history-dependent policy as $ \\bm{\\pi}^{R}=({\\bm{d}}_{1}^{R},{\\bm{d}}_{2}^{R},\\dots,{\\bm{d}}_{T}^{R}) $ .", "\\newline Similarly, entity agent\u2019s history-dependent policy is $ \\bm{\\pi}^{E}=({\\bm{d}}_{1}^{E},{\\bm{d}}_{2}^{E},\\dots,{\\bm{d}}_{T}^{E}) $ .", "Entity agent can acquire its embedding of last step $ {\\bm{e}}_{t-1} $ , entity space embeddings $ {\\bm{E}}_{t} $ , its history $ {\\bm{h}}_{t}^{E}=\\text{LSTM}({\\bm{h}}_{t-1}^{E},{\\bm{e}}_{t-1}) $ , and the probability distribution of entities $ {\\bm{d}}^{E}_{t} $ as follows.", "\\newline <equationgroup> <equation> $ {\\bm{d}}^{E}_{t}=\\sigma({\\bm{E}}_{t}\\times{\\bm{W}}_{3}\\text{ReLU}% ({\\bm{W}}_{4}[{\\bm{h}}_{t}^{E};{\\bm{r}}_{q};{\\bm{e}}_{s};{\\bm{e}}_{t}])) $ $ {\\bm{d}}^{E}_{t} $ $ =\\sigma({\\bm{E}}_{t}\\times{\\bm{W}}_{3}\\text{ReLU}({\\bm{W}}_{4}[{% \\bm{h}}_{t}^{E};{\\bm{r}}_{q};{\\bm{e}}_{s};{\\bm{e}}_{t}])) $ </equation> </equationgroup> where $ {\\bm{W}}_{3} $ and $ {\\bm{W}}_{4} $ is trainable parameters.", "Note that entity agent uses a different LSTM to encode the entity history.", "\\newline </subsection> <subsection> <title> 4.4 Reward </title> Intuitively, the relation agent prefers paths which mostly direct the way to the correct entity for a query relation.", "Thus, given a relation path, we give reward according to its confidence level, which we call the rule guidance reward $ R_{r} $ .", "These confidence scores are retrieved from the rule miner.", "In our experiments, we only use C rules generated by AnyBURL [@bib:Meilicke2019AnytimeBR] to compute the reward.", "A confidence score is defined as the number of grounded rule paths divided by the number of grounded body relations.", "We also add a Laplace smoothing $ p_{c}=5 $ to the confidence score for the final rule guidance reward.", "\\newline In addition to the rule guidance reward, the agent will also receive a hit reward $ R_{h} $ , which is 1 if the predicted triple $ \\epsilon=(e_{s},r_{q},e_{T})\\in\\mathcal{G} $ .", "Otherwise, we use the embedding of $ \\epsilon $ to measure reward as in [@bib:lin-etal-2018-multi-hop] .", "\\newline <equationgroup> <equation> $  R_{h}=\\mathbb{I}(\\epsilon\\in\\mathcal{G})+(1-\\mathbb{I}(\\epsilon% \\in\\mathcal{G})f(\\epsilon), $ $  R_{h}=\\mathbb{I}(\\epsilon\\in\\mathcal{G})+(1-\\mathbb{I}(\\epsilon% \\in\\mathcal{G})f(\\epsilon), $ </equation> </equationgroup> where $ \\mathbb{I}(\\cdot) $ is an indicator function, $ f(e_{s},r_{q},e_{T}) $ is a composition function for reward shaping using embeddings.", "\\newline </subsection> <subsection> <title> 4.5 Training </title> We train the framework in four stages.", "Firstly, we use embedding approaches to train relation and entity embeddings as one-hop reasoning.", "Secondly, we use symbolic approach to retrieve C rules and their associated confidence scores.", "Thirdly, we pre-train the relation agent by freezing the entity agent and asking the relation agent to sample a path.", "We only use the rule miner to evaluate the path and compute $ R_{r} $ based on the pre-mined confidence score.", "The model will learn as many high-quality rules as possible and behave like a symbolic model with similar confidence score distribution.", "Finally, we jointly train the relation and entity agent to leverage the semantic information of the embeddings for hit reward shaping.", "The final reward $ R $ is the $ R_{r} $ and $ R_{h} $ with a constant factor $ \\lambda $ : $ R=\\lambda R_{r}+(1-\\lambda)R_{h} $ .", "\\newline </subsection> <subsection> <title> 4.6 Optimization </title> We train our policy network of two agents using REINFORCE [@bib:williams1992simple] algorithm to maximize the expected reward: \\newline <equation> $ J(\\theta)=\\mathbb{E}_{(e_{s},r_{q},e_{o})\\in\\mathcal{G}}[\\mathbb{E}_{r_{1},% \\dots,r_{T}\\sim\\bm{\\pi}^{R}_{\\theta};e_{1},\\dots,e_{T}\\sim\\bm{\\pi}^{E}_{\\theta% }}[ $ </equation> <equation> $ R(e_{T},r_{1},\\cdots,r_{T}|e_{s},r_{q})]] $ </equation> \\newline </subsection>  </section>"], ["<section> <title> 5 Experiments </title>  In this section, we test our model on three datasets and compare its performance with symbolic, embedding, and walk-based approaches.", "We describe the experiment setting, results, and analysis in detail.", "\\newline <subsection> <title> 5.1 Experimental Setup </title> Datasets We evaluate the effectiveness of the proposed method on four benchmark datasets.", "(1) FB15k-237 [@bib:toutanova2015representing] , (2) WN18RR [@bib:dettmers2018convolutional] , and (3) NELL-995 [@bib:xiong2017deeppath] .", "Table [@ref:LABEL:tb:kgstats] shows the density.", "Original action space contains different relation-entity pairs that have duplicate relations and entities.", "Splitting the action space into relation and entity spaces can reduce the space size for both.", "\\newline Hyperparameters We set the relation and entity embedding size to 200.", "The history encoder of each agent is a three-layer LSTM with a hidden state dimension 200.", "We fix the reasoning step to 3.", "Mini-batch size is 256 for WN18RR and FB15k-237; 128 for NELL-995.", "We set $ \\lambda $ to 0.65 for WN18RR; 0.1 for FB15k-237 and NELL-995.", "The threshold for pre-mined rule sets is 0.15 as discussed in [@ref:LABEL:ablation] .", "\\newline </subsection> <subsection> <title> 5.2 Results </title> Table [@ref:LABEL:tb:mainresults] shows the evaluation results of our proposed approach compared to symbolic-based, embedding-based, and walk-based baselines.", "Our model achieves state-of-the-art results on WN18RR and NELL-995.", "We also have a competitive performance on FB15k-237.", "This observation is expected given AnyBURL\u2019s performance on these datasets.", "In FB15k-237, the relation space is much larger, which means it is harder for the relation agent to select a valid rule that is traceable.", "\\newline We find symbolic-based models perform strongly on WN18RR and we set the $ \\lambda $ to 0.65 to encourage the agent to generate more rule paths.", "We also observe that embedding-based methods perform better than walk-based methods despite their simplicity.", "One possible reason is that embedding-based methods implicitly encode the connectivity of the whole graph into the embedding space [@bib:lin-etal-2018-multi-hop] .", "By leveraging rules, we also incorporate some global information as guidance to make up for the potential searching space loss during the discrete inference process.", "\\newline Table [@ref:LABEL:tb:rulepercentage] shows the percentage of rules used on the development set using ComplEx embedding in the pre-training and training phase.", "It shows that our model abandons a few rules to further improve hit performance during the training phase.", "\\newline </subsection> <subsection> <title> 5.3 Performance Analysis </title> Ablation Study We performed an ablation study where we removed the pre-training step, freeze the relation agent after pre-training, or use a single agent on WN18RR.", "They all use ComplEx embedding for consistency."]], "target": "We observe a hit@1 and MRR decrease on the development set, as shown in Table . The performance decrease for freezing pre-trained agent shows that using rule reward is not enough. The model still needs the hit performance information to further improve. The performance decrease if pre-training is removed shows that learning to reason as a symbolic approach first then improve it as neural reasoning can have better performance. Single agent performance drop shows the effectiveness of pruning action space."}, {"tabular": ["  Task  &  N  &  CheXpert  &  CheXpert++  &  Both Wrong ", " No Finding  &  44  &  25%  &  39%  &  36% ", " Enlarged Cardiomediastinum  &  43  &  33%  &  56%  &  12% ", " Cardiomegaly  &  44  &  23%  &  61%  &  14% ", " Lung Lesion  &  9 $ {}^{*} $  &  44%  &  33%  &  22% ", " Airspace Opacity  &  46  &  26%  &  61%  &  13% ", " Edema  &  38 $ {}^{*} $  &  29%  &  66%  &  5% ", " Consolidation  &  46  &  30%  &  65%  &  4% ", " Pneumonia  &  45  &  22%  &  62%  &  16% ", " Atelectasis  &  46  &  24%  &  70%  &  7% ", " Pneumothorax  &  41  &  22%  &  76%  &  2% ", " Pleural Effusion  &  45  &  31%  &  58%  &  9% ", " Pleural Other  &  8 $ {}^{*} $  &  38%  &  13%  &  50% ", " Fracture  &  14 $ {}^{*} $  &  21%  &  57%  &  21% ", " Support Devices  &  45  &  29%  &  62%  &  7% ", " Micro Average  &    &  27%  &  60%  &  12%  "], "ref_sec": [["<section> <title> 1 Introduction </title>  In many non-mdecial settings, annotating even large datasets is relatively feasible via crowd-sourcing.", "In medicine, however, producing high-quality labels requires human expertise, and is thus much more expensive and impractical.", "To compensate for these difficulties, in several recent large radiology datasets [@bib:johnson_mimic-cxr:_2019,irvin_chexpert:_2019,bustos_padchest:_2019,wang_chestx-ray8:_2017] , researchers have relied on various rule-based, expert-defined labeling systems, most recently the CheXpert Labeler [@bib:irvin_chexpert:_2019] , which labels chest X-rays by running their corresponding free-text reports through a rule-based NLP model to predict either positive, negative, or uncertain mentions of 14 radiographic diagnostic categories.", "CheXpert has been used to define classification labels for two recent chest X-ray datasets [@bib:johnson_mimic-cxr:_2019,irvin_chexpert:_2019] , and has been used to provide a metric for clinical accuracy in radiology report generation [@bib:liu_clinically_2019] , all in the absence of any ground-truth labels for these tasks.", "\\newline CheXpert builds on top of NegBio [@bib:peng_negbio:_2017] which provides regular expression (regex) infrastructure for uncertainty and negation detection.", "As a result of being a regex-based system, CheXpert is not differentiable, and its outputs are binary labels, not continuous probabilities.", "CheXpert can also only be run on the CPU, rendering it relatively slow compared to GPU-optimized systems like neural network models.", "CheXpert has been extremely valuable to the research community, but these three issues (lack of differentiability, probabilistic output, and runtime speed) induce significant hurdles for several exciting research directions.", "\\newline For example, [@bib:liu_clinically_2019] attempt to use CheXpert to enforce that automatically generated free-text radiology reports are clinically accurate by comparing their generated text to the ground truth in the CheXpert label space.", "However, as CheXpert is non-differentiable, they were forced to use a reinforcement learning policy gradient solution to optimize through the discrete labeler, a process which induced significant computational cost and, likely added additional instability to the results [@bib:liu_clinically_2019] .", "In a different direction, several notable researchers have also questioned the validity of CheXpert\u2019s labels [@bib:oakden-rayner_exploring_2017] , a finding we reproduce here. Were CheXpert to output probabilistic labels, we could attempt to improve these labels by employing active learning strategies leveraging the model\u2019s output uncertainty as a signal of which inputs most required human relabeling, or we could attempt to compensate for these label errors by using a small set of human labels to re-calibrate the model and then weighting our downstream use of these labels by their confidence scores.", "As it stands, we can do none of these things.", "\\newline In this work, we solve these problem via CheXpert++ , a Bidirectional Encoder Representations from Transformers (BERT; [@bib:devlin_bert:_2018] ) model trained to yield an extremely high-fidelity approximation to CheXpert.", "CheXpert++ is differentiable, yields probabilistic outputs, and is fully GPU-ready.", "CheXpert++ is able to match CheXpert labels 99.81% of the time on held-out radiology report sentences, so it can serve as a viable drop-in replacement for CheXpert, all while running approximately 1.8 times faster.", "Additionally, it is fully differentiable, and thus can be integrated natively into downstream neural pipelines.", "Finally, it yields probabilistic outputs, and therefore can be used with active learning systems.", "Unexpectedly, we also find that in the 0.07% of cases where the two systems disagree, CheXpert++ labels are actually more often preferred over CheXpert labels by a clinician, suggesting some inductive bias may be enabling CheXpert++ to correct errors in its own training labels.", "\\newline Building on that finding and to demonstrate the utility of this approximator, we additionally build a proof-of-concept active learning system and show that after only one epoch of re-training under solicited clinician annotations, CheXpert++ is able to improve accuracy by 8% in matching true clinician-defined gold standard labels over CheXpert.", "Note that in a concordant work, released while this paper was under review, [@bib:smit2020chexbert] has also found that mixed training a BERT based model on CheXpert labels and human labels can offer improvements to overall labeling performance.", "Whereas our active learning study is a proof-of-concept, their work uses a larger set of gold-standard annotations and should be regarded as a more full-fledged model aimed at improving labeler performance alone [@bib:smit2020chexbert] .", "\\newline In the rest of this work, we will first briefly outline the original CheXpert labeler and the construction, training, and performance of our approximation model CheXpert++ , both as an approximation (e.g., in terms of fidelity to CheXpert) and in comparison to CheXpert via clinician evaluation.", "Next, we walk through in detail some projected use cases of this approximation model that would not be possible with the original CheXpert labeler, to better justify why we believe it is sensible to create such an approximation in the first place, focusing specifically on our proof-of-concept active learning system and the labeling improvements it imparts.", "Finally, we close with future work and concluding thoughts.", "\\newline <subsection> <title> Generalizable Insights about Machine Learning in the Context of Healthcare </title> Technically, this paper is (intentionally) quite simple.", "It uses existing models and algorithms, applying them in relatively standard ways.", "Our goal is not to demonstrate a new method or procedure for collecting labels or for modeling clinical text.", "Instead, our goal is two-fold: \\newline <list> \\ To provide as a resource a drop-in replacement for CheXpert that solves the 3 major hurdles identified above which inhibit novel research directions.", "Based on our own collective research history, we are confident this solution will prove useful to the community.", "\\newline \\ \\ To challenge the assumption that moving beyond silver labels is generally prohibitive, even if it requires additional annotations.", "\\newline \\ </list> \\newline This second point is our primary source of generalizable insight.", "Too often in machine learning for healthcare do we settle for sub-par labels before exploring procedures to intelligently and efficiently increase label quality, such as, for example, active learning.", "Evidence of this is the numerous works using the CheXpert labeler over the MIMIC-CXR dataset despite its known inadequacies.", "In this work, with only roughly 3 hours of labeling time by a clinical coauthor and a single pass of fine-tuning, we were able to improve over the raw performance of CheXpert by approximately 8% (averaged across all 14 CheXpert tasks) on a disjoint (also clinician-labeled) test set.", "The ease with which we obtain this gain is suggestive that these problems are not as insurmountable as they appear, and that machine learning for health, as a field, would be well served investing more in co-learning, model-approximation, and active-learning techniques to efficiently and rapidly improve label quality.", "\\newline </subsection>  </section>"], ["<section> <title> 2 From CheXpert to CheXpert++ </title>  <paragraph> <title> CheXpert Model Details </title> CheXpert [@bib:irvin_chexpert:_2019] processes a span of text, and produces classification predictions across 14 different disease categories: {Support Devices} , {Airspace Opacity} , {Atelectasis} , {Cardiomegaly} , {Consolidation} , {Edema} , {Enlarged Cardiomediastinum} , {Fracture} , {Lung Lesion} , {No Finding} , {Pleural Effusion} , {Pleural Other} , {Pneumonia} , and {Pneumothorax} .", "For each disease category, CheXpert classifies the text as either containing {no mention} , an {uncertain mention} (indicating that the physician is uncertain), a {negative mention} , or a {positive mention} of the disease category.", "\\newline CheXpert is an extension upon NegBio [@bib:peng_negbio:_2017] , a rule-based algorithm for detecting the 14 label categories used with the NIH Chest X-Ray 14 dataset [@bib:wang_chestx-ray8:_2017] .", "Being largely rule-based, CheXpert is non-differentiable and yields only predictions, not probabilities.", "Additionally, in practice, we have found it relatively slow and only suitable for offline labeling, even when run with many parallel processes.", "\\newline NegBio (in particular the labels assigned to the NIH Chest X-Ray 14 dataset) have been previously criticized for their inaccuracy [@bib:oakden-rayner_exploring_2017] , so a valid question is whether these weaknesses extend to CheXpert.", "[@bib:irvin_chexpert:_2019] addressed these concerns by reporting comparisons to expert extracted labels for 1000 held-out radiology reports, finding strong performance for Mention F1 (0.948), and weaker but still strong performance for Negation F1 (0.899) and Uncertainty F1 (0.770).", "However, when tested on 687 reports from the (different) MIMIC-CXR dataset, [@bib:johnson_mimic-cxr:_2019] found significant reductions in performance across all three tasks (0.874, 0.565, 0.470, respectively).", "\\newline </paragraph> <paragraph> <title> CheXpert++ Model Details </title> CheXpert++ is a BERT based model, initialized from the pre-trained clinical BERT model [@bib:alsentzer_publicly_2019] , and followed by a multi-task prediction head.", "We trained CheXpert++ at the per patient and report level over the MIMIC-CXR dataset [@bib:johnson_mimic-cxr:_2019] , using an 80/10 train/test data split (602,855 train sentences and 75,748/29,166 test sentences/not-in-train test sentences).", "Note two nuances to these data; first, that while we do split the data by unique report, due to the frequency of the use of templates in this modality, some individual sentences (which is the level at which we model) are shared across all splits.", "To account for this, we analyzed our results both on the overall test set and the unseen-sentences level (in which we simply removed the sentences that were also seen in training), finding nearly identical performance in both.", "Here, to be conservative, we report performances on the unseen-sentences test set, which had minutely worse numbers (overall parity on the full test set was 99.93%, vs. 99.81% on the unseen-sentences set) and imposes a slightly stronger generalization requirement at the expense of imposing some domain shift between the train and test sentences.", "Secondly, note that the first model trained yielded such strong performance that no hyperparameter tuning was necessary, so while we originally had an additional 10% separated for validation, we did not use it at all in this work.", "The pre-trained BERT model was fine-tuned using 4 GPUs (GTX TITAN X) for 5 epochs using a batch size of 32 and an initial learning rate of $ 5\\times 10^{-5} $ .", "Full training code and the pre-trained model are available.", "\\newline </paragraph> <paragraph> <title> CheXpert++ Model Results </title> Overall Performance CheXpert++ approximates CheXpert with extremely high fidelity, matching CheXpert labels 99.81% of the time on a held out set.", "This number is an average accuracy (match %) over all 4 classes for all 14 labels.", "Each label individually also obtains over 99.7% match accuracy, and all tasks save \u201cNo Finding\u201d obtain over 99.9% match accuracy.", "Especially when considering that CheXpert itself makes errors at rates far greater than this failure-to-match rate (as evaluated by [@bib:johnson_mimic-cxr:_2019,irvin_chexpert:_2019] ), these performance numbers can justify CheXpert++ as a direct replacement for CheXpert.", "Some readers may question why we don\u2019t just train CheXpert++ directly on the underlying data backing CheXpert.", "The answer is simple: {such data do not exist.} CheXpert is an expert-defined rule-based model, built and validated on separate, non-public datasets that we do not have access to.", "Using CheXpert as our training target is therefore very appropriate \u2014 by matching at this rate we can confidently use CheXpert++ as a drop in replacement, thereby solving our 3 main pain points, with no significant risk of disagreement all without requiring {any} human-generated labels of any kind.", "Later in this work, we will discuss a proof-of-concept active learning system where we use an additional, human annotated set we construct to improve CheXpert++ over CheXpert, but up to this point we rely on no human labels whatsoever.", "\\newline Per-task Performance Breakdown For more detailed performance numbers, Table [@ref:LABEL:tab:per_task_error] lists the failure-to-match rate for all 14 labeling tasks, comparing majority class assignment performance (e.g., if one just predicted CheXpert\u2019s most frequent class, how often would this class match the CheXpert labels) to CheXpert++ .", "We also break down performance across all labels for all tasks separately with per-task confusion matrices in the Appendix, Figure [@ref:LABEL:fig:confusion_matrices] .", "We see some patterns emerge among our (very small) number of mis-classifications \u2014 in particular, we very rarely mistake a \u201cNo Mention\u201d label for a different label, whereas mis-classifications within the three other kinds of mentions are much more common.", "\\newline Error Analysis In addition to the analyses above, we also had a clinician colleague examine a random subset of up to approximately 46 discrepant examples per task (some tasks had fewer than 46 discrepancies total across the approximately 30,000 held out sentences, as noted in the table) .", "Perhaps surprisingly, we found that in a majority of cases (59%), said clinician preferred CheXpert++ labels over those of CheXpert, despite the fact that CheXpert++ was trained to match CheXpert exactly.", "In 13% of cases, both labels were deemed incorrect, and in only 28% of cases was CheXpert preferred over CheXpert++ ."]], "target": "A per-task breakdown of these results is in Table . We find that CheXpert++ is preferred in all tasks save \u201cLung Lesion\u201d (where CheXpert labels are preferred) and \u201cPleural Other,\u201d where most often both are incorrect."}, {"tabular": ["    &  CAD  &  K-CAD (rbf)  &  K-CAD (poly)  &  Iso Forest  &  LOF  &  SVM  &  EE ", " Speech  &  Train:  &  Time:  &  14.84 $ \\pm $ 0.21  &  13.54 $ \\pm $ 0.21  &  13.87 $ \\pm $ 0.33  &  2.82 $ \\pm $ 0.06  &  6.53 $ \\pm $ 0.02  &  6.12 $ \\pm $ 0.02  &  23.35 $ \\pm $ 0.25 ", " AUC:  &  34.78 $ \\pm $ 0.15  &  76.15 $ \\pm $ 2.08  &  63.69 $ \\pm $ 1.48  &  48.37 $ \\pm $ 1.38  &  53.99 $ \\pm $ 1.75  &  46.63 $ \\pm $ 1.59  &  49.16 $ \\pm $ 1.84 ", " Test:  &  Time:  &  7.16 $ \\pm $ 0.03  &  1.38 $ \\pm $ 0.03  &  1.42 $ \\pm $ 0.03  &  0.06 $ \\pm $ 00.01  &  7.31 $ \\pm $ 0.10  &  0.24 $ \\pm $ 0.01  &  0.01 $ \\pm $ 0.00 ", " AUC:  &  42.07 $ \\pm $ 10.48  &  71.23 $ \\pm $ 13.18  &  56.15 $ \\pm $ 10.48  &  45.23 $ \\pm $ 12.17  &  53.53 $ \\pm $ 12.29  &  44.55 $ \\pm $ 12.09  &  47.25 $ \\pm $ 12.54 ", " Opt digits  &  Train:  &  Time:  &  13.27 $ \\pm $ 0.11  &  26.96 $ \\pm $ 0.31  &  25.86 $ \\pm $ 0.48  &  0.81 $ \\pm $ 0.01  &  1.84 $ \\pm $ 0.02  &  3.10 $ \\pm $ 0.01  &  0.96 $ \\pm $ 0.04 ", " AUC:  &  32.67 $ \\pm $ 1.53  &  87.52 $ \\pm $ 1.76  &  77.79 $ \\pm $ 1.45  &  68.38 $ \\pm $ 4.64  &  60.84 $ \\pm $ 1.67  &  50.52 $ \\pm $ 3.81  &  39.04 $ \\pm $ 2.44 ", " Test:  &  Time:  &  11.24 $ \\pm $ 0.12  &  2.67 $ \\pm $ 0.01  &  2.59 $ \\pm $ 0.03  &  0.03 $ \\pm $ 0.01  &  2.13 $ \\pm $ 0.08  &  0.15 $ \\pm $ 00.00  &  00.01 $ \\pm $ 00.00 ", " AUC:  &  26.28 $ \\pm $ 7.10  &  88.22 $ \\pm $ 5.62  &  79.72 $ \\pm $ 4.81  &  68.36 $ \\pm $ 8.11  &  61.12 $ \\pm $ 11.65  &  37.49 $ \\pm $ 7.41  &  38.84 $ \\pm $ 4.29 ", " Arrhythmia  &  Train:  &  Time:  &  4.76 $ \\pm $ 0.02  &  2.75 $ \\pm $ 0.06  &  2.53 $ \\pm $ 0.03  &  0.20 $ \\pm $ 0.01  &  0.07 $ \\pm $ 00.00  &  0.13 $ \\pm $ 00.00  &  0.85 $ \\pm $ 0.02 ", " AUC:  &  52.89 $ \\pm $ 0.96  &  48.87 $ \\pm $ 0.51  &  73.92 $ \\pm $ 1.12  &  62.43 $ \\pm $ 2.05  &  91.04 $ \\pm $ 0.66  &  88.56 $ \\pm $ 00.87  &  80.59 $ \\pm $ 0.65 ", " Test:  &  Time:  &  1.59 $ \\pm $ 0.01  &  0.30 $ \\pm $ 0.01  &  0.28 $ \\pm $ 00.00  &  0.02 $ \\pm $ 0.01  &  0.08 $ \\pm $ 0.00  &  0.01 $ \\pm $ 00.00  &  00.01 $ \\pm $ 00.00 ", " AUC:  &  48.02 $ \\pm $ 9.06  &  48.56 $ \\pm $ 5.38  &  71.88 $ \\pm $ 9.23  &  63.07 $ \\pm $ 11.55  &  90.57 $ \\pm $ 5.47  &  90.03 $ \\pm $ 5.63  &  80.32 $ \\pm $ 4.73 ", " Wine  &  Train:  &  Time:  &  0.28 $ \\pm $ 0.00  &  0.03 $ \\pm $ 0.03  &  0.03 $ \\pm $ 0.01  &  0.09 $ \\pm $ 0.00  &  0.01 $ \\pm $ 0.00  &  0.01 $ \\pm $ 0.00  &  0.05 $ \\pm $ 0.02 ", " AUC:  &  25.59 $ \\pm $ 4.28  &  27.04 $ \\pm $ 10.66  &  92.11 $ \\pm $ 7.06  &  79.56 $ \\pm $ 10.59  &  98.70 $ \\pm $ 1.29  &  68.59 $ \\pm $ 4.25  &  59.56 $ \\pm $ 37.15 ", " Test:  &  Time:  &  0.18 $ \\pm $ 00.00  &  0.02 $ \\pm $ 00.00  &  0.01 $ \\pm $ 0.00  &  0.02 $ \\pm $ 0.00  &  0.01 $ \\pm $ 0.00  &  0.01 $ \\pm $ 0.00  &  0.01 $ \\pm $ 0.00 ", " AUC:  &  23.65 $ \\pm $ 14.45  &  40.17 $ \\pm $ 13.12  &  86.97 $ \\pm $ 2.96  &  76.09 $ \\pm $ 10.11  &  92.58 $ \\pm $ 5.69  &  91.13 $ \\pm $ 3.83  &  57.70 $ \\pm $ 38.84 ", " Musk  &  Train:  &  Time:  &  11.15 $ \\pm $ 0.20  &  9.67 $ \\pm $ 0.47  &  9.42 $ \\pm $ 0.03  &  0.98 $ \\pm $ 0.01  &  1.16 $ \\pm $ 0.03  &  3.16 $ \\pm $ 0.00  &  10.16 $ \\pm $ 0.32 ", " AUC:  &  40.69 $ \\pm $ 2.97  &  69.68 $ \\pm $ 2.97  &  93.45 $ \\pm $ 1.46  &  99.91 $ \\pm $ 00.00  &  41.93 $ \\pm $ 3.34  &  57.99 $ \\pm $ 7.34  &  99.99 $ \\pm $ 00.00 ", " Test:  &  Time:  &  4.89 $ \\pm $ 0.02  &  0.98 $ \\pm $ 0.02  &  0.98 $ \\pm $ 0.01  &  0.03 $ \\pm $ 0.00  &  1.24 $ \\pm $ 0.01  &  0.17 $ \\pm $ 0.00  &  0.01 $ \\pm $ 0.00 ", " AUC:  &  30.30 $ \\pm $ 10.37  &  50.00 $ \\pm $ 0.00  &  93.80 $ \\pm $ 3.77  &  99.95 $ \\pm $ 0.00  &  39.00 $ \\pm $ 10.55  &  5.71 $ \\pm $ 3.63  &  100 $ \\pm $ 0.00  "], "ref_sec": [["<section> <title> 1 Introduction </title>  Anomaly detection, instance ranking, and prototype selection are important tasks in data mining.", "Anomaly detection refers to finding outliers or anomalies which differ significantly from the normal data points [@bib:emmott2013systematic] .", "There exist many applications for anomaly detection such as fraud detection, intrusion detection, medical diagnosis, and damage detection [@bib:chandola2009anomaly] .", "\\newline Ranking data points (instances) according to their importance can be useful for better representation of data, omitting the dummy or noisy points, better discrimination of classes in classification tasks, etc [@bib:ghojogh2018principal] .", "Prototype selection is referred to finding the best data points in terms of representation of data, discrimination of classes, information of points, etc [@bib:garcia2012prototype] .", "It can also be useful for better storage and processing time efficiency.", "Prototype selection can be done either using ranking the points and then discarding the less important points or by merely retaining a portion of data and discarding the others.", "\\newline In this paper, we propose Curvature Anomaly Detection (CAD) and inverse CAD (iCAD) for anomaly detection and prototype selection, respectively.", "We also propose their kernel versions which are Kernel CAD (K-CAD) and Kernel iCAD (K-iCAD).", "The idea of proposed algorithms is based on polyhedron curvature where every point is imagined to be the vertex of a hypothetical polyhedron defined by its neighbors.", "We also define anomaly landscape and anomaly path which can have different applications such as image denoising.", "In the following, we mention the related work for anomaly detection and prototype selection.", "Then, we explain the background for polyhedron curvature.", "Afterwards, the proposed CAD, K-CAD, iCAD, and K-iCAD are explained.", "Finally, the experiments are reported.", "\\newline Anomaly Detection: Local Outlier Factor (LOF) [@bib:breunig2000lof] is one of the important anomaly detection algorithms.", "It defines a measure for local density of every data point according to its neighbors.", "It compares the local density of every point with its neighbors and find the anomalies.", "One-class SVM [@bib:scholkopf2000support] is another method which estimates a function which is positive on the regions of data with high density and negative elsewhere.", "Therefore, the points with negative values of that function are considered as anomalies.", "If the data are assumed to have Gaussian distribution as the most common distribution, Elliptic Envelope (EE) can be fitted to data [@bib:rousseeuw1999fast] and the points having low probability in the fitted envelope are considered to be anomaly.", "Isolation forest [@bib:liu2008isolation] is an isolation-based anomaly detection method [@bib:liu2012isolation] which isolates the anomalies using an ensemble approach.", "The ensemble includes isolation trees where the more depth of tree for isolating a point is a measure of its normality.", "\\newline Prototype Selection: Prototype selection [@bib:garcia2012prototype] is also referred to as instance ranking and numerosity reduction.", "Edited Nearest Neighbor (ENN) [@bib:wilson1972asymptotic] is one of the oldest prototype selection method which removes the points having most of its neighbors from another class.", "Decremental Reduction Optimization Procedure 3 (DROP3) [@bib:wilson2000reduction] has the opposite perspective and removes a point if ite removal improves the $ k $ -Nearest Neighbor ( $ k $ -NN) classification accuracy.", "Stratified Ordered Selection (SOS) [@bib:kalegele2012demand] starts with boundary points and then recursively finds the median points noticing that boundary and median points are informative.", "Shell Extraction (SE) [@bib:liu2017efficient] introduces a reduction sphere and removes the points falling in this hyper-sphere in order to approximate the support vectors.", "Principal Sample Analysis (PSA) [@bib:ghojogh2018principal] , which is extended for regression and clustering tasks in [@bib:ghojogh2019principal] , considers the scatter of data as well as the regression of prototypes for better representation.", "Instance Ranking by Matrix Decomposition (IRMD) [@bib:ghojogh2019instance] decomposes the matrix of data and makes use of the bases of decomposition.", "The more similar points to the bases are considered to be more important.", "\\newline  </section>"], ["<section> <title> 2 Background on Polyhedron Curvature </title>  A polytope is a geometrical object in $ \\mathbb{R}^{d} $ whose faces are planar.", "The special cases of polytope in $ \\mathbb{R}^{2} $ and $ \\mathbb{R}^{3} $ are called polygon and polyhedron , respectively.", "Some examples for polyhedron are cube, tetrahedron, octahedron, icosahedron, and dodecahedron with four, eight, and twenty triangular faces, and twelve flat faces, respectively [@bib:coxeter1973regular] .", "Consider a polygon where $ \\tau_{j} $ and $ \\mu_{j} $ are the interior and exterior angles at the $ j $ -th vertex; we have $ \\tau_{j}+\\mu_{j}=\\pi $ .", "A similar analysis holds in $ \\mathbb{R}^{3} $ for Fig. [@ref:LABEL:figure_polyhedron] -a.", "In this figure, a vertex of a polyhedron and its opposite cone are shown where the opposite cone is defined to have perpendicular faces to the faces of the polyhedron at the vertex.", "The intersection of a unit sphere centered at the vertex and the opposite cone is shown in the figure.", "This intersection is a geodesic on the unit sphere.", "According to Thomas Harriot\u2019s theorem proposed in 1603 [@bib:markvorsen1996curvature] , if this geodesic on the unit sphere is a triangle, its area is $ \\mu_{1}+\\mu_{2}+\\mu_{3}-\\pi=2\\pi-(\\tau_{1}+\\tau_{2}+\\tau_{3}) $ .", "The generalization of this theorem from a geodesic triangular polygon ( $ 3 $ -gon) to an $ k $ -gon is $ \\mu_{1}+\\dots+\\mu_{k}-k\\pi+2\\pi=2\\pi-\\sum_{a=1}^{k}\\tau_{a} $ [@bib:markvorsen1996curvature] , where the polyhedron has $ k $ faces meeting at the vertex.", "\\newline The Descartes\u2019s angular defect at a vertex $ \\boldsymbol{x} $ of a polyhedron is [@bib:descartes1890progymnasmata] : $ \\mathcal{D}(\\boldsymbol{x}):=2\\pi-\\sum_{a=1}^{k}\\tau_{a} $ .", "The total defect of a polyhedron is defined as the summation of the defects over the vertices.", "It can be shown that the total defect of a polyhedron with $ v $ vertices, $ e $ edges, and $ f $ faces is: $ \\mathcal{D}:=\\sum_{i=1}^{v}\\mathcal{D}(\\boldsymbol{x}_{i})=2\\pi(v-e+f) $ .", "The term $ v-e+f $ is Euler-Poincar\u00e9 characteristic of the polyhedron [@bib:richeson2019euler,hilton1982descartes] ; therefore, the total defect of a polyhedron is equal to its Euler-Poincar\u00e9 characteristic.", "According to Fig. [@ref:LABEL:figure_polyhedron] -b, the smaller $ \\tau $ angles result in sharper corner of the polyhedron.", "Therefore, we can consider the angular defect as the curvature of the vertex.", "\\newline  </section>"], ["<section> <title> 3 Anomaly Detection </title>  <subsection> <title> 3.1 Curvature Anomaly Detection </title> The main idea of the Curvature Anomaly Detection (CAD) method is as follows.", "Every data point is considered to be the vertex of a hypothetical polyhedron (see Fig. [@ref:", "LABEL:figure_polyhedron] -a).", "For every point, we find its $ k $ -Nearest Neighbors ( $ k $ -NN).", "The $ k $ neighbors of the point (vertex) form the $ k $ faces of a polyhedron meeting at that vertex.", "Then, the more curvature that point (vertex) has, the more anomalous it is because it is far away (different) from its neighbors.", "Therefore, anomaly score $ s_{A} $ is proportional to the curvature.", "\\newline Since, according to the equation of angular effect, the curvature is proportional to minus the summation of angles, we can consider the anomaly score to be inversely proportional to the summation of angles.", "Without loss of generality, we assume the angles to be in range $ [0,\\pi] $ (otherwise, we take the smaller angle).", "The less the angles between two edges of the polyhedron, the more their cosine.", "As the anomaly score is inversely proportional to the angles, we can use cosine for the anomaly score: $ s_{A}(\\boldsymbol{x}_{i})\\propto 1/\\tau_{a}\\propto\\cos(\\tau_{a}) $ .", "We define the anomaly score to be the summation of cosine of the angles of the polyhedron faces meeting at that point: $ s_{A}(\\boldsymbol{x}_{i}):=\\sum_{a=1}^{k}\\cos(\\tau_{a})=\\sum_{a=1}^{k}(\\breve{% \\boldsymbol{x}}_{a}^{\\top}\\breve{\\boldsymbol{x}}_{a+1})/(||\\breve{\\boldsymbol{% x}}_{a}||_{2}||\\breve{\\boldsymbol{x}}_{a+1}||_{2}) $ where $ \\breve{\\boldsymbol{x}}_{a}:=\\boldsymbol{x}_{a}-\\boldsymbol{x}_{i} $ is the $ a $ -th edge of the polyhedron passing through the vertex $ \\boldsymbol{x}_{i} $ , $ \\boldsymbol{x}_{a} $ is the $ a $ -th neighbor of $ \\boldsymbol{x}_{i} $ , and $ \\breve{\\boldsymbol{x}}_{a+1} $ denotes the next edge sharing the same polyhedron face with $ \\breve{\\boldsymbol{x}}_{a} $ where $ \\breve{\\boldsymbol{x}}_{k+1}=\\breve{\\boldsymbol{x}}_{1} $ .", "\\newline Note that finding the pairs of edges which belong to the same face is difficult and time-consuming so we relax this calculation to the summation of the cosine of angles between all pairs of edges meeting at the vertex $ \\boldsymbol{x}_{i} $ : \\newline <equationgroup> <equation> $  s_{A}(\\boldsymbol{x}_{i}):=\\sum_{a=1}^{k-1}\\sum_{b=a+1}^{k}\\frac% {\\breve{\\boldsymbol{x}}_{a}^{\\top}\\breve{\\boldsymbol{x}}_{b}}{||\\breve{% \\boldsymbol{x}}_{a}||_{2}||\\breve{\\boldsymbol{x}}_{b}||_{2}}, $ $  s_{A}(\\boldsymbol{x}_{i}):=\\sum_{a=1}^{k-1}\\sum_{b=a+1}^{k}\\frac% {\\breve{\\boldsymbol{x}}_{a}^{\\top}\\breve{\\boldsymbol{x}}_{b}}{||\\breve{% \\boldsymbol{x}}_{a}||_{2}||\\breve{\\boldsymbol{x}}_{b}||_{2}}, $ </equation> </equationgroup> where $ \\breve{\\boldsymbol{x}}_{a}:=\\boldsymbol{x}_{a}-\\boldsymbol{x}_{i} $ , $ \\breve{\\boldsymbol{x}}_{b}:=\\boldsymbol{x}_{b}-\\boldsymbol{x}_{i} $ , and $ \\boldsymbol{x}_{a} $ and $ \\boldsymbol{x}_{b} $ denote the $ a $ -th and $ b $ -th neighbor of $ \\boldsymbol{x}_{i} $ .", "In Eq. ( [@ref:LABEL:equation_anomaly_score] ), we have omitted the redundant angles because of symmetry of inner product.", "Note that the Eq. ( [@ref:LABEL:equation_anomaly_score] ) implies that we normalize the $ k $ neighbors of $ \\boldsymbol{x}_{i} $ to fall on the unit hyper-sphere centered at $ \\boldsymbol{x}_{i} $ and then compute their cosine similarities (see Fig. [@ref:LABEL:figure_polyhedron] -c).", "\\newline The mentioned relaxation is valid for the following reason.", "Take two edges meeting at the vertex $ \\boldsymbol{x}_{i} $ .", "If the two edges belong to the same polyhedron face, the relaxation is exact.", "Consider the case where the two edges do not belong to the same face.", "These two edges are connected with a set of polyhedron faces.", "If we tweak one of the two edges to increase/decrease the angle between them, the angle of that edge with its neighbor edge on the same face also increases/decreases.", "Therefore, the changes in the additional angles of relaxation are consistent with the changes of the angles between the edges sharing the same faces.", "\\newline After scoring the data points, we can sort the points and find a suitable threshold visually using a scree plot of the scores.", "However, in order to find anomalies automatically, we apply K-means clustering, with two clusters, to the scores.", "The cluster with the larger mean is the cluster of anomalies because the higher the score, the more anomalous the point.", "\\newline For finding anomalies for out-of-sample data, we find $ k $ -NN for the out-of-sample point where the neighbors are from the training points.", "Then, we calculate the anomaly score using Eq. ( [@ref:LABEL:equation_anomaly_score] ).", "The K-means cluster whose mean is closer to the calculated score determines whether the point is normal or anomaly.", "It is noteworthy that one can see anomaly detection for out-of-sample data as novelty detection [@bib:pimentel2014review] .", "\\newline </subsection> <subsection> <title> 3.2 Kernel Curvature Anomaly Detection </title> The pattern of normal and anomalous data might not be linear.", "Therefore, we propose Kernel CAD (K-CAD) to work on data in the feature space.", "In K-CAD, the two stages of finding $ k $ -NN and calculating the anomaly score are performed in the feature space.", "Let $ \\boldsymbol{\\phi}:\\mathcal{X}\\rightarrow\\mathcal{H} $ be the pulling function mapping the data $ \\boldsymbol{x}\\in\\mathcal{X} $ to the feature space $ \\mathcal{H} $ .", "In other words, $ \\boldsymbol{x}\\mapsto\\boldsymbol{\\phi}(\\boldsymbol{x}) $ . Let $ t $ denote the dimensionality of the feature space, i.e., $ \\boldsymbol{\\phi}(\\boldsymbol{x})\\in\\mathbb{R}^{t} $ while $ \\boldsymbol{x}\\in\\mathbb{R}^{d} $ .", "Note that we usually have $ t\\gg d $ .", "The kernel over two vectors $ \\boldsymbol{x}_{1} $ and $ \\boldsymbol{x}_{2} $ is the inner product of their pulled data [@bib:hofmann2008kernel] : $ \\mathbb{R}\\ni k(\\boldsymbol{x}_{1},\\boldsymbol{x}_{2}):=\\boldsymbol{\\phi}(% \\boldsymbol{x}_{1})^{\\top}\\boldsymbol{\\phi}(\\boldsymbol{x}_{2}) $ .", "The Euclidean distance in the feature space is [@bib:scholkopf2001kernel] : $ ||\\boldsymbol{\\phi}(\\boldsymbol{x}_{i})-\\boldsymbol{\\phi}(\\boldsymbol{x}_{j})|% |_{2}=\\sqrt{k(\\boldsymbol{x}_{i},\\boldsymbol{x}_{i})-2k(\\boldsymbol{x}_{i},% \\boldsymbol{x}_{j})+k(\\boldsymbol{x}_{j},\\boldsymbol{x}_{j})} $ .", "Using this distance, we find the $ k $ -NN of the dataset in the feature space.", "\\newline After finding $ k $ -NN in the feature space, we calculate the score in the feature space.", "We pull the vectors $ \\breve{\\boldsymbol{x}}_{a} $ and $ \\breve{\\boldsymbol{x}}_{b} $ to the feature space so $ \\breve{\\boldsymbol{x}}_{a}^{\\top}\\breve{\\boldsymbol{x}}_{b} $ is changed to $ k(\\breve{\\boldsymbol{x}}_{a},\\breve{\\boldsymbol{x}}_{b})=\\boldsymbol{\\phi}(% \\breve{\\boldsymbol{x}}_{a})^{\\top}\\boldsymbol{\\phi}(\\breve{\\boldsymbol{x}}_{b}) $ .", "Let $ \\boldsymbol{K}_{i}\\in\\mathbb{R}^{k\\times k} $ denote the kernel of neighbors of $ \\boldsymbol{x}_{i} $ whose $ (a,b) $ -th element is $ k(\\breve{\\boldsymbol{x}}_{a},\\breve{\\boldsymbol{x}}_{b}) $ .", "The vectors in Eq. ( [@ref:LABEL:equation_anomaly_score] ) are normalized.", "In the feature space, this is equivalent to normalizing the kernel $ k(\\breve{\\boldsymbol{x}}_{a},\\breve{\\boldsymbol{x}}_{b}):=k(\\breve{\\boldsymbol% {x}}_{a},\\breve{\\boldsymbol{x}}_{b})/\\sqrt{k(\\breve{\\boldsymbol{x}}_{a},\\breve% {\\boldsymbol{x}}_{a})\\,k(\\breve{\\boldsymbol{x}}_{b},\\breve{\\boldsymbol{x}}_{b})} $ [@bib:ah2010normalized] .", "If $ \\boldsymbol{K}^{\\prime}_{i} $ denotes the normalized kernel $ \\boldsymbol{K}_{i} $ , the anomaly score in the feature space is: \\newline <equationgroup> <equation> $  s_{A}(\\boldsymbol{x}_{i}):=\\sum_{a=1}^{k-1}\\sum_{b=a+1}^{k}% \\boldsymbol{K}^{\\prime}_{i}(a,b), $ $  s_{A}(\\boldsymbol{x}_{i}):=\\sum_{a=1}^{k-1}\\sum_{b=a+1}^{k}% \\boldsymbol{K}^{\\prime}_{i}(a,b), $ </equation> </equationgroup> where $ \\boldsymbol{K}^{\\prime}_{i}(a,b) $ is the $ (a,b) $ -th element of the kernel.", "The K-means clustering and out-of-sample anomaly detection are similarly performed as in CAD.", "\\newline Our observations in experiments showed that the anomaly score in K-CAD is ranked inversely for some kernels such as Radial Basis Function (RBF), Laplacian, and polynomial (different degrees) in various datasets.", "In other words, for example, in K-CAD with linear (i.e., CAD), cosine, and sigmoid kernels, the more anomalous points have greater score but in K-CAD with RBF, Laplacian, and polynomial kernels, the smaller score is assigned to the more anomalous points.", "We conjecture that the reason lies in the characteristics of the kernels.", "We defer more investigations for the reason as a future work.", "In conclusion, for the mentioned kernels, we should either multiply the scores by $ -1 $ or take the K-means cluster with smaller mean as the anomaly cluster.", "\\newline </subsection> <subsection> <title> 3.3 Anomaly Landscape and Anomaly Paths </title> We define anomaly landscape to be the landscape in the input space whose value at every point $ \\boldsymbol{x}_{i} $ in the space is the anomaly score computed by Eq. ( [@ref:LABEL:equation_anomaly_score] ) or ( [@ref:LABEL:equation_anomaly_score_kernel] ).", "The point $ \\boldsymbol{x}_{i} $ in the space can be either the training or out-of-sample point but the $ k $ -NN is obtained from the training data.", "We can have two types of anomaly landscape where all the training data points or merely the non-anomaly training points are used for $ k $ -NN.", "In the latter type, the training phase of CAD or K-CAD are performed before calculating the anomaly landscape for the whole input space.", "\\newline We also define the anomaly path as the path that an anomalous point has traversed from its not-known-yet normal version to become anomalous.", "Conversely, it is the path that an anomalous point should traverse to become normal.", "In other words, {an anomaly path can be used to make a normal sample anomalous or vice-versa} .", "At every point on the path, we calculate the $ k $ -NN again because the neighbors may change slightly during the path.", "For anomaly path, we use the second type of anomaly landscape where the path is like going up/down the mountains in this landscape.", "For finding the anomaly path for every anomaly point, we use gradient descent where the gradient of the Eq. ( [@ref:LABEL:equation_anomaly_score] ) is used: \\newline <equationgroup> <equation> $ \\frac{\\partial s_{A}(\\boldsymbol{x}_{i})}{\\partial\\boldsymbol{x}_% {i}}=\\sum_{a=1}^{k-1}\\sum_{b=a+1}^{k}\\bigg{[}\\frac{1}{||\\breve{\\boldsymbol{x}}% _{a}||_{2}||\\breve{\\boldsymbol{x}}_{b}||_{2}}\\Big{[}\\!-(\\breve{\\boldsymbol{x}}% _{a}+\\breve{\\boldsymbol{x}}_{b})+\\breve{\\boldsymbol{x}}_{a}^{\\top}\\breve{% \\boldsymbol{x}}_{b}\\big{(}\\frac{\\breve{\\boldsymbol{x}}_{a}}{||\\breve{% \\boldsymbol{x}}_{a}||_{2}^{2}}+\\frac{\\breve{\\boldsymbol{x}}_{b}}{||\\breve{% \\boldsymbol{x}}_{b}||_{2}^{2}}\\big{)}\\Big{]}\\bigg{]}, $ $ \\frac{\\partial s_{A}(\\boldsymbol{x}_{i})}{\\partial\\boldsymbol{x}_% {i}} $ $ =\\sum_{a=1}^{k-1}\\sum_{b=a+1}^{k}\\bigg{[}\\frac{1}{||\\breve{% \\boldsymbol{x}}_{a}||_{2}||\\breve{\\boldsymbol{x}}_{b}||_{2}}\\Big{[}\\!-(\\breve{% \\boldsymbol{x}}_{a}+\\breve{\\boldsymbol{x}}_{b})+\\breve{\\boldsymbol{x}}_{a}^{% \\top}\\breve{\\boldsymbol{x}}_{b}\\big{(}\\frac{\\breve{\\boldsymbol{x}}_{a}}{||% \\breve{\\boldsymbol{x}}_{a}||_{2}^{2}}+\\frac{\\breve{\\boldsymbol{x}}_{b}}{||% \\breve{\\boldsymbol{x}}_{b}||_{2}^{2}}\\big{)}\\Big{]}\\bigg{]}, $ </equation> </equationgroup> whose derivation is eliminated for brevity (see supplementary material at the end of this article).", "The anomaly path can be computed in CAD and not K-CAD because the gradient in K-CAD cannot be computed analytically.", "The anomaly path can have many applications one of which is image denoising as explained in our experiments.", "\\newline </subsection>  </section>"], ["<section> <title> 4 Prototype Selection </title>  <subsection> <title> 4.1 Inverse Curvature Anomaly Detection </title> If the anomaly detection uses scores, we can see instance ranking and numerosity reduction in the opposite perspective of anomaly detection.", "Therefore, the ranking scores can be considered as the anomaly scores multiplied by $ -1 $ : $ s_{R}(\\boldsymbol{x}_{i}):=-1\\times s_{A}(\\boldsymbol{x}_{i})=-\\sum_{a=1}^{k-1% }\\sum_{b=a+1}^{k}(\\breve{\\boldsymbol{x}}_{a}^{\\top}\\breve{\\boldsymbol{x}}_{b})% /(||\\breve{\\boldsymbol{x}}_{a}||_{2}||\\breve{\\boldsymbol{x}}_{b}||_{2}) $ .", "We sort the ranking scores in descending order.", "The data point with larger ranking score is more important.", "As the order of ranking scores is inverse of the order of anomaly scores, we name this method as inverse CAD (iCAD) .", "\\newline Prototype selection can be performed in two approaches: (I) the data points are sorted and a portion of the points having the best ranks is retained, or (II) a portion of data points is retained as prototypes and the rest of points are discarded.", "Some examples of the fist approach is IRMD, PSA, SOS, and SE.", "DROP3 and ENN are examples for the second approach.", "The iCAD can be used for both approaches.", "The first approach is ranking the points with the ranking score.", "For the second approach, we apply K-means clustering, with two clusters, to the ranking scores and take the points of the cluster with larger mean.", "\\newline </subsection> <subsection> <title> 4.2 Kernel Inverse Curvature Anomaly Detection </title> We can perform iCAD in the feature space to have Kernel iCAD (K-iCAD) .", "The ranking score is again the anomaly score multiplied by $ -1 $ to reverse the ranks of scores: $ s_{R}(\\boldsymbol{x}_{i}):=-1\\times s_{A}(\\boldsymbol{x}_{i})=-\\sum_{a=1}^{k-1% }\\sum_{b=a+1}^{k}\\boldsymbol{K}^{\\prime}_{i}(a,b) $ .", "Again, we have two approaches where the points are ranked or K-means is applied on the scores.", "Note that for what was mentioned before, we do not multiply by $ -1 $ for some kernels including RBF, Laplacian, and polynomial.", "Note that iCAD and K-iCAD are task agnostic and can be used for data reduction in classification, regression, and clustering.", "For classification, we apply the method for every class while in regression and clustering, the method is applied on the entire data.", "\\newline </subsection>  </section>"], ["<section> <title> 5 Experiments </title>  <subsection> <title> 5.1 Experiments for Anomaly Detection </title> Synthetic Datasets: We examined CAD and iCAD on three two-dimensional synthetic datasets, i.e., two moons and two homogeneous and heterogeneous clusters.", "Figure [@ref:LABEL:figure_synthetic] shows the results for CAD and K-CAD with RBF and polynomial (degree three) kernels.", "As expected, the abnormal and core points are correctly detected as anomalous and normal points, respectively.", "The boundary points are detected as anomaly in CAD while they are correctly recognized as normal points in K-CAD.", "In heterogeneous clusters data, the larger cluster is correctly detected as normal in CAD but not in K-CAD; however, if the threshold is manually changed (rather than by K-means) in K-CAD, the larger cluster will also be correctly recognized.", "As seen in this figure, the scores are reverse in EBF and polynomial kernels which is consistent to our previous note in the paper.", "We also show the anomaly landscape and anomaly paths for CAD in Fig. [@ref:LABEL:figure_synthetic] .", "The K-CAD does not have anomaly paths as mentioned before.", "The landscapes in this figure are of the second type and the paths are shown by red traces which simulates climbing down the mountains in the landscape.", "\\newline Real Datasets: We did experiments on several real datasets of anomaly detection.", "The datasets, which are taken from [@bib:web_anomaly_datasets] , are speech, opt.", "digits, arrhythmia, wine, and musk with 1.65%, 3%, 15%, 7.7%, and 3.2% portions of anomalies, respectively.", "The sample size of these datasets are 3686, 5216, 452, 129, and 3062 and their dimensionality are 400, 64, 274, 13, and 166, respectively.", "We compared CAD and K-CAD with RBF and polynomial (degree 3) kernels to Isolation forest, LOF, one-class SVM (RBF kernel), and EE.", "We used $ k=10 $ in LOF, CAD, and K-CAD."]], "target": "The average area under the ROC curve (AUC) and the average time for both training and test phases over 10-fold Cross Validation (CV) are reported in Table . For wine data, because of small sample size, we used 2-fold CV. The system running the methods was Intel Core i7, 3.60 GHz, with 32 GB RAM. In most cases, K-CAD has better performance than CAD; although CAD is useful and effective as we will see for anomaly path and also instance ranking. For speech and optdigits datasets, RBF kernel has better performance than polynomial and for other datasets, polynomial kernel is better. Mostly, K-CAD is faster in both training and test phases because K-CAD uses kernel matrix and normalizing the matrix rather than element-wise cosines in CAD. In speech and optdigits datasets, we outperform all the baseline methods in both trainign and test AUC rates. In arrhythmia data, K-CAD with polynomial kernel has beter results than isolation forest. For wine dataset, K-CAD with polynomial kernel is better than isolation forest, SVM, and EE. In musk data, K-CAD with both RBF and polynomial kernels is better than isolation forest and SVM."}, {"tabular": ["  Usage  &  Morning rush  &  Afternoon rush  &  Sat morning  &  Relax 1  &  Relax 2  &  Home  &  Work 1  &  Work 2  &  Sleep ", " Social  &  0.326  &  0.418***  &  0.317  &  0.351  &  0.338***  &  0.22***  &  0.409  &  0.4  &  0.073 ", " Video  &  0.342  &  0.412  &  0.336**  &  0.361***  &  0.36*  &  0.238*  &  0.376  &  0.367  &  0.091* ", " Music  &  0.446*  &  0.436**  &  0.302  &  0.316  &  0.309  &  0.217  &  0.344  &  0.391  &  0.07 ", " Reading  &  0.441**  &  0.391  &  0.321  &  0.318  &  0.291  &  0.207  &  0.393  &  0.402  &  0.068 ", " Game  &  0.398***  &  0.411  &  0.323  &  0.34  &  0.323  &  0.229**  &  0.379  &  0.381  &  0.087** ", " Shopping  &  0.338  &  0.413  &  0.31  &  0.353  &  0.308  &  0.189  &  0.427***  &  0.423***  &  0.059 ", " Restaurant  &  0.226  &  0.461*  &  0.332***  &  0.439*  &  0.355**  &  0.143  &  0.398  &  0.357  &  0.041 ", " Transportation  &  0.354  &  0.402  &  0.369*  &  0.374**  &  0.304  &  0.208  &  0.386  &  0.385  &  0.076 ", " office  &  0.356  &  0.392  &  0.316  &  0.334  &  0.306  &  0.206  &  0.429**  &  0.424**  &  0.086*** ", " stocks  &  0.192  &  0.195  &  0.074  &  0.062  &  0.092  &  0.058  &  0.815*  &  0.489*  &  0.016  "], "ref_sec": [["<section> <title> 1 Introduction </title>  As reported by UN , up to 2018, $ 55\\% $ of the world\u2019s population lives in urban areas, and this proportion is expected to increase to $ 68\\% $ by 2050.", "The modern life style, the expanding urban population and the increasing complicated city structure bring changeable type, intensity and distribution of the residents activities, which which raise challenges to city management, ranging from traffic monitoring, resource scheduling to city planning.", "\\newline From the viewpoint of time, the changeable residents activities at different time lead the city transform between different states.", "For example, at rush hours when most residents are on the main road with crowded traffic, the city belongs to a state; while in working hours when most residents are concentrated in office area, the city belongs to another state.", "In order to build smart cities which are both efficient and livable, understanding how the transform of city state along with the time, i.e., urban dynamics, has become an urgent demand for policymaker, city governors and urban planners [@bib:xia2019revealing] .", "\\newline Previous understanding of residents\u2019 activities comes from conducting surveys on human agents [@bib:morenoff2001neighborhood] , which provides detailed information about people\u2019s behaviors.", "However, collecting such kind of data is costly, and also has limitations in terms of generalization and geographical scope.", "Luckily, smart phones and mobile network are popular and ubiquitous everywhere, which makes it available for us to collect large-scale mobility data.", "Recently, many works have investigated urban dynamics through resident\u2019 mobile behaviors.", "Sofiane {et al.} [@bib:spatiotemporaldynamics] built activity time series for London and Doha, and found that close neighborhoods tend to share similar rhythms.", "Louail {et al.} [@bib:louail2014mobile] demonstrated that the city shape and hot-spots change with the course of the day.", "Fabio {et al.} [@bib:cityrhythm] captured the spatio-temporal activity in a city across multiple temporal resolutions, and visualized different activity levels in different time slots.", "Xia {et al.} [@bib:xia2019revealing] revealed the daily activity patterns by learning offline mobility and online App usage together.", "However, these previous works are either based on statics [@bib:spatiotemporaldynamics,louail2014mobile] , or case studies of several regions [@bib:cityrhythm,xia2019revealing,fan2014cityspectrum] , which do not consider the spatial distribution of residents\u2019 activities in the city, thus are not able to present urban dynamics in a comprehensive and concise way.", "\\newline To bring meaningful and useful insights in understanding urban dynamics, three key questions are raised: \\newline <list> \\ What mobility features should be used to characterize urban dynamics from high-dimensional activities, considering that mobility has a spatial distribution in the city? \\newline \\ \\ What are the basic components, i.e., city states, in urban dynamics? \\newline \\ \\ What are the long-term periodicity and short-term regularity of urban dynamics? \\newline \\ </list> \\newline In this paper, we propose UrbanRhythm to address these questions.", "\\newline Firstly, after dividing the mobility data into different time slots, we look into the dynamics reflected by the mobility changing with these time slots.", "Yuan et al. [@bib:Zhenyu2015] has proved the moving-in and moving-out flow can be used to discover urban functional regions, and commuting is the most important activity in the city.", "Thus, for each region in the city in each time slot, we extract {staying, leaving, arriving} three attributes to represent the mobility within it.", "Considering the spatial distribution of mobility in the city, for each time slot, we map the mobility of different regions in the city to a three-channel city image, where a pixel on the image represents a region, and three channels correspond to {staying, leaving, arriving} attributes.", "Then image processing methods could be utilized to capture the mobility spatial distribution feature in the city.", "Compared with ordinary image processing tasks, we lack supervision and enough data to train a deep learning network.", "Thus we redefine an unsupervised image processing method Saak transform [@bib:saak,lossysaak] , and utilize it to capture the mobility spatial distribution feature in the city.", "\\newline To solve the second question, we detect city states, i.e., certain kinds of mobility distribution, by utilizing unsupervised clustering after calculating mobility distribution features for each time slot.", "Several city states are identified, such as working state, sleeping state, which highly correspond to residents\u2019 daily behaviors.", "The detected states could be further divided into sub-states.", "For example, sleeping state could be divided into deep-sleeping state and light-sleeping state.", "\\newline For the third question, we first visualize the urban dynamics by full time mapping and 24-hour mapping to directly observe the long-term periodicity of urban dynamics.", "As a result, we find the long-term periodicity of urban dynamics highly correspond to the periodicity of weekdays, weekends and festival holidays.", "Then, to investigate the short-term regularity of urban dynamics, we design a novel motif analysis method and implement it on the city state series, discovering motifs of various lengths and their hierarchy relationships.", "We consider motifs in urban dynamics reflect residents\u2019 regular behaviors and the composition pattern of motifs actually tells how residents\u2019 long-term regular behaviors are composed with short-term regular behaviors.", "\\newline Finally, we carry out two experiments on two real-life datasets of Beijing and Shanghai.", "Besides, a validation experiment is done by employing a TF-IDF analysis [@bib:Paik2013] on the relation between App usage and city states, which support our interpretation of the detected city states and further demonstrate that urban dynamics could be revealed from human mobility.", "\\newline To summarize, the contribution of our work is four-fold: \\newline <list> \\ We propose a novel system UrbanRhythm to reveal daily urban dynamics.", "To the best of our knowledge, we are the first to consider the spatial distribution of mobility in the city when studying urban dynamics.", "\\newline \\ \\ We identify specific city states including working time, sleeping time, relaxing time, rush hours and other states corresponding to residents\u2019 daily life.", "These found states can be further divided into sub-states, like deep-sleeping and light-sleeping.", "\\newline \\ \\ We find the long-term periodicity of urban dynamics correspond to weekdays, weekends and holidays.", "Besides, we use a novel motif analysis method to investigate the short-term regularity of urban dynamics, as well as their hierarchy relationships.", "This brings knowledge about residents\u2019 basic regular behaviors and how these behaviors compose long-term regular behaviors.", "\\newline \\ \\ We evaluate our method in two mobility datasets, Beijing and Shanghai, from different sources.", "A validation experiment is done by analyzing App usage records, which further support our revealing of urban dynamics. .", "\\newline \\ </list> \\newline  </section>"], ["<section> <title> 2 Overview </title>  <subsection> <title> 2.1 Problem Statement </title> In order to characterize urban dynamics from mobility data, we have the following definitions: \\newline Definition 1 (Region) In this problem, we partition a city into a $ X\\times Y $ grid map based on the longitude and latitude where a grid denotes a region, as shown in Fig. [@ref:LABEL:fig:definition] (a).", "Here, region in $ i $ - $ th $ row and $ j $ - $ th $ column is denoted by $ R_{i,j} $ .", "\\newline Definition 2 (City Image, City Image Series) After dividing a city into $ X\\times Y $ grids, we can describe the characters of the city by a three-channel image, where each channel presents one character and each pixel presents one region.", "Here we define the channels of an image as an staying-channel, a leaving-channel and an arriving-channel presenting that how many people stay at, leave from and arrive in the region during a given time slot, respectively [@bib:Zhenyu2015] .", "The 3-channel image of a given time slot is shown in Fig. [@ref:LABEL:fig:definition] (b).", "City images at different time slots form a city image series, which reveal the variation of human mobility along with the time.", "A city image series is shown in Fig. [@ref:LABEL:fig:definition] (c), where $ N $ is the total number of time slots.", "We denote the city image series by $ I=\\{I_{1},I_{2},...,I_{n},..,I_{N}\\} $ with $ I_{n} $ denoting the image at $ n $ - $ th $ time slot.", "\\newline Definition 3 (City State, City State Series) We divide city images into several kinds.", "A city state represents a typical kind of city images and further represents a typical kind of mobility distribution.", "Similar city images share the same city state.", "We define the total number of city states to be $ K $ and the state of city image $ m_{n} $ to be $ s_{n} $ , where $ s_{n}=0,1,...,K-1 $ .", "\\newline Definition 4 (Urban Dynamics) We classify each city image in city image series to a city state, forming a city state series $ S=\\{s_{1},s_{2},...,s_{n},..,s_{N}\\} $ with $ s_{n} $ denoting the city state at $ n $ - $ th $ time slot.", "We define urban dynamics as the transform of city states along with time.", "\\newline Definition 5 (Motif, Motif Class) We define a sub-sequence of $ S $ to be $ S_{a,l} $ , where $ a,l $ denote the start time slot and the length of this sub-sequence, respectively.", "For $ S_{a,l} $ , if there exist one or more sub-sequences $ S_{b,l} $ similar to $ S_{a,l} $ and $ b\\neq a $ , we call these sub-sequences motifs, and refer as $ M_{a,l},M_{b,l} $ ,\u2026 $ M_{z,l} $ respectively.", "A motif class $ C_{n} $ refers to a set of similar motifs of the same length.", "\\newline Definition 6 (Motif relationship) A motif could be a sub-sequence of another motif.", "We define the father/son relationships between motifs: If $ M_{a_{1},l_{1}} $ and $ M_{a_{2},l_{2}} $ are two motifs and $ a_{1}<a_{2} $ and $ l_{1}>l_{2} $ , then call $ M_{a_{1},l_{1}} $ a father of $ M_{a_{2},l_{2}} $ and $ M_{a_{2},l_{2}} $ a son of $ M_{a_{1},l_{1}} $ .", "For two motif class $ C_{i} $ and $ C_{j} $ , if there exist $ M_{a_{1},l_{1}}\\in C_{i},M_{a_{2},l_{2}}\\in C_{j} $ and $ M_{a_{1},l_{1}} $ is the father of $ M_{a_{2},l_{2}} $ , then we call $ C_{i} $ a father of $ C_{j} $ , and vice versa.", "In the following sections, without specification, we refer the relationships between motif classes as motif relationships.", "\\newline In this paper, we aim to reveal urban dynamics.", "To do it, we answer three questions: what mobility feature to be used, what city states could be found, what long-term periodicity and short-term regularity would be. \\newline For the first question, we divide the mobility data into different time slots and aim to calculate the mobility feature within each time slot.", "For the second question, we aim to detect city states after extract mobility feature from each time slot and interpret the detected city states.", "For the third question, we aim to directly observe the long-term periodicity of urban dynamics from the visualization of the city state series and investigate the short-term regularity by analyzing motifs in the city state series.", "\\newline </subsection> <subsection> <title> 2.2 System Framework </title> Our system is shown in Fig. [@ref:LABEL:fig:system] .", "To deal with the first question, we extract the mobility attributes {staying, leaving, arriving} at different time slots to form the city image series $ I $ and then conduct multi-channel Saak transform on each city image to calculate the spatial distribution pattern of mobility, get mobility features $ V $ .", "\\newline To solve the second question, we employ hierarchical clustering on the mobility features $ V $ to detect city states.", "Also, we interpret each city states according to the temporal distribution of states and the spatial distribution of the mobility.", "\\newline After that, to answer the third question, we visualize urban dynamics by full time mapping and 24-hour mapping to observe long-term periodicity.", "Besides, we perform a novel motif analysis on the city state series to investigate the short-term regularity of urban dynamics, which includes three parts: discovering motifs, determining motif classes based on the discovered motifs, and finally investigate the hierarchical relationship of motif classes.", "\\newline In the end, we take an App usage analysis to validate our detection and interpretation of city states.", "\\newline </subsection>  </section>"], ["<section> <title> 3 Algorithm Design </title>  In this section, we introduce the algorithms used in this paper, including calculating mobility features by Saak transform, extracting city states by hierarchy clustering, investigating the long-term periodicity and short-term regularity of urban dynamics by visualization and motif analysis.", "\\newline <subsection> <title> 3.1 Calculate mobility features </title> To calculate the spatial distribution of mobility in the city, our basic method is Saak transform.", "In this section, we introduce Saak transform under our problem definition and how we adapt it to our multi-channel city images.", "\\newline Kuo and Chen [@bib:saak] proposed Saak transform recently.", "Saak transform converts a single-channel image $ A_{n} $ to a feature vector $ V_{n} $ in spectral space by implement Karhunen-Loeve transform (KLT) step by step.", "Chen et al. [@bib:lossysaak] put forward lossy saak transform, in which he uses the principal component analysis (PCA) instead of KLT to save time and space.", "\\newline Under our problem definition, with images series as input, each stage of Saak transform has the following three steps: \\newline 1) Assemble adjacent regions: We first choose the size of area in which we calculate the local distribution pattern.", "In practice, we choose the basic scale of $ 2\\times 2 $ .", "Let value in region $ R_{i,j} $ denoted by $ r_{i,j}\\in R^{D} $ , $ i,j=1,2\\dots L_{in} $ , where $ L_{in} $ is the input width and height.", "For each city image, assemble each 4 adjacent regions to be a new grid, denoted as $ G_{i,j}\\in R^{4D} $ , $ i,j=1,2\\dots L_{out} $ .", "\\newline <equationgroup> <equation> $  g_{i,j}=Concatenate(r_{2i-1,2j-1},r_{2i-1,2j},r_{2i,2j-1},r_{2i,% 2j}) $ </equation> <equation> $  L_{out}=L_{in}/2 $ </equation> </equationgroup> \\newline 2) Calculate local distribution pattern: We conduct principal component analysis (PCA) on grids vectors from all $ N $ city images.", "In this way, for each grid $ G_{i,j} $ , a comparison with other grids among all city images is implemented and the variation pattern is calculated and expressed as output vectors $ G^{*}_{i,j} $ .", "\\newline To avoid the change of sign in two consecutive stage, we conduct a sign-to-position(S/P) transform , with $ G^{*}_{i,j} $ as input and $ G^{\\prime}_{i,j} $ as output.", "\\newline <equationgroup> <equation> $  g^{\\prime}_{2k-1}=ReLU(g^{*}_{k}),k=1,2\\dots 4D $ $  g^{\\prime}_{2k-1}=ReLU(g^{*}_{k}),k=1,2\\dots 4D $ </equation> <equation> $  g^{\\prime}_{2k}=ReLU(-g^{*}_{k}),k=1,2\\dots 4D $ $  g^{\\prime}_{2k}=ReLU(-g^{*}_{k}),k=1,2\\dots 4D $ </equation> </equationgroup> \\newline 3) Generate new image: Refill each gird $ G_{i,j} $ with the transform vector $ g^{\\prime}_{i,j}\\in R^{8D} $ .", "Form $ N $ new images with half the original width and height.", "The spatial relationship between grids are kept for the next stage transform.", "\\newline The scale of $ 2\\times 2 $ is the smallest scale we can choose.", "Using bigger scale like 3*3 or 4*4 may miss the influence of small district pattern to city state. And the same as [@bib:lossysaak] did, we reserve components with explained variance ratios lager than $ 3\\% $ in PCA, which has been proved to be an acceptable compromise between efficiency and reserving discriminative information [@bib:lossysaak] .", "\\newline The first stage of Saak transforms is illustrated in Fig. [@ref:LABEL:fig2] .", "In $ k $ stage of Saak transform, the local spatial pattern of $ 2^{k}\\times 2^{k} $ scale is calculated.", "Put together the outputs of all stages, the spatial distribution pattern of mobility is calculated.", "\\newline Multi-channel Saak transform The original Saak transform only deals with one channel at one single time.", "We can\u2019t directly concatenate three channels of city images and apply Saak transform because people\u2019s staying, leaving, arriving are obviously correlated.", "Thus we design Multi-channel Saak transform.", "We apply KLT on channels to do decorrelation and use KLT-transformed images as input for Saak transform.", "For each city image, put together the outputs for all stages of Saak transform as the feature vector for this city image.", "\\newline </subsection> <subsection> <title> 3.2 Extract city states </title> After Saak transform, each city image $ I_{n} $ can be represented as feature vectors $ V_{n} $ ( $ n=1,2\\dots N $ ).", "To save time and space for clustering, we apply PCA on feature vectors to reduce their dimensions to 128, uniformly.", "The choice of this dimension is under the consideration of the explained variance ratio of PCA.", "\\newline Intuitively, human mobility behaviors usually have intrinsic periods of day and week; city state of different time could be alike.", "Thus unsupervised clustering method can be utilized on city images to find those with similar mobility features.", "However, the problem of totally unsupervised clustering is that we don\u2019t have a specific standard to evaluate the cluster results and due to that it\u2019s hard for us to specify a number of clusters.", "On the other hand, we\u2019re not only curious about a specific set of city states or a specific kind of city dynamics, but also their inclusion relationships.", "So to better understand the process of clustering and the relationship between clusters, we use hierarchical clustering method to cluster feature vectors.", "\\newline We conduct hierarchical clustering in these obtained feature vectors $ V_{n} $ ( $ n=1,2\\dots N $ ) of city images.", "The basic idea of hierarchical clustering is to generate a tree of clusters where two son clusters merge to form a father cluster.", "The leaf node of this tree is the input $ N $ feature vectors. And then from bottom to up iteratively merge the most suitable two clusters until the stop condition is met.", "We define the suitability of two clusters\u2019 merging according to Ward\u2019s method [@bib:Jr1963Hierarchical] , to minimize the variance of the clusters after merging.", "By applying hierarchical clustering instead of distance-based or density-based clustering, we could analyze the dynamic states at different levels.", "\\newline </subsection> <subsection> <title> 3.3 Visualization </title> To directly observe the periodicity of urban dynamics, we visualize the obtained urban dynamics in two aspects as follows: \\newline 1) Full time mapping: We plot the obtained state series along time axis, presenting the transform of city state over time.", "By doing this, we hope to reveal the period of urban dynamics and roughly locate the times when the city experiences different dynamics.", "\\newline 2) 24-hour mapping: We show each 48 time slots in the same day as a 24-hour pie chart.", "Besides, according to the coarse localization of different dynamics, we divide the time slots into weekend, weekday and holidays to show 24-hour pie charts respectively.", "By doing this, dynamics within a day can be observed and different kinds of dynamics are presented and compared.", "\\newline </subsection> <subsection> <title> 3.4 Motif analysis </title> To investigate the short-term regularity of urban dynamics in a fine-grained style, we design a motif analysis method, which contains three parts: discovering motifs, determining motif classes, and investigating motif relationships.", "Among the three parts, the first part is inspired by [@bib:Motif] .", "\\newline Discovering motifs: To discovery motifs of arbitrary length, we use a divide-merge strategy.", "A pseudocode of discovering motifs is shown in Algorithm 1, which includes the following steps: \\newline 1) Cut the city state series into windows of length $ l_{w} $ , with stride $ s_{w} $ .", "Denote the sub-sequence within the $ i-th $ window as $ w_{i} $ .", "\\newline 2) Compare the similarity between windows and map the results into a collision matrix $ Mat $ , where \\newline <equationgroup> <equation> $  Mat(i,j)=\\left\\{\\begin{array}[]{rcl}True&&{Dis(w_{i},w_{j})\\leq% \\sigma_{w}}\\\\ False&&{Dis(w_{i},w_{j})>\\sigma_{w}}\\end{array}\\right.", "$ $  Mat(i,j)=\\left\\{\\begin{array}[]{rcl}True&&{Dis(w_{i},w_{j})\\leq% \\sigma_{w}}\\\\ False&&{Dis(w_{i},w_{j})>\\sigma_{w}}\\end{array}\\right.", "$ </equation> </equationgroup> $ Dis $ is the Hamming distance function.", "\\newline 3) Extract traces from $ Mat $ .", "Each trace correspond to two similar sub-sequences, which are composed with several windows.", "We transfer traces to motifs $ M_{i,l} $ .", "\\newline 4) Transfer the extracted traces to motifs $ M_{i,l} $ .", "\\newline <float> Discovering motifs from city state series \\ City state series $ S $ , window length $ l_{w} $ , window difference threshold $ \\sigma_{w} $ , stride $ s_{w} $ .", "\\ \\ Motifs $ M $ .", "\\ \\ \\ 1) Cut $ S $ into windows: \\ \\ for $ i\\in range(0,len(S),l_{w}) $ do \\ \\ $ w[i]=S[i*s_{w}:i*s_{w}+l_{w}] $ \\ \\ end for \\ \\ \\ 2) Compare windows and map the results into $ Mat $ : \\ \\ for $ i\\in range(0,len(w)) $ do \\ \\ for $ j\\in range(i,len(w)) $ do \\ \\ $ Mat[i,j]=Similar(w_{i},w_{j}) $ \\ \\ end for \\ \\ end for \\ \\ \\ 3) Extract traces from $ Mat $ : \\ \\ $ traces=\\{\\} $ \\ \\ for $ i $ in $ range(len(Mat)) $ do \\\\ Find traces \\ \\ for $ i $ in $ range(len(Mat)) $ do \\ \\ if $ Mat[i,j]==True $ then \\ \\ $ L=1 $ \\ \\ while $ Mat[i+1,j+1]==True $ do \\ \\ $ L=L+1 $ \\ \\ end while \\ \\ $ trace[L].append(i) $ \\ \\ end if \\ \\ end for \\ \\ end for \\ \\ \\ 4) Convert traces to motifs: \\ \\ $ M=\\{\\} $ \\ \\ for $ L $ in $ traces $ do \\\\ convert traces to motifs \\ \\ for $ start $ in $ traces[L] $ do \\ \\ $ i=start*s_{w} $ \\ \\ $ l=l_{w}+(L-1)*s_{w} $ \\ \\ $ M[l].append(S[i:i+l]) $ \\ \\ end for \\ \\ end for \\ \\ \\ return $ M $ \\ </float> \\newline Determine motif classes: We use DBSCAN clustering [@bib:ester1996density] to cluster motifs of same length.", "Define each cluster to be a motif class $ C_{n} $ .", "\\newline Investigate motif relationships: We investigate the inclusion relationships of motifs classes and visualize them with the following steps: \\newline 1) For each pair of motif classes, determine their relationship according to Definition 6. \\newline 2) For each motif class, delete its \u201cgrandson\u201d: if $ C_{x} $ is a father of $ C_{y} $ , and both of them are sons of $ C_{z} $ , then delete $ C_{y} $ from sons of $ C_{z} $ .", "\\newline 3) Build a directed graph based on motif relationships.", "\\newline By employing the above analysis method, motifs of different lengths will first be discovered and then be categorised into different classes.", "Finally, a directed graph is build to present the hierarchy relationship of motifs.", "\\newline </subsection>  </section>"], ["<section> <title> 4 Performance Evaluation </title>  In this section, we evaluate our algorithm in two different datasets and investigate the basic components and the regularity in urban dynamics.", "\\newline <subsection> <title> 4.1 Datasets </title> We collect two large scale real world mobility datasets to apply and evaluate our methodology.", "The datasets are collected from two different metropolis: Beijing and Shanghai, China.", "The features of the datasets are presented in Table [@ref:LABEL:tab:data] .", "Shanghai dataset also contains the mobile applications (App) the mobile users are currently using, by resolving the $ URI $ of $ HTTP\\ requests $ .", "We use this App usage records to further validate the city states identified by analyzing mobility features.", "\\newline Beijing: This dataset is collected from the mobile devices in Beijing by a popular mobile application vendor.", "It records the spatio-temporal information of mobile users whenever they request localization services in the applications, such as check-in and location-based social network.", "The localization of the mobility records is mainly achieved by GPS modules on the mobile devices plus network-based enhancement.", "This dataset is large scale in terms of tracing 18,916,166 mobile users in one month.", "\\newline Shanghai: This dataset is collected by a major cellular network operator in Shanghai, China.", "It is a large scale mobility dataset also covering 1,700,000 mobile users with the duration of 5 days.", "It records the spatio-temporal information of mobile subscribers when they access cellular network.", "(i.e., making phone calls, sending texts, or consuming data plan).", "Thus, the recorded locations are at the granularity of cellular base stations.", "It also contains the mobile applications (App) the mobile users are currently using, by resolving the $ URI $ of $ HTTP\\ requests $ .", "Such associations also provide insights about understanding the dynamics from another angle.", "There are 10 types of Apps in our dataset: Social, Video, Music, Reading, Game, Shopping, Restaurant, Transportation, Office, Stock.", "\\newline Privacy and ethical concerns: We have taken the following procedures to address the privacy and ethical concerns of dealing with such sensitive data.", "First, all of the researchers have been authorized by the application vendor and cellular network operator to utilize these two datasets for research purposes, and are bounded by strict non disclosure agreements.", "Second, the data is completely anonymized by replacing the users\u2019 identifiers with random sequence.", "Third, we store all the data in a secure off-line server, and only the core researchers can access the data.", "\\newline </subsection> <subsection> <title> 4.2 Pre-processing </title> We divide Beijing into $ 1km\\times km $ grids, and finally remain the areas in downtown with total grid number of $ 61\\times 65 $ .", "For Shanghai, to evaluate the flexibility of our framework, we divide its city areas into $ 256\\times 256 $ grid map, where each grid has a granularity of $ 200m\\times 200m $ .", "Besides, we calculate the mobility features {staying, leaving, arriving} for each half hour.", "Thus, the number of city images for Beijing is 1440 and for Shanghai is 240.", "\\newline </subsection> <subsection> <title> 4.3 Feature Space Visualization </title> We apply PCA on features vectors after Saak transform to reduce their dimensions and as the input for clustering.", "We conduct t-SNE [@bib:tsne] to visualize the relationship of all 128-dimensional features.", "Results for Beijing and Shanghai are shown in [@ref:LABEL:fig:features] (a) and [@ref:LABEL:fig:features] (b), respectively.", "From these two figures, we can explicitly observe that in the feature space, the time slots of the same state distribute closely to each other, while the time slots of different states generally have a larger distance.", "Therefore, it demonstrates that the Saak and PCA transform is effective to represent the feature of time slots.", "\\newline </subsection> <subsection> <title> 4.4 Revealing urban dynamics </title> <subsubsection> <title> 4.4.1 Hierarchical Clustering Structure </title> Since hierarchical clustering is utilized, the structure of clustering results from up to bottom could be clearly observed.", "By default, we display the cluster hierarchy using several circles, where child clusters are nested within their parent cluster.", "This gives a clear view of the hierarchical relationships of different clusters.", "Circle sizes reflect the number of time slots in the cluster, which allows us to quickly identify the most prevalent states.", "\\newline For Beijing, we show the 3-level results for 3, 7, 11 clusters exhibited in circles with the color from blue to white in Fig. [@ref:LABEL:fig_circle] .", "We also label the semantics for each state when the time slots are divided into 11 clusters.", "Obviously, the outermost three circles represent three basic states in city that people are working, relaxing and sleeping.", "When the number of clusters increases, the time slots can be divided into more detailed states.", "For example, the basic sleeping state of Beijing can be divided into four states {Home} , {Sleep 1} , {Sleep 2} and {Sleep 3} , which represent different levels of people\u2019s staying home and movement in the city.", "The latter three sub-states could be further interpreted as different levels of how many people are sleeping, respectively.", "The same is to Shanghai.", "We show the 3-level results for 3, 6, 9 clusters in Fig. [@ref:LABEL:fig_circle] .", "\\newline To conclude, the hierarchical relationships of different time slots is consistent with our intuitions to the states of city, which is pave the way for our understanding of urban dynamics.", "\\newline </subsubsection> <subsubsection> <title> 4.4.2 The extracted city states </title> To analyze specific city states and investigate how they correspond to residents\u2019 daily life, we set the number of clusters to be 11 and 9 for Beijing and Shanghai respectively and interpret the physical meaning of each state by analyzing the temporal distribution pattern of states, the spatial distribution pattern of mobility, and the relationship between states and sub-states.", "We also show the full time mapping and 24-hour mapping visualization of city state series in Fig. [@ref:LABEL:fig:bj_clustering] and Fig. [@ref:LABEL:fig:sh_clustering] .", "\\newline For Beijing, the state series is shown in Fig. [@ref:LABEL:fig:bj_clustering] (a), and the 24-hour pie chart is shown in Fig. [@ref:LABEL:fig:bj_clustering] (b).", "Since the dataset of Beijing covers a whole month, we can easily observe the period of day and week in the state transform process.", "The distribution of state on the time axis is very symmetrical and neat, which is consistent with the regularity of people\u2019s daily commuting.", "To explain these states in more detail, we align the states on the time axis and display them in 24-hour pie chart, where each circle presents one day and time slots in the same state are exhibited in the same color.", "We summarize the characters of each state as follows: \\newline Sleep States : These states include Sleep 1, Sleep 2 and Sleep 3.", "In these states, most people are sleeping and few people are moving in the city, reflected by bigger value in staying-channel than arriving-channel and leaving-channel.", "Besides, values in all three channels in Sleep states are much smaller than others states, suggesting few people are using the mobile application.", "Values in all three channels decrease from Sleep 3 to Sleep 2 to Sleep 1, which means more and more people become asleep.", "\\newline Home State : This state usually covers 23:00-23:30 and 7:00-7:30 in all days.", "It is similar to Sleep states with larger value in staying-channel and smaller value in leaving-channel and arriving-channel, according to the clustering structure in Fig. [@ref:LABEL:fig:bj_clustering] .", "However we are surprised to find it also appears in some non-weekdays afternoons.", "\\newline Rush States : These states include Rush 1 and Rush 2.", "In these states, most people are moving in traffics, reflected by bigger value in arriving-channel and leaving-channel than staying-channel.", "The distribution of people in city address the main road.", "Specifically, Rush 1 only appears in weekdays, corresponding to go-to-work and off-work rush.", "Rush 2 appears both weekdays are non-weekdays.", "Compared to Rush 1, people presents more staying, less leaving and arriving.", "\\newline Work States : These state include Work 1 and Work 2 state, both appearing in only weekdays.", "In these state, most people are working reflected by high values of official areas in staying-channel.", "Thus, we conclude in these states most people are at work.", "Besides, in Work 2, people\u2019s movement is more frequent than in Work 1.", "We are surprised to find that people\u2019s movement in noon is close to that in the beginning and end of office time.", "\\newline Relax States : These states include Relax 1,2,3.", "Relax 1 covers most day-time in holidays when many people travel far away from the city.", "Relax 2 covers day-time in weekends, 22:00-22:30 and 7:30-8:00 in weekdays and it presents larger value in all three channels than Relax 1.", "Relax 3 appears mostly after Relax 2 or in non-weekdays mornings, with much lower arriving value and leaving value than Relax 2.", "\\newline For Shanghai, the state time series is shown in Fig. [@ref:LABEL:fig:sh_clustering] (a), and the 24-hour pie chart is shown in Fig. [@ref:LABEL:fig:sh_clustering] (b).", "Since the dataset of Shanghai covers only five days, we can only observe the period of day. But the distribution of state on the time axis is still very symmetrical and neat.", "We summarize the characters of each state as follows: \\newline Sleep State : This state mainly covers 23:30-06:00.", "Most people are sleeping and few people are moving in the city.", "Values in arriving-channel and leaving-channel are very low.", "\\newline Work States : These states include Work 1 and Work 2.", "Most people are at work with slight movement in the specific office district.", "Specifically, people in Work 2 state have more movement than Work 1.", "\\newline Rush States : These states include Morning rush, Afternoon rush and Saturday morning.", "In these rush states, people\u2019s moving is much stronger than work and sleep states.", "Movement in theses states addresses downtown areas.", "The value of leaving-channel in Morning rush higher than that of arriving-channel.", "However, it is just opposite in Afternoon rush.", "In Saturday morning, value in both leaving-channel and arriving-channel is high, suggesting the movement in Saturday morning is more directionless than that in weekdays.", "\\newline Relax States : These states include Weekends relax and Evening relax.", "The movement is more frequent than work hours and less frequent than rush hours, as well as less concentrated in office areas and downtown areas.", "This indicates people are moving all around the city without very heavy traffic.", "Thus we conclude people are moving for relaxing in these two states.", "\\newline Home State : This state usually covers 21:30-23:30 and 6:00-7:00 in all days.", "It is similar to Relax states, for they belong to the same root state according to Fig. [@ref:LABEL:fig_circle] .", "However the values of leaving-channel and arriving-channel are smaller than that in Relax states, but larger than that in Sleep states.", "Thus Home state corresponds to the time when people are at home with few movement.", "\\newline To conclude, observing the state in the 24-hour pie chart from clockwise, we have that the dynamics of city from morning to night, from day to month, which reveal the regularity of people\u2019s mobility behavior from inactive to active, and last back to inactive in circle of one day.", "\\newline </subsubsection> <subsubsection> <title> 4.4.3 City Images for States </title> To further explain the states obtained through hierarchical clustering, we show the spatial distribution of the three original mobility features for different time slots and compare their difference.", "Limited by space, we only compare Morning rush , Afternoon rush , Sleep 1 , Work 1 states in Shanghai, whose physical meanings are go-to-work rush, off-work rush, sleeping, working as shown Fig. [@ref:LABEL:fig:cityImageSH] .", "The heatmap is colored with the relative density.", "\\newline 1) In Shanghai, Compared with working state, people\u2019s staying is distributed more uniformly with low arriving and leaving in sleeping state.", "However, for working state, people are staying in some specific area with higher arriving and leaving than sleeping state.", "The reasonable explanation is that people are staying at home and the living area in the city is distributed more uniformly than office areas.", "\\newline 2) As for go-to-work rush and off-work rush, the arriving-channel and leaving-channel have higher values than other states.", "The distribution of mobility in city address the downtown area and main road.", "These show that these two states are much about traffic.", "Interestingly, staying people in off-work rush are more than those in go-to-work rush. And this may due to that people usually have a uniform time to go to work, but do not have uniform off work time.", "Someone keep staying office while others are on the way home.", "We also find that the arriving-channel and leaving-channel in go-to-work rush is similar to the leaving-channel and arriving-channel of off-work rush.", "This implies that off-work rush is the opposite process of go-to-work rush.", "\\newline </subsubsection> </subsection> <subsection> <title> 4.5 Long-term periodicity </title> We directly observe the long-term periodicity of urban dynamics from the full time mapping and 24-hour mapping visualization.", "\\newline From [@ref:LABEL:fig:bj_clustering] (a) and Fig. [@ref:LABEL:fig:sh_clustering] (a), we observe that there are several kinds of dynamics daily repeating, which well correspond to the weekday and weekend periodicity.", "Specifically, for Beijing, Qingming Festival and May Day also present different dynamics.", "\\newline From [@ref:LABEL:fig:bj_clustering] (b) and Fig. [@ref:LABEL:fig:sh_clustering] (b), after dividing days into weekends, weekdays, Qingming Festival and May Day, the difference of different kinds of days could be observed.", "Besides, the same kind of days present similar dynamics.", "\\newline We conclude that the long-term periodicity of urban dynamics are caused by the periodicity of weekdays, weekends and festival holidays.", "The deeper reason is that residents tend to behave similarly in the same kind of days, while behave differently in different kinds of days.", "Readers could refer to our interpretation of city states to compare residents\u2019 different behaviors in different dynamics.", "\\newline Another finding is that, urban dynamics are highly repeating not only in days, but also in hours.", "Hours-regularity are investigated by motif analysis in the next section.", "\\newline </subsection> <subsection> <title> 4.6 Short-term regularity </title> As Shanghai dataset covers only 5 days and Beijing dataset covers 30 days, the regularity of urban dynamics in Beijing is more general and convincing.", "Thus, we only implement motif analysis on Beijing dataset.", "\\newline <subsubsection> <title> 4.6.1 Implementation details </title> We set $ l_{w}=6,s_{w}=2,\\sigma_{w}=1 $ .", "To investigate short-term regularities, we limit each motif within a day and set a frequency threshold $ f_{threshold}=3 $ , that only motifs with more than $ f_{threshold} $ will be counted.", "An exception is for the motifs whose time lengths are 24 hours (we set $ f_{threshold}=1 $ for them), for we wish to know all the kinds of daily dynamic the city will experience.", "In DBSCAN clustering, we set min samples to be 2 and eps to be 0.25 multiply the length of motifs (with a max eps threshold $ eps_{max}=8 $ ).", "Also, we ignore some nodes with out-degree and in-degree equal to 1 when plotting the motif family graph.", "These parameters and operations are for a clean but meaningful motif visualization.", "Parameters are chosen from experiments.", "We infer the corresponding resident behavior for each motif by analyzing how this motif is composed with different city states and when this motif appears.", "\\newline </subsubsection> <subsubsection> <title> 4.6.2 Motifs </title> After employing our motif analysis method on the detected city state series, 59 motifs of different lengths are identified.", "Two motif examples are shown in Fig. [@ref:LABEL:fig:motif_example] .", "We refer two motif examples as sleeping motif and weekday motif , respectively.", "For the sleeping motif, it contains a sequence of Sleep 3 - Sleep 2 - Sleep 1 - Sleep 2 - Sleep 3 - Home.", "We consider it correspond to residents\u2019 sleeping behaviors.", "For the weekday motif, it starts from 0 am and last to 24 am in weekdays, with a state transform from sleeping states, to rush states, to working states, to relaxing states and finally back to sleeping states.", "This motif well corresponds to residents\u2019 behaviors within a weekday and keep repeating in all weekdays.", "\\newline </subsubsection> <subsubsection> <title> 4.6.3 Motif relationships </title> Note that, the sleeping motif is a sub-sequence of the weekday motif, which means the regular sleeping behavior is a component of the regular weekday behavior.", "This kind of relationship also exist between other motifs.", "To further investigate the relationship of motifs, we plot a directed graph to present the hierarchy motif family, shown in Fig. [@ref:LABEL:fig:motif_family] .", "In this graph, every node represent a motif class $ C_{i} $ .", "The inclusion relationship of motifs are represented by an directional edge, from the father motif to its son motif.", "For example, the sleeping motif $ C_{9} $ is a component of the weekday motif $ C_{1} $ .", "Thus from the node with mark 1, go along with the arrow, pass the son and some grandsons, we can reach the node with mark 9.", "In the figure, we also divide motifs into different layers according to their lengths.", "The top layer motifs have longest length, which is 24 hours time.", "While the lowest layer motifs have the shortest length, which is determined by the window length.", "\\newline From Fig. [@ref:LABEL:fig:motif_family] , we could observe 6 top motif classes, which means at the level of 24 hours, the city experiences 6 kinds of states transforms, and further indicates residents have 6 kinds of regular 24-hour behaviors.", "Among these 6 motif classes, $ C_{0} $ occurs only in weekends, and corresponds to residents regular behaviors in weekends.", "$ C_{1} $ occurs only in weekdays, and includes a working time motif.", "Thus we refer $ C_{1} $ as weekday motif, which represents residents\u2019 behaviors in weekdays.", "We denote $ C_{2} $ as holiday motif for the same reason.", "$ C_{3} $ , $ C_{4} $ , $ C_{5} $ occurs only once in the 30 days of Beijing dataset.", "\\newline We could observe that $ C_{0} $ , $ C_{1} $ , $ C_{2} $ has lush families which contain a lot of sons and grandsons.", "This means these long-term regular behaviors is composed with many short-term regular behaviors.", "To dig deeper into it, we visualize how a long-term motif is composed with short-term motifs and analyze how this related to people\u2019s behaviors.", "The visualization is shown in Fig. [@ref:LABEL:fig:familyexample] .", "\\newline Interestingly, some short-term motifs are shared by many different long-term motifs.", "We visualize the shared motifs in Fig. [@ref:LABEL:fig:shared_motif] .", "We speculate this kind of short-term motifs correspond to people\u2019s basic behaviors.", "Sleeping, as a very basic behaviors, are shared by all days.", "Though different days may have different form of sleeping behaviors (see $ C_{57} $ and $ C_{55} $ ), a basic sleeping is common (see $ C_{9} $ and $ C_{6} $ ).", "Another interesting finding is $ C_{35} $ , that the last evening of Qingming festival shares the same $ C_{35} $ with weekends evenings.", "(see the analysis below) \\newline </subsubsection> </subsection> <subsection> <title> 4.7 Special dynamic Patterns </title> By observing dynamics in Fig. [@ref:LABEL:fig:bj_clustering] and Fig. [@ref:LABEL:fig:sh_clustering] and comparing different motifs, we find some interesting dynamic patterns.", "Some of our finding well match people\u2019s intuition while some give surprises.", "\\newline Weekends vs Holidays : Two holidays are detected through our method, i.e., Qingming Festival and May Day.", "People have intuition that weekends and holidays are different, but wonder why and how.", "As shown $ C_{0} $ and $ C_{2} $ , in weekends, Relax 3 covers very morning time and Relax 2 covers other day time and some evening time.", "However in holidays like Qingming Festival and May day, Relax 1 covers almost all the time.", "Relax 3 covers very morning time and almost all the evening time.", "This shows that people\u2019s movements pattern are similar in weekends\u2019 and holidays\u2019 mornings and evenings, while differ in their day-time.", "we conclude that in holidays\u2019 day-time, people tend to travel far away from the city, while in mornings and evenings, people haven\u2019t set off or have backed the city, following the same pattern as weekends.", "\\newline Last evening of holidays : We usually have a sense that on the last evening of holidays, our pace of life back to normal.", "Interestingly, as shown in Fig. [@ref:LABEL:fig:shared_motif] , we find that $ C_{35} $ is shared by both weekends evenings and the evening night of Qingming Festival.", "This indicates that in the last evening of Qingming Festival, the city\u2019s dynamic back to weekends patterns, where a Rush 2 state appears first, then followed Relax 2 and Relax 3.", "It matches with our intuition that people come back city in the last day of holiday, causing a traffic jam, then most people get home while some people still hang out.", "Note that our data only covers the first two days of May Day, so this motif doesn\u2019t appear in May Day.", "\\newline Symmetric night : We find that the sleeping behaviors of residents are more symmetric than expected.", "This pattern is for all the days, regardless weekdays or not.", "As shown in $ C_{9} $ , city\u2019s states in night are : Home - Sleep 3 - Sleep 2 - Sleep 1 - Sleep 2 - Sleep 3 - Home.", "Though this comes from people\u2019s movement patterns, but well matched people\u2019s sleeping habits.", "The government can properly arrange resources like illumination and construction according to this night dynamics.", "\\newline Unexpected peace in afternoons : We find Home state surprisingly appears in two holiday afternoons and one weekend afternoon.", "This suggests people\u2019s slight movement, which means at these moments, the city is as \u2019quite\u2019 and \u2019peaceful\u2019 as about-to-sleep hours.", "\\newline </subsection> <subsection> <title> 4.8 Validation with App Usage </title> In order to validate our explanations of urban dynamics through App usage, we first analyze the normalized App usage at different time slots.", "To make it clear, we show the normalized curve of some Apps in Fig. [@ref:LABEL:fig:app] .", "From these figures, we can observe that the usage of different Apps are various in hour and day.", "For example, Stock Apps are used most frequently during 9:00-11:00 and 13:00-15:00, which is Stock market time.", "Thus, App usage could be utilized to validate the states we identify, and further to explain the urban dynamics from these states.", "\\newline Considering that the numbers of Apps in each App category are different, we can not compare the absolute usage count in the same state directly.", "In order to address this problem, we use TF-IDF statistic to analyze the relationship between App usage and city states [@bib:Paik2013] .", "We denote $ U $ as the absolute usage count of each App, where $ U_{i,j} $ means the usage of $ i $ - $ th $ App under $ j $ - $ th $ state.", "Thus, the transformed App usage $ U^{\\prime} $ can be calculated as follows, \\newline <equation> $ U^{\\prime}_{i,j}=\\frac{U_{i,j}}{\\sum\\limits_{j}{U_{i,j}}}\\times\\log\\frac{\\sum% \\limits_{i}{U_{i,j}}}{U_{i,j}}. $ </equation>"]], "target": "The result is shown in Table ,where we can observe that: 1) In Sleep state, the usage of all Apps are lowest. 2) In Work states, including Work 1 and Work 2, the usage of Stock and Office are highest. 3) In Rush states, the usage of Transportation Apps is high in Morning rush, Afternoon rush, and highest in Saturday morning. Interestingly, in Morning rush and Afternoon rush, the usage of Music and Restaurant is highest. 4) In Relax states, including Relax 1 and Relax 2, the usage of Restaurant, Video, Transportation, Social Apps are high. Specifically, in Relax 1 state where some people tend to travel far in weekends, the usage of Transportation Apps is higher than that in Relax 2 state where people get fewer movement. 5) In Home state, the usage of all Apps is low and the usage of Video and Game are highest among them. People tend to stay home, rest and relax."}]