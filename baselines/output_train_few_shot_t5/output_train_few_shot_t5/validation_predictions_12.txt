< extra_id_0 > table 2 shows the throughput for processing the treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ’ s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while fold ’ s folding technique shows better performance on training thanks to its gpu exploitation .
< extra_id_0 > table 1 : throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as well as the linear dataset when the batch size increases from 1 to 25 . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization .
< extra_id_0 > hyper parameters activation func . activation func . activation func . activation func . activation func . activation func . activation func . activation func . activation func . activation func . activation func . activation func . activation func . activation func . activation func . activation func . activation func . activation func .
< extra_id_0 > [ bold ] best f1 ( in 5 - fold ) without sdp c > [ bold ] best f1 ( in 5 - fold ) with sdp c > [ bold ] best f1 ( in 5 - fold ) without sdp c > [ bold ] best f1 ( in 5 - fold ) without sdp c > [ bold ] best f1 ( in 5 - fold ) without sdp c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > r - f1 50 % c - f1 100 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c c c c c c c c c c c c c c c c c c c c c c c c c c c c c
< extra_id_0 > , and paragraph level acc . c > paragraph level acc . c > paragraph level acc . c > paragraph level acc . c > paragraph level acc . c > paragraph level acc . c > paragraph level acc . c > paragraph level acc . c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c >
< extra_id_0 > 4 shows the c - f1 ( 100 % ) for the two indicated systems ; essay vs . paragraph level . note that the mean performances are lower than the majority performances over the runs given in table 2 . note that the mean performance is lower than the majority performance over the two indicated systems .
< extra_id_0 > bleu c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu c > [ bleu ser ] c > [ bleu ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser
< extra_id_0 > table 1 : data statistics comparison for the original e2e data and our cleaned version ( number of distinct mrs , total number of textual references , ser as measured by our slot matching script , see section 3 ) .
< extra_id_0 > bleu c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu c > [ bleu c > [ bleu ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser
< extra_id_0 > table 4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set : total absolute numbers of errors we found ( added , missed , slight disfluencies , slight disfluencies , slight disfluencies ) .
< extra_id_0 > seq2seqk has a b c > 22 . 0 and all c > 24 . 6 . graphlstm ( song et al . , 2018 ) has a b c > 24 . 4 and gcnseq ( damonte et al . , 2019 ) has a b c > 25 . 6 and gcnseq ( damonte et al . , 2019 ) has a b c > 25 . 6 . graphlstm ( song et al . , 2018 ) c > 24 . 4 and gcnseq ( damonte et al . , 2019 ) has a 24 . 4 .
< extra_id_0 > gcnseq achieves 24 . 5 bleu points on amr17 . gcnseq achieves 24 . 5 bleu points on amr17 . gcnseq achieves 24 . 5 bleu points on amr17 . gcnseq achieves 24 . 5 bleu points on amr17 . gcnseq achieves 24 . 5 bleu points on amr17 . gcnseq achieves 24 . 5 bleu points on amr17 . gcnseq achieves 24 . 5 bleu points on amr17 . ggnn2
< extra_id_0 > # p c > [ bold ] english - czech # p c > [ bold ] english - czech # p c > [ bold ] english - czech # p c > [ bold ] english - czech # p c > [ bold ] english - czech # p c > [ bold ] english - czech # p c > [ bold ] english - czech # p c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [
< extra_id_0 > and 3 c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] c >
< extra_id_0 > rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes .
< extra_id_0 > dcgcn ( 3 ) has 22 . 8m and dcgcn ( 4 ) has 22 . 8m . dcgcn ( 4 ) has 22 . 8m and dcgcn ( 4 ) has 22 . 8m and dcgcn ( 4 ) has 22 . 8m . dcgcn ( 4 ) has 22 . 8m and dcgcn ( 4 ) has 22 . 8m and dcgcn ( 4 ) has 22 . 8m . dcgcn ( 4 ) has 22 . 8m and dcgcn ( 4 ) has 22 . 8m . the model has a better performance than dcgcn ( 1 ) and dcgcn ( 4 ) with 22 . 8m and 52 . 4m .
< extra_id_0 > table 8 : ablation study for density of connections on the dev set of amr15 . - i dense block denotes removing the dense connections in the i - th block . - i dense block denotes removing the dense connections in the i - th block .
< extra_id_0 > 9 : ablation study for modules used in the graph encoder and the lstm decoder . using the lstm decoder and the dcgcn4 encoder , the lstm decoder performs better than the dcgcn4 encoder and the lstm decoder . the lstm decoder performs better than the dcgcn4 encoder .
< extra_id_0 > table 7 : scores for initialization strategies on probing tasks . glorot c > 35 . 1 c > 70 . 8 c > 78 . 7 c > 79 . 7 c > [ bold ] 79 . 7 c > [ bold ] 79 . 7 c > [ bold ] 79 . 7 c > [ bold ] 79 . 7 c > [ bold ] 79 . 7 c > [ bold ] c >
< extra_id_0 > bshift c > depth c > objnum c > topconst c > subjnum c > objnum c > objnum c > objnum c > objnum c > objnum c > topconst c > topconst c > topconst c > topconst c > wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc
< extra_id_0 > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > cmow / 784 c > 79 . 2 c >
< extra_id_0 > the results are presented in table 3 . the hybrid model achieves the highest score on unsupervised downstream tasks . the hybrid model achieves the highest score on unsupervised downstream tasks . the hybrid model achieves the highest score on unsupervised downstream tasks .
< extra_id_0 > mrpc and mpqa mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc .
< extra_id_0 > cmow - c performs better than sts12 and sts14 , respectively . cmow - c performs better on the unsupervised downstream tasks than cbow - c and cmow - c .
< extra_id_0 > the bshift c > tense c > objnum c > topconst c > topconst c > wc r > cmow - c c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > c > c >
< extra_id_0 > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > cmow - c c > 79 . 9 c > [ bold ] c > 79 . 4 c > [ bold ] c > cmow - r c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > all org c > all misc c > all misc c > all loc c > all org c > all misc c > all org c > all misc c > mil - nd c > 57 . 15 c > 89 . 46 c > 89 . 46 c > 89 . 46 c > 89 . 46 c > 89 . 46 c >
< extra_id_0 > all p c > all f1 c > in [ italic ] e + p c > in [ italic ] e + r c > all f1 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 38 c >
< extra_id_0 > ref gen gen gen gen gen ref gen gen gen ref gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent
< extra_id_0 > bold > bleu / bold > bold > bleu / bold > bold > bleu / bold > bold > bleu / bold > bold > bleu / bold > bold > g2s - gin c > 22 . 55 0 . 17 0 . 16 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 bold > bold > bold > g2s - ggnn ldc2015e86 ldc2015e86 ldc2015e86 ldc2015e86 ldc2015e86 cao et al . ( 2018 ) c > - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
< extra_id_0 > 3 : results on ldc2015e86 test set when models are trained with additional gigaword data . c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > 4 shows the results of the ablation study on the ldc2017t10 development set . bold > meteor / bold > bold > meteor / bold > c > 57 . 6m c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > bold > graph diameter / bold > 14 - 20
< extra_id_0 > table 8 shows the fraction of elements in the output that are not present in the input ( added ) and the fraction of elements in the input graph that are missing in the generated sentence ( miss ) , for the test set of ldc2017t10 . the token lemmas are used in the comparison .
< extra_id_0 > table 4 : sem and pos tagging accuracy using features extracted from the 4th nmt encoding layer , trained with different target languages on a smaller parallel corpus ( 200k sentences ) .
< extra_id_0 > 2 : pos and sem tagging accuracy with baselines and an upper bound . pos and sem tagging accuracy with baselines and an upper bound . pos and sem tagging accuracy with baselines .
< extra_id_0 > fr c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > 89 . 4 c > 87 . 9 c > 87 . 9 c > 89 . 4 c > 87 . 9 c >
< extra_id_0 > table 5 shows pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders are shown in table 5 .
< extra_id_0 > attacker ’ s performance on different datasets is shown in table 8 . is the difference between the attacker ’ s performance and the corresponding adversary ’ s accuracy on different datasets . is the difference between the attacker ’ s score and the corresponding adversary ’ s accuracy on different datasets .
< extra_id_0 > table 1 summarizes the accuracies when training directly towards a single task . for pan16 , the accuracies are significantly higher than for pan16 and pan16 . for pan16 , the accuracies are significantly higher than for pan16 .
< extra_id_0 > 2 : protected attribute leakage : balanced & unbalanced data splits . pan16 shows a significant difference between the two datasets ( table 2 : protected attribute leakage and unbalanced task leakage ) .
< extra_id_0 > is the difference between the attacker score and the adversary ’ s accuracy . is the difference between the attacker score and the corresponding adversary ’ s accuracy . is the difference between the attacker score and the adversary ’ s accuracy .
< extra_id_0 > table 6 shows the accuracies of the protected attribute with different encoders with different encoders . leaky achieves 64 . 5 accuracies compared to rnn and guarded , respectively .
< extra_id_0 > ptb + finetune c > wt2 + finetune c > wt2 + dynamic c > yang et al . ( 2018 ) compared to yang et al . ( 2018 ) compared to yang et al . ( 2018 ) compared to yang et al . ( 2018 ) compared to yang et al . ( 2018 ) compared to yang et al . ( 2018 ) . yang et al . ( 2018 ) c >
< extra_id_0 > + bert acc c > base time c > base time c > base time c > base time c > base time c > base time c > base time c > base time c > base time c > base time c > base time c > base time c > base time c > base time c > base time c > base time c > base time c > + bert acc c >
< extra_id_0 > amapolar err c > yahoo err c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > c > c >
< extra_id_0 > table 3 shows the tokenized bleu score on wmt14 english - german translation task . bleu scores on gnmt and olrn are similar to those on newstest2014 dataset . bleu scores on newstest2014 are similar to those on newstest2014 dataset .
< extra_id_0 > “ # params ” : the parameter number of elmo . rnet * : the results published by wang et al . ( 2017 ) . rnet * : the results of wang et al . ( 2018 ) .
< extra_id_0 > “ # params ” : the parameter number in conll - 2003 english ner task . “ # params ” : the parameter number in ner task . “ # params ” : the parameter number in ner task .
< extra_id_0 > table 7 : test accuracy on snli task with base + ln setting and test perplexity on ptb task with base + ln setting and test perplexity on snli task with base + ln setting and test perplexity on snli task with base + ln setting . elrn and glrn perform better on snli task with base + ln setting .
< extra_id_0 > w / system retrieval [ bold ] r - 2 c > [ italic ] w / system retrieval [ bold ] r - 2 c > [ italic ] w / system retrieval [ bold ] # sent c > [ italic ] w / system retrieval [ bold ] r - 2 c > [ italic ] w / system retrieval [ bold ] r - 2 c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] w / system retrieval [ bold ] r - 2 c > [ italic ] w / system retrieval [ bold ] # sent c > w / system retrieval [ bold ] w / system retrieval [ bold ] r - 2 c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > the highest standard deviation among automatic systems is 1 . 0 , with statistical significance marked with ( approximation randomization test , p0 . 0005 ) . the highest standard deviation among automatic systems is highlighted in bold , with statistical significance marked with ( approximation randomization test , p0 . 0005 ) . the best result among automatic systems is highlighted in bold . the highest standard deviation among all automatic systems is 1 . 0 . the highest standard deviation among automatic systems is highlighted in bold .
< extra_id_0 > tlqs and docsub tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlq
< extra_id_0 > tlqs and docsub tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlq
< extra_id_0 > dlqs c > dsim c > docsub c > hclust c > dsim c > dsim c > dsim c > dsim c > dsim c > dsim c > dsim c > dsim c > dsim c > dsim c > dsim c > df c > dsim c > dsim c > dsim c > dsim c > dsim c > dsim c > dsim c > dsim c > dsim c > dsim c > dsim c > dsim c > dsim c > dsim c > dsim c > dsim c > dsim c > dsim c > dsim c > dsim c > dsim clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust
< extra_id_0 > dlqs and docsub perform better than the corpus dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs dlqs
< extra_id_0 > avgdepth is 9 . 9 and avgdepth is 9 . 9 and avgdepth is 9 . 8 . europarl has avgdepth of 79 and avgdepth of 79 . europarl has avgdepth of 79 and avgdepth of 79 . europarl has avgdepth of 79 and avgdepth of 79 . europar
< extra_id_0 > s and d denote question type , answer score sampling , and hidden dictionary learning , respectively . lf is the enhanced version as we mentioned in table 1 . ndcg % comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . ndcg % comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 .
< extra_id_0 > table 2 : performance ( ndcg % ) of ablative studies on different models on visdial v1 . 0 validation set . p2 indicates the most effective one ( i . e . , hidden dictionary learning ) shown in table 1 .
< extra_id_0 > table 5 : comparison on hard and soft alignments . cs - en de - en fi - en lv - en cap > cap > cap > cap > cap > cap > cap > cap > cap > cap > cap > cap > cap > cap > cap > cap
< extra_id_0 > de - en c > zh - en c > zh - en c > zh - en c > zh - en c > zh - en c > zh - en c > zh - en c > zh - en c > zh - en c > zh - en c > zh - en c > c > c >
< extra_id_0 > : inf . < extra_id_1 > l bold > qual / bold > bold > qual / bold > bold > qual / bold > bold > qual / bold > bold > qual / bold > bold > qual / bold > bold > qual / bold > bold > qual / bold > bold > inf . bold >
< extra_id_0 > m1 m2 m1 m2 m2 m1 m2 m2 m1 m2 m2 m1 m2 m2 m1 m2 m2 m1 m2 m2 m1 m2 m1 m2 m2 m1 m2 m1 m2 m2 .
< extra_id_0 > m1 : m0 has a score of 22 . 3 compared to 22 . 3 for sim and gm . m5 : m0 [ italic ] + cyc + para has a score of 0 . 752 compared to 22 . 3 for sim and gm . m4 : m0 [ italic ] + cyc + para has a score of 0 . 754 compared to 22 . 3 for sim and gm . m5 : m0 [ italic ] + cyc + para has a score of 0 . 754 compared to 22 . 3 compared to 22 . 3 compared to 22 . 3 compared to 22 . 3 compared to 22 . 3 compared to 22 . 3 compared to 22 . 3 compared to 22 . 8 compared to 22 . 3 compared to 22 . 3 compared to 22 . 3 compared to 22 . 3 compared to 22 . 8 compared to 22 . 8 compared to 22 . 8 compared to 22 . 3 compared to 22 . 3 compared to 22 . 3 compared to 22 . 3 compared to 22 . 8 compared to 22 . 8 compared to 22 . 8 compared to 22 . 3 compared to 22 . 3 compared to 22 . 3 compared to 22 . 8 compared to 22 . 8 compared to 22 . 8 compared to 22 . 8 compared to 22 . 8 compared to 22 . 3 compared to 22 . 3 compared to 22 . 8 compared to 22 . 8 compared to 22 . 8 compared to 22 . 8 compared to 22 . 3 compared to 22 . 3 compared to 22 . 3 compared to 22 . 8 compared to 22 . 8 compared to 22 . 8 compared to 22 . 8 compared to 22 . 8 compared to 22 . 8 compared to 22 . 8 compared to 22 . 8 compared to 22 . 8 compared to 22 . 8 compared to 22 . 8 compared to 22 . 8 compared to 22 . 8 compared to 22 . 8 compared to 22 . 8 compared to 22 . 8 compared to 22 . 8 compared to 22 . 8 compared to 22 . 8 compared to 22 . 8 compared to 22 . 8 compared to 22 . 8 compared to 22 . 8 compared to 22 . 8 compared to 22 . 8 compared to 22 . 8 compared to 22 . 8 compared to 22 . 8 compared to 22 . 8 compared to 22 . 8 compared to 22 . 8 compared to 22 . 8 compared to 22 . 8 compared to 22 . 8 compared to 22 . 8 compared to 22 . 8 compared to 22 . 8 compared to 22 . 8 compared to 22 . 8 compared to 22 . 8
< extra_id_0 > transfer quality a > b c > models a c > models b c > models a c > models b c > models b c > models a c > models b c > models b c > models a c > models b c > models b c > models a c > models b c > models b c > models b c > models b c > models b c > models b c > models b c > models b c > models b c > models b c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p
< extra_id_0 > see text for validation of gm ; see text for validation of acc ; see text for validation of pp ; see text for validation of acc ; see text for validation of pp ; see text for validation of acc ; see text for validation of gm ; see text for validation of acc ; see text for validation of acc ; see text for validation of pp ; see text for validation of acc ; see text for validation of acc ; see text for validation of pp ; see text for validation of acc .
< extra_id_0 > we observe that m0 and m5 have significantly better performance than m0 and m5 : m0 , respectively . we observe that m0 and m5 have significantly better performance than m0 and m5 : m0 , respectively . we observe that m0 and m5 have significantly better performance than m0 and m5 : m0 , respectively . we observe that m0 and m5 have significantly better performance than m0 and m5 : m0 , respectively .
< extra_id_0 > bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . our best models achieve higher bleu than previous work at similar levels of acc , but untransferred sentences achieve the highest bleu than prior work at similar levels of bleu . our best models achieve higher bleu than previous work at similar levels of bleu . our best models achieve higher bleu than previous work , but untransferred sentences achieve the highest bleu .
< extra_id_0 > table 2 summarizes the percentage of reparandum tokens that were correctly predicted as disfluent . rephrased tokens have a higher rate of disfluency than rephrased tokens . rephrased tokens have a higher rate of disfluency than rephrased tokens . rephrased tokens have a higher rate of disfluency than rephrased tokens . rephrased tokens have a higher rate of disfluency than restart tokens .
< extra_id_0 > table 3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) , either the reparandum or repair ( content - function ) , or in neither of these categories .
< extra_id_0 > c > [ bold ] dev mean c > [ bold ] test mean c > [ bold ] dev best c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > c > c > c > c > c > c > early c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > accuracy ( % ) agree c > accuracy ( % ) disagree c > accuracy ( % ) disagree c > accuracy ( % ) unrelated c > average of word2vec embedding c > 12 . 43 c > 01 . 30 c > 53 . 24 c > 79 . 53 c > 81 . 72 c > [ bold ] c > [ bold ] c > [ bold ] c > c >
< extra_id_0 > table 2 shows the accuracy ( % ) of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the unified model significantly outperforms all previous models .
< extra_id_0 > table 3 shows the effectiveness of both word attention and graph attention for this task . this results show the effectiveness of both word attention and graph attention for this task .
< extra_id_0 > embedding + t has 1 / 1 and embedding + t has 1 / n and embedding + t has 59 . 8 points . embedding + t has 59 . 8 points and embedding + t has 59 . 8 points . embedding + t has 59 . 8 points and embedding + t has 59 . 8 points . embedding + t has 59 . 8 points and embedding + t has 59 . 8 points . embedding + t has 69 . 8 points .
< extra_id_0 > classification ( % ) c > [ bold ] trigger [ bold ] identification ( % ) c > [ bold ] trigger [ bold ] identification ( % ) c > [ bold ] classification ( % ) c > [ bold ] identification ( % ) c > [ bold ] classification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] classification ( % ) c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold ] c > [ bold c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold ] c > [ bold c > [ bold
< extra_id_0 > wer
< extra_id_0 > table 4 shows the results on the dev set and on the test set using discriminative training with only subsets of the code - switched data . the results on the dev set and on the test set using discriminative training with only subsets of the code - switched data .
< extra_id_0 > table 5 : accuracy on the dev set and on the test set , according to the type of the gold sentence in the set : code - switched ( cs ) vs . monolingual ( mono ) . cs and fine - tuned - disc have higher accuracy than fine - tuned - disc .
< extra_id_0 > table 7 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset ( * marks statistically significant improvement ) for all three eye - tracking datasets .
< extra_id_0 > table 5 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features on the conll - 2003 dataset ( * marks statistically significant improvement ) .
< extra_id_0 > the hpcd ( full ) is from the original paper , and it uses syntactic skipgram . the results on belinkov2014exploring ’ s ppa test set are summarized in table 1 . the glove - retro and glove - extended synset embeddings are compared to the lstm - pp embeddings obtained by rothe and schütze ( 2015 ) .
< extra_id_0 > table 2 summarizes the results from rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . the rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . the rbg dependency parser with features coming from various pp attachment predictors and oracle attachments .
< extra_id_0 > table 3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . the effect of removing sense priors and context sensitivity ( attention ) from the model is shown in table 3 .
< extra_id_0 > adding subtitle data and domain tuning for image caption translation ( bleu % scores ) . adding subtitle data and domain tuning for image caption translation ( bleu % scores ) . all results with marian amun .
< extra_id_0 > and mscoco17 . the subs1m subs1m subs1m subs1m subs1m subs1m subs1m subs1m subs1m subs1m subs1m subs1m subs1m subs1m subs1m subs1m subs1m subs1m subs1m subs1m subs1m subs1m subs1m subs1m subs1m subs1m en - de en - de
< extra_id_0 > autocap 1 - 5 ( concat ) and autocap 1 - 5 ( concat ) improves the performance of en - de on flickr16 and mscoco17 compared to en - de on flickr16 , mscoco17 and mscoco17 . adding autocap 1 - 5 ( concat ) improves the performance of en - de on flickr16 , mscoco17 , mscoco17 , mscoco17 , en - de on flickr16 and mscoco17 improves the performance of autocap 1 - 5 ( dual attn . ) on flickr16 .
< extra_id_0 > mscoco17 and en - de c > flickr16 c > flickr17 c > en - de c > flickr16 c > flickr17 c > mscoco17 c > mscoco17 c > enc - gate c > 68 . 30 c > dec - gate c > 37 . 47 c > enc - gate c > 68 . 47 c >
< extra_id_0 > mscoco17 and en - fr c > flickr16 c > mscoco17 c > en - fr c > en - fr c > en - fr c > en - fr c > en - fr c > en - fr c > en - fr c > en - fr c > en - fr c >
< extra_id_0 > en - fr - trans - ff has a better performance than en - fr - smt - back and en - fr - trans - ff , respectively . en - fr - trans - ff has a better performance than en - fr - trans - ff , whereas en - fr - trans - ff has a better performance than en - fr - trans - ff with a better performance than en - fr - trans - ff has a better performance than en - t
< extra_id_0 > table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . the number of parallel sentences in the train , test and development splits for the language pairs we used is shown in table 1 .
< extra_id_0 > table 2 : training vocabularies for the english , french and spanish data used for our models . the training vocabularies for the english , french and spanish data used for our models are shown in table 2 .
< extra_id_0 > table 5 summarizes the automatic evaluation scores ( bleu and ter ) for the rev systems . the bleu and ter scores for the rev systems are shown in table 5 .
< extra_id_0 > results on flickr8k . the row labeled vgs is the visually supervised model from chrupala2017representations . the row labeled vgs is the visually supervised model from chrupala2017representations . the median recall rate is 0 and the median recall rate is 0 . 0 .
< extra_id_0 > results on synthetically spoken coco . the row labeled vgs is the visually supervised model from chrupala2017representations . the row labeled vgs is the visually supervised model from chrupala2017representations .
< extra_id_0 > turns in a u > screenplay screenplay screenplay screenplay screenplay screenplay screenplay screenplay screenplay that ’ s so clever you want hate hate hate hate hate hate hate . we report further examples in the appendix . we report further examples in table 1 .
< extra_id_0 > bold > rnn / bold > bold > dan / bold > bold > dan / bold > bold > dan / bold > bold > dan / bold > bold > dan / bold > bold > dan / bold > bold > dan / bold > bold > dan / bold > 3 4
< extra_id_0 > the last two rows correspond to the case where negative labels are flipped to positive and vice versa . the numbers indicate that the score increases in positive and negative sentiment with respect to the original sentence and vice versa .
< extra_id_0 > it n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t
