< extra_id_0 > table 2 shows throughput for processing the treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ’ s iterative approach , with the large movie review dataset . the recursive framework performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation .
< extra_id_0 > table 1 : throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but the linear dataset exhibits only a small room of improvement .
< extra_id_0 > activation func . c > [ bold ] hyper parameters activation func . c > [ bold ] hyper parameters learning rate c > [ bold ] hyper parameters learning rate c > [ bold ] hyper parameters learning rate c > [ bold ] hyper parameters learning rate c > [ bold ] hyper parameters learning rate c > [ bold ] hyper parameters learning rate c > [ bold ] hyper parameters learning rate c > [ bold ]
< extra_id_0 > [ bold ] best f1 ( in 5 - fold ) without sdp c > [ bold ] best f1 ( in 5 - fold ) with sdp c > [ bold ] best f1 ( in 5 - fold ) without sdp c > [ bold ] best f1 ( in 5 - fold ) without sdp c > [ bold ] best f1 ( in 5 - fold ) with sdp c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > 50 % c - f1 100 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1
< extra_id_0 > r - f1 and paragraph level r - f1 are similar to paragraph level r - f1 and paragraph level r - f1 compared to paragraph level r - f1 compared to paragraph level r - f1 compared to paragraph level r - f1 compared to paragraph level r - f1 compared to paragraph level r - f1 compared to paragraph level r - f1 compared to paragraph level r - f1 compared to paragraph level r - f1 compared to mate
< extra_id_0 > the mean performance is lower than the majority performance over the two indicated systems ; essay vs . paragraph level . note that the mean performance is lower than the majority performance over the two indicated systems ; essay vs . paragraph level .
< extra_id_0 > bleu nist nist rouge - l rouge - l rouge - l rouge - l rouge - l rouge - l rouge - l rouge - l rouge - l rouge - l rouge - l rouge - l rouge - l rouge - l rouge - l rouge - l
< extra_id_0 > mrs , total number of textual references , ser as measured by our slot matching script , see section 3 . cap > [ bold ] dataset c > [ bold ] mrs c > [ bold ] refs c > [ bold ] ser ( % ) c > [ bold ] ser ( % ) c > [ bold ] train c > 4 , 862 c > 4 , 672 c > [ bold ] ser ( % )
< extra_id_0 > bleu c > [ bold ] nist c > [ bold ] rouge - l c > [ bold ] rouge - l c > [ bold ] rouge - l c > [ bold ] rouge - l c > [ bold ] rouge - l c > [ bold ] rouge - l c > [ bold ] c > [ bold ] c > [ bold ] c >
< extra_id_0 > table 4 : results of manual error analysis of tgen on a sample of 100 instances from the original test set : total absolute numbers of errors we found ( added , missed , wrong values , slight disfluencies , slight disfluencies ) .
< extra_id_0 > seq2seqk ( song et al . , 2017 ) c > - c > 22 . 0 r > graphlstm ( song et al . , 2018 ) c > - c > 24 . 4 r > graphlstm ( song et al . , 2018 ) c > - c > 24 . 4 r > graphlstm ( song et al . , 2018 ) c > 24 . 4 r > 24 . 4 r > 24 . 4 r > 24 . 4 r > 24 . 4 r > 24 . 4 r > 24 . 4 r > 24 . 4 r > 24 . 4 r > 24 . 4 r > 24 . 4 r > 24 . 4 r > 24 . 4 r > 24 . 4 r > 24 . 4 r > 24 . 4 r > 24 . 4 r > 24 . 4 r > 24 . 4 r > 24 . 4 r > 24 . 4 r > 24 . 4 r > 24 . 4 r > 24 . 4 r > 24 . 4 r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r >
< extra_id_0 > shows the model size in terms of parameters on amr17 . gcnseq ( damonte and cohen , 2019 ) achieves 24 . 5 bleu points . # p shows the model size in terms of parameters on amr17 . ggnn2seq ( damonte and cohen , 2019 ) achieves 24 . 5 bleu points . # p shows the model size in terms of parameters on amr17 .
< extra_id_0 > german # p c > [ bold ] english - czech # p c > [ bold ] english - czech # p c > [ bold ] english - czech # p c > [ bold ] english - czech # p c > [ bold ] english - czech # p c > [ bold ] english - czech # p c > [ bold ] english - czech # p c >
< extra_id_0 > c > 2 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 4
< extra_id_0 > rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc
< extra_id_0 > c > # p c > # p c > # p c > # p c > # p c > # p c > # p c > # p c > # p c > # p c > # p c > # p c > # p c > # p c > # p c > # p c > # p c > # p c >
< extra_id_0 > 8 : ablation study for density of connections on the dev set of amr15 . - i dense block denotes removing the dense connections in the i - th block . - i dense block denotes removing the dense connections in the i - th block . - i dense block denotes removing dense connections in the i - th block .
< extra_id_0 > - direction aggregation c > 23 . 7 c > 53 . 2 c > - linear combination c > 24 . 9 c > 55 . 4 c > - global node c > 24 . 9 c > 54 . 6 c > - linear combination c > 54 . 9 c > 54 . 6 c > - coverage mechanism c > 54 . 9 c > 54 . 6 c > c >
< extra_id_0 > table 7 summarizes the initialization strategies on probing tasks . glorot c > 35 . 1 c > 70 . 8 c > [ bold ] 72 . 8 c > [ bold ] 72 . 8 c > [ bold ] 72 . 9 c > [ bold ] 72 . 9 c > [ bold ] 72 . 9 c > [ bold ] 72 . 9 c > [ bold ] 72 . 9 c > [ bold ] 72 . 9 c > [ bold ]
< extra_id_0 > and topconst < extra_id_1 > and topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst
< extra_id_0 > mrpc has a better performance than sick - e and sick - b compared to mrpc and sick - e . sick - e has a better performance than sick - e and sick - b compared to sick - e . sick - e has a better performance than sick - e and sick - b compared to sick - e . sick - e has a better performance than sick - e . sick - e has a better performance than sick - b .
< extra_id_0 > 3 shows the relative change on unsupervised downstream tasks attained by our models . rows starting with “ cmp . ” show the relative change on unsupervised downstream tasks attained by our models . cmow c > + 26 . 6 % c > + 44 . 2 % c > + 49 . 7 % c > + 49 . 7 % .
< extra_id_0 > has a score of 69 . 6 c > [ bold ] 69 . 6 c > [ bold ] 69 . 6 c > [ bold ] 69 . 6 c > [ bold ] 69 . 6 c > [ bold ] 69 . 6 c > [ bold ] 69 . 6 c > [ bold ] 69 . 6 c > [ bold ] 69 . 6 c > [ bold
< extra_id_0 > c > sts12 c > sts13 c > sts14 c > sts15 c > sts16 c > cmow - c c > [ bold ] 43 . 5 c > [ bold ] 52 . 2 c > [ bold ] 52 . 2 c > [ bold ] 52 . 2 c > [ bold ] 52 . 2 c > [ bold ] 52 . 2 c > [ bold ]
< extra_id_0 > and topconst . topconst performs better than cmow - c . topconst performs better than cmow - c . topconst performs better than cmow - c . topconst performs better than cmow - c . topconst performs better than cmow - c .
< extra_id_0 > cmow - r has a better performance than sick - e compared to sick - e compared to sick - e compared to sick - e compared to sick - e compared to sick - e compared to sick - e compared to sick - e compared to sick - e compared to sick - e compared to sick - e compared to sick - e compared to sick - e compared to sick - e compared to sick - r
< extra_id_0 > 51 . 14 mil - nd mil - nd mil - nd mil - nd mil - nd mil - nd mil - nd mil - nd mil - nd mil - nd mil - nd mil - nd mil - nd mil - nd mil - nd mil - nd mil - nd mil - nd mil - nd
< extra_id_0 > all p < extra_id_1 > all f1 0 . 72 0 . 72 0 . 72 0 . 72 0 . 72 0 . 72 0 . 72 0 . 72 0 . 72 0 . 72 0 . 72 0 . 72 0 . 72 0 . 72 0 . 72 0 . 72 mil - nd ( model 2 ) 0 . 72 0 . 72 0 . 72
< extra_id_0 > gen ref gen gen gen con / bold > gen ref gen con / bold > gen ref gen con / bold > gen ref gen con / bold > gen ref gen con / bold > gen
< extra_id_0 > < extra_id_1 > > bleu / bold > bold > bleu / bold > bold > bleu / bold > bold > bleu / bold > bold > bleu / bold > bold > bleu / bold > bold > bleu / bold > bold > bleu / bold > bold > bleu / bold > bold >
< extra_id_0 > 3 : < extra_id_1 > bold > bleu / bold > bold > bleu / bold > bold > bleu / bold > bold > bleu / bold > bold > bleu / bold > bold > bleu / bold > bold > bleu / bold > bold > bleu / bold > bold > bleu / bold > r >
< extra_id_0 > 4 : results of the ablation study on the ldc2017t10 development set . bilstm c > 22 . 50 c > 30 . 42 c > 57 . 6m c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 >
< extra_id_0 > table 8 shows the fraction of elements in the output that are not present in the input ( added ) and the fraction of elements in the input graph that are missing in the generated sentence ( miss ) for the test set of ldc2017t10 . the token lemmas are used in the comparison .
< extra_id_0 > table 4 shows sem and pos tagging accuracy using features extracted from the 4th nmt encoding layer , trained with different target languages on a smaller parallel corpus ( 200k sentences ) . sem and pos tagging accuracy using features extracted from the 4th nmt encoding layer , trained with different target languages .
< extra_id_0 > 2 : pos and sem tagging accuracy with baselines and an upper bound . mft c > 91 . 95 c > 87 . 06 c > 95 . 55 c > word2tag .
< extra_id_0 > c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c >
< extra_id_0 > pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual encoders , averaged over all non - english target languages .
< extra_id_0 > is the difference between the attacker ’ s performance and the corresponding adversary ’ s accuracy . is the difference between the attacker ’ s performance and the corresponding adversary ’ s accuracy . is the difference between the attacker ’ s performance and the corresponding adversary ’ s accuracy . is the difference between the attacker ’ s performance and the corresponding adversary ’ s accuracy .
< extra_id_0 > c > 67 . 4 r > [ italic ] c > [ empty ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ]
< extra_id_0 > table 2 : protected attribute leakage : balanced & unbalanced data splits . mention & dial & dial & dial & dial & dial & dial & dial & dial splits .
< extra_id_0 > is the difference between the attacker score and the adversary ’ s accuracy . is the difference between the attacker score and the adversary ’ s accuracy . is the difference between the attacker score and the adversary ’ s accuracy . is the difference between the attacker score and the adversary ’ s accuracy .
< extra_id_0 > c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c >
< extra_id_0 > and wt2 + finetune c > wt2 + finetune c > wt2 + dynamic r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r >
< extra_id_0 > # params c > # params c > # params c > # params c > # params c > # params c > # params c > # params c > # params c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > yelppolar err compared to yelppolar err compared to yelppolar err compared to yelppolar err compared to yelppolar err compared to yelppolar err compared to yelppolar err compared to yelppolar err compared to yelppolar err compared to yelppolar err compared to yelppolar err compared to yelppolar time compared to yahoo time
< extra_id_0 > table 3 . train : time in seconds per training batch measured from 0 . 2k training steps on tesla p100 . decode : time in milliseconds used to decode one sentence measured on newstest2014 dataset . train : time in seconds measured from 0 . 2k training steps on tesla p100 .
< extra_id_0 > “ # params ” : the parameter number of base . “ # params ” : the parameter number of elmo . rnet * : results published by wang et al . ( 2017 ) .
< extra_id_0 > “ # params ” : the parameter number in conll - 2003 english ner task . “ # params ” : the parameter number in conll - 2003 english ner task .
< extra_id_0 > table 7 : test accuracy on snli task with base + ln setting and test perplexity on ptb task with base + ln setting . ptb task with base + ln setting and elrn task with base + ln setting .
< extra_id_0 > retrieval [ bold ] b - 2 c > [ italic ] w / system retrieval [ bold ] b - 4 c > [ italic ] w / system retrieval [ bold ] # sent c > [ italic ] w / system retrieval [ bold ] r - 2 c > [ italic ] w / system retrieval [ bold ] # word c > [ italic ] w /
< extra_id_0 > the highest standard deviation among automatic systems is highlighted in bold . the highest standard deviation among automatic systems is highlighted in bold . the highest standard deviation among automatic systems is highlighted in bold . the highest standard deviation among all automatic systems is highlighted in bold .
< extra_id_0 > tlqs cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster clust
< extra_id_0 > tlqs cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster clust
< extra_id_0 > tlqs cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster clust
< extra_id_0 > slqs is better than slqs and hlqs . hlqs is better than hlqs and hlqs . hlqs is better than hlqs and hlqs . hlqs is better than hlqs and hlqs . hlqs is better than hlqs and hlqs . hlqs is better than hlqs . hlqs is better than hlqs .
< extra_id_0 > avgdepth : 9 . 9 9 . 9 9 . 9 9 . 9 9 . 9 9 . 9 9 . 9 9 . 9 9 . 9 9 . 9 9 . 9 9 . 9 9 . 9 9 . 9 9 . 9 9 . 9 9 . 9 9 . 9 9 . 9 9 . 9 9 . 9 9 . 9 9 . 9 9 . 9 9 . 9 9 . 9 9 . 9 9 . 9 9 . 9 9 . 9 9 . 9 9 . 9 .
< extra_id_0 > r0 , r1 , r2 , r3 denote regressive loss , weighted softmax loss , and hidden dictionary learning , respectively . lf is the enhanced version as we mentioned in table 1 . ndcg % comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . lf is the enhanced version as we mentioned in table 1 .
< extra_id_0 > p2 indicates the most effective one ( i . e . , hidden dictionary learning ) on visdial v1 . 0 validation set . p2 indicates the most effective one ( i . e . , hidden dictionary learning ) shown in table 2 .
< extra_id_0 > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > c > c > c > c > c > c >
< extra_id_0 > zh - en bold > de - en bold > zh - en bold > zh - en bold > zh - en bold > zh - en bold > zh - en bold > zh - en bold > zh - en bold > zh - en bold > zh - en bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold >
< extra_id_0 > and sfhotel bold > qual / bold > bold > qual / bold > bold > qual / bold > bold > qual / bold > bold > qual / bold > bold > qual / bold > bold > qual / bold > bold > qual / bold > bold > qual / bold > bold >
< extra_id_0 > m1 m2 m1 m2 m2 m1 m2 m2 m1 m2 m1 m2 m2 m1 m2 m1 m2 m1 m2 m2 m1 m2 m1 m2 m2 m1 m2 m1 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 .
< extra_id_0 > m1 : m0 and m5 : m0 have significantly better performance compared to m1 : m0 and m5 : m0 , respectively . m5 : m0 has better performance compared to m5 : m0 and m5 : m0 , respectively . m5 : m0 has better performance compared to m5 : m0 and m5 : m0 , respectively .
< extra_id_0 > m2 and m3 have m0 and m7 , respectively . yelp has m0 and m7 , respectively . semantic preservation a > b and semantic preservation b > a have m0 and m7 , respectively . semantic preservation b > a and semantic preservation b > a have m0 and m7 , respectively . semantic preservation b > a and semantic preservation tie have m0 and m7 , respectively .
< extra_id_0 > see text for validation of gm ; see text for validation of gm ; see text for validation of gm ; see text for validation of gm ; see text for validation of gm ; see text for validation of gm ; see text for validation of gm ; see text for validation of gm ; see text for validation of gm ; see text for validation of gm ; see text for validation of gm ; see text for validation of gm ; see text for validation of gm ; see text for validation of acc ;
< extra_id_0 > m5 : m0 has a better performance than m5 : m0 , whereas m5 : m0 has a better performance than m5 : m0 . m5 : m0 has a better performance than m5 : m0 , whereas m5 : m0 has a better performance than m5 : m0 . m5 : m0 has a better performance than m5 : m0 , whereas m5 : m0 has a better performance than m5 : m0 , whereas m5 : m0 has better performance is better than m5 : m0 .
< extra_id_0 > bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . our best models achieve higher bleu than previous work at similar levels of acc , but untransferred sentences achieve the highest bleu than previous work at similar levels of bleu . our best models achieve higher bleu than previous work at similar levels of bleu .
< extra_id_0 > reparandum length [ bold ] 3 - 5 c > [ bold ] reparandum length [ bold ] 6 - 8 c > [ bold ] reparandum length [ bold ] 3 - 5 c > [ bold ] reparandum length [ bold ] 3 - 5 c > [ bold ] reparandum length [ bold ] 3 - 5 c > [ bold ] – 0 c >
< extra_id_0 > table 3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) , either in the reparandum or repair ( content - function ) or in neither .
< extra_id_0 > c > [ bold ] test mean c > [ bold ] test best c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c >
< extra_id_0 > agree c > accuracy ( % ) disagree c > accuracy ( % ) disagree c > accuracy ( % ) disagree c > accuracy ( % ) disagree c > accuracy ( % ) disagree c > accuracy ( % ) unrelated c > 81 . 72 c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ]
< extra_id_0 > the unified model significantly outperforms all previous models on the apw and nyt datasets for the document dating problem ( higher is better ) . the unified model significantly outperforms all previous models .
< extra_id_0 > table 3 shows the effectiveness of both word attention and graph attention for this task . this results show the effectiveness of both word attention and graph attention for this task .
< extra_id_0 > 1 / n c > [ bold ] 1 / 1 c > [ bold ] 1 / n c > [ bold ] 1 / 1 c > [ bold ] 1 / n c > [ bold ] all c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c >
< extra_id_0 > [ bold ] identification ( % ) c > [ bold ] trigger [ bold ] identification ( % ) c > [ bold ] trigger [ bold ] identification ( % ) c > [ bold ] trigger [ bold ] identification ( % ) c > [ bold ] trigger [ bold ] identification ( % ) c > [ bold ] trigger [ bold ] identification ( % ) c > [ bold ] classification ( % ) c > [ bold ]
< extra_id_0 > wer
< extra_id_0 > c > 50 % train dev c > 50 % train dev c > 50 % train test c > 75 % train dev c > 75 % train test c > 75 % train dev c > 75 % train test c > cs - only c > 58 . 4 c > 58 . 9 c > [ bold ] 73 . 0 c > [ bold ] 73 . 0 c > [ bold ]
< extra_id_0 > table 5 shows the accuracy on the dev set and on the test set , according to the type of the gold sentence in the set : code - switched ( cs ) vs . monolingual ( mono ) .
< extra_id_0 > table 7 shows precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset ( * marks statistically significant improvement ) .
< extra_id_0 > table 5 shows precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features on the conll - 2003 dataset ( * marks statistically significant improvement ) .
< extra_id_0 > hpcd ( full ) is from the original paper , and it uses syntactic skipgram . glove - retro is a glove vector retrofitted by faruqui et al . ( 2015 ) to wordnet 3 . 1 , and glove - extended refers to the glove embeddings obtained by autoextension rothe and schütze ( 2015 ) .
< extra_id_0 > results from rbg with features coming from various pp attachment predictors and oracle attachment predictors . rbg + hpcd ( full ) c > 94 . 17 c > 88 . 51 .
< extra_id_0 > table 3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . the effect of removing sense priors and context sensitivity ( attention ) is shown in table 3 .
< extra_id_0 > c > en - de c > flickr16 c > flickr17 c > mscoco17 c > mscoco17 c > mscoco17 c > en - de c > en - de c > en - de c > en - de c > mscoco17 c > mscoco17 c > mscoco17 c >
< extra_id_0 > en - fr c > flickr16 c > mscoco17 c > en - fr c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > c >
< extra_id_0 > autocap 1 - 5 ( concat ) and autocap 1 - 5 ( concat ) compared to en - de and mscoco17 , respectively . adding automatic image captioning to en - fr and mscoco17 compared to en - de and mscoco17 , respectively .
< extra_id_0 > mscoco17 and en - fr c > flickr16 c > flickr17 c > mscoco17 c > mscoco17 c > mscoco17 c > en - fr c > flickr16 c > flickr17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c >
< extra_id_0 > en - fr c > flickr16 c > mscoco17 c > en - fr c > en - fr c > en - fr c > en - fr c > en - fr c > en - fr c > en - fr c > en - fr c > en - fr c > en - fr c > c >
< extra_id_0 > en - fr - trans - ff scores better than en - fr - trans - ff , whereas en - fr - trans - ff scores better than en - fr - trans - ff , whereas en - fr - trans - ff scores better than en - fr - trans - ff , whereas en - fr - trans - ff scores better than en - fr - trans - ff scores better than en - fr - trans -
< extra_id_0 > table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . the train , test and development splits show the number of parallel sentences in the train , test and development splits .
< extra_id_0 > table 2 : training vocabularies for the english , french and spanish data used for our models . the training vocabularies for the english , french and spanish data are shown in table 2 .
< extra_id_0 > table 5 summarizes the automatic evaluation scores ( bleu and ter ) for the rev systems . bleu and ter score the rev systems .
< extra_id_0 > vgs is the visually supervised model from chrupala2017representations . the row labeled vgs is the visually supervised model from chrupala2017representations . the row labeled vgs is the visually supervised model from chrupala2017representations .
< extra_id_0 > table 1 : results on synthetically spoken coco . the row labeled vgs is the visually supervised model from chrupala2017representations . the row labeled vgs is the visually supervised model from rsaimage .
< extra_id_0 > turns in a u > screenplay that u > at the edges ; it ’ s so clever you want hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate
< extra_id_0 > < extra_id_1 > > bold > rnn / bold > bold > rnn / bold > bold > rnn / bold > bold > rnn / bold > bold > rnn / bold > bold > / bold > bold > / bold > bold > / bold > bold > / bold > bold > / bold >
< extra_id_0 > the numbers indicate the changes in percentage points with respect to the original sentence . the last two rows correspond to the case where negative labels are flipped to positive and vice versa . and indicate that the score increases in positive sentiment .
< extra_id_0 > bold > sst - 2 / bold > negative bold > pubmed / bold > bold > sst - 2 / bold > bold > sst - 2 / bold > bold > sst - 2 / bold > bold > sst - 2 / bold > bold > sst - 2 / bold > bold > sst - 2 / bold > bold >
