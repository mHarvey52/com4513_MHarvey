< extra_id_0 > 2 : throughput for processing the treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ’ s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation .
< extra_id_0 > table 1 shows throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as well as the linear dataset when the batch size increases from 1 to 25 . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization .
< extra_id_0 > activation func . activation func . activation func . activation func . activation func . activation func . activation func . activation func . activation func . activation func . activation func . activation func . activation func . activation func . activation func . activation func . activation func . activation func . activation func .
< extra_id_0 > [ bold ] best f1 ( in 5 - fold ) without sdp compared to [ bold ] best f1 ( in 5 - fold ) with sdp compared to [ bold ] best f1 ( in 5 - fold ) without sdp compared to [ bold ] best f1 ( in 5 - fold ) without sdp compared to [ bold ] best f1 without sdp compared to [ bold ] best f1 ( in 5 - fold ) without sdp compared to [ bold relationship compared to [ bold relationship compared to [ bold relationship without sdp compared to [ bold relationship without sdp compared to [ bold relationship without sdp compared to [ bold relationship without sdp compared to [ bold relationship without sdp compared to [ bold relationship without sdp ) without sdp ( in 5 - fold ) without sdp ( in 5 - fold ) without sdp ( in 5 - fold ) with sdp ) without sdp ( in table 1 ) .
< extra_id_0 > 30 . 30 / bold > and 40 . 30 / bold > . compared to y - 3 , y - 3 , y - 3 , y - 3 , y - 3 , y - 3 , y - 3 , y - 3 , y - 3 , y - 3 , y - 3 , y - 3 , y - 3 , y - 3 , y - 3 , y - 3 , y - 3 , y - 3 , y - 3 , y - 3 , y - 3 , y - 3 , y - 3 ,
< extra_id_0 > , and paragraph level acc . c > paragraph level acc . c > paragraph level c - f1 c > paragraph level r - f1 c > paragraph level f1 c > paragraph level r - f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c >
< extra_id_0 > table 4 shows c - f1 ( 100 % ) in % for the two indicated systems ; essay vs . paragraph level . note that the mean performances are lower than the majority performances over the runs given in table 2 . note that the average performance is lower than the majority performance over the two indicated systems .
< extra_id_0 > bleu cider nist rouge - l rouge - l ser rouge - l rouge - l rouge - l rouge - l rouge - l rouge - l rouge - l rouge - l rouge - l rouge - l rouge - l rouge - l rouge - l ser rouge - l ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser rec rec rec rec rec rec rec rec rec rec rec rec rec rec rec rec rec rec rec rec rec rec rec rec rec rec rec rec rec rec rec rec rec rec rec rec rec rec rec rec rec rec rec rec rec rec rec rec rec rec rec rec rec rec rec
< extra_id_0 > table 1 shows the comparison of the original e2e data and our cleaned version ( number of distinct mrs , total number of textual references , ser as measured by our slot matching script , see section 3 ) . the original e2e data is compared with the cleaned version .
< extra_id_0 > bleu cider rouge - l cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider
< extra_id_0 > table 4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set : total absolute numbers of errors we found ( added , missed , wrong values , slight disfluencies , slight disfluencies ) . the results of manual error analysis are presented in table 4 .
< extra_id_0 > graphlstm ( song et al . , 2018 ) has a higher all score than snrg ( song et al . , 2018 ) and tree2seq ( song et al . , 2018 ) with a higher all score than snrg ( song et al . , 2018 ) with a higher all score than snrg ( song et al . , 2018 ) with a higher all score than snrg ( song et al . , 2018 ) has a higher all score of 24 . 4 ( damonte and cohen , 2019 ) and graphlstm ( damonte and cohen , 2019 ) . graphlstm ( damonte and cohen , 2019 ) with a higher all score of 24 . 4 .
< extra_id_0 > gcnseq ( damonte and cohen , 2019 ) achieves 24 . 5 bleu points on amr17 . gcnseq achieves 24 . 5 bleu points on amr17 . gcnseq achieves 24 . 5 bleu points on amr17 . gcnseq achieves 24 . 5 bleu points on amr17 . gcnseq achieves 24 . 5 bleu points on amr17 . gcnseq achieves 24 . 5 bleu points on amr17 .
< extra_id_0 > english - czech b c > [ bold ] english - czech b c > [ bold ] english - czech b c > [ bold ] english - czech b c > [ bold ] english - czech b c > [ bold ] english - czech b c > [ bold ] english - german b c > [ bold ] english - german c c > [ bold ] english - german b czech c c > [ beck et al . , 2017 ] c > [ beck et al . , 2017 ] c > [ beck et al . , 2018 ] c > [ beck et al . , 2018 ] c > [ beck et al . , 2018 ] c > [ beck et al . , 2018 ] c > [ beck et al . , 2018 ] c > [ beck et al . , 2018 ] c > [ beck et al . , 2018 ] c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > [ italic ] n c > [ italic ] m c > [ italic ] n c > [ italic ] m c > [ italic ] n c > [ italic ] n c > [ italic ] m c > [ italic ] n c > [ italic ] m c > [ italic ] n c > [ italic ] m c > [ italic n c >
< extra_id_0 > rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes a baseline denotes a baseline denotes a baseline denotes a baseline denotes a baseline denotes a baseline denoted by a baseline denoted by a baseline denoted by a baseline denoted by a baseline denoted by a baseline denoted by a baseline denoted by a baseline denoted by a baseline denoted by a baseline denoted by a baseline denoted by a baseline denoted by a baseline denoted by a baseline denoted by a baseline denoted by a baseline denoted by a baseline denoted by a baseline denoted by a baseline denoted by a baseline denoted by a gcn + rc denoted by rc denoted by rc denoted by rc denoted by rc denoted by rc denoted by rc denoted by a baseline .
< extra_id_0 > dcgcn ( 2 ) and dcgcn ( 4 ) have significantly better performance than dcgcn ( 4 ) and dcgcn ( 4 ) . dcgcn ( 4 ) and dcgcn ( 4 ) have significantly better performance than dcgcn ( 4 ) and dcgcn ( 4 ) . dcgcn ( 4 ) and dcgcn ( 4 ) have significantly better performance .
< extra_id_0 > ablation study for density of connections on the dev set of amr15 . - i dense block denotes removing dense connections in the i - th block . - i dense block denotes removing dense connections in the i - th block .
< extra_id_0 > 9 : ablation study for modules used in the graph encoder and the lstm decoder . the results of the ablation study are presented in table 9 . the ablation study for the decoder and the lstm encoder is shown in table 9 . the decoder and the lstm encoder are shown in table 9 .
< extra_id_0 > table 7 summarizes the scores for initialization strategies on probing tasks . the bshift and topconst models have the best initialization scores on probing tasks . glorot achieves the best initialization scores on probing tasks .
< extra_id_0 > the bshift c > subjnum c > tense c > topconst c > subjnum c > subjnum c > objnum c > objnum c > objnum c > topconst c > topconst c > wc c > wc c > wc c > wc c > wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc
< extra_id_0 > cmow / 784 achieves 79 . 2 and 79 . 6 respectively . cmow / 784 achieves 79 . 2 and 79 . 6 respectively . cmow / 784 achieves 79 . 2 and 79 . 6 respectively . cmow / 784 achieves 79 . 2 and 79 . 6 respectively . cmow / 784 achieves 79 . 2 and 79 . 6 respectively . cmow / 784 achieves 79 . 2 and 79 . 6 respectively . cmow / 7
< extra_id_0 > the results are presented in table 3 . the hybrid model achieves the best score on unsupervised downstream tasks . the hybrid model achieves the best score on unsupervised downstream tasks . the hybrid model achieves the best score on unsupervised downstream tasks .
< extra_id_0 > table 8 summarizes the performance of glorot vs . glorot vs . glorot vs . glorot vs . glorot vs . glorot vs . glorot vs . glorot vs . glorot vs . glorot vs . glorot vs . glorot vs . glorot vs . glorot vs . glorot vs . glorot vs . glorot vs . glorot vs . glorot vs . glorot vs . glorot vs .
< extra_id_0 > cmow - c achieves the highest score on the unsupervised downstream tasks . cmow - c achieves the highest score on the unsupervised downstream tasks .
< extra_id_0 > cmow - r has a better understanding of bshift vs . objnum vs . topconst vs . somo vs . cmow - r . cmow - r has a better understanding of bshift vs . objnum vs . topconst vs . somo vs . cmow - r . cmow - r has a better understanding of bshift vs . cmow - r has a better understanding .
< extra_id_0 > cmow - r achieves a similar performance to the subj mrpc and mpqa mrpc . cmow - r achieves a similar performance to the sick - e and sick - r in terms of performance . cmow - r achieves a similar performance to cmow - r in terms of performance . cmow - r achieves a similar performance to cmow - r in terms of performance .
< extra_id_0 > all misc and all loc and all orgs , and all miscs and miscs are presented in table 1 . mil - nd and mil - nd achieve better performance than mil - nd and mil - nd , respectively . mil - nd and mil - nd achieve better performance than mil - nd and mil - nd , respectively . mil - nd achieves better performance than mil - nd and mil - nd , respectively .
< extra_id_0 > all p c > all r c > all f1 c > in [ italic ] e + p c > in [ italic ] e + r c > in [ italic ] e + f1 , all p c > all f1 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 38 c >
< extra_id_0 > ref gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gin gin gin gin gin gin gin gin gin gin gin gin gin gin gin gin gin gin gin gin gin gin gin gin ggnn
< extra_id_0 > the model outperforms all the other models except for the meteor and the g2s - ggnn models . g2s - gin achieves a significant improvement over s2s - gin and g2s - gin , respectively , with a significant improvement over s2s - gin and g2s - ggnn .
< extra_id_0 > 3 : results on ldc2015e86 test set when models are trained with additional gigaword data . c > konstas et al . ( 2017 ) c > 200k c > 27 . 40 , c > song et al . ( 2018 ) c > 31 . 60 .
< extra_id_0 > 4 shows the results of the ablation study on the ldc2017t10 development set . bold > meteor / bold > bold > get / italic > + bilstm c > 26 . 33 c > 32 . 42 c > 59 . 6m bilstm c > 27 . 37 c > 33 . 30 bilstm c > 57 . 6m bilstm bils
< extra_id_0 > bold > graph diameter / bold > 14 - 20
< extra_id_0 > table 8 shows the fraction of elements in the output that are not present in the input ( added ) and the fraction of elements in the input that are missing in the generated sentence ( miss ) , for the test set of ldc2017t10 . the token lemmas are used in the comparison . the gold refers to the reference sentences .
< extra_id_0 > table 4 : sem and pos tagging accuracy using features extracted from the 4th nmt encoding layer , trained with different target languages on a smaller parallel corpus ( 200k sentences ) . the sem and pos tagging accuracy is shown in table 4 .
< extra_id_0 > 2 shows the pos and sem tagging accuracy with baselines and an upper bound . pos and sem tagging accuracy with baselines and an upper bound is shown in table 2 .
< extra_id_0 > , fr c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c >
< extra_id_0 > table 5 shows pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english targets .
< extra_id_0 > attacker ’ s performance on different datasets . attacker ’ s performance on different datasets is shown in table 8 . is the difference between the attacker ’ s score and the corresponding adversary ’ s accuracy on different datasets .
< extra_id_0 > task c > accuracy c > 81 . 2 c > [ empty ] c > [ italic ] gender c > 67 . 7 c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > 2 : protected attribute leakage : balanced & unbalanced data splits . in pan16 , we see a significant difference between the two datasets ( table 2 : protected attribute leakage : balanced and unbalanced data splits ) .
< extra_id_0 > 3 : performances on different datasets with an adversarial training . is the difference between the attacker score and the adversary ’ s accuracy . is the difference between the attacker score and the corresponding adversary ’ s accuracy .
< extra_id_0 > 6 : accuracies of the protected attribute with different encoders . embedding leaky and guarded significantly outperform both rnn and guarded , respectively .
< extra_id_0 > ptb + finetune c > ptb + dynamic c > wt2 + finetune c > wt2 + dynamic c > yang et al . ( 2018 ) compared yang et al . ( 2018 ) with yang et al . ( 2018 ) with yang et al . ( 2018 ) . yang et al . ( 2018 ) compared yang et al . ( 2018 ) with yang et al .
< extra_id_0 > + bert acc c > + ln acc c > base time c > + bert acc c > + bert time c > + ln + bert time c > + ln + bert time c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > amapolar err c > yahoo err c > yelppolar time c > yelppolar time c > yelppolar err c > yelppolar err c > yelppolar err c > yelppolar time c > zhang et al . ( 2015 ) compared zhang et al . ( 2015 ) compared zhang et al . ( 2015 ) compared zhang et al . ( 2015 ) c > c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time et al . ( 2015 ) et al . ( 2015 ) et al . ( 2015 ) et al . ( 2015 ) et al . ( 2015 ) et al . ( 2015 ) et al . ( 2015 ) et al . ( 2015 ) et al . ( 2015 ) et al . ( 2015 ) et al . ( 2015 ) et al . ( 2015 ) et al . ( 2015 ) et al . ( 2015 ) et al . ( 2015 ) et al . ( 2015 ) et al . ( 2015 ) et al . ( 2015 ) et al . ( 2015 ) et al . ( 2015 ) et al . ( 2015 ) et al . ( 2015 ) et al . ( 2015 ) et al . ( 2015 ) et al . ( 2015 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 )
< extra_id_0 > table 3 shows the tokenized bleu score on the wmt14 english - german translation task . the bleu score on the gnmt dataset is shown in table 3 . the bleu score on the newstest2014 dataset is shown in table 3 .
< extra_id_0 > table 4 : exact match / f1 - score on squad dataset . rnet * : results published by wang et al . ( 2017 ) . rnet * : results published by wang et al . ( 2018 ) .
< extra_id_0 > “ # params ” : the parameter number in conll - 2003 english ner task . lstm * denotes the reported result lample et al . ( 2016 ) .
< extra_id_0 > table 7 : test accuracy on snli task with base + ln setting and test perplexity on ptb task with base + ln setting and test accuracy on snli task with base + ln setting and test perplexity on snli task with base + ln setting and test accuracy on snli task with ptb setting .
< extra_id_0 > b - 2 and r - 2 , respectively , and system retrieval [ bold ] w / r - 2 and r - 2 , respectively , and system retrieval [ bold ] w / r - 2 and r - 2 , respectively . c > [ italic ] w / system retrieval [ bold ] and # sent c > [ italic ] w / system retrieval [ bold ] and # sent c > [ italic ] w / system retrieval [ bold ] c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > the highest standard deviation among all automatic systems is 1 . 0 . the highest standard deviation among all automatic systems is highlighted in bold . the best result among automatic systems is highlighted in bold . the highest standard deviation among all automatic systems is 1 . 0 . the highest standard deviation among all automatic systems is highlighted in bold . the highest standard deviation among all automatic systems is highlighted in bold .
< extra_id_0 > dsim , docsub and docsub are the main features of lang cluster and patt cluster and docsub clust and docsub clust and docsub clust and docsub clust and docsub clust and docsub clust cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster clust clust clust clust clust clust clust clust clust clust clust clust .
< extra_id_0 > dsim and docsub , respectively , and tf c > patt c > docsub c > docsub c > docsub c > docsub c > docsub c > docsub c > hclust c > europarl c > [ bold ] c > 0 . 1192 c > 0 . 0083 c > 0 . 0056 c > 0 . 0056 c > 0 . 0056 c > docsub clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust
< extra_id_0 > slqs and docsub are the only corpus that supports docsub and docsub , respectively . the p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > europar
< extra_id_0 > dsim , docsub and docsub are based on a corpus - based corpus . the europarl model has a totaldepth of 21 and avgdepth of 21 and avgdepth of 21 and avgdepth of 21 and avgdepth of 21 and avgdepth of 21 and avgdepth of 21 and avgdepth of 21 and avgdepth is 1 and 2 respectively , respectively , respectively , respectively .
< extra_id_0 > avgdepth is 9 . 9 and avgdepth is 9 . 9 and avgdepth is 9 . 8 . europarl outperforms all other corpus models except for docsub and docsub . europarl outperforms all corpus models except docsub and docsub in terms of avgdepth and avgdepth . europarl outperforms all corpus models except docsub and docsub in terms of avgdepth .
< extra_id_0 > lf is the enhanced version as we mentioned in table 1 . ndcg % comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . lf is the baseline version as we mentioned in table 1 . ndcg % comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . lf is the enhanced version as we mentioned in table 1 .
< extra_id_0 > table 2 shows the performance ( ndcg % ) of ablative studies on different models on visdial v1 . 0 validation set . using p2 indicates the most effective one ( i . e . , hidden dictionary learning ) shown in table 1 . using p2 indicates the most effective one .
< extra_id_0 > fi - en c > lv - en c > cs - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > bold > cs - en c > bold > direct assessment / bold > zh - en c > bold > direct assessment / bold > lv - en c > bertscore - f1 c > 0 . 552 c > 0 . 646 c > 0 . 646 c > 0 . 661 c > 0 . 646 c > 0 . 661 c > 0 . 661 c > cs - en c > bold > direct assessment / bold > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en
< extra_id_0 > bold > inf / bold > sfhotel bold > qual / bold > sfhotel bold > qual / bold > sfhotel bold > qual / bold > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r >
< extra_id_0 > leic ( * ) has a m1 score of 0 . 750 and m2 score of 0 . 750 . baselines based on spice and bertscore - recall have m1 score of 0 . 750 and m2 score of 0 . 750 , respectively . the results are summarized in table 1 .
< extra_id_0 > m0 has a statistically significant improvement over the previous two models . m0 has a statistically significant improvement over the previous two models . m0 has a statistically significant improvement over the previous two models . m0 has a significant improvement over the previous two models , with a significant improvement over the previous two models .
< extra_id_0 > transfer quality a > b transfer quality tie transfer quality a > b transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie transfer quality tie
< extra_id_0 > acc and pp are validated in table 5 . acc and pp are validated using spearman ’ s [ italic ] and spearman ’ s [ italic ] datasets ; b / w negative pp and human ratings of fluency are validated using spearman ’ s [ italic ] datasets . gm is validated using spearman ’ s [ italic ] and spearman ’ s [ italic ] datasets ; see text for validation of gm .
< extra_id_0 > m6 has a better performance than m0 and m5 has a better performance . m0 has a better performance than m0 and m5 has a better performance . m0 has a better performance than m0 and m5 has a better performance . m0 has a better performance than m0 and m5 has a better performance .
< extra_id_0 > bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . our best models achieve higher bleu than previous work at similar levels of acc , but untransferred sentences achieve the highest bleu than prior work at similar levels of bleu . our best models achieve higher bleu than previous work , but untransferred sentences achieve the highest bleu than prior work at similar levels .
< extra_id_0 > table 2 shows the percentage of reparandum tokens that were correctly predicted as disfluent . rephrase and restart tokens have a similar distribution of reparandum tokens . rephrase and restart tokens have a similar distribution . rephrase and restart tokens have a similar distribution . rephrase and restart tokens have a similar distribution .
< extra_id_0 > table 3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both reparandum and repair ( content - content ) or in neither reparandum or repair ( content - function ) or in neither of these categories .
< extra_id_0 > , [ bold ] dev mean , [ bold ] test mean , [ bold ] dev best c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > c > c > c > c > early c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > text + innovations c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c
< extra_id_0 > accuracy ( % ) agree c > accuracy ( % ) disagree c > accuracy ( % ) unrelated c > embedding compared with the state - of - art algorithms on the fnc - 1 test dataset . our model achieves a better performance than the state - of - art ones .
< extra_id_0 > table 2 shows the accuracy ( % ) of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the unified model significantly outperforms all previous models for the document dating problem .
< extra_id_0 > table 3 shows the accuracy ( % ) comparisons of component models with and without attention . the results show the effectiveness of both word attention and graph attention for this task . please see section 6 . 2 for more details .
< extra_id_0 > embedding + t has 1 / 1 and embedding + t has 1 / 1 and embedding + t has a 1 / n and embedding + t has a 1 / n and embedding + t has a 1 / n and embedding + t has a 1 / n and embedding + t has a 1 / n and embedding + t has a 1 / n and embedding + t has a better performance than all other models . embedding + t has a better performance on both datasets .
< extra_id_0 > identification ( % ) c > [ bold ] trigger [ bold ] classification ( % ) c > [ bold ] trigger [ bold ] classification ( % ) c > [ bold ] trigger [ bold ] identification ( % ) c > [ bold ] trigger [ bold ] identification ( % ) c > [ bold ] classification ( % ) c > [ bold ] classification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( % )
< extra_id_0 > wer
< extra_id_0 > table 4 shows the results on the dev set and on the test set using discriminative training with only subsets of the code - switched data . the results on the dev set and on the test set using discriminative training with only subsets of the code - switched data .
< extra_id_0 > table 5 shows the accuracy of the gold sentence on the dev set and on the test set , according to the type of the gold sentence in the set : code - switched ( cs ) vs . monolingual ( mono ) .
< extra_id_0 > 7 : precision ( p ) , recall ( r ) and f1 - score ( f1 ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset ( * marks statistically significant improvement ) . type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset ( * marks statistically significant improvement ) .
< extra_id_0 > table 5 summarizes the performance of type - aggregated gaze features on the conll - 2003 dataset ( p ) , recall ( r ) and f1 - score ( f1 ) for using type - aggregated gaze features on the conll - 2003 dataset ( * marks statistically significant improvement ) . type - aggregated gaze features achieve better recall , recall and f1 - score ( f1 ) for using type - aggregated gaze features .
< extra_id_0 > table 1 summarizes the results on belinkov2014exploring ’ s ppa test set . syntactic - sg and lstm - pp have the best initialization and embedding accuracy . lstm - pp and glove - extended have the best results .
< extra_id_0 > table 2 summarizes results from rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . rbg + hpcd ( full ) achieves 94 . 17 and 88 . 51 points respectively .
< extra_id_0 > table 3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . the effect of removing sense priors and context sensitivity ( attention ) is shown in table 3 .
< extra_id_0 > add subtitle data and domain tuning for image caption translation ( bleu % scores ) . all results with marian amun are shown in table 2 . add subtitle data and domain tuning for multi30k and mscoco17 .
< extra_id_0 > and mscoco17 , respectively . subs1m and lm + ms - coco outperform domain - tuned and lm + ms - coco . lm + ms - coco outperforms lm + ms - coco and lm + ms - coco in terms of labeling performance .
< extra_id_0 > autocap 1 - 5 ( concat ) and autocap 1 - 5 ( concat ) outperforms autocap 1 - 5 ( concat ) and autocap 1 - 5 ( concat ) in terms of automatic image captioning . adding automatic image captioning improves the performance of en - de flickr16 , mscoco17 , en - de flickr16 , mscoco17 , mscoco17 , en - de multi30k and multi30k .
< extra_id_0 > and mscoco17 , respectively . the results of enc - gate , enc - gate and dec - gate are summarized in table 1 . the results of enc - gate and dec - gate are comparable to those of enc - gate and enc - gate . however , enc - gate and dec - gate have significantly lower performance than enc - gate and enc - gate .
< extra_id_0 > mscoco17 and en - fr c > flickr16 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > subs3m [ italic ] [ italic ] c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > s3m s3m s3m s3m s3m s3m s3m s3m s3m s3m s3m s3m s3m s3m s3m s3m s3m s3m s3m s3m s3m s3m s3m s3m s3m
< extra_id_0 > we see that en - fr - trans - ff outperforms both en - fr - trans - ff and en - fr - smt - back in terms of performance . mtld outperforms both en - fr - trans - ff and en - fr - trans - back in terms of performance .
< extra_id_0 > table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . the number of parallel sentences in the train , test and development splits for the language pairs is shown in table 1 .
< extra_id_0 > table 2 : training vocabularies for the english , french and spanish data used for our models . the training vocabularies used for the english , french and spanish data are shown in table 2 .
< extra_id_0 > table 5 shows the automatic evaluation scores ( bleu and ter ) for the rev systems . the en - fr - rnn - rev and en - fr - trans - rev systems achieve the best performance .
< extra_id_0 > recall @ 10 ( % ) c > rsaimage c > 0 c > 0 . 0 c > 0 . 0 c > 0 . 0 c > 0 . 0 c > vgs c > 0 c > 0 . 0 c > 0 . 0 c > 0 . 0 c > 0 . 0 c > 0 . 0 c > 0 . 0 c > 0 . 0 c > 0 . 0 c > 0 . 0 c >
< extra_id_0 > table 1 summarizes the results on synthetically spoken coco . recall @ 10 ( % ) is significantly higher than rsaimage and mfcc , respectively . rsaimage achieves a higher recall rate than rsaimage and a higher chance rate than rsaimage .
< extra_id_0 > is so clever that you want hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate . we report further examples in the appendix . we report further examples in table 1 .
< extra_id_0 > there is a significant difference in the number of nouns in table 2 : the number of nouns in table 2 is significantly higher than the number of nouns in table 2 : the number of nouns outperforms the number of nouns in table 2 : the number of nouns outperforms the number of nouns in table 2 : the number of nouns outperforms the number of nouns in table 2 : the number of nouns outperforms the number of nouns ( table 2 : table 2 : table 2 : table 2 : table 2 : table 2 : table 2 : table 2 : table 2 : table 2 : table 2 : table 2 : table 2 : table 2 : table 2 : table 2 : table 2 : table 2 : table 2 : table 2 : table 2 : table 2 : table 2 : table 2 : table 2 : table 2 : table 2 : table 2 : table 2 : nouns ( table 2 : table 2 : table 2 : table 2 : table 2 : table 2 : table 2 : table 2 : table 2 : table 2 : table 2 : table 2 : table 2 : table 2 : table 2 : table 2 : table 2 : table 2 : table 2 : table 2 : table 2 : table 2 : table 2 : nouns ( table 2 : nouns ( additional nouns ( additional nouns ( additional nouns ( additional nouns ( additional nouns ( additional nouns ( additional nouns ( additional nouns ( additional nouns ( additional nouns ( additional nouns ( additional nouns ( additional nouns ( additional nouns ( additional nouns ( additional nouns ( additional nouns ( additional nouns ) ) . ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )
< extra_id_0 > the last two rows correspond to the case where negative labels are flipped to positive and vice versa . the numbers indicate that the score increases in positive and negative sentiment with respect to the original sentence .
< extra_id_0 > it n ’ t evaluative c > best c > it c > it c > it c > it c > it c > it c > it c > it c > it c > it c > it c > it c > it c > it c > it c > it c > it c > it c > it c > it c > it c >
