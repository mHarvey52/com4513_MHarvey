< extra_id_0 > cap > table 2 : throughput for processing the treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ’ s iterative approach , with the large movie review dataset . the recursive framework performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation .
< extra_id_0 > table 1 : throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as well as the linear dataset when the batch size increases from 1 to 25 .
< extra_id_0 > in table 2 : hyper parameter optimization results for each model with different representation . the max pooling strategy consistently performs better in all model variations . the max pooling strategy consistently performs better in all model variations . the max pooling strategy consistently performs better in all model variations .
< extra_id_0 > [ bold ] best f1 ( in 5 - fold ) without sdp c > [ bold ] best f1 ( in 5 - fold ) without sdp c > [ bold ] best f1 ( in 5 - fold ) without sdp c > [ bold ] best f1 ( in 5 - fold ) without sdp c > [ bold ] best f1 ( in 5 - fold ) without sdp c > + 19 . 90 c >
< extra_id_0 > 50 % c - f1 100 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1
< extra_id_0 > r - f1 c > paragraph level r - f1 c > paragraph level r - f1 c > paragraph level r - f1 c > paragraph level r - f1 c > paragraph level r - f1 c > paragraph level r - f1 c > paragraph level r - f1 c > paragraph level r - f1 c > paragraph level r - f1 c > paragraph level r - f1
< extra_id_0 > r > c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ]
< extra_id_0 > c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ]
< extra_id_0 > mrs , total number of textual references , ser as measured by our slot matching script , see section 3 ) . cap > table 1 : data statistics comparison for the original e2e data and our cleaned version ( number of distinct mrs , total number of textual references , ser as measured by our slot matching script , see section 3 .
< extra_id_0 > c > [ bold ] system c > [ bold ] system c > [ bold ] system c > [ bold ] system c > [ bold ] system c > [ bold ] system c > [ bold ] system c > [ bold ] system c > [ bold ] system c > [ bold ] system c > [ bold ] system c > [ bold ] system c >
< extra_id_0 > table 4 : results of manual error analysis of tgen on a sample of 100 instances from the original test set : total absolute numbers of errors we found ( added , missed , wrong values , slight disfluencies ) .
< extra_id_0 > 22 . 0 r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r >
< extra_id_0 > shows the model size in terms of parameters on amr17 . gcnseq ( damonte and cohen , 2019 ) achieves 24 . 5 bleu points . # p shows the model size in terms of parameters on amr17 . gcnseq ( damonte and cohen , 2019 ) achieves 24 . 5 bleu points . # p shows the model size in terms of parameters on amr17 . gcnseq achieves 24 . 5 bleu points . # p shows the model size in terms of parameters
< extra_id_0 > b c > [ bold ] english - czech # p c > [ bold ] english - czech # p c > [ bold ] english - czech # p c > [ bold ] english - czech # p c > [ bold ] english - czech # p c > [ bold ] english - czech # p c > [ bold ] english - czech # p c > [ b
< extra_id_0 > c > 2 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4 c > 4
< extra_id_0 > rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc
< extra_id_0 > c > # p c > # p c > # p c > # p c > # p c > # p c > # p c > # p c > # p c > # p c > # p c > # p c > # p c > # p c > 420 c > 11 . 3m c > 22 . 9m c > 23 . 9m c > [ bold
< extra_id_0 > - i dense block denotes removing the dense connections in the i - th block . - i dense block denotes removing the dense connections in the i - th block . - i dense block denotes removing the dense connections in the i - th block . - i dense block denotes removing the dense connections in the i - th block . - i dense block denotes removing the dense connections in the i -
< extra_id_0 > table 9 : ablation study for modules used in the graph encoder and the lstm decoder . table 9 : ablation study for modules used in the graph encoder and the lstm decoder . table 9 : ablation study for modules used in the graph encoder and the lstm decoder .
< extra_id_0 > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r >
< extra_id_0 > c > bshift c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst
< extra_id_0 > c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c >
< extra_id_0 > c > sts12 c > sts13 c > sts14 c > sts14 c > sts15 c > sts16 c > sts16 c > sts16 c > sts16 c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > table 8 : our paper c > [ bold ] 70 . 6 c > [ bold ] 70 . 6 c > [ bold ] 70 . 6 c > [ bold ] 70 . 6 c > [ bold ] 70 . 6 c > [ bold ] 70 . 6 c > [ bold ] 70 . 6 c > [ bold ] 70 . 6 c > [ bold ] 70 . 6 c > [ bold ] 70 . 6 c > [ bold
cap > cap > cap > cap > cap > cap > cap > cap > cap > cap > cap > cap > cap > cap > cap > cap > cap > cap > cap > cap > cap > cap > cap > cap > cap > cap > cap > cap > cap >
< extra_id_0 > c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > [ bold ] c >
< extra_id_0 > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > sick - e c > sick - e c > sick - e c > sick - e c > sick - e c > sick - e c > sick - e c > sick - e c > sick - e c > sick - e c >
< extra_id_0 > mil - nd mil - nd mil - nd mil - nd mil - nd mil - nd mil - nd mil - nd mil - nd mil - nd mil - nd mil - nd mil - nd mil - nd mil - nd mil - nd mil - nd mil - nd mil - nd mil
< extra_id_0 > all p c > 35 . 87 c > 35 . 87 c > 35 . 87 c > 35 . 87 c > 35 . 87 c > 35 . 87 c > 35 . 87 c > 35 . 87 c > 35 . 87 c > 35 . 87 c > 35 . 87 c > 35 . 87 c > 35 . 87 c > 35 . 87 c > 35 . 87 c > 35 . 87
< extra_id_0 > bold > ent / bold > bold > neu / bold > bold > neu / bold > bold > neu / bold > bold > neu / bold > bold > neu / bold > bold > neu / bold > bold > neu / bold > bold > neu / bold > bold >
< extra_id_0 > bold > bleu / bold > bold > bleu / bold > bold > bleu / bold > bold > bleu / bold > bold > bleu / bold > bold > bleu / bold > bold > bleu / bold > bold > bleu / bold > bold > bleu / bold > bold > bold >
< extra_id_0 > 3 : results on ldc2015e86 test set when models are trained with additional gigaword data . c > bold > bleu / bold > c > bold > bleu / bold > c > song et al . ( 2018 ) c > 200k c > 31 . 60 r > c > bold > bleu / bold > c > c > 200k c
< extra_id_0 > 4 : results of the ablation study on the ldc2017t10 development set . c > bold > meteor / bold > c > bold > meteor / bold > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > / bold > 7 - 13
< extra_id_0 > 8 shows the fraction of elements in the output that are not present in the input ( added ) and the fraction of elements in the input graph that are missing in the generated sentence ( miss ) for the test set of ldc2017t10 . the token lemmas are used in the comparison .
< extra_id_0 > table 4 : sem and pos tagging accuracy using features extracted from the 4th nmt encoding layer , trained with different target languages on a smaller parallel corpus ( 200k sentences ) . sem and pos tagging accuracy using features extracted from the 4th nmt encoding layer .
< extra_id_0 > 2 : mft ; unsupemb : classifier using unsupervised word embeddings ; word2tag : upper bound encoder - decoder . table 2 : pos tagging accuracy with baselines and an upper bound .
< extra_id_0 > c > fr c > zh c > fr c > zh c > zh c > fr c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c >
< extra_id_0 > pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . uni tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual encoders .
< extra_id_0 > is the difference between the attacker ’ s performance and the corresponding adversary ’ s accuracy . is the difference between the attacker ’ s performance and the corresponding adversary ’ s accuracy . is the difference between the attacker ’ s performance and the corresponding adversary ’ s accuracy . is the difference between the attacker ’ s performance and the corresponding adversary ’ s accuracy . is the difference between the attacker ’ s performance and the corresponding adversary ’ s accuracy . is the difference between the
< extra_id_0 > 1 : accuracies when training directly towards a single task . cap > table 1 : accuracies when training directly towards a single task . cap > table 1 : accuracies when training directly towards a single task .
< extra_id_0 > table 2 : protected attribute leakage : balanced & unbalanced data splits . r > c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empt
is the difference between the attacker score and the adversary ’ s accuracy . is the difference between the attacker score and the adversary ’ s accuracy . is the difference between the attacker score and the adversary ’ s accuracy . is the difference between the attacker score and the adversary ’ s accuracy .
< extra_id_0 > r > c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ]
< extra_id_0 > + finetune c > wt2 + finetune c > wt2 + finetune c > wt2 + finetune c > wt2 + finetune c > wt2 + finetune c > wt2 + finetune c > wt2 + finetune c > wt2 + dynamic c > yang et al . ( 2018 ) c >
< extra_id_0 > c > # params c > # params c > # params c > # params c > # params c > # params c > # params c > # params c > # params c > # params c > # params c > # params c > # params c > # params c >
< extra_id_0 > model c > # params c > yelppolar err c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c >
< extra_id_0 > table 3 shows the tokenized bleu score on wmt14 english - german translation task . train : time in seconds per training batch measured from 0 . 2k training steps on tesla p100 . decode : time in milliseconds used to decode one sentence on newstest2014 dataset .
< extra_id_0 > “ # params ” : the parameter number of base . “ # params ” : the parameter number of elmo . rnet * : results published by wang et al . ( 2017 ) .
< extra_id_0 > “ # params ” : the parameter number in ner task . “ # params ” : the parameter number in ner task . “ # params ” : the parameter number in ner task .
< extra_id_0 > 7 : test accuracy on snli task with base + ln setting and test perplexity on ptb task with base + ln setting . table 7 : test accuracy on snli task with base + ln setting and test perplexity on ptb task with base + ln setting .
< extra_id_0 > retrieval [ bold ] b - 2 c > [ italic ] w / system retrieval [ bold ] b - 4 c > [ italic ] w / system retrieval [ bold ] # sent c > [ italic ] w / system retrieval [ bold ] b - 2 c > [ italic ] w / system retrieval [ bold ] # word c > [ italic ] w /
< extra_id_0 > ( p0 . 0005 ) . the highest standard deviation among automatic systems is highlighted in bold , with statistical significance marked with ( approximation randomization test , p0 . 0005 ) . the highest standard deviation among automatic systems is highlighted in bold . the highest standard deviation among automatic systems is highlighted in bold .
< extra_id_0 > c > dsim c > docsub c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c >
< extra_id_0 > dsim c > hlqs c > docsub c > hlqs c > docsub c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c >
< extra_id_0 > dsim c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs
< extra_id_0 > c > 957 c > 957 c > 957 c > 957 c > 957 c > 957 c > 957 c > 836 c > 836 c > 836 c > 836 c > 836 c > 957 c > 957 c > 957 c > 957 c > 957 c > 957 c >
< extra_id_0 > 980 c > 980 c > 984 c > 849 c > europarl c > 980 c > 980 c > 980 c > 996 c > 996 c > 996 c > 996 c > 996 c > 996 c > 996 c > 996 c > 996 c > 996 c > 996 c >
< extra_id_0 > table 1 shows the performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . table 1 shows the performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . lf is the enhanced version as we mentioned in table 1 .
< extra_id_0 > performance ( ndcg % ) of ablative studies on different models on visdial v1 . 0 validation set . p2 indicates the most effective one ( i . e . , hidden dictionary learning ) shown in table 2 .
< extra_id_0 > c > cs - en c > fi - en c > lv - en c > lv - en c > lv - en c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > > zh - en c > bold > direct assessment / bold > lv - en c > bold > direct assessment / bold > ru - en c > bold > direct assessment / bold > zh - en c > bold > direct assessment / bold > lv - en c > bold > direct assessment / bold > zh - en c >
< extra_id_0 > > qual / bold > bold > inf / bold > bold > qual / bold > bold > qual / bold > bold > qual / bold > bold > qual / bold > bold > qual / bold > bold > qual / bold > bold > qual / bold > bold > qual
< extra_id_0 > m1 m2 m2 m1 m2 m2 m1 m2 m2 m1 m2 m2 m1 m2 m2 m1 m2 m2 m1 m2 m2 m1 m2 m2 m1 m2 m1 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2 m2
< extra_id_0 > shows that m1 : m0 [ italic ] + para c > 0 . 728 c > 0 . 747 c > 0 . 747 c > [ bold ] 22 . 3 c > [ empty ] 22 . 3 c > [ bold ] 12 . 8 c > [ bold ] 12 . 8 c > [ empty ] 12 . 8 c > [ bold ] 12 . 8 c > [ bold ] 12 . 8 c > [ bold ] 12 . 8
< extra_id_0 > models a c > models b c > models a c > models b c > models a c > models b c > models b c > models a c > models b c > models b c > models b c > models a c > models b c > models b c > models b c > models b c >
< extra_id_0 > table 5 : human sentence - level validation of metrics ; 100 examples for each dataset for validating acc ; 150 for sim and pp ; see text for validation of gm ; see text for validation of gm ; see text for validation of gm ; see text for validation of gm ; see text for validation of gm ; see text for validation of gm ; see text for validation of gm ; see text for validation of gm ; see text for validation of gm ; see text for validation of acc ; see text for
< extra_id_0 > shows that m5 : m0 [ italic ] + cyc + para + lang c > 0 . 818 c > 0 . 813 c > 0 . 813 c > 0 . 813 c > 0 . 813 c > 0 . 813 c > 0 . 813 c > 0 . 813 c > 0 . 813 c > 0 . 813 c > 0 . 813 c > 0 . 813 c > 0 . 813 c >
< extra_id_0 > bleu than prior work at similar levels of acc , but untransferred sentences achieve the highest bleu than prior work at similar levels of acc , but untransferred sentences achieve the highest bleu than prior work at similar levels of bleu . our best models achieve higher bleu than prior work at similar levels of acc , but untransferred sentences achieve the highest bleu than previous work at similar levels of bleu . our best models achieve higher bleu than previous work at similar
< extra_id_0 > table 2 : percent of reparandum tokens that were correctly predicted as disfluent . * statistics for repetition tokens exclude repetition tokens .
< extra_id_0 > reparandum length [ bold ] 1 - 2 c > [ bold ] reparandum length [ bold ] 3 - 5 c > [ bold ] reparandum length [ bold ] 1 - 2 c > [ bold ] reparandum length [ bold ] 3 - 5 c > [ bold ] reparandum length [ bold ] 3 - 5 c > [ bold ] rephrases correctly predicted as disflu
< extra_id_0 > c > [ bold ] test mean c > [ bold ] test best c > [ bold ] test mean c > [ bold ] test mean c > [ bold ] test best c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic
< extra_id_0 > ( % ) agree c > accuracy ( % ) disagree c > accuracy ( % ) disagree c > accuracy ( % ) disagree c > accuracy ( % ) disagree c > accuracy ( % ) disagree c > rnn - based sentence embedding c > 24 . 54 c > 05 . 06 c > 53 . 24 c > 79 . 53 c > 81 . 72 c > [ bold
< extra_id_0 > cap > table 2 : accuracy ( % ) of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the unified model significantly outperforms all previous models .
< extra_id_0 > table 3 : accuracy ( % ) comparisons of component models with and without attention . this results show the effectiveness of both word attention and graph attention for this task .
< extra_id_0 > [ bold ] 1 / 1 c > [ bold ] 1 / n c > [ bold ] 1 / 1 c > [ bold ] 1 / 1 c > [ bold ] 1 / 1 c > [ bold ] 1 / 1 c > [ bold ] 1 / 1 c > [ bold ] 1 / 1 c > [ bold ] 1 / 1 c > [ bold ] 1 / 1 c > [ bold ] 1 / 1 c > [ empt
< extra_id_0 > [ bold ] classification ( % ) c > [ bold ] trigger [ bold ] classification ( % ) c > [ bold ] trigger [ bold ] classification ( % ) c > [ bold ] trigger [ bold ] classification ( % ) c > [ bold ] trigger [ bold ] classification ( % ) c > [ bold ] trigger [ bold ] classification ( % ) c > [ bold ] classification ( % ) c >
< extra_id_0 > wer
cap > table 4 : results on the dev set and on the test set using discriminative training with only subsets of the code - switched data .
cap > table 5 : accuracy on the dev set and on the test set , according to the type of the gold sentence in the set : code - switched ( cs ) vs . monolingual ( mono ) .
< extra_id_0 > 7 : precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset ( * marks statistically significant improvement ) . table 7 : precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset .
< extra_id_0 > 5 : precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features on the conll - 2003 dataset ( * marks statistically significant improvement ) . table 5 : precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features on the conll - 2003 dataset ( * marks statistically significant improvement ) .
< extra_id_0 > 1 : results on belinkov2014exploring ’ s ppa test set . hpcd ( full ) is from the original paper , and it uses syntactic skipgram . glove - retro uses glove vectors retrofitted by faruqui et al . ( 2015 ) to wordnet 3 . 1 , and glove - extended uses glove vectors retrofitted to wordnet 3 . 1 .
< extra_id_0 > 2 : results from rbg dependency parser with features coming from various pp attachment predictors and oracle attachment predictors .
< extra_id_0 > table 3 : effect of removing sense priors and context sensitivity ( attention ) from the model . the effect of removing sense priors and context sensitivity ( attention ) from the model .
< extra_id_0 > c > en - de c > flickr16 c > flickr17 c > mscoco17 c > en - fr c > en - de c > en - de c > en - de c > en - de c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c >
< extra_id_0 > c > flickr16 c > flickr17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco
< extra_id_0 > en - fr c > flickr16 c > flickr17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > c >
< extra_id_0 > c > flickr16 c > flickr17 c > mscoco17 c > mscoco17 c > mscoco17 c > en - fr c > flickr16 c > flickr17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > en - de c > c >
< extra_id_0 > en - fr c > flickr16 c > flickr17 c > mscoco17 c > en - fr c > en - fr c > en - fr c > en - fr c > en - fr c > en - fr c > en - fr c > en - fr c > en - fr c >
< extra_id_0 > en - fr - trans - ff c > 0 . 7107 c > 0 . 8656 c > 0 . 8656 c > 0 . 8656 c > 0 . 8656 c > 0 . 8656 c > 0 . 8656 c > 0 . 8656 c > 0 . 8656 c > 0 . 8656 c > 0 . 8656 c > 0 . 8656 c > 0 . 8656 c > 0 . 8656 c >
< extra_id_0 > table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . the number of parallel sentences in the train , test and development splits for the language pairs we used is shown in table 1 .
< extra_id_0 > 2 : training vocabularies for the english , french and spanish data used for our models . r > c > src c > trg r > c > en – fr c > 113 , 132 c > 131 , 104 c > 168 , 195 cap > table 2 : training vocabularies for the english , french and spanish data used for our models .
cap > table 5 : automatic evaluation scores ( bleu and ter ) for the rev systems ( en - fr - rnn - rev and en - fr - trans - rev ) .
< extra_id_0 > 2 : results on flickr8k . the row labeled vgs is the visually supervised model from chrupala2017representations . the row labeled vgs is the visually supervised model from chrupala2017representations . the row labeled vgs is the visually supervised model from flickr8k .
< extra_id_0 > table 1 : results on synthetically spoken coco . the row labeled vgs is the visually supervised model from chrupala2017representations . the row labeled vgs is the visually supervised model from chrupala2017representations .
< extra_id_0 > she turns in a u > screenplay screenplay screenplay screenplay screenplay screenplay screenplay screenplay screenplay screenplay screenplay screenplay screenplay screenplay screenplay of u > edges edges edges curves so clever you want hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate
< extra_id_0 > 3 c > 3 c > 3 c > 3 c > 4 c > 3 c > 4 c > 3 c > 4 c > 3 c > 4 c > 3 c > 4 c > 3 c > 4 c > 4 c >
< extra_id_0 > the numbers indicate the changes in percentage points with respect to the original sentence . the last two rows correspond to the case where negative labels are flipped to positive and vice versa . and indicate that the score increases in positive and negative sentiment .
< extra_id_0 > bold > sst - 2 / bold > negative bold > pubmed / bold > bold > sst - 2 / bold > bold > sst - 2 / bold > bold > sst - 2 / bold > bold > sst - 2 / bold > bold > sst - 2 / bold > bold > sst - 2 / bold > bold >
