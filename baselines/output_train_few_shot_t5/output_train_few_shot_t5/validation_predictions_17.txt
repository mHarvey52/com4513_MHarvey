< extra_id_0 > 2 : throughput for processing the treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ’ s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation .
< extra_id_0 > table 1 : throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as well as the linear dataset when the batch size increases from 1 to 25 instances .
< extra_id_0 > hyper parameters l2 reg . c > [ bold ] hyper parameters learning rate c > [ bold ] hyper parameters learning rate c > [ bold ] hyper parameters learning rate c > [ bold ] hyper parameters learning rate c > [ bold ] hyper parameters learning rate c > [ bold ] hyper parameters learning rate c > [ bold ] hyper parameters learning rate c > [ bold ] hyper parameters learning rate c > [ bold ] hyper parameters learning rate c > [ bold [ bold ] hyper parameters learning rate c > [ bold [ bold ] hyper parameters learning rate c > [ bold [ bold ] [ bold [ bold ] [ bold ] [ bold [ bold ] [ bold ] [ bold ] [ bold ] [ bold ] [ bold [ bold ] [ bold ] [ bold [ bold ] [ bold [ bold ] [ bold [ bold ] [ bold ] [ bold [ bold ] [ bold ] [ bold [ bold ] [ bold ] [ bold [ bold ] [ bold [ bold ] [ bold ] [ bold ] [ bold [ bold ] [ bold ] [ bold [ bold ] [ bold [ bold ] [ bold [ bold ] [ bold ] [ bold [ bold ] [ bold ] [ bold ] [ bold ] [ bold ] [ bold [ bold ] [ bold ] [ bold [ bold ] [ bold ] [ bold [ bold ] [ bold ] [ bold [ bold ] [ bold [ bold ] [ bold ] [ bold ] [ bold [ bold ] [ bold ] [ bold ] [ bold ] [ bold [ bold ] [ bold [ bold ] [ bold [ bold ] [ bold [ bold ] [ bold ] [ bold ] [ bold ] [ bold [ bold ] [ bold ] [ bold [ bold ] [ bold ] [ bold [ bold ] [ bold ] [ bold [ bold ] [ bold [ bold ] [ bold ] [ bold ] [ bold [ bold ] [ bold ] [ b
< extra_id_0 > [ bold ] best f1 ( in 5 - fold ) without sdp compared to [ bold ] best f1 ( in 5 - fold ) without sdp compared to [ bold ] best f1 ( in 5 - fold ) without sdp compared to [ bold ] best f1 ( in 5 - fold ) without sdp compared to [ bold ] best f1 ( in 5 - fold ) without sdp compared to [ bold ] best f1 ( in 5 - fold ) compared to [ bold relationship ( in 5 - fold ) without sdp compared to [ bold relationship ( in 5 - fold ) without sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp sdp
< extra_id_0 > the r - f1 50 % and c - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f1 50 % r - f
< extra_id_0 > , and the paragraph level acc . , respectively , and the paragraph level acc . , respectively , and the paragraph level acc . , respectively , and the paragraph level r - f1 , respectively , and the paragraph level acc . , respectively , and the paragraph level acc . , respectively , and the paragraph level acc . , respectively , and the paragraph level acc . , respectively , and the paragraph level acc . , respectively , and the paragraph level acc . , respectively , and tables c - f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 respectively , respectively , respectively , respectively , respectively , respectively , respectively , respectively , respectively , and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 and f1 respectively , respectively , respectively , respectively , respectively , respectively , respectively , respectively , respectively , respectively , respectively , respectively , respectively , respectively , respectively , respectively , respectively , respectively , respectively , respectively , respectively , respectively , respectively , respectively , respectively , respectively , respectively , respectively , respectively , respectively , respectively , respectively , respectively , respectively .
< extra_id_0 > the mean c - f1 performance is lower than the majority performance over the two indicated systems ; essay vs . paragraph level . note that the mean performance is lower than the majority performance over the two indicated systems ; essay vs . paragraph level .
< extra_id_0 > bleu cider nist rouge - l rouge - l rouge - l rouge - l rouge - l rouge - l rouge - l rouge - l rouge - l rouge - l rouge - l rouge - l rouge - l rouge - l rouge - l ser rouge - l ser rouge - l ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser
< extra_id_0 > table 1 : data statistics comparison for the original e2e data and our cleaned version ( number of distinct mrs , total number of textual references , ser as measured by our slot matching script , see section 3 ) .
< extra_id_0 > bleu c > [ bold ] nist c > [ bold ] meteor c > [ bold ] add c > [ bold ] miss c > [ bold ] ser r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r
< extra_id_0 > table 4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set : total absolute numbers of errors we found ( added , missed , wrong values , slight disfluencies , slight disfluencies ) . the results of manual error analysis are shown in table 4 .
< extra_id_0 > graphlstm ( song et al . , 2018 ) has a b c > 22 . 0 c > whereas snrg ( song et al . , 2018 ) has a b c > 25 . 6 c > whereas graphlstm ( song et al . , 2018 ) has a b c > 25 . 6 c > whereas snrg ( song et al . , 2018 ) has a b c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r >
< extra_id_0 > gcnseq achieves 24 . 5 bleu points on amr17 . gcnseq achieves 24 . 5 bleu points on amr17 . gcnseq achieves 24 . 5 bleu points on amr17 . gcnseq achieves 24 . 5 bleu points on amr17 . gcnseq achieves 24 . 5 bleu points on amr17 . gcnseq achieves 24 . 5 bleu points on amr17 . gcnseq achieves 24 . 5 bleu points on amr17 . bleu points on amr17 .
< extra_id_0 > english - czech b c > [ bold ] english - german # p c > [ bold ] english - czech # p c > [ bold ] english - czech # p c > [ bold ] english - czech # p c > [ bold ] english - czech # p c > [ bold ] english - czech # p c > [ bold ] english - german b c > [ beck et al . , 2017 c > [ beck et al . , 2017 c > [ beck et al . , 2017 c > [ beck et al . , 2018 c > [ beck et al . , 2018 c > [ beck et al . , 2018 c > [ beck et al . , 2018 c > [ beck et al . , 2018 ] c > [ beck et al . , 2018 ] c > [ beck et al . , 2018 ] c > [ beck et al . , 2018 ] c > [ beck et al . , 2018 ] c > [ beck et al . , 2018 ] c > [ beck et al . , 2018 ] c > [ beck et al . , 2018 ] c > [ beck et al . , 2018 ] c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c
< extra_id_0 > b c > [ italic ] n c > [ italic ] m c > [ italic ] n c > [ italic ] m c > [ italic ] n c > [ italic ] n c > [ italic ] m c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic n c > [ italic n c > [ italic ] n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c
< extra_id_0 > gcn has a + rc denoted by a + rc denoted by a + rc denoted by a + rc denoted by a + rc denoted by a + rc denoted by a + rc denoted by a + rc denoted by a + rc denoted by a + rc denoted by a + rc denoted by a + rc denoted by a + rc denoted by a + rc denoted by a + rc denoted by a denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted de
< extra_id_0 > dcgcn ( 1 ) and dcgcn ( 2 ) perform better than dcgcn ( 3 ) and dcgcn ( 4 ) . dcgcn ( 4 ) and dcgcn ( 4 ) perform better than dcgcn ( 4 ) and dcgcn ( 4 ) , respectively . dcgcn ( 4 ) and dcgcn ( 4 ) perform better than dcgcn ( 4 ) and dcgcn ( 4 ) .
< extra_id_0 > table 8 shows the ablation study for density of connections on the dev set of amr15 . - i dense block denotes removing the dense connections in the i - th block . - i dense block denotes removing the dense connections in the i - th block .
< extra_id_0 > 9 : ablation study for modules used in the graph encoder and the lstm decoder . - graph attention and linear combination have a significant improvement over the baseline of the encoder and the lstm decoder .
< extra_id_0 > 7 : scores for initialization strategies on probing tasks . in glorot , initialization scores are significantly higher than in glorot . in glorot , initialization scores are significantly higher than in glorot . in glorot , initialization scores are significantly higher than in glorot .
< extra_id_0 > the bshift c > subjnum c > tense c > topconst c > subjnum c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > wc c > wc c > wc c > wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc
< extra_id_0 > hybrid cmow / 784 achieves 79 . 2 outperforms all other methods except for subj and mpqa . hybrid cmow / 784 achieves 79 . 2 outperforms all other methods except for sick - e and sick - r . hybrid cmow / 784 achieves 79 . 2 outperforms all other methods except for sick - e and sick - r . hybrid cmow / 784 achieves 79 . 2 outperforms all other methods achieves 79 . 2 outperforms all other methods except for sick - r .
< extra_id_0 > we present the scores on unsupervised downstream tasks attained by our models in table 3 . the hybrid model achieves a higher score than the hybrid model . the hybrid model achieves a higher score on unsupervised downstream tasks than the hybrid model .
< extra_id_0 > table 8 summarizes the performance of glorot vs . glorot vs . glorot vs . glorot vs . glorot vs . glorot vs . glorot vs . glorot vs . glorot vs . glorot vs . glorot vs . glorot vs . glorot vs . glorot vs . glorot vs . glorot vs . glorot vs . glorot vs . glorot vs . glorot vs . glorot vs .
< extra_id_0 > cmow - c achieves the best performance on the unsupervised downstream tasks . cmow - c achieves the best performance on both the unsupervised and unsupervised tasks ( table 6 ) .
< extra_id_0 > cmow - c performs better than cmow - c in terms of depth c > tense c > objnum c > objnum c > topconst c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > cmow - c c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > cmow - c c > cmow - c c > cmow - c c > cmow - c c > cmow - c c > cmow - c c > cmow - c c > cmow - c c > cmow - c c > cmow - c c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > cmow - r achieves a similar performance to mrpc and mpqa . cmow - r achieves a similar performance to mrpc and mrpc . cmow - r achieves a similar performance to mrpc and mrpc . cmow - r achieves a similar performance to cbow - c in terms of performance .
< extra_id_0 > mil - nd has a better performance than mil - nd and mil - nd , respectively . mil - nd has a better performance than mil - nd and mil - nd , respectively . mil - nd has a better performance than mil - nd and mil - nd , respectively . mil - nd has a better performance than mil - nd and mil - nd , respectively . mil - nd has a better performance than mil - nd and mil - nd . mil - nd has a better performance .
< extra_id_0 > all p c > all r c > all f1 c > in [ italic ] e + p c > in [ italic ] e + r c > in [ italic ] e + f1 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 37 . 42 c > 37 . 38 c >
< extra_id_0 > ref gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gin gin gin gin gin gin gin gin gin gin gin gin gin gin gin gin gin gin gin gin gin gin gin gin gin gin
< extra_id_0 > bold > bleu / bold > bold > meteor / bold > bold > bleu / bold > bold > g2s - gin c > 22 . 55 0 . 17 0 . 16 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - g2s - g2s - g2s - g2s - g2s - g2s - g2s - g2s - g2s - g2s - g2s - g2s - g2s - g2s - g2s - g2s - g2s - g2s - g2s - g2s - g2s - g2s - g2s - g2s - g2s - g2s - g2s - g2s - g2s - g2s - g2s - g2s - g2s - g2s - g2s - g2s - g2s - g2s - g2s - g2s - g2s - g2
< extra_id_0 > 3 : results on ldc2015e86 test set when models are trained with additional gigaword data . c > konstas et al . ( 2017 ) c > 27 . 40 on 200k , whereas song et al . ( 2018 ) c > 31 . 60 on 200k . c > guo et al . ( 2018 ) c > 31 . 60 on 200k .
< extra_id_0 > 4 shows the results of the ablation study on the ldc2017t10 development set . bold > meteor / bold > bold > bleu / bold > c > 57 . 6m c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > model is 0 - 7 and bold > graph diameter / bold > 14 - 20
< extra_id_0 > table 8 shows the fraction of elements in the output that are not present in the input ( added ) and the fraction of elements in the input that are missing in the generated sentence ( miss ) , for the test set of ldc2017t10 . the token lemmas are used in the comparison . the gold refers to the reference sentences .
< extra_id_0 > table 4 : sem and pos tagging accuracy using features extracted from the 4th nmt encoding layer , trained with different target languages on a smaller parallel corpus ( 200k sentences ) . the sem and pos tagging accuracy is significantly higher than the pos tagging accuracy .
< extra_id_0 > 2 shows the pos and sem tagging accuracy with baselines and an upper bound . pos and sem tagging accuracy with baselines and an upper bound are shown in table 2 .
< extra_id_0 > , 87 . 9 and 87 . 8 respectively . pos tagging accuracy compared to pos tagging and sem tagging accuracy . pos tagging accuracy compared to pos tagging and sem tagging accuracy compared to pos tagging and sem tagging accuracy .
< extra_id_0 > table 5 shows pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders are shown in table 5 .
< extra_id_0 > attacker ’ s performance on different datasets is shown in table 8 . is the difference between the attacker ’ s score and the corresponding adversary ’ s accuracy on different datasets . is the difference between the attacker ’ s score and the corresponding adversary ’ s accuracy .
< extra_id_0 > table 1 summarizes the results for each task in table 1 . for each task , the accuracy is significantly higher than for the pan16 dataset . for each task , the accuracy is significantly higher than for the pan16 dataset . for each task , the accuracy is significantly higher than for the pan16 dataset . for each task , the accuracy is significantly higher than for the pan16 dataset .
< extra_id_0 > 2 : protected attribute leakage : balanced & unbalanced data splits . pan16 shows the highest percentage of protected attribute leakage among all datasets , with the highest percentage of protected attribute leakage among all datasets ( table 2 ) .
< extra_id_0 > 3 : performances on different datasets with an adversarial training . is the difference between the attacker score and the adversary ’ s accuracy . is the difference between the attacker score and the corresponding adversary ’ s accuracy .
< extra_id_0 > 6 : accuracies of the protected attribute with different encoders . leaky achieves 64 . 5 vs . guarded and 54 . 8 vs . rnn , respectively .
< extra_id_0 > ptb + finetune c > ptb + dynamic c > wt2 + finetune c > wt2 + dynamic c > yang et al . ( 2018 ) reported that yang et al . ( 2018 ) reported that yang et al . ( 2018 ) reported that yang et al . ( 2018 ) reported that yang et al . ( 2018 ) reported that yang et al . ( 2018 ) reported that yang et al . ( 2018 ) reported that yang et al . ( 2018 ) et al . ( 2018 ) et al . ( 2018 ) et al . ( 2018 ) et al . ( 2018 ) reported that yang et al . ( 2018 ) reported that yang et al . ( 2018 ) reported that yang et al . ( 2018 ) reported that yang et al . ( 2018 ) reported that yang et al . ( 2018 ) reported that yang et al . ( 2017 ) reported that yang et al . ( 2018 ) reported that yang et al . ( 2018 ) reported that yang et al . ( 2018 ) reported that yang et al . ( 2018 ) reported that yang et al . ( 2018 ) reported that yang et al . ( 2018 ) reported that yang et al . ( 2018 ) reported that yang et al . ( 2018 ) reported that yang et al . ( 2018 ) reported that yang et al . ( 2018 ) reported that yang et al . ( 2018 ) reported that yang et al . ( 2018 ) reported that yang et al . ( 2018 ) reported that yang et al . ( 2018 ) reported that yang et al . ( 2018 ) reported that yang et al . ( 2018 ) reported that yang et al . ( 2018 ) reported that yang et al . ( 2018 ) reported that yang et al . ( 2018 ) reported that yang et al . ( 2018 ) reported that yang et al . ( 2018 ) reported that yang et al . ( 2018 ) reported that yang et al . ( 2018 ) reported that yang et al . ( 2018 ) reported that yang et al . ( 2018 ) reported that yang et al . ( 2018 ) reported that yang et al . ( 2018 ) reported that yang et al . ( 2018 ) reported that yang et al . ( 2018 ) reported that yang
< extra_id_0 > model c > # params c > base acc c > base time c > + ln acc c > + bert acc c > + ln + bert time c > + ln + bert time c > + ln + bert time c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > amapolar err c > yahoo err c > yelppolar err c > yelppolar err c > yelppolar err c > yelppolar err c > yelppolar err c > yelppolar time c > zhang et al . ( 2015 ) compared to zhang et al . ( 2015 ) compared to zhang et al . ( 2015 ) c > yelppolar err c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar err c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar err c > yelppolar
< extra_id_0 > table 3 shows the tokenized bleu score on the wmt14 english - german translation task . the bleu score on the gnmt dataset is significantly higher than the bleu score on the newstest2014 dataset . the bleu score on the gnmt dataset is higher than the bleu score on the newstest2014 dataset .
< extra_id_0 > “ # params ” : the parameter number of elmo . rnet * : the results published by wang et al . ( 2017 ) . lstm achieves a score of 71 . 1 / 79 . 5 on the lstm dataset .
< extra_id_0 > table 6 shows the f1 score on conll - 2003 english ner task . “ # params ” : the parameter number in conll - 2003 english ner task .
< extra_id_0 > 7 : test accuracy on snli task with base + ln setting and test perplexity on ptb task with base + ln setting and test accuracy on snli task with base + ln setting and test perplexity on snli task with base + ln setting and test accuracy on snli task with base + ln setting and test perplexity on ptb task with base setting .
< extra_id_0 > b - 2 and r - 2 , respectively , and system retrieval [ bold ] w / r - 2 and r - 2 , respectively , and system retrieval [ bold ] w / r - 2 and r - 2 , respectively , and system retrieval [ bold ] w / r - 2 and r - 2 , respectively , and system retrieval [ bold ] w / r - 2 and r - 2 , respectively , and system retrieval [ bold ] w / r - 2 and r - 2 , respectively , and r - 2 , respectively , respectively , respectively , respectively , respectively , respectively , respectively , respectively , respectively , respectively , respectively , respectively , et al . , respectively , et al . , et al . , et al . , et al . , et al . , et al . , et al . , et al . , et al . , et al . , et al . , et al . , et al . , et al . , et al . , et al . , et al . , et al . , et al . , et al . , et al . , et al . , et al . , et al . , et al . , et al . , et al . , et al . , et al . , et al . , et al . , et al . , et al . , et al . , et al . , et al . , et al . , et al . , et al . , et al . , et al . , et al . , et al . , et al . , et al . , et al . , et al . , et al . , et al . , et al . , et al . , et al . , et al . , et al . , et al . , et al . , et al . , et al . , et al . , et al . ,
< extra_id_0 > the best result among automatic systems is highlighted in bold . the highest standard deviation among automatic systems is 1 . 0 , with statistical significance marked with ( approximation randomization test , p0 . 0005 ) . the highest standard deviation among automatic systems is highlighted in bold . the best result among automatic systems is highlighted in bold . the highest standard deviation among all automatic systems is 1 . 0 .
< extra_id_0 > slqs and docsub are similar in terms of p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c
< extra_id_0 > dsim and docsub , respectively , and dlqs and hclust , respectively . the p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > slqs and docsub , respectively . the p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c
< extra_id_0 > dsim , docsub and docsub are derived from the same corpus as the europarl corpus . the europarl corpus and the europarl corpus are derived from the same corpus as the europarl corpus and the europarl corpus . the europarl corpus and the europarl corpus are derived from the same corpus as the europarl corpus and the europarl corpus . the europarl corpus and the europarl corpus are derived from df and docsub , respectively , and hclust .
< extra_id_0 > avgdepth is 9 . 9 and avgdepth is 9 . 8 . europarl has avgdepth of 9 . 8 . europarl has avgdepth of 9 . 8 . europarl has avgdepth of 9 . 8 . europarl has avgdepth of 9 . 8 . europarl has avgdepth of 9 . 8 . europarl has avgdepth of 9 . 8 . europar
< extra_id_0 > qt , s and d denote question type , answer score sampling , and hidden dictionary learning , respectively . lf is the enhanced version as we mentioned in table 1 . ndcg compares the performance ( ndcg % ) for the experiments of applying our principles on the validation set of visdial v1 . 0 on the validation set of visdial v1 . 0 .
< extra_id_0 > table 2 shows the performance ( ndcg % ) of ablative studies on different models on visdial v1 . 0 validation set . p2 indicates the most effective one ( i . e . , hidden dictionary learning ) shown in table 1 .
< extra_id_0 > fi - en c > lv - en c > cs - en c > lv - en c > lv - en c > lv - en c > lv - en c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > bold > cs - en c > bold > direct assessment / bold > zh - en c > bold > direct assessment / bold > lv - en c > bertscore - f1 c > 0 . 672 c > 0 . 661 c > 0 . 646 c > 0 . 661 c > 0 . 646 c > 0 . 661 c > 0 . 646 c > 0 . 661 c > c > bold > lv - en cs - en c > / bold > lv - en c > c > c > c > bertscore - f1 c > 0 . 661 c > 0 . 661 c > 0 . 661 c > 0 . 661 c > 0 . 661 c > 0 . 661 c > 0 . 661 c > 0 . 661 c > 0 . 661 c > 0 . 661 c > 0 . 661 c > 0 . 661 c > 0 . 661 c > 0 . 661 c > 0 . 661 c > 0 . 661 c > 0 . 661 c > 0 . 661 c > 0 . 661 c > 0 . 661 c > 0 . 661 c > 0 . 661 c > 0 . 661 c > 0 . 661 c > 0 . 661 c > 0 . 661 c > 0 . 661 c > 0 . 661 c > 0 . 661 c > 0 . 661 c > 0 . 661 c > 0 . 661 c > 0 . 661 c > 0 . 661 c > 0 . 661 c > 0 . 661 c > 0 . 661 c > 0 . 661 c > 0 . 661 c > 0 . 661 c > 0 . 661 c > 0 . 661 c > 0 . 661 c > 0 . 661 c > 0 . 661 c > 0 . 661 c > 0 . 661 c >
< extra_id_0 > : inf . < extra_id_1 > l bold > nat / bold > sfhotel bold > qual / bold > sfhotel bold > qual / bold > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r >
< extra_id_0 > m1 and m2 respectively , and leic ( * ) and spice ( * ) have m1 and m2 scores , respectively , and m1 and m2 have m1 and m2 scores , respectively , and m1 and m2 scores have m1 and m2 scores , respectively , and m1 and m2 scores have m1 and m2 scores , respectively , and m1 and m2 scores have m1 and m2 scores , respectively , and m1 and m2 scores have m1 and m2 scores , respectively , respectively , respectively .
< extra_id_0 > sim has 22 . 3 outperforms all other models except for sim and gm . the results show that m0 and m0 have significantly higher performance than m0 and m0 , respectively . m0 and m0 have significantly lower performance than m0 and m0 , respectively . m0 and m0 have a higher performance than m0 and m0 , respectively .
< extra_id_0 > transfer quality is 9 . 0 and m2 is 9 . 6 . transfer quality is 9 . 6 and m2 is 9 . 6 . transfer quality is 9 . 6 and m2 is 9 . 6 . semantic preservation is 9 . 6 . semantic preservation tie is 9 . 3 and m2 is 9 . 8 . semantic preservation tie is 9 . 3 and m2 is 9 . 3 respectively . semantic preservation tie is 9 . 3 and m2 is 9 . 3 respectively . semantic preservation tie is 9 . 3 and m2 is 9 . 3 respectively . semantic preservation tie is 9 . 3 respectively . semantic preservation tie is 9 . 3 respectively . semantic preservation tie is 9 . 3 respectively . semantic preservation tie is 9 . 3 respectively . semantic preservation tie is 9 . 3 respectively .
< extra_id_0 > acc and pp are validated in table 5 . acc and pp are validated using spearman ’ s [ italic ] and spearman ’ s [ italic ] datasets ; b / w negative pp and human ratings of fluency are validated using spearman ’ s [ italic ] and spearman ’ s [ italic ] datasets ; see text for validation of gm and gm ; see text for validation of gm .
< extra_id_0 > m0 has a better performance than shen - 1 and cyc + para compared to 2d and cyc + para compared to 2d and cyc + para compared to 2d and cyc + para compared to 2d and cyc + para compared to 2d and cyc + para compared to 2d and cyc + para compared to 2d and cyc + para compared to 2d and 3d , respectively , respectively , respectively , respectively , respectively .
< extra_id_0 > bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . our best models achieve higher bleu than previous work at similar levels of acc , but untransferred sentences achieve the highest bleu than prior work at similar levels of bleu . the results on yelp sentiment transfer are shown in table 6 . our best models achieve higher bleu than previous work , but untransferred sentences achieve the highest bleu than previous work at similar levels of acc . our best models achieve higher bleu .
< extra_id_0 > table 2 shows the percentage of reparandum tokens that were correctly predicted as disfluent . rephrase , rephrase , rephrase , rephrase , rephrase and restart tokens show the highest percentage of reparandum tokens that were correctly predicted as disfluent . rephrase , rephrase and restart tokens show the highest percentage of reparandum tokens that were correctly predicted as disfluent .
< extra_id_0 > table 3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both reparandum and repair ( content - content ) or in neither reparandum or repair ( content - function ) or in neither of these categories .
< extra_id_0 > [ bold ] dev mean c > [ bold ] test mean c > [ italic ] test mean c > [ italic ] test mean c > [ italic ] test mean c > [ italic ] test mean c > [ italic ] test mean c > [ italic ] test mean c > [ italic ] test mean c > [ italic ] test mean c > [ italic ] test mean c > [ italic ] test mean c > [ italic ] test best c > [ italic ] test best c > [ italic ] c > [ italic ] c > [ italic ] c > c > [ italic ] c > [ italic ] c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > early c > early c > early c > early c > early c > early c > early c > early c > early c > early c > early c > early c > early c > early c > early c > early c > early c > early c > early c > early c > early c > early c > early c > early c > text + raw c > innovations c > innovations c > innovations c > innovations c > innovations c > innovations c > innovations c > innovations c > innovations c > innovations c > innovation c > innovation c > innovation c > innovation c > innovation c > innovation c > innovation c > innovation c > innovation c > innovation c > innovation c > innovation c > 86 . 53 c > 86 . 53 c >
< extra_id_0 > accuracy ( % ) agree accuracy ( % ) disagree accuracy ( % ) unrelated table 2 : performance comparison with the state - of - art algorithms on the fnc - 1 test dataset . our model performs better than the state - of - art algorithms on the fnc - 1 test dataset .
< extra_id_0 > table 2 shows the accuracy ( % ) of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the unified model significantly outperforms all previous models for the document dating problem .
< extra_id_0 > table 3 shows the effectiveness of both word attention and graph attention for this task . this results show the effectiveness of both word attention and graph attention for this task .
< extra_id_0 > embedding + t has 1 / 1 and embedding + t has a 1 / 1 and embedding + t has a 1 / 1 and embedding + t has a 1 / 1 and embedding + t has a 1 / 1 and a 1 / n stage . embedding + t has a better performance than dmcnn and embedding + t and embedding + t , respectively . embedding + t has a better performance than cnn and jmee has a better performance , respectively .
< extra_id_0 > trigger [ bold ] identification ( % ) c > [ bold ] trigger [ bold ] classification ( % ) c > [ bold ] trigger [ bold ] identification ( % ) c > [ bold ] classification ( % ) c > [ bold ] trigger [ bold ] identification ( % ) c > [ bold ] classification ( % ) c > [ bold ] classification ( % ) c > [ bold ] classification ( % ) c > [ bold ] identification ( % ) c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > acc test perp test wer test perp test wer test perp test wer test perp test wer test perp test wer test perp test wer test wer test wer test wer test wer test perp test perp test wer test wer test wer test wer test wer test wer all : cs - only - lm tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester tester test
< extra_id_0 > and the test set using discriminative training with only subsets of the code - switched data . the results on the dev set and on the test set using discriminative training with only subsets of the code - switched data are shown in table 4 .
< extra_id_0 > table 5 shows the accuracy on the dev set and on the test set , according to the type of the gold sentence in the set : code - switched ( cs ) vs . monolingual ( mono ) . on the dev set and on the test set , according to the type of the gold sentence in the set : code - switched ( lm ) vs . fine - tuned .
< extra_id_0 > 7 shows the precision ( p ) , recall ( r ) and f1 - score ( f1 ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset ( * marks statistically significant improvement ) . type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset are shown in table 7 .
< extra_id_0 > table 5 shows the p , recall ( r ) and f1 - score ( f1 ) for using type - aggregated gaze features on the conll - 2003 dataset ( * marks statistically significant improvement ) for using type - aggregated gaze features .
< extra_id_0 > table 1 summarizes the results on belinkov2014exploring ’ s ppa test set . syntactic - sg and lstm - pp have the best initialization and embedding accuracy . lstm - pp and glove - extended have the best embedding accuracy .
< extra_id_0 > table 2 summarizes results from rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . rbg + hpcd ( full ) achieves 94 . 17 and 88 . 51 points respectively , respectively .
< extra_id_0 > table 3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . the effect of removing sense priors and context sensitivity ( attention ) from the model is shown in table 3 .
< extra_id_0 > adding subtitle data and domain tuning for image caption translation ( bleu % scores ) . all results with marian amun are shown in table 2 .
< extra_id_0 > en - de and mscoco17 perform better than en - fr and mscoco17 . lm + ms - coco outperforms en - de and mscoco17 in terms of labeling . lm + ms - coco outperforms en - de and mscoco17 in terms of labeling . lm + ms - coco outperforms lm + ms - coco and lm + ms - coco in terms of labeling .
< extra_id_0 > adding automatic image captioning is shown in table 4 . adding automatic image captioning is shown in table 4 . adding automatic image captioning is shown in table 4 : adding automatic image captioning is shown in table 4 . adding automatic image captioning is shown in table 4 . adding automatic image captioning is shown in table 4 . adding automatic image captioning is shown in table 4 : adding automatic image captioning is shown in table 4 . adding automatic image captioning is shown in table 4 . adding automatic image captioning is shown in table 4 .
< extra_id_0 > mscoco17 and en - de perform better than enc - gate and dec - gate compared to enc - gate and dec - gate compared to enc - gate and enc - gate compared to flickr16 , mscoco17 and mscoco17 , respectively . enc - gate and dec - gate perform better than enc - gate and enc - gate , respectively , while enc - gate and dec - gate perform better than enc - gate perform better than enc - gate , respectively .
< extra_id_0 > mscoco17 and en - fr c > flickr16 c > mscoco17 c > mscoco17 c > en - fr c > subs3m [ italic ] [ italic ] [ italic ] lm detectron gn2048 c > 68 . 30 c > 61 . 75 c > 53 . 06 c > text - only c > c > subs3m [ italic ] [ italic ] c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c
< extra_id_0 > en - fr - trans - ff has a better performance than en - fr - trans - back and en - fr - trans - back . mtld has a better performance than en - fr - trans - back and en - fr - trans - back , respectively . mtld has a better performance than en - fr - trans - back and en - fr - trans - ff , respectively .
< extra_id_0 > table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . the number of parallel sentences in the train , test and development splits for the language pairs is shown in table 1 .
< extra_id_0 > table 2 : training vocabularies for the english , french and spanish data used for our models . the training vocabularies for the english , french and spanish data used for our models are shown in table 2 .
< extra_id_0 > table 5 shows the automatic evaluation scores ( bleu and ter ) for the rev systems . the bleu and ter scores for the rev systems are shown in table 5 .
< extra_id_0 > recall @ 10 ( % ) c > rsaimage c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > vgs c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c >
< extra_id_0 > table 1 summarizes the results on synthetically spoken coco . the vgs model has a mean recall of 10 ( % ) and a median rank of 0 . 5 ( % ) on the rsaimage model . the rsaimage model has a mean recall of 1 , 414 and a median rank of 0 . 0 ( table 1 ) .
< extra_id_0 > she turns in a u > screenplay that u > at the edges edges edges curves so clever you want hate hate hate hate hate hate . we report further examples in the appendix .
< extra_id_0 > table 2 : nouns cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn
< extra_id_0 > the last two rows correspond to the case where negative labels are flipped to positive and vice versa . the numbers indicate that the score increases in positive and negative sentiment with respect to the original sentence .
< extra_id_0 > it n ’ t elicit a positive c > it c > it c > it c > it c > it c > it c > it c > it c > it c > it c > it c > it c > it c > it c > it c > it c > it c > it c > it c > it < extra_id_1 >
