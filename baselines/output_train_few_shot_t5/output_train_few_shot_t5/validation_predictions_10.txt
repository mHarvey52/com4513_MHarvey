< extra_id_0 > table 2 : throughput for processing the treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ’ s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while fold ’ s folding technique shows better performance on training thanks to its gpu exploitation .
< extra_id_0 > table 1 : throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as well as the linear dataset when the batch size increases from 1 to 25 .
< extra_id_0 > hyper parameter optimization results for each model with different representation . the max pooling strategy consistently performs better in all model variations . the max pooling strategy consistently performs better in all model variations . the max pooling strategy consistently performs better in all models with different representation .
< extra_id_0 > [ bold ] best f1 ( in 5 - fold ) without sdp c > [ bold ] best f1 ( in 5 - fold ) without sdp c > [ bold ] best f1 ( in 5 - fold ) without sdp c > [ bold ] best f1 ( in 5 - fold ) without sdp c > [ bold ] best f1 ( in 5 - fold ) without sdp c > [ bold ] c > [ bold ] c > c > [ bold c > [ bold ] c > [ bold ] with sdp c > [ bold ] with sdp c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > r - f1 50 % compared to r - f1 100 % compared to r - f1 50 % compared to r - f1 50 % compared to r - f1 50 % compared to f1 50 % compared to y - 3 and y - 3 and y - 3 , respectively . the results show that y - 3 and y - 3 perform better than y - 3 and y - 3 , respectively .
< extra_id_0 > compared to the paragraph level c - f1 and the essay level r - f1 compared to the paragraph level f1 and r - f1 compared to the paragraph level f1 and r - f1 compared to the paragraph level f1 and r - f1 , respectively . the results show that mst - parser performs better than mst - parser in terms of performance . we observe that mst - parser performs better than mst - parser in terms of performance .
< extra_id_0 > table 4 shows the c - f1 ( 100 % ) in % for the two indicated systems ; essay vs . paragraph level . note that the mean performances are lower than the majority performances over the runs given in table 2 .
< extra_id_0 > bleu c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu c > c > [ bleu c > [ bleu c > [ bleu ] c > [ bleu ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser
< extra_id_0 > table 1 shows the comparison between the original e2e data and our cleaned version ( number of distinct mrs , total number of textual references , ser as measured by our slot matching script , see section 3 ) .
< extra_id_0 > bleu c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu c >
< extra_id_0 > table 4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set : total absolute numbers of errors we found ( added , missed , wrong values , slight disfluencies , slight disfluencies ) .
< extra_id_0 > seq2seqk has a b c > 22 . 0 , whereas graphlstm ( song et al . , 2018 ) has a b c > 24 . 4 , whereas snrg ( song et al . , 2018 ) has a b c > 25 . 6 , whereas snrg ( song et al . , 2018 ) has a b c > 25 . 6 , whereas tree2seqk ( song et al . , 2017 ) has a b c > 24 . 4 , compared to tree2
< extra_id_0 > table 2 shows the model size in terms of parameters on amr17 . gcnseq ( damonte and cohen , 2019 ) achieves 24 . 5 bleu points . table 2 shows the model size in terms of parameters on amr17 . gcnseq achieves 24 . 5 bleu points . ggnn2seq achieves 24 . 5 bleu points . ggnn2seq achieves 24 . 5 points .
< extra_id_0 > english - czech # p c > [ bold ] english - czech # p c > [ bold ] english - czech # p c > [ bold ] english - czech # p c > [ bold ] english - czech # p c > [ bold ] english - czech # p c > [ bold ] english - czech # p c > [ bold ] c > [ bold ] c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r >
< extra_id_0 > < extra_id_1 > c > [ italic ] n c > [ italic ] m c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] m c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] c
< extra_id_0 > table 6 : comparisons with baselines . gcn + rc ( 2 ) has b 16 . 8 compared to dcgcn3 ( 27 ) and dcgcn4 ( 36 ) with b 47 . 9 compared to dcgcn4 ( 36 ) and dcgcn4 ( 36 ) with b 16 . 8 compared to dcgcn4 ( 36 ) and dcgcn4 ( 36 ) with b 16 . 8 compared to dcgcn4 ( 36 ) and dcgcn4 ( 36 ) with b 16 . 8 compared to dcgcn4 ( 36 ) and dcgcn4 ( table 6 ) .
< extra_id_0 > dcgcn ( 2 ) c > 180 c > 10 . 9m c > 22 . 9m c > 22 . 9m c > 52 . 4m c > 54 . 4m c > dcgcn ( 4 ) c > 180 c > 11 . 3m c > 22 . 8m c > 23 . 9m c > 54 . 4m c > 54 . 4m c > 54 . 4m c > 54 . 4m c > 54 . 4m c >
< extra_id_0 > table 8 : ablation study for density of connections on the dev set of amr15 . - i dense block denotes removing the dense connections in the i - th block . - i dense block denotes removing the dense connections in the i - th block .
< extra_id_0 > 9 : ablation study for modules used in the graph encoder and the lstm decoder . compared to the lstm decoder , the lstm decoder performs better than the dcgcn4 model .
< extra_id_0 > table 7 : scores for initialization strategies on probing tasks . glorot c > 35 . 1 c > 70 . 8 c > 79 . 7 c > 59 . 4 c > 59 . 4 c > 59 . 4 c > 59 . 4 c > 59 . 4 c > 59 . 4 c > 59 . 4 c > 59 . 4 c > 59 . 4 c >
< extra_id_0 > method c > subjnum c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > wc c >
< extra_id_0 > mrpc and mpqa perform better than mrpc and mrpc , respectively . mrpc and mpqa perform better than mrpc and mrpc , whereas hybrid performs better than hybrid and mrpc .
< extra_id_0 > table 3 shows the scores on unsupervised downstream tasks attained by our models . we show the relative change with respect to hybrid and hybrid models .
< extra_id_0 > our paper has a lower score than glorot with a lower score than glorot with a lower score than glorot with a lower score than glorot with a lower score than glorot with a lower score than glorot with a lower score than glorot with a higher score than glorot with a higher score than glorot with a higher score than glorot with a higher score than glorot with a higher score than glorot with a higher score than glorot with a higher score than glorot with a higher score than glorot with a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a higher score of a
< extra_id_0 > cmow - c performs better than cmow - c on the unsupervised downstream tasks . cmow - c performs better than cbow - c on the unsupervised tasks .
< extra_id_0 > cmow - c performs better than bshift and topconst . cmow - c performs better than bshift and topconst . cmow - c performs better than cbow - c in terms of depth and tense .
< extra_id_0 > cmow - r has a better performance than mrpc and mrpc . cmow - r has a better performance than sick - e and sick - b . cmow - r has a better performance than mrpc and mrpc .
< extra_id_0 > all loc c > all org c > all misc c > in [ italic ] e + loc c > all org c > all misc c > all per c > all misc c > mil - nd c > 57 . 15 c > 89 . 47 c > 89 . 47 c > 89 . 47 c > 89 . 47 c > 89 . 47 c >
< extra_id_0 > all p c > all r c > all f1 c > all p c > all p c > all f1 c > all p c > all f1 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > c >
< extra_id_0 > gen ref gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent ent
< extra_id_0 > bold > bleu / bold > bold > bleu / bold > bold > bleu / bold > bold > bleu / bold > bold > bleu / bold > bold > bleu / bold > bold > g2s - ggnn c > 24 . 50 0 . 16 0 . 16 0 . 16 0 . 16 0 . 16 bold > g2s - gin
< extra_id_0 > 3 : results on ldc2015e86 test set when models are trained with additional gigaword data . c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > g2
< extra_id_0 > table 4 summarizes the results of the ablation study on the ldc2017t10 development set . using bilstm and bilstm , bilstm achieves 57 . 6m compared to bilstm .
< extra_id_0 > the model has a 0 - 7 model and a graph diameter of 14 - 20
< extra_id_0 > table 8 shows the fraction of elements in the output that are not present in the input ( added ) and the fraction of elements in the input that are missing in the generated sentence ( miss ) , for the test set of ldc2017t10 . the token lemmas are used in the comparison .
< extra_id_0 > table 4 shows the sem and pos tagging accuracy using features extracted from the 4th nmt encoding layer , trained with different target languages on a smaller parallel corpus ( 200k sentences ) . the sem and pos tagging accuracy is shown in table 4 .
< extra_id_0 > 2 shows mft and unsupemb tagging accuracy with baselines and an upper bound . unsupemb and word2tag tagging accuracy with baselines and an upper bound .
< extra_id_0 > c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c >
< extra_id_0 > table 5 shows pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional encoders .
< extra_id_0 > attacker ’ s performance on different datasets . results are on a training set 10 % held - out . is the difference between the attacker ’ s score and the corresponding adversary ’ s accuracy .
< extra_id_0 > table 1 : accuracies when training directly towards a single task . pan16 has the highest accuracies when training directly towards a single task . pan16 has the highest accuracies when training directly towards a single task .
< extra_id_0 > 2 : protected attribute leakage : balanced & unbalanced data splits . pan16 shows a significant difference between the two datasets ( table 2 ) .
< extra_id_0 > table 3 shows the performance on different datasets with an adversarial training . is the difference between the attacker score and the corresponding adversary ’ s accuracy . is the difference between the attacker score and the adversary ’ s accuracy .
< extra_id_0 > table 6 summarizes the accuracies of the protected attribute with different encoders . leaky achieves 64 . 5 % accuracies compared to rnn and guarded , respectively .
< extra_id_0 > ptb + finetune c > wt2 + dynamic c > lstm c > 22m c > 55 . 97 c > 54 . 44 c > 47 . 69 c > 40 . 68 c > 44 . 60 c > this c > yang et al . ( 2018 ) compared to yang et al . ( 2018 ) compared to yang et al . ( 2018 ) .
< extra_id_0 > model c > # params c > model c > # params c > base acc c > base time c > + ln acc c > + bert acc c > + ln + bert time c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > yelppolar err and yelppolar time , respectively . zhang et al . ( 2015 ) reported that the model performed better than the gru model in terms of err compared to yelppolar time and yelppolar time . zhang et al . ( 2015 ) reported that the model performed better than the gru model in terms of err .
< extra_id_0 > table 3 shows the tokenized bleu score on wmt14 english - german translation task . gnmt achieves a higher bleu score than olrn and olrn on the newstest2014 dataset .
< extra_id_0 > “ # params ” : the parameter number of elmo . rnet * : the results published by wang et al . ( 2017 ) . rnet * : the results of wang et al . ( 2018 ) .
< extra_id_0 > “ # params ” : the parameter number in conll - 2003 english ner task . “ # params ” : the parameter number in conll - 2003 english ner task .
< extra_id_0 > table 7 : test accuracy on snli task with base + ln setting and test perplexity on ptb task with base + ln setting and test perplexity on snli task with base + ln setting .
< extra_id_0 > r - 2 and r - 2 , respectively . < extra_id_1 > system retrieval [ bold ] r - 2 and r - 2 , respectively . c > [ italic ] w / system retrieval [ bold ] r - 2 c > [ italic ] w / system retrieval [ bold ] # sent c > [ italic ] w / system retrieval [ bold ] r - 2 c > [ italic ] w / system retrieval [ bold ] r - 2 , respectively .
< extra_id_0 > the highest standard deviation among automatic systems is highlighted in bold . the highest standard deviation among automatic systems is 1 . 0 , with statistical significance marked with ( approximation randomization test , p0 . 0005 ) . the highest standard deviation among automatic systems is highlighted in bold . the highest standard deviation among automatic systems is highlighted in bold . the highest standard deviation among automatic systems is highlighted in bold .
< extra_id_0 > tlqs , docsub and docsub perform better than tlqs and docsub , respectively . tlqs performs better than tlqs and docsub , respectively . tlqs performs better than tlqs and docsub .
< extra_id_0 > dlqs and docsub , respectively , and hclust , respectively . the results show that the p compared to the dlqs and docsub , respectively , shows that the dlqs and docsub perform better than the dlqs and docsub in terms of p compared to the dlqs and docsub in terms of p compared to the dlqs and docsub .
< extra_id_0 > tlqs and docsub are similar to dlqs and docsub , respectively . tlqs and docsub are similar to dlqs and docsub , respectively . tlqs and docsub perform better than dlqs and hclust .
< extra_id_0 > dsim and docsub perform better than hlqs and hlqs , respectively . hlqs and docsub perform better than hlqs and hlqs and hllust .
< extra_id_0 > avgdepth : 9 . 9 c > 980 c > 984 c > 849 c > 1 c > 1 c > 1 c > 1 c > 1 c > 1 c > 1 c > 1 c > 1 c > 1 c > 1 c > 1 c > 1 c > 1 c > 1 c > 1 c > 1 c > 1 c > 1 c > europar
< extra_id_0 > table 1 shows the performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . lf is the enhanced version as we mentioned in table 1 . ndcg % comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 .
< extra_id_0 > table 2 : performance ( ndcg % ) of ablative studies on different models on visdial v1 . 0 validation set . p2 indicates the most effective one ( i . e . , hidden dictionary learning ) shown in table 1 .
< extra_id_0 > table 5 : comparison on hard and soft alignments . < extra_id_1 > cs - en c > de - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > de - en and zh - en , respectively , and zh - en and zh - en , respectively , and zh - en and zh - en , respectively , and zh - en and zh - en , respectively . bertscore - f1 and ruse - f1 achieve the best performance .
< extra_id_0 > . < extra_id_1 > l bold > qual / bold > bold > qual / bold > bold > qual / bold > bold > qual / bold > bold > qual / bold > bold > qual / bold > bold > qual / bold > bold > qual / bold > bold > qual / bold > bold > bold > bold > bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold
< extra_id_0 > m1 compared to m2 compared to spice ( * ) and spice ( * ) compared to spice ( * ) and spice ( * ) compared to spice ( * ) and spice ( * ) compared to spice ( * ) and spice ( * ) compared to spice ( * ) and spice ( * ) compared to spice ( * ) and spice ( * ) compared to spice ( * ) and spice ( * ) compared to spice ( * ) compared to spice ( * ) and spice ( * ) and spice ( * ) scores .
< extra_id_0 > m0 has a better performance than m0 with a better performance than m0 with a better performance than m5 with a better performance than m5 with a better performance than m5 with a better performance than m5 with a better performance than m5 with a better performance than m5 with a better performance than m5 with a better performance than m5 with a better performance than m5 with a better performance than m5 with a better performance than m5 with a better performance than m0 with a better performance than m0 with a better performance than m0 ( table 3 ) .
< extra_id_0 > compared to m0 and m2 respectively . compared to m0 and m2 respectively , yelp and yelp perform better than yelp in terms of transfer quality . compared to yelp , yelp performs better than yelp in terms of transfer quality .
< extra_id_0 > see text for validation of gm ; see text for validation of acc ; see text for validation of pp ; see text for validation of acc ; see text for validation of gm ; see text for validation of acc ; see text for validation of acc ; see text for validation of pp ; see text for validation of acc ; see text for validation of gm .
< extra_id_0 > m0 has a better performance than m0 and m5 has a better performance than m0 and m5 has a better performance than m0 and m5 has a better performance than m0 and m5 has a better performance than m0 and m5 has a better performance than m0 and m5 has a better performance than m0 and m5 has a better performance than m0 and m5 has a better performance than m0 and m5 has a better performance than m0 and m5 has a better performance on both metrics .
< extra_id_0 > bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . our best models achieve higher bleu than previous work , but untransferred sentences achieve the highest bleu than prior work at similar levels of acc . our best models achieve higher bleu than previous work at similar levels of bleu .
< extra_id_0 > table 2 shows the percentage of reparandum tokens that were correctly predicted as disfluent . rephrased tokens have a higher rate of disfluency than rephrased tokens . rephrased tokens have a higher rate of disfluency than rephrased tokens .
< extra_id_0 > table 3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both reparandum and repair ( content - content ) , either the reparandum or repair ( content - function ) or in neither .
< extra_id_0 > c > [ bold ] dev mean c > [ bold ] dev best c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c >
< extra_id_0 > table 2 : performance comparison with the state - of - art algorithms on the fnc - 1 test dataset . our model achieves a better performance than the state - of - art algorithms .
< extra_id_0 > table 2 shows the accuracy ( % ) of different methods on the apw and nyt datasets for the document dating problem . the unified model significantly outperforms all previous models .
< extra_id_0 > table 3 . this results show the effectiveness of both word attention and graph attention for this task . this results show the effectiveness of both word attention and graph attention for this task .
< extra_id_0 > [ bold ] 1 / n c > [ bold ] 1 / 1 c > [ bold ] 1 / n c > [ bold ] all c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > c >
< extra_id_0 > c > [ bold ] trigger [ bold ] identification ( % ) c > [ bold ] trigger [ bold ] identification ( % ) c > [ bold ] classification ( % ) c > [ bold ] trigger [ bold ] identification ( % ) c > [ bold ] classification ( % ) c > [ bold ] classification ( % ) c > [ bold ] classification ( % ) c > [ bold ] classification ( % ) c > [ bold ] identification ( % )
< extra_id_0 > acc dev perp test wer
< extra_id_0 > table 4 summarizes the results on the dev set and on the test set using discriminative training with only subsets of the code - switched data . the results on the dev set and on the test set are shown in table 4 .
< extra_id_0 > table 5 : accuracy on the dev set and on the test set , according to the type of the gold sentence in the set : code - switched ( cs ) vs . monolingual ( mono ) .
< extra_id_0 > table 7 . precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset ( * marks statistically significant improvement ) .
< extra_id_0 > table 5 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features on the conll - 2003 dataset ( * marks statistically significant improvement ) .
< extra_id_0 > hpcd ( full ) is from the original paper , and it uses syntactic skipgram . the results on belinkov2014exploring ’ s ppa test set are summarized in table 1 .
< extra_id_0 > table 2 summarizes the results of rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . the results are presented in table 2 .
< extra_id_0 > table 3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . the effect of removing sense priors and context sensitivity ( attention ) from the model .
< extra_id_0 > adding subtitle data and domain tuning for image caption translation ( bleu % scores ) . all results with marian amun are shown in table 2 .
< extra_id_0 > en - de and mscoco17 perform better than en - fr and mscoco17 . compared to en - de and mscoco17 , subs1m performs better than en - fr and mscoco17 . compared to en - de , subs1m performs better than en - fr and mscoco17 .
< extra_id_0 > autocap 1 - 5 ( concat ) and autocap 1 - 5 ( concat ) have significantly better performance than en - de and flickr16 , mscoco17 and mscoco17 . adding autocap 1 - 5 ( concat ) and autocap 1 - 5 ( concat ) has significantly better performance than en - de and flickr16 .
< extra_id_0 > en - de and dec - gate perform better than enc - gate and dec - gate on flickr16 , mscoco17 and mscoco17 , respectively . enc - gate and dec - gate perform better than enc - gate and dec - gate on flickr16 , mscoco17 and mscoco17 .
< extra_id_0 > mscoco17 and en - fr perform better than en - fr and mscoco17 . compared to en - fr and mscoco17 , mscoco17 performs better than en - fr and mscoco17 . compared to en - fr , mscoco17 performs better than en - fr and mscoco17 .
< extra_id_0 > the en - fr - trans - ff scores better than the en - fr - trans - ff scores . the en - fr - trans - ff scores better than the en - fr - trans - ff scores . the en - fr - trans - ff scores better than the en - fr - trans - ff scores .
< extra_id_0 > table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . the number of parallel sentences in the train , test and development splits is shown in table 1 .
< extra_id_0 > table 2 shows the training vocabularies for the english , french and spanish data used for our models . the training vocabularies are shown in table 2 .
< extra_id_0 > table 5 shows the automatic evaluation scores ( bleu and ter ) for the rev systems . the bleu and ter scores for the rev systems are shown in table 5 .
< extra_id_0 > results on flickr8k . the row labeled vgs is the visually supervised model from chrupala2017representations . the row labeled vgs is the visually supervised model from flickr8k .
< extra_id_0 > table 1 shows the results on synthetically spoken coco . the row labeled vgs shows the results on the visually supervised model from chrupala2017representations . the row labeled vgs shows the results on the synthetically spoken coco .
< extra_id_0 > turns in a u > screenplay screenplay screenplay screenplay screenplay screenplay screenplay screenplay screenplay that ’ s so clever you want hate hate hate hate hate hate . we report further examples in the appendix .
< extra_id_0 > table 2 summarizes the rnp and dan scores . the rnp scores are significantly higher than the rnp scores . the rnp scores are significantly higher than the rnp scores .
< extra_id_0 > the numbers indicate the changes in percentage points with respect to the original sentence . the last two rows correspond to the case where negative labels are flipped to positive and vice versa . and indicate that the score increases in positive and negative sentiment .
< extra_id_0 > it n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t .
