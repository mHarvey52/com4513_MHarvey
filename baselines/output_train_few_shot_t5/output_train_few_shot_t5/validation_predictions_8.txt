< extra_id_0 > table 2 : throughput for processing the treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ’ s iterative approach , with the large movie review dataset . the recursive framework performs the best on inference with efficient parallel execution of tree nodes , while fold ’ s folding technique shows better performance on training thanks to its gpu exploitation .
< extra_id_0 > table 1 : throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as well as the linear dataset when the batch size increases from 1 to 25 .
< extra_id_0 > hyper parameter optimization results for each model with different representation in table 2 . the max pooling strategy consistently performs better in all model variations . the max pooling strategy consistently performs better in all model variations . the max pooling strategy consistently performs better in all model variations . the max pooling strategy consistently performs better in all model variations .
< extra_id_0 > [ bold ] best f1 ( in 5 - fold ) without sdp c > [ bold ] best f1 ( in 5 - fold ) without sdp c > [ bold ] best f1 ( in 5 - fold ) without sdp c > [ bold ] best f1 ( in 5 - fold ) without sdp c > [ bold ] best f1 ( in 5 - fold ) with sdp c > [ bold ] c > [ bold ] c > c > [ bold c > [ bold ] c > [ bold ] c > [ bold c > [ bold ] with sdp c > [ bold ] with sdp c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > r - f1 50 % c - f1 100 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 100 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 %
< extra_id_0 > essay level acc . c > paragraph level acc . c > paragraph level c - f1 c > paragraph level r - f1 c > paragraph level f1 c > paragraph level r - f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c >
< extra_id_0 > table 4 shows the c - f1 ( 100 % ) in % for the two indicated systems ; essay vs . paragraph level . note that the mean performances are lower than the majority performances over the runs given in table 2 .
< extra_id_0 > bleu c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ser c > [ bleu ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser
< extra_id_0 > table 1 compares the original e2e data with our cleaned version ( number of distinct mrs , total number of textual references , ser as measured by our slot matching script , see section 3 ) .
< extra_id_0 > bleu c > [ bold ] nist c > [ bold ] meteor c > [ bold ] miss c > [ bold ] miss c > [ bold ] miss c > [ bold ] miss c > [ bold ] miss c > [ bold ] miss c > [ bold ] miss c > [ bold ] miss c > [ bold ] miss c > [ bold ser c > ser c > ser c > ser c > ser c > ser c > ser c > ser c > ser c > ser c > ser c > ser c > ser c > ser c > ser c > ser c > ser c > ser c > ser c > ser c > ser c > ser c > ser c > ser c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > table 4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set : total absolute numbers of errors we found ( added , missed , wrong values , slight disfluencies ) .
< extra_id_0 > seq2seqk ( song et al . , 2017 ) has a b c > 22 . 0 c > whereas graphlstm ( song et al . , 2018 ) has a b c > 24 . 4 c > whereas tree2str ( song et al . , 2017 ) has a b c > 25 . 6 c > whereas tree2str ( song et al . , 2017 ) has a b c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c
< extra_id_0 > shows the model size in terms of parameters on amr17 . gcnseq ( damonte and cohen , 2019 ) achieves 24 . 5 bleu points on amr17 . gcnseq achieves 24 . 5 bleu points on amr17 . gcnseq achieves 24 . 5 bleu points on amr17 . gcnseq achieves 24 . 5 bleu points on amr17 . ggnn2seq achieves 24 . 5 bleu points on amr17 . ggnn2
< extra_id_0 > german # p c > [ bold ] english - czech # p c > [ bold ] english - czech # p c > [ bold ] english - czech # p c > [ bold ] english - czech # p c > [ bold ] english - czech # p c > [ bold ] english - czech # p c > [ bold ] english - czech # p c >
< extra_id_0 > [ italic ] n c > [ italic ] m c > [ italic ] n c > [ italic ] n c > [ italic ] m c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] m c > [ italic ] c > 23 . 5 c >
< extra_id_0 > rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc .
< extra_id_0 > dcgcn ( 3 ) c > 180 c > 12 . 9m c > 22 . 9m c > 22 . 9m c > 22 . 9m c > 23 . 9m c > 23 . 9m c > 23 . 9m c > 23 . 9m c > 23 . 9m c > 23 . 9m c > 23 . 9m c > 23 . 9m c > 23 . 9m c > 23 . 9m c > 23 . 9m c > 23 . 9m c > 23 . 9m c > 23 . 9m c > 23 . 9m c > 24 . 4m c > 24 . 4m c > 24 . 4m c > 24 . 4m c > 24 . 4m c > 24 . 4m c > dcgcn ( 4 ) c > dcgcn ( 4 ) c > dcgcn ( 4 ) c > dcgcn ( 4 ) c > dcgcn ( 4 ) c > dcgcn ( 4 ) c > dcgcn ( 4 ) c > dcgcn ( 4 ) c > dcgcn ( 4 ) c > dcgcn ( 4 ) c > dcgcn ( 4 ) c > dcgcn ( 4 ) c > dcgcn ( 4 ) c > dcgcn ( 4 ) c > dcgcn ( 4 ) c > dcgcn ( 4 ) c > dcgcn ( 4 ) c > 14 . 0m c > 14 . 0m c > 14 . 0m c > 14 . 0m c > 14 . 0m c > 14 . 0m c > 14 . 0m c > 14 . 0m .
< extra_id_0 > table 8 : ablation study for density of connections on the dev set of amr15 . - i dense block denotes removing the dense connections in the i - th block . - i dense block denotes removing the dense connections in the i - th block .
< extra_id_0 > 9 : ablation study for modules used in the graph encoder and the lstm decoder ( table 9 : ablation study for modules used in the graph encoder and the lstm decoder ) . - graph attention and linear combination ( table 9 : ablation study for modules used in the graph encoder and the lstm decoder ( table 9 : ablation study ) .
< extra_id_0 > table 7 : scores for initialization strategies on probing tasks . glorot c > 35 . 1 c > 70 . 8 c > wc c > 79 . 7 c > [ bold ] 79 . 7 c > [ bold ] 79 . 7 c > [ bold ] 79 . 7 c > [ bold ] 79 . 7 c > [ bold ] 79 . 7 c > wc c >
< extra_id_0 > method c > subjnum c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c >
< extra_id_0 > cmow / 784 has 79 . 2 c > 79 . 2 c > [ bold ] 79 . 2 c > [ bold ] 79 . 6 c > [ bold ] 79 . 6 c > [ bold ] 79 . 6 c > [ bold ] 79 . 6 c > [ bold ] 79 . 6 c > [ bold ] 79 . 6 c > [ bold ] 79 . 6 c > [ bold ] 79 . 6 c > [ bold ] 79 . 6 c > [ bold ] 79 . 6 c > [ bold ] 79 . 6 c > [ bold ] 79 . 8 c > [ bold ] 79 . 8 c > [ bold ] 79 . 8 c > [ bold ] 79 . 8 c > [ bold ] 79 . 8 c > [ bold ] 79 . 8 c > [ bold ] 79 . 8 c > [ bold ] 79 . 8 c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > sick - e c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c >
< extra_id_0 > c > sts14 c > sts14 c > sts15 c > sts16 c > c > c > c > c > c > c > c > cmp . c > cmp . cmp . cmp . cmp . cmp . cmp . cmp . cmp . cmp . cmp . cmp . cmp . cmp .
< extra_id_0 > our paper c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c >
< extra_id_0 > c > sts12 c > sts13 c > sts14 c > sts15 c > sts16 c > cmow - c c > [ bold ] 43 . 5 c > [ bold ] 52 . 2 c > [ bold ] 43 . 5 c > [ bold ] 52 . 2 c > [ bold ] 52 . 2 c > [ bold ] 52 . 2 c > [ bold ] 52 . 2 c > [ bold ) c > c > c > c > c > c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ]
< extra_id_0 > cmow - r performs better than cmow - r . cmow - r performs better than cmow - r . cmow - r performs better than cmow - r . cmow - r performs better than cmow - r . cmow - r performs better than cmow - r . cmow - r performs better than cmow - r . cmow - r performs better than cmow - r in terms of depth and tense .
< extra_id_0 > cmow - r has a better performance than subj and mrpc . cmow - r has a better performance than sick - e and sick - b . cmow - r has a better performance than sick - e and sick - b . cmow - r has a better performance than cmow - r . cmow - r has a better performance than sick - e and sick - b .
< extra_id_0 > all loc c > all org c > all misc c > all misc c > all loc c > all org c > all misc c > mil - nd c > 57 . 15 c > 89 . 48 c > 89 . 46 c > 89 . 46 c > 89 . 46 c > 89 . 46 c > 89 . 46 c >
< extra_id_0 > all p c > all r c > all f1 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > gen ref gen con / bold > gen con / bold > gen ref gen con / bold > gen ref gen con / bold > gen ref gen con / bold > gen ref gen ref gen gen ref
< extra_id_0 > bold > bleu / bold > bold > bleu / bold > bold > bleu / bold > bold > bleu / bold > bold > bleu / bold > bold > bleu / bold > bold > bleu / bold > bold > bleu / bold > bold > bleu / bold > bold > g2s - ggnn / bold > bold > g2s - ggnn / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold >
< extra_id_0 > 3 : results on ldc2015e86 test set when models are trained with additional gigaword data . c > bold > model / bold > bold > external / bold > bold > model / bold > bold > external / bold > bold > model / bold > bold > model / bold > bold > model / bold > bold >
< extra_id_0 > 4 shows the results of the ablation study on the ldc2017t10 development set . bilstm has a 57 . 6m r > compared to bilstm and bilstm , respectively .
< extra_id_0 >
< extra_id_0 > table 8 shows the fraction of elements in the output that are not present in the input ( added ) and the fraction of elements in the input graph that are missing in the generated sentence ( miss ) , for the test set of ldc2017t10 . the token lemmas are used in the comparison .
< extra_id_0 > table 4 : sem and pos tagging accuracy using features extracted from the 4th nmt encoding layer , trained with different target languages on a smaller parallel corpus ( 200k sentences ) . sem and pos tagging accuracy is shown in table 4 .
< extra_id_0 > table 2 shows mft and unsupemb tagging accuracy with baselines and an upper bound . pos and sem tagging accuracy with baselines and an upper bound are shown in table 2 .
< extra_id_0 > fr c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > 87 . 9 c >
< extra_id_0 > table 5 shows pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders .
< extra_id_0 > is the difference between the attacker ’ s performance and the corresponding adversary ’ s accuracy . is the difference between the attacker ’ s score and the corresponding adversary ’ s accuracy . is the difference between the attacker ’ s score and the corresponding adversary ’ s accuracy .
< extra_id_0 > c > task c > accuracy c > 67 . 4 r > c > mention c > 67 . 4 r > c > [ empty ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c >
< extra_id_0 > c > balanced leakage c > unbalanced leakage c > unbalanced leakage c > unbalanced leakage c > unbalanced leakage c > unbalanced leakage c > unbalanced leakage c > unbalanced leakage c > unbalanced leakage c > unbalanced leakage c >
< extra_id_0 > is the difference between the attacker score and the corresponding adversary ’ s accuracy . is the difference between the attacker score and the adversary ’ s accuracy . is the difference between the attacker score and the adversary ’ s accuracy .
< extra_id_0 > c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c >
< extra_id_0 > ptb + finetune wt2 + dynamic wt2 + finetune wt2 + dynamic r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > r > l
< extra_id_0 > model c > # params c > # params c > # params c > base time c > + ln acc c > + bert acc c > + ln + bert time c > + ln + bert time c > + ln + bert time c > + ln + bert time c > + ln + bert time c > c > c >
< extra_id_0 > yelppolar err c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > c >
< extra_id_0 > table 3 shows the tokenized bleu score on wmt14 english - german translation task . bleu scores on wmt14 are similar to those on newstest2014 dataset . bleu scores on newstest2014 are similar to those on newstest2014 .
< extra_id_0 > “ # params ” : the parameter number of base . rnet * : results published by wang et al . ( 2017 ) . rnet * : results published by wang et al . ( 2017 ) .
< extra_id_0 > “ # params ” : the parameter number in conll - 2003 english ner task . “ # params ” : the parameter number in ner task . “ # params ” : the parameter number .
< extra_id_0 > 7 : test accuracy on snli task with base + ln setting and test perplexity on ptb task with base + ln setting and test perplexity on snli task with base + ln setting .
< extra_id_0 > w / system retrieval [ bold ] r - 2 c > [ italic ] w / system retrieval [ bold ] # sent c > [ italic ] w / system retrieval [ bold ] r - 2 c > [ italic ] w / system retrieval [ bold ] # word c > [ italic ] w / system retrieval [ bold ] # sent c > [ italic ] c >
< extra_id_0 > the highest standard deviation among automatic systems is highlighted in bold . the highest standard deviation among automatic systems is 1 . 0 , with statistical significance marked with ( approximation randomization test , p0 . 0005 ) . the highest standard deviation among automatic systems is highlighted in bold , with statistical significance marked with ( approximation randomization test , p0 . 0005 ) . the highest standard deviation among automatic systems is highlighted in bold . the best results among automatic systems are highlighted in bold .
< extra_id_0 > tlqs and docsub tlqs perform better than dlqs and docsub tlqs . tlqs performs better than dlqs and docsub tlqs . tlqs performs better than dlqs and docsub tlqs . tlqs performs better than dlqs and tlqs .
< extra_id_0 > tlqs and docsub tlqs perform better than tlqs and docsub tlqs . tlqs performs better than tlqs and hclust . tlqs performs better than tlqs and hclust . tlqs performs better than tlqs and tlqs . tlqs performs better than tlqs and tlqs .
< extra_id_0 > tlqs and docsub tlqs perform better than dlqs and docsub tlqs . tlqs performs better than dlqs and docsub tlqs . tlqs performs better than tlqs and hclust . tlqs performs better than tlqs and tlqs . tlqs performs better than tlqs and tlqs .
< extra_id_0 > dsim and docsub perform better than hlqs and hlqs , respectively . hlqs performs better than hlqs and hlqs , while hlqs performs better than hlqs and hlqs . hlqs performs better than hlqs and hlqs .
< extra_id_0 > avgdepth : 980 984 996 996 996 996 996 996 996 996 996 996 996 996 996 996 996 996 996 996 996 996 996 996 996 996 996 996 996 996 996 996 996 996 996 996 996 996 996 996 996 996 996 .
< extra_id_0 > qt , s and d denote question type , answer score sampling , and hidden dictionary learning , respectively . lf is the enhanced version as mentioned in table 1 . ndcg % comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . ndcg % comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 .
< extra_id_0 > performance ( ndcg % ) of ablative studies on different models on visdial v1 . 0 validation set . p2 indicates the most effective one ( i . e . , hidden dictionary learning ) shown in table 2 .
< extra_id_0 > lv - en c > cs - en c > fi - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > c > c >
< extra_id_0 > lv - en and zh - en , respectively . bertscore - f1 c > 0 . 552 c > 0 . 720 c > 0 . 720 c > 0 . 720 c > 0 . 720 c > 0 . 720 c > 0 . 720 c > 0 . 720 c > 0 . 720 c > 0 . 720 c > 0 . 720 c > 0 . 720 c > 0 . 720 c > 0 . 720 c > 0 . 720 c > 0 . 720 c > 0 . 720 .
< extra_id_0 > and sfhotel bold > qual / bold > bold > qual / bold > bold > qual / bold > bold > qual / bold > bold > qual / bold > bold > qual / bold > bold > qual / bold > bold > qual / bold > bold > qual / bold > bold > bold > qual / bold > bold > qual >
< extra_id_0 > m1 m2 m1 m2 m2 m1 m2 m2 m1 m2 m2 m2 m1 m2 m2 m1 m2 m2 m2 m1 m2 m2 m1 m2 m2 m1 m2 m2 m2 m2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2 r2
< extra_id_0 > m1 : m0 has a better performance than m2 : shen - 1 with a better performance than m4 : m0 ( cyc + para + lang ) with a better performance than m5 : m0 ( cyc + para + lang ) with a better performance than m5 : m0 ( cyc + para + lang ) with a better performance than m5 : m0 ( cyc + para + lang ) and a better performance than m5 : m0 ( cyc + para + lang ) has a better performance than m0 ( cyc + para + lang ) .
< extra_id_0 > m2 and m3 are similar in terms of transfer quality and transfer quality , respectively . compared to yelp and yelp , yelp and yelp have m0 and m7 , respectively . compared to yelp and yelp , yelp and yelp have m0 and m7 respectively .
< extra_id_0 > acc , pp , spearman ’ s and spearman ’ s ( see text for validation of gm ) . acc , pp , spearman ’ s ( see text for validation of gm ) and spearman ’ s ( see text for validation of gm ) .
< extra_id_0 > m5 : m0 [ italic ] + cyc + para + lang c > 0 . 818 c > 27 . 3 c > 21 . 6 c > 21 . 6 c > 21 . 6 c > 21 . 6 c > 21 . 6 c > 21 . 6 c > 21 . 6 c > 21 . 6 c > 21 . 6 c > 21 . 6 c > 21 . 6 c > 21 . 6 c > 21 . 6 c > 21 . 6 cyc + para + lang cyc + para cyc + para cyc + para cyc + para cyc + para cyc + para cyc + para cyc + para cyc + para cyc + para cyc + para cyc + para cyc + para cyc + para cyc + para cyc + para cyc + para cyc + para cyc + para cyc + para cyc + para cyc + para cyc + para cyc + para cyc + para cyc + para cyc + para cyc + para cyc + para cyc + para cyc + para cyc + para cyc + para cyc + para cyc + para cyc + para cyc + para cyc + para cyc + para cyc + para cyc + para cyc + para cyc + para cyc + para cyc + para cyc + para cyc + para cyc + para cyc + para cyc + para cyc + para cyc + para cyc + para cyc + para cyc + para
< extra_id_0 > untransferred sentences achieve the highest bleu than prior work at similar levels of acc . our best models achieve higher bleu than previous work at similar levels of bleu , but untransferred sentences achieve the highest bleu than prior work at similar levels of acc . our best models achieve the highest bleu than previous work at similar levels of bleu , but untransferred sentences achieve the highest bleu than previous work at similar levels .
< extra_id_0 > table 2 shows the percentage of reparandum tokens that were correctly predicted as disfluent . reparandum tokens that were correctly predicted as disfluent are shown in table 2 . reparandum tokens that were correctly predicted as disfluent are shown in table 2 . reparandum tokens that were correctly predicted as disfluent are shown in table 2 . reparandum tokens that were correctly predicted as disfluent are shown in table 2 . reparandum tokens that were correctly predicted as disfluent are shown in table 2 .
< extra_id_0 > table 3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) , either the reparandum or repair ( content - function ) or in neither .
< extra_id_0 > c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c >
< extra_id_0 > accuracy ( % ) agree c > accuracy ( % ) disagree c > accuracy ( % ) disagree c > accuracy ( % ) disagree c > accuracy ( % ) disagree c > accuracy ( % ) unrelated c > 81 . 72 c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [
< extra_id_0 > the unified model significantly outperforms all previous models on the apw and nyt datasets ( higher is better ) . the unified model significantly outperforms all previous models .
< extra_id_0 > table 3 shows the effectiveness of both word attention and graph attention for this task . this results show the effectiveness of both word attention and graph attention for this task .
< extra_id_0 > [ bold ] 1 / n c > [ bold ] 1 / n c > [ bold ] 1 / n c > [ bold ] all r > c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > embedding + t c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > ( % ) c > [ bold ] trigger [ bold ] classification ( % ) c > [ bold ] trigger [ bold ] classification ( % ) c > [ bold ] trigger [ bold ] classification ( % ) c > [ bold ] trigger [ bold ] classification ( % ) c > [ bold ] classification ( % ) c > [ bold ] classification ( % ) c > [ bold ] classification ( % ) c > [ bold ] classification ( % )
< extra_id_0 > acc dev perp test wer
< extra_id_0 > and on the test set using discriminative training with only subsets of the code - switched data . the results on the dev set and on the test set using discriminative training with only subsets of the code - switched data .
cap > table 5 : accuracy on the dev set and on the test set , according to the type of the gold sentence in the set : code - switched ( cs ) vs . monolingual ( mono ) .
< extra_id_0 > table 7 shows precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset ( * marks statistically significant improvement ) .
< extra_id_0 > table 5 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features on the conll - 2003 dataset ( * marks statistically significant improvement ) .
< extra_id_0 > hpcd ( full ) is from the original paper , and it uses syntactic skipgram . the results on belinkov2014exploring ’ s ppa test set are shown in table 1 . lstm - pp and lstm - pp are shown in table 1 .
< extra_id_0 > table 2 summarizes the results from rbg dependency parser with features coming from various pp attachment predictors and oracle attachments .
< extra_id_0 > table 3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . the effect of removing sense priors and context sensitivity ( attention ) from the model is shown in table 3 .
< extra_id_0 > adding subtitle data and domain tuning for image caption translation ( bleu % scores ) . all results with marian amun . all results with marian amun .
< extra_id_0 > mscoco17 and en - fr c > flickr16 c > flickr17 c > mscoco17 c > mscoco17 c > mscoco17 c > en - fr c > flickr16 c > flickr17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > c >
< extra_id_0 > autocap 1 - 5 ( concat ) and autocap 1 - 5 ( concat ) improves the performance of en - de and flickr16 compared to mscoco17 . adding autocap 1 - 5 ( concat ) improves the performance of en - de and flickr16 . adding autocap 1 - 5 ( concat ) improves the performance of mscoco17 .
< extra_id_0 > mscoco17 and en - de c > flickr16 c > flickr17 c > mscoco17 c > flickr17 c > mscoco17 c > flickr16 c > flickr17 c > mscoco17 c > flickr16 c > flickr17 c > flickr17 c > mscoco17 c > dec - gate c > dec - gate
< extra_id_0 > mscoco17 and subs3m [ italic ] [ italic ] [ italic ] lm detectron gn2048 c > 68 . 30 c > 62 . 45 c > 53 . 04 c > 53 . 04 c > 53 . 04 c > 53 . 04 c > 53 . 04 c > 53 . 04 c > 53 . 04 c > 53 . 04 c > 53 . 04 c > en - fr
< extra_id_0 > en - fr - trans - ff scores better than en - fr - trans - ff scores better than en - fr - trans - ff scores . en - fr - trans - ff scores better than en - fr - trans - ff scores . en - fr - trans - ff scores better than en - fr - trans - ff scores . en - fr - trans - ff scores better than en - fr - trans - ff scores better than en -
< extra_id_0 > table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . the number of parallel sentences in train , test and development splits for the language pairs we used is shown in table 1 .
< extra_id_0 > r > c > src c > trg r > c > training vocabularies for the english , french and spanish data used for our models in table 2 .
< extra_id_0 > table 5 shows the automatic evaluation scores ( bleu and ter ) for the rev systems . the bleu and ter scores ( en - es - rnn - rev ) are shown in table 5 .
< extra_id_0 > results on flickr8k . the row labeled vgs is the visually supervised model from chrupala2017representations . the row labeled vgs is the visually supervised model from chrupala2017representations . the row labeled vgs is the visually supervised model from chrupala2017representations .
< extra_id_0 > results on synthetically spoken coco . the row labeled vgs is the visually supervised model from chrupala2017representations . the row labeled vgs is the visually supervised model from chrupala2017representations .
< extra_id_0 > is so clever you want hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate .
< extra_id_0 > prep . < extra_id_1 > . bold > rnn / bold > . bold > dan / bold > . bold > dan / bold > . bold > dan / bold > . bold > dan / bold > . bold > dan / bold > . bold > dan / bold > . bold > / bold > . bold >
< extra_id_0 > the numbers indicate the changes in percentage points with respect to the original sentence . the last two rows correspond to the case where negative labels are flipped to positive and vice versa . and indicate that the score increases in positive and negative sentiment .
< extra_id_0 > it n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t n ’ t .
