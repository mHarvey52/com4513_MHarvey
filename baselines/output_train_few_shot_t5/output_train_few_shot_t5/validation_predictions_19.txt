< extra_id_0 > 2 : throughput for processing the treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ’ s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while fold ’ s folding technique shows better performance on training thanks to its gpu exploitation .
< extra_id_0 > table 1 shows the throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits the highest throughput thanks to the high degree of parallelization , but at the same time does not improve as well as the linear dataset when the batch size increases from 1 to 25 instances .
< extra_id_0 > activation func . activation func . activation func . activation func . activation func . activation func . activation func . activation func . activation func . activation func . activation func . activation func . activation func . activation func . activation func . activation func . activation func . activation func . activation
< extra_id_0 > [ bold ] best f1 ( in 5 - fold ) without sdp compared to [ bold ] best f1 ( in 5 - fold ) without sdp compared to [ bold ] best f1 ( in 5 - fold ) without sdp compared to [ bold ] best f1 ( in 5 - fold ) without sdp compared to [ bold ] best f1 ( in 5 - fold ) with sdp .
< extra_id_0 > 30 . 30 c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c
< extra_id_0 > , and paragraph level r - f1 , respectively . paragraph level c - f1 and paragraph level r - f1 are presented in table 3 . mst - parser outperforms both mst - parser and mst - parser in terms of accuracies . mst - parser outperforms both [ empty ] and [ empty ] in terms of accuracies . mst - parser outperforms both [ empty ] and [ empty .
< extra_id_0 > the mean c - f1 performance is lower than the majority performance over the two indicated systems ; essay vs . paragraph level . note that the mean performance is lower than the majority performance over the two indicated systems ; essay vs . paragraph level .
< extra_id_0 > bleu c > [ bold ] nist c > [ bold ] rouge - l c > [ bold ] meteor c > [ bold ] add c > [ bold ] miss c > [ bold ] add c > [ bold ] ser c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05 c > 00 . 05
< extra_id_0 > table 1 shows the comparison of the original e2e data and our cleaned version ( number of distinct mrs , total number of textual references , ser as measured by our slot matching script , see section 3 ) . the original e2e data is compared to the cleaned version ( number of distinct mrs , total number of textual references , ser as measured by our slot matching script ) .
< extra_id_0 > bleu c > [ bold ] nist c > [ bold ] meteor c > [ bold ] add c > [ bold ] miss c > [ bold ] correct c > [ bold ] correct c > [ bold ] correct c > [ bold ] correct c > [ bold ] correct c > [ bold ] correct c > [ bold ] correct c > [ bold bleu c > [ bold ] add ser c > [ bold ] add ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser sel sel sel sel sel sel sel sel sel sel sel sel sel sel sel sel sel sel sel sel sel sel sel sel sel sel sel sel sel sel sel sel sel sel sel sel sel sel sel sel sel sel sel sel sel sel sel sel sel sel sel sel sel sel sel sel sel sel sel select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select
< extra_id_0 > table 4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set : total absolute numbers of errors we found ( added , missed , wrong values , slight disfluencies , slight disfluencies ) .
< extra_id_0 > graphlstm ( song et al . , 2018 ) achieves a bold score of 22 . 0 compared to 24 . 4 for snrg ( song et al . , 2018 ) and gcnseq ( damonte and cohen , 2019 ) . graphlstm ( song et al . , 2018 ) achieves a bold score of 24 . 4 compared to seq2seqk ( song et al . , 2017 ) and snrg ( song et al . , 2018 ) achieves 24 . 4 . graphlseq ( damonte and cohen , 2019 ) and 24 . 4 .
< extra_id_0 > gcnseq ( damonte and cohen , 2019 ) achieves 24 . 5 bleu points on amr17 . gcnseq achieves 24 . 5 bleu points on amr17 . gcnseq achieves 24 . 5 bleu points on amr17 . gcnseq achieves 24 . 5 bleu points on amr17 . gcnseq achieves 24 . 5 bleu points on amr17 . gcnseq achieves 24 . 5 bleu points on amr17 .
< extra_id_0 > english - czech # p c > [ bold ] english - czech # p c > [ bold ] english - czech # p c > [ bold ] english - czech # p c > [ bold ] english - czech # p c > [ bold ] english - czech # p c > [ bold ] english - czech # p c > [ bold ] english - czech # p c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c
< extra_id_0 > n c > [ italic ] m c > [ italic ] n c > [ italic ] m c > [ italic ] n c > [ italic ] m c > [ italic ] n c > [ italic ] m c > [ italic ] n c > [ italic ] m c > [ italic ] n c > [ italic ] m c >
< extra_id_0 > gcn has a + rc denoted by a + rc denoted by a + rc denoted by a + rc denoted by a + rc denoted by a + rc denoted by a + rc denoted by a + rc denoted by a + rc denoted by a + rc denoted by a + rc denoted by a + rc denoted by a + rc denoted by a + rc denoted by a + rc denoted by a denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted denoted de
< extra_id_0 > dcgcn ( 2 ) has 22 . 2m and dcgcn ( 4 ) has 22 . 8m . dcgcn ( 4 ) has 22 . 8m and dcgcn ( 4 ) has 22 . 8m . dcgcn ( 4 ) has 22 . 8m and dcgcn ( 4 ) has 22 . 8m . the dcgcn ( 4 ) model has 22 . 8m and dcgcn ( 4 ) has 22 . 8m . the dcgcn ( 4 ) model has 22 . 8m and dcgcn ( 4 ) has 22 . 8m . the dcgcn ( 4 ) model has 22 . 8m and dcgcn ( 4 ) has 22 . 8m .
< extra_id_0 > ablation study for density of connections on the dev set of amr15 . - i dense block denotes removing the dense connections in the i - th block . - i dense block denotes removing the dense connections in the i - th block .
< extra_id_0 > 9 : ablation study for modules used in the graph encoder and the lstm decoder . the results of the ablation study are summarized in table 9 . the decoder and the lstm encoder have significantly better performance than the encoder and the lstm decoder .
< extra_id_0 > initialization scores for bshift and topconst on probing tasks . glorot outperforms glorot in terms of depth and tense on probing tasks . glorot outperforms glorot in terms of tense and depth on probing tasks .
< extra_id_0 > bshift c > depth c > objnum c > topconst c > subjnum c > depth c > depth c > objnum c > topconst c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc >
< extra_id_0 > mrpc and mpqa achieve similar results in subj and mrpc . hybrid cmow ( 784 and cmow / 784 achieve similar results in subj and mrpc ( 784 and 784 respectively ) and mrpc ( 784 and 784 respectively ) achieve similar results in sick - e and sick - r ( 784 and 784 respectively ) and sick - r ( 784 and 784 respectively ) .
< extra_id_0 > we present the scores on unsupervised downstream tasks attained by our models in table 3 . the hybrid model achieves a score of 43 . 5 on the unsupervised downstream tasks attained by our models . the hybrid model achieves a score of 52 . 2 on the unsupervised downstream tasks .
< extra_id_0 > our paper has a significant improvement over the previous version of glorot . glorot has a significant improvement over the previous version of glorot . glorot has a significant improvement over the previous version of glorot . glorot has a significant improvement over the previous version of glorot . glorot has a significant improvement over the previous version .
< extra_id_0 > cmow - c performs better than sts12 and 33 . 2 on the unsupervised downstream tasks . cmow - c performs better than cbow - c on the unsupervised downstream tasks .
< extra_id_0 > topconst c > bshift c > tense c > objnum c > objnum c > topconst c > wc c > cmow - c c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > cmow - c c > c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > cmow - c cmow - c cmow - c cmow - c cmow - c cmow - c cmow - c cmow - c cmow - c cmow - c cmow - c cmow - c cmow - c cmow - c cmow - c cmow - c cmow - c cmow - c cmow - c wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc
< extra_id_0 > mrpc and mpqa mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc mrpc
< extra_id_0 > and 34 . 03 respectively . in [ italic ] e + org c > all misc c > all loc c > all org c > all per c > all misc c > mil - nd c > 57 . 15 c > mil - nd c > 89 . 46 c > 89 . 46 c > 89 . 46 c > 89 . 46 c > 89 . 46 c >
< extra_id_0 > all p c > all r c > all f1 c > all p c > in [ italic ] e + p c > in [ italic ] e + r c > in [ italic ] e + f1 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 38 c >
< extra_id_0 > ref gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gen gin gin gin gin gin gin gin gin gin gin gin gin gin gin gin gin gin gin gin gin gin gin gin gin gin gin gin
< extra_id_0 > bold > bleu / bold > bold > meteor / bold > ldc2015e86 c > 23 . 00 c > 30 . 10 c > damonte et al . ( 2018 ) c > 23 . 00 c > 23 . 00 c > 30 . 10 c > 30 . 10 c > damonte et al . ( 2018 ) c > 23 . 00 c > 30 . 10 c >
< extra_id_0 > 3 : results on ldc2015e86 test set when models are trained with additional gigaword data . konstas et al . ( 2017 ) achieve 27 . 40 on the 200k test set , whereas song et al . ( 2018 ) achieve 28 . 20 on the 200k test set .
< extra_id_0 > 4 shows the results of the ablation study on the ldc2017t10 development set . bold > meteor / bold > shows that bilstm achieves 57 . 6m bleu compared to bilstm and bilstm , respectively .
< extra_id_0 > bold > graph diameter / bold > 14 - 20
< extra_id_0 > table 8 shows the fraction of elements in the output that are not present in the input ( added ) and the fraction of elements in the input that are missing in the generated sentence ( miss ) , for the test set of ldc2017t10 . the token lemmas are used in the comparison . the gold refers to the reference sentences .
< extra_id_0 > table 4 : sem and pos tagging accuracy using features extracted from the 4th nmt encoding layer , trained with different target languages on a smaller parallel corpus ( 200k sentences ) . the sem and pos tagging accuracy is shown in table 4 .
< extra_id_0 > pos and sem tagging accuracy with baselines and an upper bound . pos and sem tagging accuracy with baselines and an upper bound .
< extra_id_0 > fr c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > 87 . 9 c > 87 . 9 c >
< extra_id_0 > table 5 shows pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . uni / bidirectional / residual nmt encoders have the best pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders .
< extra_id_0 > attacker ’ s performance on different datasets is shown in table 8 . is the difference between the attacker ’ s score and the corresponding adversary ’ s accuracy on different datasets . is the difference between the attacker ’ s performance on different datasets .
< extra_id_0 > table 1 summarizes the results for each task . for each task , the number of participants is significantly higher than the number of participants in pan16 . in pan16 , the number of participants is significantly higher than the number of participants in pan16 . in pan16 , the number of participants is significantly higher than in pan16 .
< extra_id_0 > 2 : protected attribute leakage : balanced & unbalanced data splits . pan16 shows a significant difference between the two datasets ( table 2 : protected attribute leakage : balanced and unbalanced data splits ) .
< extra_id_0 > is the difference between the attacker score and the adversary ’ s accuracy . is the difference between the attacker score and the corresponding adversary ’ s accuracy . is the difference between the attacker score and the adversary ’ s accuracy .
< extra_id_0 > 6 : accuracies of the protected attribute with different encoders . leaky outperforms guarded , rnn outperforms rnn and rnn in terms of accuracy .
< extra_id_0 > ptb + finetune c > ptb + dynamic c > wt2 + finetune c > yang et al . ( 2018 ) compared yang et al . ( 2018 ) with yang et al . ( 2018 ) with yang et al . ( 2018 ) with yang et al . ( 2018 ) with yang et al . ( 2018 ) comparing yang et al . ( 2018 ) with yang et al . ( 2018 ) with yang et al . ( 2018 ) with yang et al . ( 2018 ) with yang
< extra_id_0 > model c > # params c > base acc c > base time c > + ln acc c > + bert acc c > + bert time c > + ln acc c > + bert time c > + ln acc c > c > c > c > c > c > c > c > c > lstm c >
< extra_id_0 > amafull err c > yahoo err c > yelppolar err c > yelppolar err c > yelppolar time c > lstm c > 76k c > 76k c > 76k c > 76k c > 76k c > 76k c > 76k c > 76k c > yelppolar time c >
< extra_id_0 > bleu score on wmt14 english - german translation task . train : time in seconds per training batch measured from 0 . 2k training steps on tesla p100 . decode : time in milliseconds used to decode one sentence measured in table 3 .
< extra_id_0 > table 4 : exact match / f1 - score on squad dataset . rnet * : results published by wang et al . ( 2017 ) . rnet * : results published by wang et al . ( 2018 ) .
< extra_id_0 > table 6 shows the f1 score on conll - 2003 english ner task . lstm * denotes the parameter number in ner task . lstm * denotes the reported result .
< extra_id_0 > table 7 : test accuracy on snli task with base + ln setting and test perplexity on ptb task with base + ln setting and test accuracy on snli task with base + ln setting and test accuracy on snli task with base + ln setting and test perplexity on snli task with ptb task with base + ln setting and test perplexity on snli task with base setting .
< extra_id_0 > b - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 , r - 2 ,
< extra_id_0 > the highest standard deviation among all automatic systems is 1 . 0 , with statistical significance marked with ( approximation randomization test , p0 . 0005 ) . human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) is highlighted in bold , with statistical significance marked with ( approximation randomization test , p0 . 0005 ) . the highest standard deviation among all automatic systems is 1 . 0 . seq2seq achieves the best performance among automatic
< extra_id_0 > slqs and docsub , respectively , and tf c > patt c > docsub c > docsub c > hclust c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > pt c > pt c > pt c > pt c > pt c > pt c > pt c > pt c > pt c > pt c > pt c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > slqs and docsub are the only corpus that performs better than dlqs and hclust . europarl outperforms all other corpus models except for docsub and docsub in terms of performance .
< extra_id_0 > slqs and docsub are based on df and docsub and hclust . europarl outperforms both df and docsub in terms of p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > europarl outperforms both df and docsub in terms of p
< extra_id_0 > slqs and docsub are derived from the same corpus as the dsim and docsub corpus . the europarl corpus and docsub corpus perform better than the other corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus
< extra_id_0 > slqs and docsub are based on avgdepth of 9 . 9 and 9 . 9 respectively . europarl and patt c > dsim and docsub are based on avgdepth of 9 . 9 and 9 . 9 respectively . europarl and patt c > dsim and docsub are based on avgdepth of 9 . 9 and 9 . 9 respectively . europarl is based on avgdepth of 9 . 9 and 9 . 9 respectively .
< extra_id_0 > r0 , r1 , r2 and r3 denote question type , answer score sampling , and hidden dictionary learning , respectively . lf is the enhanced version as we mentioned in table 1 . lf is the baseline version as we mentioned in table 1 . lf is the enhanced version as we mentioned in table 1 . ndcg % comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 is shown in table 1 .
< extra_id_0 > table 2 shows the performance ( ndcg % ) of ablative studies on different models on visdial v1 . 0 validation set . p2 indicates the most effective one ( i . e . , hidden dictionary learning ) shown in table 1 .
< extra_id_0 > table 5 : comparison on hard and soft alignments . cs - en c > cs - en c > fi - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > c > c > c > c >
< extra_id_0 > de - en c > bold > direct assessment / bold > zh - en c > bertscore - f1 c > 0 . 552 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 661 c > c > c > c > c > c > c > c > c > bertscore - f1 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c >
< extra_id_0 > inf : bold > nat / bold > bold > qual / bold > bold > qual / bold > bold > qual / bold > bold > nat / bold > bold > nat / bold > bold > nat / bold > bold > nat / bold > c > bleu - 1 c > c > c > c >
< extra_id_0 > leic ( * ) scores m1 and m2 respectively , while spice scores m1 and m2 consistently outperforms spice and bertscore - recall ( * ) . the results of spice and bertscore - recall ( * ) are summarized in table 1 .
< extra_id_0 > compared to shen - 1 and cyc + para , respectively . compared to shen - 1 and cyc + para , sim vs . sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim vs sim v
< extra_id_0 > transfer quality a > b c > transfer quality tie c > semantic preservation a > b c > semantic preservation tie sim c > semantic preservation tie sim c > semantic preservation tie sim c > semantic preservation tie sim c > semantic preservation tie sim c > semantic preservation tie c > m0 c > m7 c > m7 c > m7 c > m6 c > m7 c > m6 c > m7 c > m6 c > m6 c > m6 c > sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim
< extra_id_0 > acc , pp and pp are validated using spearman ’ s [ italic ] and spearman ’ s [ italic ] datasets . spearman ’ s [ italic ] and spearman ’ s [ italic ] datasets contain 100 examples for each dataset for human sentence - level validation ; see text for validation of gm and gm .
< extra_id_0 > we found that < extra_id_1 > outperforms all the other models except for the m0 and m0 variants . we found that the m0 and m0 variants outperform the m0 and m0 variants in terms of para and cyc + 2d variants . compared to the m0 and m0 variants , the m0 and m0 variants have significantly lower performance than the m0 variants .
< extra_id_0 > bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . our best models achieve higher bleu than previous work at similar levels of acc , but untransferred sentences achieve the highest bleu than prior work at similar levels of bleu . the results on yelp sentiment transfer are shown in table 6 . our best models achieve higher bleu than previous work , but untransferred sentences achieve the highest bleu than previous work at similar levels of acc . our best models achieve higher bleu .
< extra_id_0 > table 2 shows the percentage of reparandum tokens that were correctly predicted as disfluent . rephrase , rephrase , restart , rephrase , rephrase , rephrase , rephrase and restart tokens show the highest percentage of reparandum tokens that were correctly predicted as disfluent . rephrase , restart , rephrase and restart tokens show the highest percentage of tokens that were correctly predicted as disfluent .
< extra_id_0 > the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both reparandum and repair ( content - content ) , either the reparandum or repair ( content - function ) , or in neither . the fraction of tokens belong to each category .
< extra_id_0 > c > [ bold ] dev mean c > [ bold ] test mean c > [ italic ] test mean c > [ italic ] test mean c > [ italic ] test mean c > [ italic ] test mean c > [ italic ] test mean c > [ italic ] test mean c > [ italic ] test mean c > [ italic ] test mean c > [ italic ] test mean c > [ italic ] test mean c > [ italic ] test best c > [ italic ] test best c > [ italic ] test best c > single c > early c > single c > early c > early c > early c > early c > early c > early c > early c > early c > early c > early c > early c > early c > early c > early c > early c > early c > early c > early c > early c > early c > early c > early c > early c > early c > text + innovations c > text + innovations c > innovations c > innovations c > innovations c > innovations c > innovations c > innovations c > innovations c > innovations c > innovations c > innovations c > innovations c > innovations c > innovations c > innovations c > innovations c > innovations c > innovations c > innovations c > innovations c > innovations c > innovations c > 86 . 54 c > 86 . 54 c > 86 . 54 c > 86 . 54 c > 86 . 54 c > 86 . 54 c > 86 . 54 c > 86 . 54 c > 86 . 54 c > 86 . 54 c > 86 . 54 c > 86 . 54 c > 86 . 54
< extra_id_0 > accuracy ( % ) agree accuracy ( % ) disagree accuracy ( % ) unrelated table 2 : performance comparison with the state - of - art algorithms on the fnc - 1 test dataset . our model performs better than the state - of - art algorithms on the fnc - 1 test dataset .
< extra_id_0 > table 2 shows the accuracy ( % ) of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the unified model significantly outperforms all previous models .
< extra_id_0 > table 3 shows the effectiveness of both word attention and graph attention for this task . the results show the effectiveness of both word attention and graph attention for this task .
< extra_id_0 > embedding + t has 1 / 1 and embedding + t has 1 / n and embedding + t has 59 . 8 points . embedding + t has 59 . 8 points and embedding + t has 59 . 8 points . embedding + t has 57 . 8 points and embedding + t has 57 . 8 points . embedding + t has 57 . 8 points and embedding + t has 57 . 8 points .
< extra_id_0 > classification ( % ) c > [ bold ] trigger [ bold ] identification ( % ) c > [ bold ] trigger [ bold ] identification ( % ) c > [ bold ] classification ( % ) c > [ bold ] trigger [ bold ] identification ( % ) c > [ bold ] classification ( % ) c > [ bold ] trigger [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( % )
< extra_id_0 > wer
< extra_id_0 > the results on the dev set and on the test set using discriminative training with only subsets of the code - switched data are shown in table 4 . the results on the dev set and on the test set using discriminative training with only subsets of the code - switched data are shown in table 4 . the results on the dev set and on the test set using discriminative training are shown in table 4 .
< extra_id_0 > table 5 shows the accuracy of the gold sentence on the dev set and on the test set , according to the type of the gold sentence in the set : code - switched ( cs ) vs . monolingual ( mono ) .
< extra_id_0 > table 7 shows the precision ( p ) , recall ( r ) and f1 - score ( f1 ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset ( * marks statistically significant improvement ) . type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset ( * marks statistically significant improvement ) .
< extra_id_0 > conll - 2003 shows the best p , recall ( r ) and f1 - score ( f1 ) for using type - aggregated gaze features on the conll - 2003 dataset ( * marks statistically significant improvement ) . type - aggregated gaze features achieve the best p , recall ( r ) and f1 - score ( f1 ) for using type - aggregated gaze features .
< extra_id_0 > table 1 summarizes the results on belinkov2014exploring ’ s ppa test set . syntactic skipgram achieves 88 . 7 on the glove - retro and glove - extended synset embeddings , and lstm - pp achieves 84 . 8 on the glove - retro and glove - extended synset .
< extra_id_0 > table 2 summarizes results from rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . rbg + hpcd ( full ) achieves 94 . 17 compared to 88 . 51 for full uas .
< extra_id_0 > table 3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . the effect of removing sense priors and context sensitivity ( attention ) from the model is shown in table 3 .
< extra_id_0 > add subtitle data and domain tuning for image caption translation ( bleu % scores ) . all results with marian amun are shown in table 2 .
< extra_id_0 > and mscoco17 . subs1m outperforms en - de and mscoco17 in terms of labeling . lm + ms - coco outperforms en - fr and mscoco17 in terms of labeling . lm + ms - coco outperforms en - fr and mscoco17 in terms of labeling .
< extra_id_0 > autocap 1 - 5 ( concat ) outperforms [ bold ] and [ mscoco17 ] in terms of automatic image captioning . adding automatic image captioning improves the performance of en - de flickr16 flickr17 mscoco17 . adding automatic image captioning improves the performance of en - de flickr16 mscoco17 . adding automatic image captioning improves the performance of en - de multi30k 52 . 9 compared to [ bold ] 32 . 0 .
< extra_id_0 > and mscoco17 . enc - gate and dec - gate achieve better performance than enc - gate and dec - gate , respectively . enc - gate and dec - gate achieve better performance than enc - gate and dec - gate , respectively . enc - gate and dec - gate achieve better performance than enc - gate and dec - gate .
< extra_id_0 > mscoco17 and en - fr c > flickr16 c > flickr17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > subs3m c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > s3m c > s3m s3m s3m s3m s3m s3m s3m s3m s3m s3m s3m s3m s3m s3m s3m s3m s3m s3m s3m s3m s3m s3m s3m s3m s3m s3m s3m s3m s3m s3m s3m s3m s3m s3m s3m s3m s
< extra_id_0 > we see that en - fr - trans - ff outperforms both en - fr - trans - ff and en - fr - smt - back in the yule ’ s i and yule ’ s i datasets . we observe that en - fr - trans - ff achieves a better performance than en - fr - trans - ff and en - fr - trans - ff . we observe that en - fr - trans - back achieves better performance over en - fr
< extra_id_0 > table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . the number of parallel sentences in the train , test and development splits for the language pairs is shown in table 1 .
< extra_id_0 > table 2 : training vocabularies for the english , french and spanish data used for our models . the training vocabularies for the english , french and spanish data used for our models are shown in table 2 .
< extra_id_0 > table 5 shows the automatic evaluation scores ( bleu and ter ) for the rev systems . the bleu and ter scores for the rev systems are shown in table 5 .
< extra_id_0 > recall @ 10 ( % ) and median rank on flickr8k . the row labeled vgs is the visually supervised model from chrupala2017representations . the mean mfcc is 0 and the mean mfcc is 0 . 0 .
< extra_id_0 > table 1 summarizes the results on synthetically spoken coco . recall @ 10 ( % ) is significantly higher than rsaimage and mfcc , respectively . rsaimage achieves a better recall rate than rsaimage and mfcc , respectively .
< extra_id_0 > is so clever that you want hate hate hate hate hate hate hate hate hate hate hate hate . we report further examples in the appendix .
< extra_id_0 > the rnp outperforms the rnp and dan in terms of verbs and punctuation . the rnp outperforms the rnp and dan in terms of verbs and punctuation . the rnp outperforms the rnp in terms of verbs and punctuation .
< extra_id_0 > the last two rows correspond to the case where negative labels are flipped to positive and vice versa . the numbers indicate that the score increases in positive and negative sentiment with respect to the original sentence .
< extra_id_0 > it is good c > it is bad c > to evaluate c > to evaluate c > to evaluate c > to evaluate c > to clarify c > to evaluate c > to evaluate c > to evaluate c > to clarify c > to evaluate c > to clarify c > to evaluate c > to evaluate c > to clarify c > to evaluate c > to evaluate c > to evaluate c > to evaluate .
