2 shows the performance of the treelstm model on inference and training datasets , with the exception of the large movie review dataset , where the recursive approach performs the best on inference with efficient parallel execution of tree nodes .
shown in table 1 , the balanced dataset exhibits the highest throughput thanks to the high degree of parallelization . however , it does not improve as well as the linear dataset when the batch size increases from 1 to 25 .
2 shows the performance of the max pooling strategies for each model with different number of parameters . the maximum pooling strategy consistently performs better in all models with different representation size . moreover , it has the advantage of minimizing the number of error messages .
1 shows the effect of using the shortest dependency path on each relation type . our model achieves the best f1 ( in 5 - fold ) with sdp compared to the macro - averaged model . we also observe that our model outperforms all the other models in terms of f1 without sdp .
results are reported in table 3 . we observe that y - 3 significantly outperforms y - 2 in f1 and r - f1 by a margin of 3 . 5 % .
results are reported in table 3 . our proposed method outperforms all previous methods except mst - parser in terms of paragraph level f1 and f1 scores .
4 shows the c - f1 scores for the two indicated systems , lstm - parser and stagblcc . note that the mean performances are lower than the majority performances over the runs given in table 2 .
results are shown in table 3 . the original and the original results are presented in bold . our system outperforms all the other methods in terms of bleu , nist and cider by a significant margin . the results are summarized in table 4 . the best performing system is sc - lstm , which has a 2 . 86 % improvement on the original and 3 . 93 % improvement over the original .
shown in table 1 , the original and the cleaned versions have the highest number of distinct mrs and the number of textual references as measured by our slot matching script , see section 3 .
results are shown in table 2 . original and tgen models outperform the original in all but one of the four cases . the results are summarized in table 1 . original model outperforms the original on all but two of the three cases . for example , sc - lstm outperforms both the original and the tgen model by a margin of 2 . 26 points .
4 shows the results of manual error analysis on a sample of 100 instances from the original test set of tgen . we found that adding incorrect values significantly increased the chance of errors being detected in the training set .
3 shows the performance of our dcgcn model on the external and external datasets compared to the previous best state - of - the - art models . the results are summarized in table 3 . for the external dataset , all models outperform all the other models except seq2seqk .
results on amr17 are presented in table 2 . our model achieves 24 . 5 bleu points on the model size in terms of parameters , compared to the previous best state - of - the - art model , seq2seqb .
3 presents the results for english - german and english - czech . the results are presented in table 3 . the best performing models are bow + gcn ( bastings et al . , 2018 ) and birnn ( beck et al . 2018 ) . the english - language models outperform the german and czech models in terms of number of participants .
5 shows the effect of the number of layers inside dc on the quality of the layers in the network . for example , for example , there are 3 layers of layers that contribute to the overall performance of the network , while for other layers , there is only one .
results are shown in table 6 . rc denotes gcn with residual connections , rc + la ( 2 ) denotes a gcn that has residual connections to residual connections . the results are similar for dcgcn2 ( 27 ) and dcgcnl4 ( 36 ) in terms of gcn performance .
3 shows the performance of dcgcn with respect to b and c . the results are summarized in table 3 . the best performance is achieved on the large - scale datasets , where the average number of participants is 15 . 5m and the average b - max is 22 . 2 .
8 shows the ablation study results for amr15 with respect to dense blocks . the results are shown in table 8 . table 8 shows that removing the dense connections in the i - th block improves the performance of the model by 2 . 5 points .
9 shows the ablation study for the graph encoder and the lstm decoder . the results are summarized in table 9 . the best performing model is dcgcn4 , with a gap of 2 . 6 % in coverage between the two models .
results are shown in table 7 . our model outperforms the previous state - of - the - art methods in all but one of the three domains .
results are presented in table 4 . we observe that the h - cbow method outperforms the other methods in terms of depth and subtraction . however , it does not have the best performance on the subtraction test set .
cbow / 784 outperforms all other methods except sst2 and sst5 except for subj , which is closer to the best performing subj model . cbow also outperforms both subj and mpqa in terms of mrpc score . however , it does not perform as well as sst1 , which results in lower mrpc scores .
results on unsupervised downstream tasks attained by our models are shown in table 3 . our model outperforms both hybrid and cmp . cbow shows the relative change in performance with respect to hybrid .
8 shows the performance of the initialization strategies on supervised downstream tasks . our model outperforms all the other approaches except sst2 and sst5 in terms of mrpc score . it also outperforms the sst1 baseline by 3 points .
6 shows the performance for different training objectives on the unsupervised downstream tasks . cmow - c outperforms cbow - r on all three tasks except sts13 and sts14 .
results are shown in table 3 . we observe that cbow - c outperforms all the other methods in terms of depth and subtraction . however , it does not have the best performance on the subjnum and subjnum datasets .
results are presented in table 3 . we observe that the best performing cbow - r model outperforms both sst2 and sst5 on all metrics except mrpc . it is clear that the cbow model has superior recall ability on both datasets , which suggests that it can be used to improve recall accuracy .
3 shows the e + and per scores for the system in [ italic ] and [ bold ] e + e + per scores . our system outperforms all the other systems except for the one that does not have the best e + org score . the results are summarized in table 3 . our model outperforms the previous state - of - the - art model in all but one of the three cases .
results on the test set under two settings are shown in table 2 . our system outperforms all previous models in e + p , f1 score and e + f1 scores . we observe that the best performing model is mil - nd , which improves upon the previous state - of - the - art model .
6 : entailment ( ent ) and ref ( g2s - gat ) results are shown in table 6 . the model outperforms all the other models in terms of ref , ref and f1 scores , with the exception of s2s , where ref outperforms ref .
results are presented in table 3 . the best performing models are g2s and gat , which outperform the ldc2017t10 and ldc2015e86 in terms of bleu score .
3 shows the results on ldc2015e86 test set when models are trained with additional gigaword data . the results are summarized in table 3 . our model outperforms all previous models except for konstas et al . , 2017 .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . bilstm significantly outperforms the other models in terms of size and performance .
results are shown in table 3 . we observe that g2s - gin has the best overall performance on all metrics , with the exception of the graph diameter and sentence length . the results are reported in table 4 . as expected , the average number of frames per sentence is lower than the average length of the sentences .
8 shows the fraction of elements in the output that are not present in the input ( added ) and the fraction of elements that are missing in the generated sentence ( miss ) . these results are shown in table 8 . the g2s - gin model outperforms all the other models in terms of fraction and miss .
4 shows the performance of the 4th nmt encoding layer trained with different target languages on a smaller parallel corpus ( 200k sentences ) . our model outperforms the previous state - of - the - art models in terms of accuracy .
2 shows the accuracy with baselines and an upper bound . mft : most frequent tag ; unsupemb : most frequently tag ; word2tag : upper bound encoder - decoder .
results are presented in table 4 . our results are reported in tables 1 and 2 . the results are summarized in table 3 . we observe that our results are slightly superior than those of the other methods , indicating that our method outperforms the competition .
5 shows the accuracy with different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages .
performance on different datasets is shown in table 8 . the difference between the attacker score and the corresponding adversary ’ s accuracy is reported in p < . 01 .
1 shows the accuracies when training directly towards a single task . our model outperforms pan16 and pan16 in all but one of the four cases .
2 shows the performance of the hashtags in the balanced and unbalanced data splits . for the balanced data splits , we use pan16 and pan16 as models . the results are summarized in table 2 . dial and sentiment are statistically significantly worse than the other models . sentiment is slightly worse than sentiment , but is slightly better than sentiment .
performance on different datasets with an adversarial training is shown in table 3 . the difference between the attacker score and the corresponding adversary ’ s accuracy is reported in δ . in pan16 , the average age of the participants is 69 . 7 % compared to 69 . 5 % in pan16 .
6 shows the performance of the embeddings for different encoders . for example , rnn performs slightly better than embedded , while embedding is slightly worse .
results are shown in table 2 . the best performing models are lstm , lrn , and f1 . these models outperform all the other models in terms of finetune performance . however , the best performing model is sru , which outperforms all the models except for the one that relies on the finetune feature set .
results are presented in table 5 . our model outperforms all the previous models in terms of training time , with the exception of the lstm model , which has the advantage of having a longer training time .
results are shown in table 4 . we observe that our model outperforms all the other models in terms of time - based err , with the exception of amafull time , which is slightly better than the lstm model .
3 shows the bleu score on wmt14 english - german translation task . our model outperforms all the other models in terms of decoding one sentence . it is clear from table 3 that our model is able to decode one sentence in seconds .
4 shows the performance of our model on squad dataset . our model outperforms all the models in terms of match / f1 score by a large margin . further , our model performs better than all models except lrn and atr .
6 shows the f1 score on conll - 2003 english ner task . lstm * denotes the parameter number in the reported result , and sru denotes the performance of the model . further , lrn performs better than the other models , with a boost of 3 . 5 points .
results on snli task with base + ln setting and test perplexity with ptb setting are shown in table 7 . our model outperforms all the other models except for elrn .
results are shown in table 2 . word embeddings are used for system retrieval , while word embedding is used for system retriev . multi - task learning ( mtr , mtr , rtr ) is used to improve the performance of the system over the previous state - of - the - art systems . sentiment is used in combination with sentence embedding to improve performance .
4 presents the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 ( k 1000 ) . our system outperforms all the other systems except seq2seq and retrieval .
results are reported in table 3 . the results are summarized in table 4 . our model outperforms all the other models in terms of performance , with the exception of eurparl , whose performance is slightly worse than that of docsub ( p < 0 . 005 ) . however , the results are slightly better than those of other models .
results are reported in table 3 . the results are summarized in table 4 . our model outperforms all the other models in terms of performance on both datasets , with the exception of eurparl , which has the best performance on tf and docsub .
results are reported in table 3 . the results are presented in tables 1 and 2 . our model outperforms all the other models in terms of performance , with the exception of eurparl , whose performance is slightly worse than that of docsub . however , the results are slightly better than those of other models .
results are reported in table 3 . the results are summarized in table 4 . our model achieves the best performance with a minimum of 3 . 5 % improvement over the previous state - of - the - art model on the metric .
results are reported in table 3 . the results are summarized in table 4 . our model achieves the best performance with a minimum of 1 . 5 % improvement over the previous state - of - the - art model . however , it does not achieve the best results with a maxdepth of 9 . 43 % .
performance ( ndcg % ) for the experiments of applying our system on the validation set of visdial v1 . 0 . lf is the enhanced version of the original visdial model , and r1 , r2 , r3 denote regressive loss , weighted softmax loss , and generalized ranking loss , respectively .
2 shows the performance ( ndcg % ) of different ablative studies on different models on the visdial v1 . 0 validation set . the best performing model is lf , which relies on hidden dictionary learning .
5 compares the performance of hmd - f1 and wmd - prec on hard and soft alignments . the results are summarized in table 5 . for the hard alignments , hmd improves by 0 . 8 % compared to the previous state - of - the - art model .
results are presented in table 3 . the results are summarized in bold . the baselines are based on meteor scores and bertscore - f1 scores . they are comparable across all three sets except for the exception of the one in which our model outperforms our model . for example , our model improves the performance of our model by 0 . 5 points on the direct assessment test compared to the previous state of the art model .
results are presented in table 2 . the bleu - 1 model outperforms all the baselines except for the bertscore - f1 model , which obtains the best performance on both datasets . the results are summarized in table 1 . the baselines are based on the best bert score , with the exception of sfhotel , where the results are significantly better than those obtained by the baseline .
results are presented in table 3 . the baselines are summarized in bold . they are based on the leic score ( p < 0 . 001 ) and bertscore - recall scores ( p > 0 . 005 ) . the results are reported in table 4 . the leic scores are computed using the weighted average of the baselines . the results show that the combination of weighted average and weighted average bert scores can improve the performance of the models by a significant margin .
results are shown in table 3 . we observe that the m0 model outperforms the m1 model in terms of scalability , with the exception of the shen - 1 model , which is more suitable for gm . further , the m2 model performs better than the m3 model on both datasets .
results are presented in table 3 . we observe that the transfer quality and semantic preservation features are the most important aspects of semantic preservation . the results are summarized in table 4 . semantic preservation is the most distinctive feature for semantic preservation , the transfer quality features are significantly better than those in semantic preservation ( e . g . , m2 and m7 ) . the semantic preservation feature is further improved with the addition of semantic and semantic features . syntactic preservation is further enhanced with the introduction of semantic features such as semantic clusters and semantic ones .
5 shows the results of human sentence - level validation . the results are summarized in table 5 . our model outperforms both the human and machine models in terms of semantic preservation , with a slight improvement in accuracy .
results are shown in table 3 . we observe that the m0 model outperforms the m2 model in terms of performance on all metrics except for the performance of the shen - 1 test set .
results on yelp sentiment transfer are shown in table 6 . our best model achieves the highest bleu and the highest acc ∗ on the 1000 - sentence corpus compared to previous work . multi - decoder outperforms all the other models except for yang2018unsupervised , which shows that the use of different classifiers in the transfer domain can improve the predictive performance of the model . we also observe that using the same classifier can improve predictive performance as well as recall performance .
2 shows the performance of nested disfluencies compared to the disfluent ones . reparandum length is reported in table 2 . the number of repetition tokens that are correctly predicted to be disfluent is estimated to be 2 . 5 times as high as the number of disfluency tokens .
3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and the repair ( content - function ) . the percentage of tokens correctly predicted to contain a word is in parentheses , indicating that the disfluency in the reparandum is caused by the presence of a word in the repair .
results are presented in table 3 . we show the best and worst results for each model on the single and multi - domain test , respectively . the best results are reported in table 4 . in the single domain , the text + innovations model outperforms all the other models in terms of dev and innovations .
2 compares our model with the state - of - art word2vec embeddings on the fnc - 1 test dataset . the results are summarized in table 2 . our model outperforms all the other models on the test dataset by a significant margin .
2 shows the performance of different methods on the apw and nyt datasets for the document dating problem . the best performing method is burstysimdater , which significantly outperforms all previous methods .
3 shows the performance of both word attention and graph attention for the word attention task . the results are shown in table 3 .
results are shown in table 1 . the best performing models are the dmcnn and jrnn models , which outperform all the other models except for trigger , which is more difficult to train .
3 presents the results of our method in cross - event settings . our method outperforms all the methods except for the one used in the event of a single event ( f1 ) . our method achieves the best results in both scenarios , with the exception of the event in which the event is named .
results are shown in table 3 . all but the spanish - only - lm model outperforms all the other models in terms of dev perp and test wer . the results are summarized in table 4 . for english , all but the fine - tuned models outperform the english - only model in both metrics .
4 shows the results on the dev set and the test set using discriminative training with only subsets of the code - switched data . the results are shown in table 4 .
5 shows the performance on the dev set and the test set , compared to monolingual ( cs ) and code - switched models .
results are shown in table 7 . for type - aggregated gaze features trained on all three eye - tracking datasets , precision ( p ) and f1 - score ( f ) are significantly better than those trained on the conll - 2003 dataset . note that the precision ( p ≤ 0 . 05 ) and recall ( f1 ) scores are significantly worse than those for type combined gaze features .
5 shows the precision and f1 scores for using type - aggregated gaze features on the conll - 2003 dataset . precision ( p ) and recall ( f1 ) are statistically significant improvements over the previous state - of - the - art model .
results on belinkov2014exploring ’ s ppa test set are shown in table 1 . syntactic - sg embeddings are used in wordnet 3 . 1 , and glove - retro is used on wordnet 4 . 1 . the syntactic embedding features are obtained by using the syntactic skipgram feature set , and the semantic embedding feature set is derived from the original wordnet 2 . 1 paper .
results from rbg are presented in table 2 . we observe that the dependency parser is able to interpret various pp attachment predictors and oracle attachments with a minimum of variation .
3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . the results are shown in table 3 . it is clear that the removal of context sensitivity has a significant effect on ppa performance .
2 shows the results of domain tuning for image caption translation ( bleu % scores ) and multi30k decoding ( marian amun et al . , 2018 ) . adding subtitle data and domain tuning results in the same direction as the original embeddings .
3 shows the performance of subs1m overdomain - tuned models in en - de and in - de . the results are summarized in table 3 . subdomain tuning improves the performance by 2 . 5 points over the previous state - of - the - art model . the results of domain tuning improve the performance for both en - fr and en - de . in - de , the subs2m model outperforms all the other models in terms of performance .
4 shows the bleu scores of the models using automatic captions . the results are summarized in table 4 . the best model is the multi30k model , while the worst one is the one with the best output . the improvement is due to the fact that the model is more compact and requires fewer training sessions .
5 compares the performance of different approaches for embedding visual information . multi30k + ms - coco + subs3mlm embeddings outperform enc - gate and dec - gate in terms of bleu % scores . we observe that the enc - gated approach improves the performance for the en - de model by 2 . 5 % compared to the previous best model .
3 shows the performance of subs3m and subs6m on en - de and in - de , respectively . the performance of the two models is slightly better than those of the other models , however , the performance is still slightly worse for the other two models . in - de models perform better than the models using text - only or multi - lingual features .
results are shown in table 3 . we observe that the best performing en - fr - ff model outperforms the best - performing en - es - ht model on mtld and en - rnn - ff , respectively .
results are shown in table 1 . the number of parallel sentences in the train , test and development splits for the language pairs we used is shown in bold .
2 shows the results for the english , french and spanish datasets . our model outperforms both the english and french datasets by a significant margin .
5 shows the bleu and ter scores for the rev systems . automatic evaluation scores ( bleu , ter ) show that the system performs better than the previous state - of - the - art systems on both datasets .
results on flickr8k are shown in table 2 . the vgs model outperforms the segmatch model in terms of recall @ 10 and f1 score .
results on synthetically spoken coco are shown in table 1 . the results are presented in the row labeled vgs , which is the visually supervised model from chrupala2017representations . segmatch and audio2vec - u achieve the best performance on both datasets .
table 1 shows the results for each classifier compared to the original on sst - 2 . we report further examples in table 1 . for example , cnn turns in a < u > screenplay that shows the edges at the edges ; it ’ s so clever you want to hate it . for dan , it shows that the edges of the screenplay are very clever . for cnn , the edges are so clever that it shows the shape of the dialog . for rnn , we show the results of using the different classifiers . we show that when the dialog is in the dialog , the dialog turns out to be more clever than in the original .
2 presents the results of fine - tuning on sst - 2 . the results are summarized in table 2 . we observe that the number of occurrences have increased , decreased or stayed the same for the majority of the words . this indicates that fine tuning has not changed the quality of the sentence .
3 shows the change in sentiment between positive and negative labels in sst - 2 . the results are shown in table 3 . positive labels are flipped to positive and vice versa .
results are presented in table 2 . the results are summarized in bold . the best results are reported in table 1 . however , there is a slight difference in performance between the best and the worst results . in addition , the results are not statistically significant , indicating that our approach is more suitable for the task . table 2 summarizes the results of our method in terms of performance .
