2 shows the performance of our recursive framework on the large movie review dataset compared to the iterative approach on the recursive framework . the results are summarized in table 2 . inference is the best performance on training , while the inference performance is the worst on inference .
shown in table 1 show that the balanced dataset exhibits the highest throughput thanks to the high degree of parallelization . however , it does not improve as well as the linear dataset , because there is only a small room of performance improvement left to train the system .
2 shows the performance of the max pooling strategy for each model with different representation . the performance of conll08 is shown in table 2 . the number of parameters with the maximum number of iterations is the same across all models with different performance metrics . the hgnll08 model achieves the best performance in all three scenarios with different iterations of the model . hgnl08 also outperforms softplus and sigmoid in all but one of the three scenarios .
1 shows the effect of using the shortest dependency path on each relation type . it can be seen in table 1 that the macro - averaged model achieves the best f1 ( in 5 - fold ) with sdp and the best diff . the relation type that relies on sdp has the highest f1 score , which indicates that the dependency path has the greatest effect on the f1 scores .
results are presented in table 3 . we observe that y - 3 significantly outperforms y - 2 in terms of f1 and f1 score with a significant margin improvement .
3 presents the results of our method on the paragraph level . the results are presented in table 3 . our method outperforms all the other methods in terms of both test and evaluation . our approach achieves the best results on the test level with a minimum of 50 % of the time compared to the average of 50 % . on the evaluation level , it achieves the highest f1 score with a maximum of 2 . 17 % and a maximum score of 3 . 17 % . the results of the method outperform all the methods except mst - parser on the validation level . we observe that the method achieves a high level of performance when trained on a single dataset .
4 shows the c - f1 scores for the two indicated systems ; the lstm - parser and the paragraph system show lower performance than the majority systems .
3 shows the performance of our system on the original and the second set of test sets . the results are presented in table 3 . the original and second set are shown in bold . the bleu system outperforms all the other test sets except for the one in which it is used . the performance of both sets is reported in table 1 . we observe that the cleaned and unsupervised tgen models outperform the original in all but one of the two sets . these results show that the original is better than the original on both sets . this is evident from the fact that both sets are trained on the same set of training data .
shown in table 1 , the original and the cleaned versions have the highest number of distinct mrs as measured by the number of textual references in our slot matching script , see section 3 . the cleaned version has the highest percentage of mrs and the highest rate of concatenation with the original .
results are shown in table 1 . original and original results are presented in bold . original results show that the tgen + model outperforms both the original and the original in all but one of the cases . the performance of the original is comparable across all the test sets except for the one in which it is tested . the results of the test set are summarized in table 2 . we observe that the original results are slightly better than the original , but still comparable to the original . the difference between original and original results is less pronounced for both sets . we notice that the difference between original and wrong results is more pronounced for the original set .
results of manual error analysis on a sample of 100 instances from the original test set are shown in table 4 . the errors in the original training set are caused by slight disfluencies in the training set . in addition , there is a slight imbalance in the disfluency scores of the added and the wrong values of the disflips . table 4 shows the absolute numbers of errors we found ( added , missed , wrong values ) and the percentage of errors that were caused by misficencies .
3 shows the performance of our dcgcn model on the external and the internal datasets . the first set of results show that the proposed dgcn bridges the gap between the external and the external datasets , improving upon the state - of - the - art approach by a significant margin . for the single dataset , we see that all the proposed seq2seqk models outperform both the original and the proposed tree2str .
results on amr17 show that dcgcn outperforms the ensemble seq2seqb model in terms of bleu points . the results are summarized in table 2 . the results show that the ensemble model performs better than the single model , which suggests that the larger ensemble model is more suitable for the task .
3 presents the results for english - german and english - czech . the results are presented in table 3 . the first set of results show that the single - language model outperforms the two - language models in terms of overall performance . the second set shows that both the english - language and the german - language datasets perform similarly to the first set . we observe that the bow + gcn model performs well in both languages , with the exception of english - korean , where it performs slightly better in the single language .
5 shows the effect of the number of layers inside dc on the overall performance of the system . table 5 shows that for all layers , there is a significant effect on the performance . for example , for example , if the layer with the most layers is aligned with the other layers , the effect is less pronounced for the two layers .
6 shows the performance of the baselines with residual connections . rc + la denotes gcns that have residual connections with other gcns . the results are summarized in table 6 . with residual connections , gcns with gcn + rc + la ( 2 ) and dcgcn + 4 ( 6 ) show that the residual connections are beneficial for gcn performance .
3 shows the performance of our dcgcn model in terms of bias metric . the results are presented in table 3 . we observe that the approach achieves the best performance with a minimum of over - fitting on both datasets . in fact , it achieves the highest performance with an absolute overfitting on all datasets .
8 shows the ablation study results on the dev set of amr15 . we observe that the dense blocks in the i - th block have the highest density of the three dense blocks , which indicates that removing the dense connections in these blocks leads to a better performance .
9 shows the ablation study for the graph encoder and the lstm decoder . the results are presented in table 9 . we observe that the multi - decoder design has the advantage of co - ordination , which leads to better interpretability . it is clear from table 9 that the multilingual design relies on the hierarchical nature of the word encoder .
7 shows the performance of our initialization strategies on probing tasks . our proposed method outperforms the previous state - of - the - art method on all three tasks . the results are summarized in table 7 . we observe that our method obtains the best performance on the two tasks .
are presented in table 4 . we show the results of our method in terms of depth and subtraction . the results show that our method obtains the best performance when subtraction is applied to the subtraction of the parameter . it obtains a superior performance on all subtraction metrics . it is clear that the h - cbow / 400 model outperforms both the cbow and cbow in both subtraction and subtention .
3 presents the results of our method on the test set of subj and mpqa . the results are presented in table 3 . subj outperforms both sst2 and sst5 in terms of mrpc score . it is clear that the cbow / 784 model performs better on both test sets . it obtains the best performance on all test sets , with a marginal drop of 0 . 2 % compared to the previous state - of - the - art cbow .
results on unsupervised downstream tasks attained by our models are shown in table 3 . we observe that the cbow model outperforms both hybrid and cmp . cbow models in terms of the relative change with respect to upstream tasks .
8 shows the performance of our initialization strategies on supervised downstream tasks . our proposed method outperforms all the alternatives except subj and mpqa . it achieves the best overall performance on all three tasks , with the exception of sst2 , which achieves the highest overall performance .
6 shows the performance for different training objectives on the unsupervised downstream tasks . cmow - c outperforms cbow - r on all three tasks except sts14 and sts15 . in both cases , it achieves the best performance .
shown in table 1 , the cbow - r method outperforms all the other methods in terms of depth and subtraction . it is clear from table 1 that the method obtains the best performance when subtraction is applied to the subtraction of the row length . it obtains a superior performance on all subtraction metrics , with the exception of the subjnum . the results are presented in table 2 .
3 presents the results of our approach on the subj and mpqa datasets . our approach outperforms both the sst2 and sst5 datasets in terms of mrpc performance . it achieves the best results on both datasets when trained on the same single - domain test set . it also outperforms all the other baselines except sick - e and sts - b by a significant margin . the results are presented in table 3 . we observe that the cbow - r model outperforms the other two methods in both datasets .
3 shows the e + and per scores for each system . all org scores are reported in table 3 . name matching and multi - task learning ( mil - nd ) outperform all the other systems except for the one that does not have a single org score . in [ italic ] e + org and misc scores , the system performs better than all the systems except mil - nd . it obtains the best e + or per scores in both systems . the results are summarized in table 1 . we observe that all the system ' s features are better than any other system except for those that do not use the feature - rich org feature .
results on the test set under two settings are shown in table 2 . our system outperforms all the previous models in e + p and f1 scores . it achieves the best e + f1 score in both settings . in [ italic ] system , it achieves the highest e − p score in all three settings . we observe that the accuracy of the system is comparable to that of the supervised learning model , which improves the general performance of both systems .
6 presents the entailment ( ent ) and ref ) results on the g2s - gat dataset . the results are presented in table 6 . the model outperforms all the other models in terms of both ref and f1 scores . for example , ref outperforms ref on both datasets , while f1 achieves a better f1 score on both sets .
results are presented in table 3 . we observe that all the models trained on the ldc2017t10 dataset outperform the previous state - of - the - art models in all but one of the three cases .
3 shows the results on ldc2015e86 test set when trained with additional gigaword data . our model outperforms the previous state - of - the - art models in both external and external settings . the results show that the g2s - ggnn model performs better in the external setting than in the internal setting .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . the results show that bilstm significantly improves the performance of the model when combined with a larger number of parameters , improving the overall performance .
results are presented in table 3 . we observe that the g2s - gin model outperforms all the other models in terms of both word and sentence length . the results are summarized in table 1 . it can be observed that the model achieves the best overall performance when the sentence length is longer than the average length of the sentence . in addition , the average f1 score drops significantly when the distance between sentences is shorter than the length of sentences . finally , the results are reported in table 2 .
8 shows the fraction of elements in the output that are missing in the input ( g2s - gin ) and the fraction of those that are not present in the generated sentence ( miss ) . the results are shown in table 8 . it is clear that the g2s model is better than the other two systems in terms of fraction of elements missing .
4 shows the accuracy of our approach with respect to target languages . we use the 4th nmt encoding layer , trained with different target languages on a smaller parallel corpus ( 200k sentences ) . the results are shown in table 4 . our approach obtains the best performance with the best accuracy with the four target languages extracted .
2 shows the accuracy of our method with baselines and an upper bound . the results are presented in table 2 . we use the word embeddings as the classifier and the upper bound as the reference tags . we also use the semantic tags as reference tags , which improve the semantic tagging performance .
results are presented in table 4 . pos tagging accuracy outperforms all the other methods except for the one in which it is tested . the results are summarized in terms of accuracy and precision . we observe that the performance of all the methods that rely on the word embeddings is comparable to that of the original pos tagging algorithm . the performance of both methods is comparable across all domains .
5 shows the accuracy with different layers of the 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . note that our approach obtains the best performance with the best accuracy with the features from the four layers of our encoder . the results are presented in table 5 .
performance on different datasets is shown in table 8 . the difference between the attacker score and the corresponding adversary ’ s accuracy is caused by the training set trained on 10 % held - out .
1 shows the performance of our system when training directly towards a single task . our system obtains the best performance with the highest accuracy .
shown in table 2 , the balanced and unbalanced data splits are caused by the asymmetric nature of the hashtags in the conversation , and by the imbalance of the sentiment and gender in the task , respectively . these data splits result in a significant drop in performance for the classifier pan16 compared to the previous state of the art .
3 shows the performance on different datasets with an adversarial training . the difference between the attacker score and the corresponding adversary ’ s accuracy is shown in table 3 . sentiment and gender are the most important factors in predicting the performance of the target . gender and age are also important factors , as are age and gender . it is clear from table 3 that the training results in a better predictive performance for the target than the trained adversary .
6 shows the performance of the embedded and guarded encoders compared to the original embeddings . the asymmetric nature of the protected attribute is evident in the fact that rnn is more likely to embed leaky information than rnn .
3 shows the performance of our proposed system on the two datasets . our proposed system outperforms the previous state - of - the - art models on both datasets . it achieves the best performance on all three datasets , with the exception of the wt2 dataset , where it performs better on the dynamic and finetune datasets . the results are presented in table 3 . we observe that our proposed sru outperforms both the original sru and the original lt2 dataset by a significant margin . the results show that sru performs better than the original lstm model in both datasets when trained on the same training set . finally , it achieves the highest performance on the " finetune " dataset with a significant improvement over the original lru model . it is clear from table 3 that the sru model is more suitable for both tasks .
results are presented in table 5 . we show the performance of our model with respect to training time . it is clear from table 5 that it is better to train with a shorter training time than using a full - fledged lstm . the results of training time with a full attention span are consistent across all metrics . when training time is used , the time taken to train is the same as when training time , we observe that training time has a generally positive effect on the task performance . our model outperforms the previous state - of - the - art model in terms of time and accuracy .
3 shows the performance of our model on the four datasets in question . the results are presented in table 3 . table 3 shows that our model outperforms the previous state - of - the - art on all four datasets . the results of our approach are summarized in table 1 . we observe that our approach significantly improves the performance on both the yahoo and the amapolar time datasets when compared to the original lstm model . finally , we observe that the improvement on both datasets is due to the increased performance of the sru in the amafull time dataset .
3 shows the bleu score on wmt14 english - german translation task . our system outperforms all the previous models in terms of decoding one sentence . we observe that the speed at which sru learns to decode one sentence is comparable to that at gnmt14 .
4 shows the performance of our model with respect to match / f1 score on squad dataset . the results published by wang et al . ( 2017 ) show that our model improves upon the baselines with the parameter number of sru and lstm by 2 . 44m . we observe that the sru model outperforms all the other baselines in terms of parameter number .
6 shows the f1 score on conll - 2003 english ner task . lstm * denotes the parameter number in the reported result ( lample et al . ( 2016 ) . lrn shows that it can improve the performance by a significant margin . its performance is comparable to that of the sru model .
results are shown in table 7 . snli model achieves the best performance with base + ln setting and test perplexity on ptb task with base setting . it also achieves the highest performance with a boost in recall score .
3 shows the performance of the word - based system - based systems . word - based methods outperform the human - based approach retrieval is the most effective , with a 2 . 36 % improvement on average compared to the previous best state - of - the - art system . sent attention is beneficial for both systems , with an overall improvement of 2 . 38 % on average over the previous state of the art system . we also use word embeddings ( mtr , mtr and word2 ) in the task - based setting . the word - driven approach is beneficial in both systems . it improves the overall performance of both systems and the overall task - level performance . it reduces the dependency on error prediction .
4 presents the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 . the best performance among all systems is shown in bold . the highest standard deviation among all is 1 . 2 , which indicates that the system performs well in the task .
results are presented in table 3 . our proposed approach outperforms both the original df and docsub embeddings . we observe that our proposed approach significantly improves the performance of the df and slqs datasets compared to the original ones . the results of our approach are summarized in table 1 . we find that our approach significantly outperforms the previous approaches in terms of performance .
results are presented in table 3 . our proposed approach outperforms both the original df and docsub embeddings . we observe that our proposed approach significantly improves the performance of the df and slqs datasets compared to the original ones . the results of our approach are summarized in table 1 . we find that our approach significantly outperforms the previous approaches in terms of performance on both datasets .
results are presented in table 3 . we observe that our proposed approach outperforms all the other approaches except for the one proposed by p < cid : eurparl , term , and docsub . our proposed approach significantly outperforms both the original and the original embeddings of both docsub and europarl . it is clear that the proposed approach is superior to the original docsub approach on both datasets .
3 shows the performance of our system on the three datasets . our system achieves the best performance on all three datasets with a minimum of 1 . 78 % boost on the metric compared to the previous best state - of - the - art system . the results are summarized in table 3 . we observe that our system has the best overall performance on both metric and metric metrics . for metric metrics , we observe that the numberroots metric has the worst performance .
3 shows the performance of our system on the two datasets . our system achieves the best performance on both datasets . it improves upon the maxdepth and maxdepth metrics by 1 . 3pp on the df and tf datasets , while improving on the tf dataset by 2pp . on the df dataset , it achieves the highest performance with a boost of 1 . 5pp on both metric and metric .
1 shows the performance ( ndcg % ) on the validation set of visdial v1 . 0 . lf is the enhanced version of the original visdial model . it obtains the best r0 , r2 and r3 scores , respectively . however , it obtains only the worst r3 score , which indicates that it is better to rely on hidden dictionary learning instead of qt .
2 shows the performance ( ndcg % ) of different ablative studies on different models on different sets of visdial v1 . 0 validation set . the best performing model is the one using the hidden dictionary learning method in section 5 .
5 compares the performance of hmd - prec and wmd - f1 on hard and soft alignments . the results are summarized in table 5 . we observe that hmd prec outperforms both the soft and hard alignments in terms of bert score .
3 presents the results of the direct assessment and bertscore - f1 tasks on the test set of ruse . the results are presented in table 3 . the results of both sets are summarized in terms of direct assessment scores . table 3 shows the performance of the baselines for both sets . for example , the results obtained by ruse ( * ) are significantly better than those obtained by the f1 task , but are slightly worse than those achieved by the svm ( * ) task .
3 presents the bagel and sfhotel scores on the test set . the baselines are presented in table 3 . they are based on bleu - 1 and bertscore - f1 scores and are comparable to the baselines on both sets . however , the difference in performance between the baseline and the baseline is less pronounced for the two sets . for example , the begel score is significantly better than the baseline score on the base set . this suggests that the difference between baseline and baseline scores may be due to the small size of the training set and the large number of training instances in which the training dataset is used .
results are presented in table 3 . the baselines are based on the leic score - recall metric and are comparable to those used in the f1 setting . leic scores are comparable across all three sets , with the exception of the m2 setting , where the leic scores are closer to the baseline . meteor scores are similar across all baselines except for those using elmo and p < 0 . 001 . however , when using bertscorerecall , the results are slightly better than those using spice score - mover .
results are presented in table 3 . we observe that the m0 + para + lang model outperforms all the other models except for the one that relies on the shen - 1 dataset . the results are summarized in table 1 . it can be seen that the combination of the two improves the performance of the model when trained on a single dataset .
results are presented in table 4 . the results are summarized in terms of transfer quality and semantic preservation . semantic preservation is the most important part of the semantic preservation task , and it achieves the best performance with a minimum of performance drop . syntactic preservation is further improved with a drop of 0 . 05 over the previous state - of - the - art setup . for semantic preservation δsim and δpp , the transfer quality quality improvement is achieved with a slight drop of 1 . 5 over the baseline . relis and relis achieve the best results with a transfer quality drop of 2 . 5 . finally , relis achieves the highest transfer quality score with a decrease of 2 points .
5 shows the results of human and machine validation on the test set of yelp and lit . the results are shown in table 5 . it is clear that the human ratings of semantic preservation are high , indicating that the machine and human judgments match the quality of the sentence .
results are presented in table 3 . we observe that the m0 + para + lang model outperforms all the other models except for the m6 model , which shows the diminishing returns of the shen - 1 model when trained on a single dataset . in addition , the performance drop of the m2 model shows that it is better to train on single dataset than on multi dataset .
results on yelp sentiment transfer are shown in table 6 . our best model achieves the highest bleu and the highest acc ∗ on the test set of fu - 1 and fu - 2 , respectively . however , it is not clear whether this is due to different classifiers in use . multi - decoder performs worse than the previous work on both test sets . it is clear from table 6 that the differences in classifiers are due to the nature of the transfer set of the training set . this is evident from the fact that it is restricted to only 1000 sentences , not 1000 . it is also clear from the results of the second set of training set that the classifier in question is used to train the model to interpret the sentiment .
2 shows the precision of nested disfluencies . the number of tokens that are correctly predicted to be disfluent is reported in table 2 . reparandum tokens are typically shorter than repetition tokens , however the number of repetition tokens is higher . the repetition tokens are shorter than the repetition tokens .
3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and the repair ( content - disfluent ) . the fraction of tokens correctly predicted to contain a word is shown in table 3 . it is clear that the disfluency predicted as a function has a significant effect on the prediction accuracy .
results are presented in table 4 . we show the best and worst performance on the single and multi - factor test sets , respectively , compared to the best state - of - the - art model . our model outperforms all the other models in terms of both dev and test mean , with the exception of the single - factor model , which achieves the best performance on both test sets . table 4 shows the performance of our model on both single - and multifactor test set .
2 shows the performance of our model with the state - of - art algorithms on the fnc - 1 test dataset . our model achieves the best performance with both agree and disagree embeddings . it also outperforms both the rnn and rnn - based systems in terms of f1 score .
2 shows the performance of different methods on the apw and nyt datasets for the document dating problem . the best performing method is burstysimdater , which significantly outperforms all previous methods except maxent - joint .
3 shows the performance of our method in terms of word attention and graph attention . it can be seen that the accuracy of our approach is comparable to that of the other two methods .
3 presents the performance of our proposed method on the test set of jvmee . the proposed method outperforms the previous state - of - the - art method on all test sets except for the one in which it is tested . the results are presented in table 3 . we observe that the proposed method performs well on both test set , with the exception of the one that is tested in the second set . it is clear from table 3 that it performs better on the two sets of test sets than the previous ones .
table 1 , we report the results of our method with respect to event identification and classification . our method obtains the best performance on both trigger and event identification tasks . we use the same method with the same classification system . the results are presented in tables 1 and 2 . table 1 shows the results for each event with the different classification methods . the method is presented in table 1 . all the classification methods used with this method yield the best results on both events . the classifier used with the most data is named after a specific event . the classification method is used to classify and classify the objects in the event .
can be seen in table 1 , all but fine - tuned models outperform all the other models in terms of test performance . for example , all except for fine - tuned - lm outperforms all the others in the test set by a significant margin .
4 shows the results on the dev set and on the test set using discriminative training with only subsets of the code - switched data . we observe that fine - tuned train dev outperforms the state - of - the - art on both train test and test set . in both sets , the performance on the full train test set is significantly better than the state of the art .
5 shows the performance of our system on the dev set and the test set . it is clear from table 5 that it performs better on both the dev and test set than the monolingual set .
shown in table 7 , type - aggregated gaze features trained on the three eye - tracking datasets and tested on the conll - 2003 dataset show statistically significant improvement in precision ( p = 0 . 005 ) and f1 - score ( p ≤ 0 . 01 ) .
5 shows the precision and f1 scores for using type - aggregated gaze features on the conll - 2003 dataset . precision ( p ) and recall ( f1 ) are both statistically significant improvements over the baseline ( p ≤ 0 . 05 ) on the pre - trained dataset . f1 - score is also statistically significant at 94 . 03 and 94 . 35 , respectively , for the pretrained dataset ( p < 0 . 01 ) . the f1 score is significantly higher for the trained set ( p > 0 . 00 ) .
results on the original wordnet test set are shown in table 1 . syntactic - sg embeddings are used in wordnet 3 . 1 and wordnet 4 . 1 . they are used to train the semantic skipgram parser , and it uses syntactic embedding as the base . they perform similarly to glove - retro on wordnet , and they use syntactic transferdings obtained in the original paper . they do not rely on syntactic synset embedding , however . they rely on the syntactic skipgrams parser , which improves the performance for the original task . they also rely on semantic embedding . we observe that syntactic mixing improves the results for the golov et al . ( 2015 ) task , but does not improve the performance .
2 shows the performance of our rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . the results are presented in table 2 . it is clear from table 2 that the dependency parser relies on pp attachment prediction features to predict pp attachment predictions .
3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . note that the attention removal reduces the ppa acc . score by 0 . 7 points .
2 shows the results of domain tuning with respect to image caption translation . the results are summarized in table 2 . adding subtitle data improves the multi30k performance by 3 . 5 % over the previous state - of - the - art model . adding domain tuning improves the performance by 2 . 5 % .
3 shows the performance of subs1m in en - de and in - de . the results are presented in table 3 . subdomain - tuned models outperform all the other models in terms of performance . the results of domain - tuning are summarized in table 1 . subdomain tuning improves the performance by 3 . 5 points over the previous state - of - the - art model . domain tuning improves performance by 2 . 3 points . in - de , the results are comparable with that of the original subs , with the exception of mscoco17 .
4 shows the bleu scores in terms of multi30k captions . the results are summarized in table 4 . we observe that the automatic captions outperform the single - attention ones . the results show that the combination of concatenated captions and the combination feature - based captions significantly improve bleus scores .
5 compares the performance of different approaches for integrating visual information . we observe that the enc - gate and dec - gate interfaces outperform the other approaches in terms of bleu % scores . in particular , we observe that decoding the visual information leads to better interpretability . in fact , decoding the information improves interpretability , improving interpretability and performance .
3 shows the performance of subs3m and subs6m in terms of multi - lingual features . the results are presented in table 3 . sub - text - only features outperform all the other features on the en - de dataset , in particular , the improvements on the text - only subset are statistically significant ( p < 0 . 01 ) . the performance of the subs3ms in the single - language subset is comparable to that of subs4m ( p > 0 . 05 ) . moreover , the performance improvement on the compact subset is statistically significant , with the exception of the reduction of noise . finally , we observe that the performance gain on compact subset refers to the quality of the visual features .
3 presents the results of our proposed system on the test set of mtld and en - fr - ff . the results are presented in table 3 . we observe that the proposed system outperforms both the original and the original embeddings in terms of translation performance . table 3 shows the performance of the proposed method .
results in table 1 show that the number of parallel sentences in the train , test and development splits for the language pairs we used is significantly larger than those in the test set .
2 presents the results of our training on the english , french and spanish datasets . the results are presented in table 2 .
5 shows the bleu and ter scores for the rev systems . the automatic evaluation scores ( bleu ) are comparable to ter scores ( 37 . 3 and 45 . 0 ) for the original rev system . ter scores are comparable with en - fr - rnn - rev and en - es - trans - rev , but are slightly better than ter scores .
results on flickr8k are shown in table 2 . the hierarchical supervised model outperforms both segmatch and rsaimage in terms of recall @ 10 .
results on synthetically spoken coco are shown in table 1 . the results are presented in the row labeled vgs . the hierarchical supervised model outperforms both rsaimage and segmatch in terms of recall . it achieves the best performance on both datasets .
1 shows the results of the different classifiers compared to the original on sst - 2 . we report further examples in table 1 . for example , cnn turns in a < u > screenplay that shows the edges at the edges ; it ’ s so clever you want to hate it . dan also shows that the edges of the screenplay are better than the edges in the original . this shows that when the edges are curved , the effect of the word " hate " is less pronounced . it shows that if the word “ hate ” is used to describe hate speech , then you want hate hate hate speech to be removed .
2 shows the results of fine - tuning on sst - 2 . the results are shown in table 2 . we observe that the number of words in the original sentence has increased , decreased or stayed the same . these results indicate that the value of the word " nouns " has not been increased or decreased , indicating the effectiveness of the system .
3 shows the change in sentiment from positive to negative in sst - 2 compared to the original sentence . note that the positive label is flipped to positive and vice versa . this indicates that the effect of the flipped negative label is positive .
results are presented in table 2 . the results are summarized in table 1 . we observe that the performance of sift is comparable to that of pimi ( p < 0 . 005 ) . the performance of ppmi is comparable with that of sst - 2 ( p > 0 . 05 ) . we note that the results are comparable across all three aspects of the evaluation process , with the exception of the positive ones . our results are consistent across all aspects of evaluation .
