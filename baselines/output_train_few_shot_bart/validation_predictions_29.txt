2 : throughput for processing the treelstm model on our recursive framework and tensorflow â€™ s iterative approach , with the large movie review dataset as the training dataset . the results are shown in table 2 . throughput performs the best on training datasets with a large number of iterations . further , it shows better performance on inference with efficient parallel execution of the tree nodes .
1 shows the performance of the balanced and linear datasets compared to the linear ones . as table 1 shows , when the batch size increases from 1 to 25 , the performance is less than that of a linear dataset , but still comparable to the level of performance of a balanced dataset . further using the recursive dataflow graphs , our model exhibits the highest throughput thanks to the high degree of parallelization , but at the same time exhibits the smallest performance improvement .
2 shows the performance of different parameter optimization strategies for each model with different representation . our approach achieves the best performance with different number of parameters in each model . as the table 2 shows , the max pooling strategy consistently performs better in all model variations . moreover , the performance gain from different iterations of the model is less pronounced for different iterations .
1 shows the effect of using the shortest dependency path on each relation type . it can be observed that macro - averaged models achieve the best f1 ( in 5 - fold ) with only sdp as dependency path . however , it is harder to distinguish between these models with different dependency paths . in particular , the difference between accuracy with sdp and diff . is most pronounced in relation type , where sdp is the most important . relation and f1 are the only ones that perform better in 5fold .
performance of the y - 3 models compared to previous stateof - the - art models is presented in table 3 . in general terms , the performance of y - 2 models is significantly better than those of y - 3 : y - 3 shows that the quality of the word - based f1 scores is superior to the average r - f1 score .
paragraph level f1 and average level of f1 are presented in table 4 . the results of the best - performing models are shown in table 5 . our model achieves state - of - the - art results on average with 50 % of the entries considered .
4 shows the c - f1 scores for the two indicated systems at the essay vs . paragraph level . note that the mean performances of the two systems are lower than the majority performances over the runs given in table 2 .
performance of original and new models on the test set is presented in table 4 . the results are presented in tables 1 and 2 . table 4 summarizes the results for each model . original and original models perform better than the other two systems on a single test set . the difference between accuracy on bleu and the original is less pronounced for some models than others . table 4 shows the performance of the models that have been trained on the original and the wrong ones . we observe that the performance obtained on the original dataset is slightly worse than those by the original , but still comparable to the best state - of - the - art model .
shown in table 1 , the original and the cleaned versions have the highest number of distinct mrs , as measured by our slot matching script , see section 3 . we also have the smallest number of instances in our original dataset , which shows the diminishing returns from mixing multiple script fragments .
performance of original and original models on the test set is presented in table 2 . original models perform better than the original models on a single test set while the original model performs slightly worse on the two test sets . the difference between accuracy between original and wrong is less pronounced for the two sets .
manual error analysis we performed on a sample of 100 instances from the original test set . results of the analysis shown in table 4 show that removing incorrect values from the training data leads to a significant drop in accuracy .
model performance on the large scale graphlstm dataset is presented in table 1 . the best performances are achieved by dcgcn ( single ) with a gap of 0 . 2m in performance compared to the previous stateof - the - art models . after applying our data augmentation on the single model , the performance gap between the two improves significantly .
results on amr17 are presented in table 2 . our model achieves 24 . 5 bleu points on the model size compared to the ensemble model of seq2seqb . the results show that dcgcn outperforms both ensemble and single - model models .
results in english - czech are shown in table 1 . the best performing models are published in english , german , french , russian , turkish , turkish and turkish . we also include the best performing model in the english - language , peyrard et al . , 2017 , 2018 , and 2019 , respectively . the results are presented in tables 1 and 2 . they show the results for english and german , both for the single and the multi - language model , as expected , the results are slightly better in english than in german , although still slightly better than in english .
5 shows the effect of the number of layers inside a layer on the performance of the model in table 5 . as table 5 shows , there is a significant imbalance in performance between layers , which shows the diminishing returns from mixing multiple layers .
6 shows the performance of rc - based gcns with residual connections . rc + la shows that dcgcn has strong residual connections with multiple gcns . with residual connections , gcn models perform better than other models with comparable performance . however , when gcn is combined with multiple connections , performance drops significantly .
model 3 shows the performance of the dcgcn models when combined with the minimum number of training instances . as shown in table 3 , dcgcnn models outperform all the other models in terms of model b and model c on both metric and bias metric , while the size of the training data increases with increasing model b performance , the performance decreases with the growth of model c .
8 shows the ablation study results for amr15 on the dev set . it shows that dense blocks reduce the number of connections in the i - th block . this shows that removing the dense blocks does improve the model performance .
show the ablation study results for the different types of modules used in the graph encoder and the lstm decoder . encoder modules use the best performing model , but their coverage mechanism is only slightly better than the best - tuned dcgcn4 decoder modules . the differences in coverage between the two models are minimal , but we note that - global node & linear combination < cid : 22 . 9 and 22 . 4 respectively , respectively , compared to the previous state - of - the - art model .
investigate the effects of different initialization strategies on probing tasks . table 7 presents the results for each initialization strategy . our paper shows that our method performs better than the previous stateof - the - art method on three out of four probing tasks ,
observe that our method outperforms all the alternatives except cbow / 400 in terms of depth and threshold sensitivity . however , it does not have the best performance on the subtense level .
subj and sick - r perform comparably to other methods except cmp . cbow shows significant performance improvement over the strong baselines on mrpc and mpqa . however , it does outperform both sick and subj models in terms of mrpc score . subj models perform better than sst2 , sst5 and sts - b models , respectively . cbow / 784 shows a slight performance drop over other methods , however , the biggest performance gain is on the mrpc dataset , which underscores the competitiveness of subj model in the hybrid and hybrid settings .
observe that cbow and hybrid perform comparably to each other on unsupervised downstream tasks attained by our models . however , on the sts15 dataset , cmp performs slightly better than cmp . cbow shows the relative change with respect to hybrid .
8 shows the performance of different initialization strategies on supervised downstream tasks . our paper shows that the best performing model is sst2 , while glorot performs better on sst - b .
6 shows the performance for different training objectives on the unsupervised downstream tasks . the best performances are on the sts13 and sts15 datasets , while cbow - r performs slightly better on the larger sts16 dataset .
observe that cbow - r outperforms other methods in terms of depth and threshold sensitivity . it obtains the best performance with a gap of 2 . 6 points in accuracy over other methods . however , it does not exceed the upper boundary of coordinv and subjnum , which shows the diminishing returns from mixing syntactic and semantic information . we observe that incorporating these features improves the performance of both methods , but does not improve the overall performance .
subj and sick - r models outperform all other methods except cbow - c except for sst2 . these models perform on par with the best mpqa models , but are slightly superior on mrpc . subj models also outperform the sst5 baseline on multiple metrics , including mrpc score , sst - e score and ssts - b score . cbow improves on both the mrpc and subj models by 3 . 8 points in overall performance compared to sst6 .
system e + org and per are presented in table 1 . all org methods outperform the best state - of - the - art systems in terms of e + per . we observe that the approach that allows the best performance to be combined with the best performing feature set , mil - nd , is the only system that performs better in all aspects . the results of using the org method show that it is better to compile all the information into a single resource than to rely on only one entity , namely , the name matching algorithm . table 1 shows the performance of all the systems that perform better than the other two .
results on the test set under two settings are shown in table 2 . supervised learning improves the general performance of the model in and corresponding models , but does not improve the e + p score significantly . it is clear that the supervised learning model performs better than the original model in terms of general performance . moreover , the accuracy of the models is relatively high , indicating that the model performs well in both settings . we observe that the accuracy obtained by automatic learning is comparable to the performance obtained by supervised learning , however , the difference between supervised learning and supervised learning is less pronounced for both scenarios are statistically significant , showing that the effectiveness of supervised learning may vary depending on the training set . further improving performance by high margins
6 shows the performance of models trained on ref and ref compared to the original embeddings . ref outperforms gen , but g2s - gat performs slightly better than ref , indicating that ref is more effective at generation of keywords . further , models trained with ref as ref or ref pre - trained models have superior performance on the ref test set compared to those without .
results reported in table 1 show that the models outperform the models in terms of bdi and eor on three of the four benchmarks , while on the ldc2017t10 .
results on ldc2015e86 test set are shown in table 3 . the models trained with additional gigaword data outperform the models trained only on external data .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . the results show that the use of bilstm improves the model ' s performance on the low - supervision development set .
results are presented in table 4 . we observe that g2s - gin significantly outperforms other models in terms of sentence length and sentence length as measured by the weighted average number of frames , indicating that the model is more suitable for the task at hand . table 4 also highlights the large difference in sentence length between models that are trained on the same graph diameter and weighted average length of frames . note that the smaller size of the sentence length alone does not impact the performance of the model in the task setting , which shows the diminishing returns from training on data that are already contained in the attachment layer . finally , we observe that , when trained on word - domain embeddings , the model performs better on sentence length than the other two baselines , showing that the syntactic and semantic information injected into the sentence are more important to the model ' s performance .
shown in table 8 , the fraction of elements that are missing in the input graph that are present in the generated sentence is much smaller than those in the output graph , indicating that the model is better at selecting the relevant tokens .
4 shows the performance of our method with respect to target language embeddings . it achieves the best performance with 96 . 7 % accuracy using the 4th nmt layer .
2 : pos and sem tagging accuracy with baselines and an upper bound . accuracies are shown in table 2 . using unsupervised word embeddings improves the semantic performance for word2tag over other classifiers .
results are presented in table 4 . our proposed method outperforms all the base methods except for the one that it performs on par with the original embeddings . we observe that the accuracy obtained by our method is comparable with those obtained by other methods .
5 shows the accuracy with different layers of 4 - layer nmt encoders , averaged over all non - english target languages . we find that combining all the features improves the results for both languages .
performance on different datasets is shown in table 8 . the average age of the attacker is 9 . 7 and the corresponding adversary is 14 . 3 . the difference between the performance of the two is statistically significant , with a difference of 2 . 3 % in accuracy on a training set 10 % held - out .
performance of the models when training directly towards a single task is presented in table 1 . as the table 1 shows , the training performance that directly interacts with the data is significantly better than those that are trained in pan16 .
2 shows the effect of the additional cost term on the balanced and unbalanced model splits . the classifier named pan16 exhibits the worst performance . dial and sentiment both show significant drop in performance , however , the difference is less pronounced in unbalanced and balanced datasets , sentiment and gender are the most prevalent classifiers , we find that the gender - neutral classifier , which affects the performance of the model , has the least effect on the model performance .
performance on different datasets with an adversarial training is shown in table 3 . the difference between the performance of the trained classifier and the corresponding adversary is significant , in particular , it is significant because of the gender difference in the performance . sentiment and emotion are the most important factors in predicting whether an attacker will miss a task or not . dial and emotion alone have a significant impact , but only on predicting whether the target will fail .
6 shows the ablation results for different encoders . embedding leaky is easier for rnn to embed than it is for embedded .
results presented in table 2 show that our approach achieves the best performance on both datasets when trained on the same model . the results also show that the finetune approach further boosts performance on the wt2 dataset , however , it does not improve significantly over the strong lemma baseline on wt2 . we observe that the performance gain on the combined model is modest but consistent with the performance of existing models , yang et al . ( 2018 ) and parallelism ( 2018 ) , both of which outperform previous stateof - the - art models on multiple metrics .
1 shows the performance of our model compared to previous models . the results are presented in table 2 . we observe that our model performs on par with the best models in terms of both acc andbert time . as the results of training on a single dataset show , the model is able to do both the tasks at once , while the time taken to train on the other datasets is considerably less . table 2 also highlights the differences in performance between models with different training set .
3 shows the performance of our model compared to other models trained on the same dataset . we observe that our model performs better than both the original and yelppolar time embeddings , indicating that the model is more suitable for both datasets . the results of " combined time " and " work " are summarized in table 3 .
3 shows the bleu score on the test set of wmt14 english - german translation task . our model improves over previous stateof - the - art models in terms of decoding one sentence , and outperforms both gru and olrn by a noticeable margin .
4 shows the performance of our model with respect to match / f1 score on squad dataset . it can be seen that our model performs better than other models with the parameter number of 2 . 44m . however , it is still inferior to other models that do not have large parameter numbers . the results published by wang et al . ( 2017 ) show that the use of # params improves the model ' s performance when trained with a minimum of training data .
6 shows the f1 score on conll - 2003 english ner task . it can be seen that all the models use the same parameter number in the model , which indicates the performance of the model in the task .
performance of our model on snli with base + ln setting and test perplexity on ptb task with base setting . the results are shown in table 7 . snli models significantly outperform other models in both tasks , with the exception of elrn .
performance of all systems when combined with a single domain is reported in table 4 . word embeddings perform best for both human and system , with the exception of system retrieval . table 4 shows the performance of the word - level systems in terms of evaluation performance . word - based evaluations ( mtr ) significantly outperform human - level evaluations on all metrics when using a single dataset , with a gap of 2 . 5 - 2 . 05 % when using all the data from the last published state - of - the - art systems . sentiment is specific to the task at hand , as it is used to train the system ' s semantic relations with the relevant domain . in general terms , all the words used for this analysis seem to belong to the same domain , although the differences are less pronounced for the other systems .
4 : human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 2 . 3 is highlighted in bold . the best performance among all the automatic systems is ranked in top 1 or 2 for overall quality .
3 shows the performance of all models trained on the new embeddings . our model outperforms the competition on every metric by a significant margin . for example , our model performs slightly better than the baseline on three of the four datasets , while on the other four datasets it performs slightly worse than the other two .
3 shows the performance of the models trained on the corpus dataset compared to the previous stateof - the - art models . our model outperforms all the other models except for those using docsub embeddings . the difference between the performance on corpus and docsub is minimal but significant , reaching the upper end of the range of 0 . 005 and 0 . 4057 on average , respectively , for the three models performing slightly better than the other two .
3 shows the performance of all models trained on the new embeddings . our model outperforms the competition on every metric by a significant margin . for example , our model performs slightly better than the baseline on three of the four datasets , while on the other four datasets it performs slightly worse than the other two .
metrics are presented in table 3 . our system achieves the best performance with a minimum of 3 . 5roots compared to the maxdepth of our baseline model , europarl . on the other hand , our system performs slightly worse than our maxdepth model , on par with the performance of other baselines , such as docsub and hclust .
3 shows the performance of our model compared to other methods . our model achieves the best performance on three of the four metrics : maxdepth , relativedepth and depthcohesion . europarl achieves the highest performance on both metrics : 9 . 43 % truedepth and 1 . 1 % averagedepth , respectively , compared to the previous best state - of - the - art metrics .
performance of our model on the validation set of visdial v1 . 0 . 0 is shown in table 1 . compared to the enhanced visdial model , lf performs better in terms of answer score sampling and hidden dictionary learning .
performance ( ndcg % ) of different ablative studies on different models on the visdial v1 . 0 validation set is shown in table 2 . using only p2 exposes the model to hidden dictionary learning . this shows the performance that using p2 can further improve the model ' s interpretability .
5 compares the performance of these models on hard and soft alignments . the results are summarized in table 5 . for the hard alignments , hmd - prec improves significantly , while wmd - lv - en improves marginally .
3 presents the results of our approach with respect to baselines . the results are summarized in table 3 . our approach significantly outperforms the baseline on three of the four metrics by a significant margin . the most striking thing about our approach is that it significantly improves upon the baseline bertscore - f1 score by 0 . 685 on average compared to the strong lemma baseline on ruse ( * ) baseline .
performance of baseline models on the bleu - 1 and sfhotel benchmarks is presented in table 2 . the results are summarized in table 1 . our proposed approach significantly improves the performance by increasing the precision of the baseline models by 0 . 3 points over the baseline baseline on all metrics , from 0 . 005 to 0 . 021 .
performance of the models according to these baselines is reported in table 3 . the results are summarized in table 1 . the summaries obtained by the models generally outperform the baselines on three of the four metrics . the difference is most prevalent in the m2 setting , where the average bert score of 0 . 939 and 0 . 749 respectively outperforms the baseline on all metrics except for those using elmo . morph - mover improves performance by 0 . 8 points over the strong lei - based baseline on m2 .
3 shows the performance of the models trained on the shen - 1 dataset compared to the previous stateof - the - art models on the simuli - vec dataset . as table 3 shows , for m0 and m6 models , their performance is slightly better than those using pure word embeddings , however , it is still slightly worse than m1 and m2 : m0 has significantly worse performance than the other two models . table 3 also highlights the differences in performance between state of the art models . for example , m1 has the better performance of m2 while m2 has the worse performance .
results are presented in table 4 . semantic preservation and transfer quality are the most important aspects of our model performance . the results of the best performing models are summarized in tables 4 and 5 . we observe that the semantic preservation approaches significantly improve over the syntactic preservation baseline , the difference is less pronounced in the final set , as the results of using semantic preservation improves , the performance is less consistent with the sentiment of the other two sets . syntactic preservation improves over semantic preservation ,
human ratings of accuracy and recall are shown in table 5 . it is clear from table 5 that both language and human evaluations have poor performance on these metrics . however , the performance of the human evaluations is high , indicating that the accuracy obtained by the model can be further improved with a better interpretability baseline .
performance of the models compared to previous stateof - the - art models on the test set is presented in table 4 . as can be seen , all models trained on the shen - 1 dataset only slightly outperform the baseline on all test set except for those using pure - language embeddings .
results on yelp sentiment transfer are shown in table 6 . our best model achieves higher bleu than any prior work at similar levels of acc âˆ— , but only slightly outperforms the best model using the classifier yang2018unsupervised . we also observe that the language embedding model performs slightly worse than the simple - decoder model , however , this is mostly due to different classifiers in use .
statistics for nested disfluencies are shown in table 2 . the percentage of repetition tokens that were correctly predicted as disfluency is slightly higher than the rate at which repetition tokens were predicted as fluent . we also observe that the number of errors for each repetition token is slightly less than the frequency of the repetition tokens .
3 shows the relative frequency of rephrases correctly predicted as disfluent for each category . as table 3 shows , the number of tokens predicted to contain a content word is less than the number predicted as containing a word in either the reparandum or the repair word . however , the fraction of tokens that contain a word is more than 50 % less likely to be misfluent in either category .
results are presented in table 4 . we observe that the best performing model is text + innovations , improving the model ' s performance by 0 . 2 points over the single model . the results also indicate that the use of text and innovations in the development of the model is beneficial for boosting model performance .
perform slightly better than the state - of - art algorithms on the fnc - 1 test dataset . table 2 shows the performance of our model on the test dataset in the low - supervision settings . it is clear from table 2 that our proposed method significantly improves the model ' s performance in the conflict and non - conflict settings .
2 shows the performance of different methods on the apw and nyt datasets for the document dating problem . it significantly outperforms all previous models except for burstysimdater .
3 shows the performance of our method with and without attention . it shows the effectiveness of word attention and graph attention for this task . compared to ac - gcn , it obtains 63 . 9 % better performance .
model performance in table 1 shows that all models trained on the proposed jnn framework outperform the baseline on every stage except for the one where jnn embeddings are tested . we observe that for all models , the performance gap between the baseline models is less than 2 % on average .
1 and table 2 show the results of all methods for each domain . our method outperforms the other methods in terms of both event identification and classification . all methods cause a significant drop in performance when using a single domain , hence leading to different classifiers contributing differently to the performance .
can be seen in table 1 , all but fine - tuned models perform better than the original spanish - only model when trained with only the word embeddings in the pre - trained model . also , the difference in performance between english - only and french - only models is less pronounced for all but fine - tuned - lm , which requires significantly less training data .
4 shows the results on the training set with only subsets of the code - switched data . fine - tuning achieves the best results with a 25 % train dev and 75 % train test improvement .
performance on the dev set and on the test set is shown in table 5 . as expected , fine - tuning gives the best performance , but monolingual performance suffers . fine - tuned - disc achieves a comparable performance with the best - tuned model , but it is slightly better .
results in table 7 show that type - aggregated gaze features significantly improve recall for the three eye - tracking datasets tested on the conll - 2003 dataset . however , the biggest performance drop is for type combined gaze features , which shows significant performance drop .
5 shows the precision and f1 scores for using the type - aggregated gaze features on the conll - 2003 dataset . while the improvement is statistically significant , it is less striking for type combined with the presence of the additional features .
results on belinkov2014exploring â€™ s ppa test set . we use glove - retro embeddings for wordnet 3 . 1 . the results on this test set are shown in table 1 . the syntactic - sg embedding gives the best performance . however , it does not outperform the syntactic ones . we observe that it is harder to learn the semantic features of skipgram than the original wordnet , and it requires significantly more training data . we also observe that syntactic embedding improves the performance of wordnet .
performance of our system using various pp attachment predictors and oracle attachments is presented in table 2 . however , the best performance comes from the combination of oracle pp and lstm - pp , which results in significantly better ppa acc . score . the results also show that using oracle pp also improves the predictive performance of the model .
3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . it shows the diminishing returns from removing the sensitivity from the models .
2 : adding subtitle data and domain tuning for image caption translation ( marian amun et al . , 2018 ) with subsfull embeddings improves the results for all models except for the one that requires domain tuning . adding the multi30k dataset improves the performance for both models .
1 shows the performance of the models trained on the en - de dataset compared to subs1m on the large - scaleickr16 dataset . the results are presented in table 1 . table 1 shows that the domain - tuned models perform better on the larger - scale datasets , with the exception of flickr17 , where the improvements are more concentrated on the high - scale settings . subdomain - tuning improves the results for all but the small - scale ones , with respect to captions , the performance remains the same on all datasets with a marginal drop of 0 . 3 % compared to the previous state of the art model .
4 shows the bleu scores of models using automatic captions . we show the results in table 4 . as expected , the better results with only one or all 5 captions are obtained . however , the improvement is slim , indicating that the use of concatenated captions leads to less variation in performance .
5 compares our approach with prior approaches on en - de and capt - de . results are summarized in table 5 . we observe that incorporating dec - gate and enc - gate improves the picture quality , but does not improve the overall picture .
performance of subs3m compared to subs6m is presented in table 4 . as the table shows , the visual features captured by the models perform best on the largerickr16 dataset , while the performance of the standalone models is slightly worse on the smallerickr17 dataset . in particular , the presence of the multi - lingual features boosts performance over the baselines , with the exception of the aspect - of - speech embeddings , the performance remains the same for all the other models except for the one using the word " intensemble " .
3 shows the performance of the different models compared to the original ones . as the table indicates , all the models trained on the word embeddings outperform their counterparts on mtld . table 3 shows that the differences in performance between the original and the alternative are minimal .
shown in table 1 , the number of parallel sentences in the train , test and development splits for the language pairs we used .
2 : training vocabularies for the english , french and spanish data used for our models .
system reference bleu and ter scores for the rev systems are shown in table 5 . the automatic evaluation scores ( bleu ) consistently show lower performance than the baseline en - fr - rev or en - es - rev .
2 shows the performance of our visually supervised model compared to the naive rsaimage embeddings . as can be seen , the effectiveness of our model is relatively high , indicating that our approach is well - equipped to handle the task .
experimental results on synthetically spoken coco are shown in table 1 . the models trained on the embeddings of chrupala2017representations are presented in bold . the average rank of the models is 0 . 0 , compared to the mean of the two baseline models . the difference in rank between baseline models is less pronounced for acoustic2vec - u ,
1 shows the results of different classifiers compared to the original on sst - 2 . for example , dan turns in a < u > screenplay that shows the edges at the edges ; it â€™ s so clever you want to hate it . table 1 shows that for cnn , the edges of the screenplay are edges edges , and for rnn , they show the shapes of the word " hate it " . as the table 1 shows , if you want hate hate hate , you can use it as a noun in the screenplay . if you want it to hate you will hate it .
2 shows the part - of - speech ( pos ) changes in sst - 2 . these numbers indicate that the number of occurrences in the sentence has increased , decreased or stayed the same through fine - tuning respectively . the results are shown in table 2 .
3 shows the sentiment changes in sst - 2 from positive to negative . the change in sentiment between positive and negative sentiment shows that the effect of the negative label is less pronounced on the original sentence .
table 2 , we report the performance of our method on the test set of pubmed andpubmed . results are presented in table 2 . overall , the results are slightly better than those of other methods , indicating that the approach has a high impact on interpretability . however , the difference between positive and negative evaluations is less pronounced for sst - 2 and other methods . table 2 presents the results of different approaches for different aspects of the evaluation .
