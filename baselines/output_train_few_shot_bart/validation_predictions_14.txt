2 : throughput for the treelstm model on the recursive framework , and tensorflow ’ s iterative approach , with the large movie review dataset as the training dataset . the recur approach performs the best on inference and training , while the inference approach shows better performance on training .
results in table 1 show that the balanced dataset exhibits the highest throughput , but still performs slightly worse than the linear dataset .
2 shows the performance of the max pooling strategies for each model with different representation . the performance of conll08 outperforms softplus and sigmoid in all but one of the four scenarios . the hgnll08 model has the best performance in all four scenarios , with the exception of the one in which it has the worst performance .
1 shows the effect of using the shortest dependency path on each relation type . our model achieves the best f1 ( in 5 - fold ) with sdp compared to macro - averaged model , and the best diff . with sdp as dependency path , the model performs slightly better than the macro model , but is slightly worse than the baseline model .
results in table 3 show that the y - 3 model outperforms both the r - f1 and f1 scores by a significant margin .
3 presents the results of our method in terms of paragraph level and f1 scores . the results are presented in table 3 . our method outperforms all the other methods except for mst - parser , where it achieves the best results .
4 shows the c - f1 scores for the two indicated systems ; essay vs . paragraph level . note that the mean performances are lower than the majority performances over the runs given in table 2 .
3 shows the performance of our system on the original and the second set of test sets . the results are presented in table 3 . our system outperforms all the other test sets except for bleu , which is better than the original . the performance of the original is slightly worse than the other two systems .
1 compares the original e2e dataset with our cleaned version . the results are summarized in table 1 . the number of distinct mrs , total number of textual references , and the number of slot matching scripts as measured by our slot matching script are shown in tables 1 .
results are shown in table 1 . original and original systems outperform the best performing baselines in all but one of the four categories . the best performing ones are bleu , nist and sc - lstm , while the others are slightly better .
4 shows the results of manual error analysis on a sample of 100 instances from the original test set of tgen . we found that adding incorrect values significantly increased the chance of errors in the training set .
3 presents the results of our model on the external and external datasets . the results are summarized in table 3 . the best performing models are the dcgcn and tree2str ( song et al . , 2016 ) and the snrg ( song and cohen , 2018 ) models .
2 presents the results on amr17 . our model achieves 24 . 5 bleu points and achieves 50 . 4 bleu points . the results are summarized in table 2 . our model outperforms all the other models in terms of parameters , with the exception of seq2seqb .
3 presents the results for english - german and english - czech . the results are presented in table 3 . the best performing models are bow + gcn ( bastings et al . , 2017 ) and birnn ( beck et al . 2018 ) . the results show that the combination of single and multi - language models can improve the english - language performance by a significant margin .
5 shows the effect of the number of layers inside dc on the performance of the model with respect to layer expansion . we observe that for all layers , there is a significant drop in performance compared to the baseline , which indicates that the model is more balanced .
6 : comparisons with baselines with residual connections . rc + la ( 2 ) and gcn ( 4 ) show that the gcn has residual connections with other gcns . however , when using residual connections , gcn shows a drop in performance .
3 shows the performance of the dcgcn model compared to the previous state - of - the - art models in terms of bias metric . the results are summarized in table 3 .
8 shows the ablation study results for amr15 in terms of the density of the connections in the dev set . the results show that removing the dense blocks reduces the performance of the model .
9 presents the ablation study for the graph encoder and the lstm decoder . the results are summarized in table 9 . the best performing model is the dcgcn4 decoder , which has the best coverage in the world .
7 shows the performance of our initialization strategies on probing tasks . our model outperforms all the other approaches except for subjnum , which shows the diminishing returns from mixing different tokens .
3 presents the results of our method in terms of depth and subtraction . the results are summarized in table 3 . the best performing method is h - cbow / 400 , which obtains the best performance on both metric . it also achieves the best results on the subtraction metric . however , it does not achieve the best result on the depth metric .
3 presents the results of our method with respect to subj and mrpc . the results are summarized in table 3 . our model outperforms all the other methods except sst2 and sst5 by a significant margin . it also outperforms both sst1 and sick - b in terms of recall .
performance on unsupervised downstream tasks attained by our models is shown in table 3 . the best performance is achieved on sts12 and sts14 , where cmp performs better than cmp . cbow shows the relative change with respect to hybrid .
8 shows the performance of our initialization strategies on supervised downstream tasks . our model outperforms all the baselines except subj and mpqa . it also outperforms both the mrpc and trec baseline .
6 shows the performance for different training objectives on the unsupervised downstream tasks . cmow - c outperforms cbow - r in all but one of the four scenarios .
results in table 3 show that cbow - r outperforms all the other methods in terms of depth and subtraction .
3 presents the results of our method on the subj and mpqa datasets . the results are summarized in table 3 . our model outperforms both the sst2 and sst5 datasets in terms of performance . however , it is still better than both the mrpc and sick - r datasets in both cases .
3 presents the results of our system in ( italic ) e + org and per . the results are presented in table 3 . all org scores are reported in tables 1 and 2 . the results show that our system outperforms all the other systems in terms of e - loc , e + per and misc scores .
results on the test set under two settings are shown in table 2 . the best performing system is mil - nd , which improves the e + p score by 2 . 5 points over the previous state - of - the - art model . it also improves the f1 score by 3 . 3 points .
6 : entailment ( ent ) and conference ( neu ) in the model outperform ref and ref in both cases . in particular , ref outperforms ref when combined with ref .
results are presented in table 3 . the best performing models are g2s and gat , which outperform all the other models in terms of bleu score .
3 shows the results on ldc2015e86 test set when models are trained with additional gigaword data . our model outperforms all the other models in terms of performance .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . the results show that bilstm significantly improves the model ' s performance when combined with a larger number of training examples .
results are presented in table 4 . we observe that g2s - gin has the best overall performance on both datasets , with the exception of the one in which it has the worst overall performance . the results are summarized in table 5 . in particular , we observe that the average sentence length of the models is significantly shorter than those of the other models , indicating that the model is more suitable for larger datasets .
8 shows the fraction of elements in the output that are not present in the input ( added ) and fraction of elements that are missing in the generated sentence ( miss ) . these results are shown in table 8 . they show that the g2s - gin model outperforms all the other models in terms of fraction and miss .
4 shows the performance of the models trained with different target languages on a smaller parallel corpus ( 200k sentences ) . the best performance is obtained using the 4th nmt encoding layer .
2 : pos and sem accuracy with baselines and an upper bound . mft : most frequent tag ; unsupemb : most frequently tag ; word2tag : upper bound encoder - decoder .
results are shown in table 4 . the results are presented in bold . our pos tagging accuracy model outperforms all the other methods except for the one in which it performs better .
5 shows the accuracy with different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages .
performance on different datasets is shown in table 8 . the difference between the attacker score and the corresponding adversary ’ s accuracy is reported in δ ( differences in performance between the two datasets ) .
results in table 1 show that when training directly towards a single task , the training accuracies are significantly higher than those of pan16 .
2 shows the performance of our classifier pan16 in the context of balanced and unbalanced data splits . the results are summarized in table 2 . the classifier consistently outperforms both the classifier and the target in both instances .
3 shows the performance on different datasets with an adversarial training . the difference between the attacker score and the corresponding adversary ’ s accuracy is shown in table 3 . the performance on the training datasets with a gender - neutral approach is significantly higher than those without .
6 shows the concatenation of the protected attribute with different encoders . the results are shown in table 6 . embedding leaky is easier than embedding guarded .
3 presents the results of our work on the model with respect to finetune . the results are summarized in table 3 . our model outperforms all the other models in terms of both model performance and model finetuning .
results are shown in table 5 . this model outperforms all the other models in terms of training time and time . it also outperforms both the lstm and bbert datasets by a significant margin . in fact , it is the only model that has achieved the best performance on both datasets .
3 shows the performance of our model compared to previous work on four different datasets . our model outperforms all the other models in terms of err and f1 scores .
3 shows the bleu score on wmt14 english - german translation task . our model outperforms all the other models by a significant margin . in particular , it has the advantage of decoding one sentence in a single task .
4 shows the performance of our model with respect to match / f1 score on squad dataset . our model outperforms all the models except lrn and sru by a significant margin . the results published by wang et al . ( 2017 ) show that our model is better than both the lstm and atr models .
6 shows the f1 score on conll - 2003 english ner task . lstm * denotes the parameter number in the reported result ( lample et al . ( 2016 ) . lrn also shows the performance improvement ( f1 score of 90 . 94 % ) compared to the original model .
results are shown in table 7 . our model obtains the best performance with base + ln setting and test perplexity on ptb task with base setting . the results are reported in tables 7 and 8 .
results are shown in table 4 . word embeddings ( mtr ) w / system retrieval and mtr w / multi - task r - 2 are the best performing methods for both systems . in general , the performance of both systems is comparable , with the best performance by human in both systems ( r - 2 and b - 4 ) . the performance of the human - based system is comparable with that of the system - based systems ,
4 : human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 ( best ) . the best performance among all systems is shown in bold , with statistical significance marked with ∗ in table 4 .
results are shown in table 4 . we observe that the best performing corpus models are europarl and ted talks , while the best ones are docsub and docsub . these models outperform all the other models in terms of performance , with the exception of docsub having the worst performance .
3 shows the performance of all the models trained on the corpus dataset . the results are presented in table 3 . our model outperforms all the other models in terms of performance . the best performing models are df , docsub and docsub , while the worst performing are docsub .
results are shown in table 4 . the results are presented in tables 1 and 2 . we observe that the best performing corpus models are europarl , df , docsub and hclust , while the worst performing docsub models are eurparl and term .
3 shows the performance of our model compared to the previous best state - of - the - art models . the results are summarized in table 3 . our model outperforms all the baselines except for the ones that do not rely on metric embeddings , such as docsub , docsub and docsub . in comparison , our model achieves the best performance on all metrics .
3 shows the performance of our model compared to other baselines . the results are summarized in table 3 . our model outperforms all the baselines except for the ones that do not belong to the corpus dataset , namely , docsub , docmax and hclust . we observe that our model achieves the best performance on both metric metrics .
performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . lf outperforms both the original visdial model and the enhanced version by a significant margin . in fact , it is better than the original model by a large margin .
2 shows the performance ( ndcg % ) of different ablative studies on different models on the visdial v1 . 0 validation set . the best performing model is lrva , which relies on hidden dictionary learning .
5 compares the performance of hmd - prec and wmd - f1 on hard alignments . the results are summarized in table 5 .
3 presents the results of our approach in terms of direct assessment and bertscore - f1 . the results are summarized in table 3 . our approach obtains the best results with the best performance on both sets . our model outperforms all the baselines except for the one in which it obtains a better performance .
3 presents the bleu - 1 and bertscore - f1 scores on the baselines . the results are summarized in table 3 . the baselines show that both the baseline and the corresponding baselines are superior in terms of performance on both sets . however , the results are slightly worse than those on the other baselines , indicating that baselines do not perform well on the two sets .
3 presents the baselines on the m2 and m2 datasets . the summaries are presented in table 3 . leic ( * ) and bertscore - recall have high precision on both datasets . meteor and spice have low precision on all metrics , which indicates that the performance of the models may vary depending on the setting .
results in table 3 show that the m0 + para + lang model outperforms the previous state - of - the - art models in all but one of the four cases . in particular , it outperforms m1 and m2 by a significant margin .
3 presents the results of our model on the transfer quality and semantic preservation datasets . the results are summarized in table 3 . the results show that the model with the best transfer quality is better than the one with the worst performance on the semantic preservation dataset . semantic preservation outperforms all the other models with a large margin of error .
5 presents the results of human sentence - level validation . the results are summarized in table 5 . our model outperforms both the human and the machine in all but one of the four cases .
results in table 3 show that the m0 + para + lang model outperforms all the other models except for those that do not use the word embeddings . in fact , it is the only model that performs better than the other ones that do .
results on yelp sentiment transfer are shown in table 6 . our best model achieves the highest bleu and the highest acc ∗ scores among all the classifiers in use . the worst performing model is yang2018unsupervised , which means it is unable to distinguish between 1000 and 1000 sentences . this indicates that the use of multiple classifiers can impact the model ' s ability to interpret sentiment .
2 shows the performance of nested disfluencies in predicting reparandum tokens . the average number of tokens that are correctly predicted to be disfluent is 3 . 5 .
3 : relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and the repair ( content - content ) . table 3 shows the percentage of tokens predicted to belong to each category , with a gap of 0 . 5 % in the number of tokens predicting to belong in each category . the fraction predicted to contain a single content word is lower than the fraction predicted as containing a single word .
model outperforms all the other models in terms of dev and innovations , with the exception of the single model , which has the best performance on the test .
2 compares our model with the state - of - art algorithms on the fnc - 1 test dataset . the results are shown in table 2 . our model achieves the best performance with both agree and disagree embeddings . it also outperforms both the rnn and rnn - based systems in the micro f1 ( % ) test set .
2 shows the performance of different methods on the apw and nyt datasets for the document dating problem . the best performing method is burstysimdater , which outperforms all previous methods .
3 compares the performance of our model with and without attention . the results are shown in table 3 . the best performing model is ac - gcn , which has the highest f1 score .
3 presents the performance of our model in each of the three stages . our model outperforms all the other models except for the one in which it performs the best .
3 presents the results of our method in the context of cross - event . our method outperforms all the other methods in terms of both event identification and event classification . in both cases , the method is more effective in both cases .
results are shown in table 4 . all except for fine - tuned - lm are better than all the other models except for the ones that do not use concatenated word embeddings .
4 shows the results on the dev set and on the test set using discriminative training with only subsets of the code - switched data in the training set .
5 shows the performance on the dev set and the test set , compared to the monolingual model . our model outperforms both the dev and test set in both cases .
results in table 7 show that type - aggregated gaze features significantly improve recall , f1 - score and recall scores for the three eye - tracking datasets , respectively .
5 shows the precision and f1 scores for using type - aggregated gaze features on the conll - 2003 dataset . precision ( p < 0 . 01 ) and recall ( p ≤ 0 . 05 ) are statistically significant improvements over the baseline , while f1 - score is significantly lower .
results on belinkov2014exploring ’ s ppa test set are shown in table 1 . syntactic - sg embeddings are used in wordnet 3 . 1 and wordnet 4 . 0 . they have high performance on ppa tests , and are comparable to the original wordnet . they also outperform glove - retro , which is used in the original work . the hpcd ( full ) is comparable with the original wordnet , but it has lower performance on the ppa set . it also outperforms the original method in the validation set .
2 presents the results of our system with features from various pp attachment predictors and oracle attachments . the results are summarized in table 2 . the best performing system is the lstm - pp model , which has the highest ppa accuracy . it also outperforms the hpcd model by a significant margin .
3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . the results are shown in table 3 . it is clear that the loss of context sensitivity has a significant effect on performance .
2 : adding subtitle data and domain tuning for image caption translation ( bleu % scores ) and adding domain tuning ( marian amun et al . , 2018 ) . the results are summarized in table 2 . subsfull domain tuning improves the multi30k performance by 3 . 7 % compared to the previous state - of - the - art .
3 shows the performance of subs1m in en - de and out - of - domain settings . the results are summarized in table 3 . sub - domain - tuned models outperform all the other models except for mscoco17 , which shows the diminishing returns from domain tuning . the results of domain - tuning outperform both the english and german models in both cases .
4 shows the bleu scores of models using automatic captions . the best ones are en - de , flickr16 and mscoco17 , while the worst ones are in - de .
5 compares the performance of different strategies for integrating visual information . we observe that the best performing enc - gate model outperforms both the en - de and dec - gate models in both datasets .
3 shows the performance of subs3m on the en - de dataset compared to subs6m in the multi - lingual setting . the results are summarized in table 4 . the best performance is obtained by combining the visual features with the semantic features of the embeddings . sub - coco also outperforms all the other models in terms of performance .
results in table 3 show that the best performing en - fr - ht model outperforms both the best and worst performing alternatives in terms of translation performance .
results in table 1 show that the number of parallel sentences in the train , test and development splits for the language pairs we used is much larger than the number in the training splits .
2 : training vocabularies for the english , french and spanish data used for our models . the results are shown in table 2 .
5 shows the bleu and ter scores for the rev systems compared to the original ones . the automatic evaluation scores ( bleu ) are significantly better than ter , but still slightly worse than en - fr .
results on flickr8k are shown in table 2 . the vgs model outperforms both segmatch and rsaimage in all but one of the cases .
results on synthetically spoken coco are shown in table 1 . the best performing model is audio2vec - u , which achieves a 3 . 9 % higher recall rate than rsaimage .
1 shows the results of the different classifiers compared to the original on sst - 2 . for example , cnn turns in a < u > screenplay that has edges at the edges ; it ’ s so clever you want to hate it . in the second example , dan shows that it is easier to turn in a screenplay screenplay than it is to turn it into a on ( in the margins ) .
2 presents the results of fine - tuning on sst - 2 . the results are summarized in table 2 . they show that the number of occurrences in the original sentence has increased , decreased or stayed the same , indicating that the quality of the sentence has not been improved .
3 shows the change in sentiment from positive to negative in sst - 2 . the results are shown in table 3 . positive labels are flipped to positive and vice versa . they show that the effect is less pronounced on cnn than on dan .
results are presented in table 3 . the results are summarized in bold . the most striking thing about the results is that there is a gap in performance between positive and negative evaluations . this suggests that the approach is more effective than either positive or negative evaluation . in addition , the results are more consistent across all aspects of the study , with the exception of pimi .
