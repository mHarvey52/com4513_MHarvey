2 : throughput for processing the treelstm model on our recursive framework and tensorflow ’ s iterative approach , with the large movie review dataset as our training example . as table 2 shows , the approach performs the best on inference with a large number of training instances . further , it shows better performance on training instances with fewer training instances than in the traditional approach . the recur and iteration methods show comparable performance on inference tasks with the traditional four - step model . table 2 also shows the performance of the iteration and recur methods , which combine the best features of the recursive and iterative approaches .
shown in table 1 , the balanced dataset exhibits the highest throughput thanks to the high degree of parallelization . however , it does not improve as well as the linear dataset , because there is only a small room of performance improvement left to train with a larger number of dataflow graphs . table 1 shows the overall performance of the treernn model , using a balanced dataset and a multilingual dataset .
2 shows the performance for each model with different hyper parameters . the maximum number of hyper parameters is chosen based on the best performing filtering rate and the number of feature maps . softplus performs better with different number of parameters than softplus , indicating that the max pooling strategy is more effective in boosting the model ' s performance . sigmoid performs similarly to softplus in all three scenarios , with the boost being greater than the dropout rate . retrieving the max parameters from the hyper parameters pooling stage with the optimal values also boosts the model performance .
1 shows the effect of using the shortest dependency path on each relation type . it can be observed that , when using only one dependency path , the model achieves the best f1 ( in 5 - fold ) with sdp and diff . with the same dependency path as in the previous experiment , it can further improve the model ' s f1 by 21 . 11 points without sacrificing too many relation types .
results are shown in table 3 . y - 3 significantly outperforms the other two models in terms of percentage of f1 , with the exception of r - f1 .
3 presents the results of our method in terms of paragraph level . our approach achieves the best results on both tests with a minimum of 50 % of the entries . our method outperforms all the other methods except for the one in which it is tested .
4 shows the c - f1 scores for the two indicated systems at the essay vs . paragraph level . note that the mean performances for the lstm - parser system are lower than the majority performances over the runs given in table 2 .
3 shows the performance of our system on the original and wrong test sets . our system outperforms all the other systems except for the one that contains tgen . the results are presented in table 3 . table 3 compares the results of all the models that have been trained on the original and false test sets . the results of the best performing model , sc - lstm , are shown in table 1 . all the other models except for those that do not have access to the original data are statistically significant ( table 3 ) . the only exception is svm , which performs slightly worse than the original on both tests .
results for the original and the cleaned versions are shown in table 1 . the number of distinct mrs , total number of textual references , and the number of slot matching scripts as measured by our matching script , see section 3 . in the original e2e dataset , there are 8 , 862 instances ( 62 . 5 % vs . 39 . 5 % ) . the cleaned version has 15 % more instances ( 59 . 5 % ) than the original , which shows the diminishing returns from mixing multiple scripts . table 1 shows the results for both sets .
performance of original and original models on the test set is presented in table 4 . the results are presented in tables 1 and 2 . original results are shown in bold . they outperform all the other methods except for the one that takes the pre - trained tgen model . table 4 compares the results of the original and the original models on all test sets . the difference between original and original is minimal , however the difference between original and original models is significant . the only exception to this is that the original model is slightly better on bleu than in the original . it is clear from table 4 that the original model is superior on both test set .
4 shows the results of manual error analysis on a sample of 100 instances from the original test set . in general terms , the errors in our system are unacceptably high ( p < 0 . 01 ) , we found no significant difference in the absolute numbers of errors found in the original training set , which shows the diminishing returns from mixing training data .
table 1 , we compare our dcgcn model with previous stateof - the - art models on the external and bias metric . the first set of models performs significantly better than the other two on both metric , with an absolute improvement of 3 . 8 points over the previous state of the art model .
2 shows the results on amr17 . our model achieves 24 . 5 bleu points on the model size compared to the previous best state - of - the - art model , seq2seqb ( beck et al . , 2018 ) . with respect to ensemble models , we observe that dcgcn achieves a comparable performance to the best state of the art model from the start .
3 shows the results for english - language and english - czech . our model outperforms all the other models with a large margin . the results are shown in table 3 . we notice a drop in performance between the single and the multi - news set , the difference between the two sets is minimal , peyrard et al . , 2017 , also outperforms the bow + gcn model in both languages with a gap of 3 . 5 points in performance from the previous state of the art model . our model achieves the best results with a 17 . 2 % overall improvement on average compared to the previous best state - of - the - art model .
5 shows the effect of the number of layers inside the dc stack on the performance of the model . table 5 shows that for every layer with at least one layer , there are two more layers that contribute strongly to the performance .
6 shows the performance of rc - based gcns with residual connections . with residual connections , gcn models generally outperform all baselines except dcgcn2 .
model 3 shows the performance of the dcgcn models compared to other stateof - the - art models . our model outperforms all the other models in terms of both bias metric and cias metric by a large margin .
8 shows the ablation study results for amr15 . it can be seen that the dense blocks reduce the number of connections in the dev set . further , the reduction in the density of the connections decreases the bias metric by 2 . 8 points .
shown in table 9 , the global encoder and the lstm decoder have remarkably similar performance on the graph encoder . the fact that the global network has the greatest coverage allows the models to achieve remarkably high precision on both the graph and the decoder is a significant improvement over the previous stateof - the - art .
investigate the effects of different initialization strategies on probing tasks . we show in table 7 the results for " depth " and " topconst " on the initialization strategies . the results show that the subtraction strategies have a significant impact on the performance of the probing tasks , as measured by the number of tokens in the subjnum .
can be seen in table 1 , the subjnum based method outperforms all the other methods except for cbow / 400 . it is clear from table 1 that the subtraction method has superior performance on both subtraction and subtraction metrics .
1 shows the performance of our model compared to other methods . our model outperforms all the other methods except subj and mpqa except for sst2 . cbow even outperforms sst5 and sst - b on mrpc , while sick - b has the advantage of training on a larger corpus . we observe that the cbow / 784 model has the best performance on both mrpc and subj .
results on unsupervised downstream tasks attained by our models are shown in table 3 . hybrid models outperform both cbow and cmp . cbow on almost all downstream tasks , except for those in the low - supervision settings . on the sts15 and sts16 datasets , cbow shows a slight improvement over the performance of hybrid models .
8 shows the performance for all initialization strategies on supervised downstream tasks . our paper shows that the best performing model is sst2 , which improves upon the performance of subj and mpqa by 3 . 8 points .
table 6 shows the performance for different training objectives on the unsupervised downstream tasks . the best performances are on the sts12 and sts15 datasets , where cbow - c performs best . however , the worst performance is on sts16 , where cmow - r performs worse than cbow .
can be seen in table 1 , the topconst method outperforms all the other methods except cbow - c in terms of precision . it is clear from table 1 that cbow has superior performance on both subtense and subtense subtasks . however , it does not have the best performance on the subtense / tense subjnum level . this shows the diminishing returns from mixing subtasks with other sophisticated methods . cbow improves upon the performance of cbow by 3 . 8 points in subtense mode .
subj , sst2 and sst5 models outperform all the other methods except for cbow - c . subj improves upon the strong performance of subj by 3 . 8 points on mrpc and sick - r . also , it boosts performance by 2 . 9 points over the sst - b model , which shows the effectiveness of selective attention .
system org and per scores are shown in table 1 . all org scores are computed using the best performing system ( mil - nd ) . table 1 shows the eorg scores for all systems except for those using plain averaged word embeddings . table 2 shows the performance of all systems using only one org score . name matching and multi - factor learning methods significantly outperform the baselines in all aspects except for org . supervised learning methods use only one type of information , namely , name matching . it is clear from table 1 that the quality of the data obtained by the system is relatively high , indicating that the system performs well across all domains . further improving upon the performance by using cross - domain learning methods is crucial for the success of the system .
results on the test set under two settings are shown in table 2 . name matching and supervised learning achieve high e + p scores ( 95 % confidence intervals ) and higher e + f1 scores ( 87 % . 03 % ) . supervised learning , on the other hand , achieves lower e − p score ( 87 . 03 % ) , 87 . 03 % f1 score ( 83 . 03 % ) and 83 . 12 % e + r score ( 86 . 53 % ) . further improvements are reported in tables 2 and 3 . these results show that the transfer learning method can improve the generalization ability of the system .
6 : entailment ( ent ) and ref compared to gen ( neu ) are presented in table 6 . all models except g2s - gat outperform their ref counterparts in terms of ref scores , ref and gen scores are substantially better than gen scores on all models except for those that use ref .
table 3 , we compare our model with previous work on the ldc datasets . the results are summarized in table 3 . the best results are reported in tables 1 and 2 . table 3 summarizes the results of the models on the 10 datasets . our model outperforms all the previous models except for the one that has been published in the previous literature .
3 shows the results on ldc2015e86 test set when trained with additional gigaword data . our model outperforms the previous stateof - the - art models in both external and logaword datasets .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . the results show that the use of bilstm improves the model ' s performance by 3 . 5pp over the strong lemma baseline .
results are shown in table 3 . the first set of results show that the g2s models have the best overall performance when compared to other models in terms of sentence length . from the second set , we also see that the model has the worst performance on the 20 - 40 δ and the 50 - 240 δ . table 3 shows the results of applying the weighted average sentence length and the average number of frames taken to compute the sentence length of the sentences . after applying these weighted average sentences , the model outperforms all the other models except for those using word - based tags . sentence length drops significantly as the model grows , but is still comparable with the other two .
8 shows the fraction of the elements in the output that are not present in the input graph that are missing in the generated sentence ( g2s - gin ) . it is clear from table 8 that the use of token lemmas has a significant impact on the performance of reference sentences .
4 shows the performance of our method with respect to target languages . we use the 4th nmt layer , trained with 200k sentences as inputs and tested on a smaller parallel corpus ( 200k sentences ) . the results are in table 4 .
2 : pos and sem tagging accuracy with baselines and an upper bound . accuracies are reported in table 2 . word2tag classifier uses unsupervised word embeddings and a lower bound encoder - decoder . sem tags have a higher accuracy rate than mft .
results in table 1 show that our method outperforms the competition on all metrics when combined with the accuracy of the other two methods . the results are presented in tables 1 and 2 .
5 shows the accuracy with different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . the results are clear from table 5 , when we add in all the features from the four layers of the encoder .
8 shows the performance of an attacker on a training set containing 10 % held - out participants . the difference between the attacker score and the corresponding adversary ’ s accuracy is shown in table 8 . in pan16 , the average age of the participants is 9 . 7 % and the average gender is 8 . 7 % .
1 shows the performance of our system when training directly towards a single task . our system obtains high performance across all three domains , with the exception of the one that requires the most training data .
2 shows the status of theected attribute leakage in the context of balanced and unbalanced data splits . the classifier pan16 ( which pretends to be trained on gender - neutral data ) is the most representative of the classifier , with a balanced task score and an unbalanced task score . dial and speech tags have the least significant effect , with the exception of the sentiment tab . sentiment tags have a significant impact on the task performance , with an absolute boost of 2 . 6 points in the overall score .
performance on different datasets with an adversarial training is shown in table 3 . the difference between the attacker score and the corresponding adversary ’ s accuracy is reported in δ ( differences are statistically significant with age , gender , and type of training ) . the classifier trained on pan16 is gender - neutral , meaning that the classifier is more likely to classify the targets as abusive . the training time is shorter than the training time for both groups .
6 shows the performance of the different embeddings for different encoders . for example , rnn is consistently better than the embedding model , while the rnn model is worse .
1 shows the performance of our model compared to other models using finetune modeling . our model outperforms all the other models with a large improvement in performance . the results are summarized in table 2 . we show that our model performs on par with the best stateof - the - art models on both the ptb and the wt2 dataset . it is clear from table 2 that the combination of feature - rich and dynamic modeling methods improves the results for both models . finally , we show the results of phase 2 on the final model of lrn .
1 shows the performance of our model compared to previous stateof - the - art models on the base acc andbert datasets . the results are summarized in table 1 . our model significantly outperforms previous models in both the base acc and thebert dataset by a significant margin . with respect to the model ' s training time , we observe that it is better to train with a shorter training time than the previous state of the art model .
3 shows the performance of our model compared to previous stateof - the - art models . the results are summarized in table 3 . our model outperforms all the other models in terms of both yahoo and google time err . yelppolar time is the better time - based model , while amafull time is more accurate . table 3 compares our model with the original lstm on both datasets . it is clear from the results that the amapolar time dataset is superior in both datasets to the original ones . finally , the results of " combined time " and " other time " metrics indicate that the model performs better in both cases . we observe that the difference in performance between the two datasets is less pronounced for this model than for other models .
3 shows the bleu score on wmt14 english - german translation task . our model improves upon the state - of - the - art model with a 2 . 67 % improvement over the performance of sru and atr . further improvements are expected in the final translation task , which takes a considerable amount of training time .
4 shows the performance of our model on squad dataset compared to the results published by wang et al . ( 2017 ) . the model performs better than all the base models except for those using elmo embeddings . with respect to match / f1 score , we observe that our model obtains the best performance with a minimum parameter number of 2 . 67m and a maximum f1 score of 79 . 83 . further improving performance by adding additional parameters to the model .
6 shows the f1 score of our model on conll - 2003 english ner task . it can be seen that our model significantly improves upon the performance of the previous stateof - the - art models by 3 . 8 points .
results in table 7 show that our model can distinguish between the performance of base + ln and test perplexity on snli task with base setting .
3 shows the performance of all systems trained on the word - based system . word - based systems ( mtr ) and system retrieval ( r - 2 ) are the most effective for both systems . table 4 shows the results of all the methods trained on this data . wordbased systems use the best performing system , with a minimum of 0 . 01 % error reduction on average . sentiment is used on all systems except for the one where it is used for the task at hand ( table 4 ) . word based systems use a high level of performance , with an average error reduction of 2 . 05 % on average compared to the previous state of the art . the word based system use the most efficient system on the data , and is used to train the system for multi - task learning . in general terms , the wordbased system is more effective than the system - based one . it is more suitable for combination of human and machine learning .
4 presents the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 . the best performance among all the systems is reported in table 4 .
3 shows the performance of all models trained on the same dataset . our model outperforms all the other models except for the one that we trained on it , eurparl , in all but one of the cases . the results are in table 3 . for df , we use the best performing model , europarl ( p < 0 . 005 ) and its variant , pt , on the other hand , performs slightly worse than the others .
3 shows the performance of all models trained on the same dataset . our model outperforms all the other models except for the one that we trained on it , eurparl , in all but one of the cases . the results are reported in table 3 . for df , we use the best performing model , europarl ( p < 0 . 005 ) and its variant , pt , on the other hand , performs significantly worse .
3 shows the performance of all models trained on the same dataset . our model outperforms all the other models except for the one using the best performing embeddings . the results are summarized in table 3 . for df , our model performs slightly better than the others on three of the four datasets . the difference between our model and the other two is minimal , but significant .
3 shows the performance of our model compared to the previous best state - of - the - art models on the three datasets . our model achieves the best performance on all three datasets , with a gap of 1 . 78 points between the official score and that of europarl . on the other hand , our model is significantly worse than the other two baselines in terms of depthcohesion . the difference between our model and the other baselines is minimal , but we see significant improvement on both datasets .
3 shows the performance of our model compared to the previous best state - of - the - art systems on the three datasets . our model achieves the best performance on all three datasets , with a gap of 1 . 5 points between the official score of eur - parl and europarl . on the other hand , our model is significantly worse than the other two baselines in terms of metric and depthcohesion . the difference between our model and the other baselines is minimal , but we see significant improvement on the metric due to the high coverage of the maxdepth .
performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . we note that the enhanced version of our model ( lf ) performs better than the original one .
2 shows the performance ( ndcg % ) of different ablative studies on different models on the visdial v1 . 0 validation set . we apply p2 as the most effective method , followed by coatt and rva as the low - frequency ones .
5 compares the performance of these models on hard and soft alignments . the results are summarized in table 5 . for example , hmd - recall + bert shows a slight improvement over the performance on hard alignments on ruse .
3 presents the results of our approach on the three sets . the first set is presented in table 3 . the results of the first set are summarized in terms of the average number of words for each setting , followed by the scores of the second set . our approach significantly improves the results for both sets , with the exception of the case of burkhard - keller , where the improvement is less pronounced . the second set , ruse - f1 , achieves an average score of 0 . 7 on the direct assessment and 0 . 8 on the baseline . the difference between the scores for brevity and absolute improvement is minimal , however significant for the comparison set , which is more consistent with the strong lemma baseline . the third set , sent - mover , achieves a performance improvement of 3 . 1 points on average compared to the previous set .
3 presents the bagel and sfhotel scores on the test set of sent - mover . the results are summarized in table 3 . the results of bleu - 1 and bertscore - f1 are significantly better than the performance of other baselines on both sets . these results are statistically significant even when using only plain averaged word embeddings . table 3 summarizes the results of the models trained on the pre - trained and unsupervised baselines . they show that the combination of high precision and high recall scores improves the scores for both baseline and baseline models .
performance of the models according to these baselines is reported in table 3 . the results are summarized in bold . the summaries obtained by the models generally outperform the baselines on all metrics except for those using pre - trained word embeddings . meteor scores are relatively high , however , on the m2 dataset , they are slightly higher than the leic scores . word - mover significantly outperforms the other models using elmo and p < 0 . 001 .
3 shows the performance of all models trained on the word embeddings . all models except m1 outperform the strong lemma baseline on both datasets except for those trained on simuli - viii . the performance of m2 and m3 is very similar , with the exception of m3 , where it has more data on the shen - 1 cluster .
results are shown in table 4 . semantic preservation and transfer quality scores are the most important aspects of semantic preservation . the results are summarized in tables 4 and 5 . syntactic preservation and semantic preservation are the top three aspects of the semantic preservation dataset . yelp , m7 and m7 achieve remarkably similar results , however , for semantic preservation , the results are only slightly better than those obtained by the other three models . complicate transfer quality features the semantic preservation features of the two datasets are strongly concentrated in one domain , yelp is the only one that achieves a better performance on both domains . relis is the most concentrated under - performed by all the other models , complicating transfer quality is the high performance on semantic preservation datasets .
human ratings of semantic preservation are shown in table 5 . table 5 shows the results of human evaluation on the three datasets for grammatical accuracy . it is clear from the table that the quality of the semantic preservation evaluations is high , indicating that the accuracy obtained by using these tools can be further improved with the use of additional tools .
3 shows the performance of the models trained on sim and pp . table 3 shows that the m0 model outperforms the m2 model on all metrics except for the shen - 1 score . it is clear from table 3 that the combination of syntactic and semantic information boosts performance for both models .
results on yelp sentiment transfer are shown in table 6 . the best models achieve higher bleu than those using the simple - transfer model ( yuan et al . , 2018 ) . the worst performing model is yang2018unsupervised , which means it is unable to distinguish between 1000 and 1000 sentences . tweets with different classifiers in use , such as delete / retrieve , multi - decoder , and retrieve achieve the highest acc score . sentiment embedding has a generally positive effect on sentiment transfer , but it is less significant than using the original embeddings . we also observe that using the classifier " retrieve " improves the generalization ability of the model , but does not improve the acc ∗ score .
statistics for nested disfluencies are shown in table 2 . reparandum length and number of tokens in the row are the most important measures for prediction accuracy . the number of reparandum tokens that are correctly predicted to be disfluent ( i . e . 1 , 2 , 3 ) is the smallest of the three types . in addition , the number of repetition tokens is the largest , at 8 .
3 shows the relative frequency of rephrases correctly predicted as disfluent for each category . the number of tokens predicted to contain a content word is in parentheses , indicating that the disfluencies in the reparandum are localized . table 3 also shows the fraction of tokens that contain a word in the reparandum and the repair function .
results are shown in table 4 . all models trained on the multi - news dataset outperform the best state - of - the - art models on every metric with α measured in the lab . the models using the best model in terms of text and innovations have the advantage of having the best overall dev score . table 4 shows the results for single and multi - news datasets , with text as the most important part of the model . in particular , the model with the best dev score is better than the model using text + innovations .
2 compares our model with the state - of - art algorithms on the fnc - 1 test dataset . our model achieves the best performance with a reasonable level of accuracy considering the fact that the topic is unrelated and the number of instances in which the embeddings are not used is relatively small .
2 shows the performance of different methods on the apw and nyt datasets for the document dating problem . the best performing method is burstysimdater , which significantly outperforms all previous methods .
3 shows the performance of our method with and without attention . it is clear from table 3 that neuraldater relies on word attention and graph attention for its task .
1 / 1 and 2 / n show the performance of models trained on state - of - the - art models . our model outperforms all the models except for the one that relies on pre - trained embedding + t embeddings . on the other hand , we observe that the model trained on dmcnn has the worst performance on all stages .
3 shows the test set for each event . all the methods used for this task are described in table 1 . table 1 shows the results for all three stages . the method used for the event is described in terms of classification and event identification . the method has the best performance on both types of event , with the exception of the case of " f1 " in which the system relies on word embeddings from the same domain . cross - event classification is the most representative of the three stages , with an absolute difference of 0 . 01 % from the previous state of the art . in all cases , the method performs better than the other two methods .
can be seen in table 1 , all the models trained on the same training set are shown in bold . all except for the one that relies on word embeddings and pre - trained models ( cs - only , fine - tuned , and fine - tuned ) are comparable in terms of performance on the dev perp and test acc tasks . note that the difference in performance between english - only and spanish - only is less pronounced for all models .
results on the dev set and on the test set are shown in table 4 . fine - tuning with only subsets of the code - switched data achieves a significant improvement over the performance of fine - tuned , which shows the effectiveness of selective training . selective training also boosts train dev performance by 2 . 8 % in the training set and in the full train test .
5 shows the performance on the dev set compared to the test set . fine - tuned - lm achieves a better performance than monolingual , comparable to the performance of fine - tuned - disc . the difference in performance between dev and test set is less pronounced for the two languages ,
results in table 7 show that type - aggregated gaze features significantly improve recall and f1 scores for the three eye - tracking datasets tested .
5 shows the precision and f1 scores for using the type - aggregated gaze features on the conll - 2003 dataset . the results are summarized in table 5 . type - aggregation features significantly improve recall ( p ≤ 0 . 01 ) and f - score ( p > 0 . 05 ) . the type combined features also improve f1 score ( p ≈ 0 . 03 ) by a factor of 1 . 01 .
results on belinkov2014exploring ’ s ppa test set are shown in table 1 . syntactic - sg embeddings are the best performing method for wordnet , and they have high performance on the ppa set . they also outperform glove - retro and wordnet 3 . 1 in the hybrid embedding stage , which means that the semantic embedding obtained by using skipgram has a high impact on the performance of wordnet . wordnet is based on the original wordnet dataset , and it has the advantage of pre - trained wordnet vectors .
2 shows the performance of our system using various pp attachment predictors and oracle attachments . the results are presented in table 2 . using oracle pp as the dependency parser , rbg improves upon the state of the art lstm - pp model by 3 . 8pp in uas and ppa acc .
3 : effect of removing sense priors and context sensitivity ( attention ) from the model . the effect of removing context sensitivity is shown in table 3 .
2 shows the results for en - de and mscoco17 . adding subtitle data and domain tuning for image caption translation ( bleu % scores ) improves the overall results for both models . adding the multi30k dataset boosts the overall score by 3 . 5pp over the previous state - of - the - art model . adding domain - tuned data boosts the performance by 3pp . with the addition of domain tuning , we get a boost of 4 . 3pp .
1 shows the results of domain - tuned subs1m on the en - de and mscoco17 datasets . the results are summarized in table 1 . table 1 shows that , when domaintuned , the model performs better than the other models on both datasets , with the exception of flickr16 , where the improvements are larger . domain - tuning improves the results for all models , with a slight improvement over the baseline on the spanish - based variant .
4 shows the bleu scores of models using automatic captions . the best models are en - de , flickr16 , and mscoco17 . adding multi30k captions improves the results for all models except for those using dual attn . the model with the best bleus scores is better than the model without .
5 compares the performance of our approach with prior approaches on en - de and on flickr16 . transformer and multi30k + subs3mlm perform similarly on both datasets , as shown in fig . 5 , the combination of enc - gate and dec - gate achieves the best results , with a bleu score of 43 . 57 / 71 . 86 on the two datasets . using transformer also improves the results for both datasets .
results are shown in table 4 . the best performing subs3m model is the one that comes with the text - only embeddings and the multi - lingual model , which allows the model to rely less on visual features . moreover , the model with the most sophisticated semantic features is able to distinguish between text - and non - text - based features , leading to a better overall performance .
table 1 , we compare the quality of our models with the best stateof - the - art systems on the mtld and chime datasets . the results are presented in tables 1 and 2 . table 1 .
results in table 1 show that the number of parallel sentences in the train , test and development splits for each language is significantly higher than the number in the standard training set .
2 : training vocabularies for the english , french and spanish data used for our models . the results are summarized in table 2 .
system reference bleu and ter scores for the rev systems are shown in table 5 . automatic evaluation scores ( bleu ) consistently show lower performance than ter and en - fr - rev .
2 shows the performance of our visually supervised model compared to the standard rsa image on flickr8k . the row labeled vgs is the median rank of our model , and is 0 . 0 . compared to the mean of our standard model , our model has a higher f1 score .
results on synthetically spoken coco are shown in table 1 . the model trained on the embedded embeddings of chrupala2017representations is significantly better than the similarly supervised audio2vec - u model . the difference is less pronounced for rsaimage , however , for vgs .
1 shows the results of different classifiers compared to the original on sst - 2 . for example , dan ( which takes the forms of ellipsis ) turns in a < u > screenplay that shows the edges at the edges ; it ’ s so clever you want to hate it . table 1 shows that dan also makes use of the edges edges of a screenplay screenplay .
2 shows the part - of - speech ( pos ) changes in sst - 2 : , , and indicate that the number of occurrences have increased , decreased or stayed the same through fine - tuning respectively . table 2 also shows the percentage of words that have been added to the original sentence that have not been added . these numbers indicate that there is a need to fine - tune the sentence to obtain the correct answer .
3 shows the change in sentiment from positive to negative . sentiment changes in sst - 2 are very small , i . e . negative sentiment is less prevalent than positive sentiment , it is clear from table 3 that the flipped sentiment labels are important to boosting sentiment .
table 2 , we present the results of our method on the positive and negative evaluations . the results are summarized in table 2 . the positive evaluation results are significantly better than the negative ones , indicating that the method has a high quality of interpretability . table 2 also highlights the competitiveness of the method with respect to topic identification . it is clear from table 2 that the use of the word " sift " improves interpretability by a large margin . in addition , the performance increase by 10 % on the negative evaluations is consistent with the quality of the sst - 2 reward . this suggests that the approach is more effective for target targets .
