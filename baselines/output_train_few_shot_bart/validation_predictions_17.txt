2 shows the performance of our recursive approach on the large movie review dataset compared to the iterative approach of recur . as table 2 shows , the approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training . with respect to the inference and recur datasets , we can also see that the use of gpu exploitation improves the performance on inference .
shown in table 1 , the balanced dataset exhibits the highest throughput thanks to the high degree of parallelization . however , it does not improve as well as the linear dataset , because there is only a small room of performance improvement left to train the system .
2 shows the performance of each model with different hyper parameters . as table 2 shows , the max pooling strategy performs better in all models with different representation . in all but one case , it achieves the best performance with different number of hyper parameters .
1 shows the effect of using the shortest dependency path on each relation type . with sdp as dependency path , our model achieves the best f1 ( in 5 - fold ) with sdp and the macro - averaged model improves the f1 by 21 . 82 % . with the same dependency path as in the previous experiment , we can see that our model performs better than the other models that rely on sdp for dependency generation .
results are shown in table 3 . in general terms , y - 3 achieves 100 % f1 and 50 % r - f1 50 % , in particular , it achieves 50 % . on the other hand , for non - empty datasets , y3 achieves only 50 % of the f1 50 % .
3 shows the results of our method on the paragraph level . our method outperforms all the other methods except for those using word embeddings . the results of the best performing method are reported in table 3 . as expected , all the results obtained by the method are in terms of f1 . however , the results are slightly worse than those obtained by mst - parser .
4 shows the c - f1 scores for the two indicated systems ; for the lstm - parser , it is 60 . 62 ± 3 . 54 and 69 . 24 ± 2 . 87 respectively compared to the majority performances of the other systems .
3 shows the performance of our system on the original and wrong bleu test set . the results are presented in table 3 . we show that our system performs on par with the best stateof - the - art systems on all three datasets except for the one where it is completely wrong . the performance on the wrong test set is reported in table 4 .
shown in table 1 , the original and the cleaned versions have the highest number of distinct mrs in the original e2e dataset and the number of instances in the cleaned version as measured by our slot matching script , see section 3 .
3 shows the performance of our system on the original and the variant variants . the results are presented in table 3 . original and variant tgen models outperform the original on all metrics except bleu and cider . in addition , the performance on the variant with the worst performance is slightly better than the variant without the best performance . table 3 summarizes the results of our proposed system . we concatenate the original with the errors from the original without re - scoring the variant .
4 shows the results of manual error analysis on a sample of 100 instances from the original test set of tgen . we found that the added errors caused by slight disfluencies in the training data are statistically significant ( p < 0 . 001 ) . further , we found that adding incorrect values to training data caused a significant drop in performance ( p > 0 . 01 ) .
3 shows the performance on the external and external datasets compared to the previous state - of - the - art models . for the external dataset , we report the performance of all the models except for seq2seqk ( konstas et al . , 2017 ) and tree2str ( learnt and schmidhuber , 2018 ) . for the graphlstm dataset , all models perform better than all the other baselines except for the two that do not belong to the corpus .
2 shows the results on amr17 . our model achieves 24 . 5 bleu points in terms of parameters , compared to the previous best state - of - the - art model , seq2seqb ( beck et al . , 2018 ) . with respect to the ensemble model , we note that the number of parameters in our model grows with each model iteration .
results in english - german and english - czech are shown in table 4 . the best performing model is bow + gcn ( bastings et al . , 2018 ) . we observe that both the english - language and the german - language embeddings perform better than the single model . we also observe that the bow - gcn model outperforms both the single and the multi - language model in terms of bias metric . with respect to english - koch , we observe that all the models trained on the single - language corpus outperform the two - language models in english .
5 shows the effect of the number of layers inside dc on the performance of the system when we layer them with layers of layers . as table 5 shows , for example , when layers are added to a layer of layers , the effect is less pronounced than when they are only contained in one layer .
6 shows the performance of concatenated gcns with residual connections . with respect to residual connections , we observe that dcgcn2 ( 27 ) outperforms all other gcns in terms of b - level performance .
model 3 shows that dcgcn outperforms all the other models in terms of bias metric by a significant margin .
8 shows the ablation study results on the dev set of amr15 . it can be seen that the dense blocks in the i - th block have a significant impact on the model ' s performance . the reduction in the number of dense blocks shows that the removal of the dense connections leads to a better model .
9 shows the ablation study for the graph encoder and the lstm decoder . the results are shown in table 9 . we observe that the hierarchical nature of the word encoder leads to different coverage mechanisms for different modules , in particular , the two types of coverage mechanisms seem to have the most impact on the performance of the two models .
7 shows the performance of our initialization strategies on probing tasks . our proposed method outperforms the previous stateof - the - art method on all three tasks except for one .
1 shows the results of our method on the subtraction test set . our method outperforms all the other methods except for cbow / 400 , which shows the diminishing returns from mixing different subjoints .
3 shows the performance of subj and sst2 on the mrpc test set . subj outperforms all the other methods except for the one using sick - e . subj performs better than sst5 on all three test sets except for sst6 . sst7 performs better on the sst1 test set than on the other two .
results on unsupervised downstream tasks attained by our models are shown in table 3 . hybrid models outperform both cbow and cmp . cbow shows the relative change with respect to hybrid models . however , when using cmp as a parameter , the difference between the two models is less pronounced .
8 shows the performance of the initialization strategies on supervised downstream tasks . our model outperforms all the baselines except subj and mpqa . it achieves the best performance with sst2 and sst5 initialization strategies .
6 shows the performance for different training objectives on the unsupervised downstream tasks . cmow - c improves on the sts12 and sts14 tasks by 3 . 6 points over the previous state of the art cbow - r .
1 shows the results of our method on the subtraction test set . cbow - r shows that it obtains the best performance on both subtraction and subtraction tests . however , it does not achieve the best results on both subsets . on the subjnum test set , we observe that cbow has the worst performance on all subsets , with a drop of 3 . 6 % on subsets from the previous state of the art .
3 shows the performance of subj and sst2 on the mrpc test set . cbow - r outperforms all subj methods except for sst5 , which is comparable to sst - b and sick - r . subj also outperforms ssts - b in terms of recall .
3 shows the e + and per scores of all systems trained on the italic e + dataset . all org scores are reported in table 3 . in [ italic ] e + org and e + per scores , the system performs better than the previous state - of - the - art systems on all three metrics . supervised learning outperforms all other approaches except for the one that does not use the org feature . with respect to e + org scores , we report the results of the best performing system , τmil - nd , in all but one case . we observe that the superior performance of all the systems that rely on org is due to the high quality of the training data .
results on the test set under two settings are shown in table 2 . name matching improves the e + p scores by 2 . 5 % and f1 scores by 69 . 12 % in [ italic ] e + f1 and 82 . 12 ± 1 . 15 in supervised learning by mil - nd ( model 2 ) . with respect to recall scores , the system performs better than the previous state - of - the - art model on all metrics except recall scores .
6 : entailment ( ent ) on the g2s - gin model . ref and ref are the only ones that ref performs better than ref . in addition , ref significantly outperforms ref in all but one of the cases where ref is used . note that the epm models do not rely on ref or ref because ref does not perform well in all cases .
3 shows the performance of the models trained on the ldc2017t10 dataset compared to the previous best stateof - the - art ldc2015e86 model . the results are presented in table 3 . the results of our model outperform all the other models except for the one that we chose for this study .
3 shows the results on ldc2015e86 test set when models are trained with additional gigaword data . our model outperforms all the previous models except for konstas et al . ( 2017 ) in terms of transfer learning .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . bilstm significantly outperforms the previous stateof - the - art models in terms of size and recall .
results are shown in table 3 . we observe that the g2s model significantly outperforms the other models in terms of sentence length and sentence length . sentence length drop significantly from 0 - 20 δ to 5 - 13 δ and the average sentence length drop consistently from 25 - 40 δ to 28 . 5 δ . finally , the performance drops significantly from the previous state - of - the - art model ( g2s - ggnn ) on the 20 - 50 δ and 50 - 240 δ datasets . in these cases , we see that the performance drop significantly due to the large size of the word embeddings in the graph diameter graph and the smaller sentence length in the memoria graph . note that the drop in sentence length indicates that the model is more sensitive to semantic information than syntactic information , which suggests that it is better to focus on sentence length than semantic information .
8 shows the fraction of elements in the output that are not present in the input ( g2s - gin ) and the fraction of elements that are missing in the generated sentence ( miss ) . in the ldc2017t10 test set , we use gold as the token lemmas for the reference sentences .
4 shows the performance of the four models trained with different target languages on a smaller parallel corpus ( 200k sentences ) .
2 shows the accuracies of mft and word2tag with baselines and an upper bound . the results are shown in table 2 . for mft , the upper bound embeddings are the most frequent tags , while for word2tag , the lower bound is the most accurate .
results are presented in table 4 . table 4 shows the performance of all the methods on the test set . our proposed method outperforms all the other methods except for the one described in the previous section . the results of our method are summarized in terms of accuracy . we observe that our method significantly outperforms both the previous methods on all metrics .
5 shows the accuracy with different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages .
performance on different datasets is shown in table 8 . the average age of the trained adversary is 9 . 7 % and gender is 14 . 3 % respectively . in pan16 , the average age is 15 . 3 % . sentiment is the difference between the accuracy of the target and the corresponding adversary ’ s current state of the art .
1 shows the accuracies when training directly towards a single task . for pan16 , we trained with the word " sentiment " . sentiment is the most difficult part of the task to predict . however , it is easier to predict whether the conversation will be positive or negative .
2 shows the status of theected attribute leakage in the context of balanced and unbalanced data splits . sentiment and gender are the most prevalent classifiers for this data , dial and sentiment are the only classifiers that are able to detect the presence of gender in the data . sentiment is the second classifier that allows the classifier to detect instances of gender - neutral speech in the generated data . the classifier pan16 is the only one that can detect instances where gender is the most important part of the conversation .
performance on different datasets with an adversarial training is shown in table 3 . sentiment is the difference between the attacker score and the corresponding adversary ’ s accuracy . dial and sentiment are predictive of the performance of a given classifier , and gender is predictive of whether the classifier is abusive or not . the gender - neutral classifier pan16 is less predictive of a classifier .
6 shows the concuracies of the protected attribute with different encoders . embedding leaky is easier for rnn to learn than it is for leaky to learn .
3 shows the performance of our model on the 3rd and 4th baselines . it can be seen that our model outperforms the previous stateof - the - art models on all three baselines with respect to finetune . we also observe that our approach outperforms both the original lstm and the original sru model by a significant margin . the results of the second and third baselines are summarized in table 3 . our approach achieves the best performance on both baselines when trained on the same training set .
5 shows the performance of our model compared to previous models . we report the average time taken to train our model . the results are shown in table 5 . it can be seen that the lstm model is significantly faster than previous models in terms of training time . with respect to training time , it can be observed that this model is more accurate in both the training time and the time to train the model .
3 shows the performance of our model compared to other models trained on the amapolar time dataset . our model outperforms all the other models in terms of err performance by a significant margin . we observe that our model performs better on both yahoo and yelp datasets , with the exception of amafull time , which is slightly worse than our model . the results of our second study are summarized in table 3 . we report the results of the second study , which compares our model against the previous state of the art model .
3 shows the bleu score on the wmt14 english - german translation task . as table 3 shows , all models trained on the p100 outperform the previous state - of - the - art models in terms of decoding one sentence . in particular , the gru model outperforms all the other models except for the sru model .
4 shows the performance of our model on squad dataset . with respect to parameter number , we observe that our model obtains the best performance with respect to match / f1 score . we also observe that the sru model outperforms all the baselines except for the lstm model , which obtains a f1 score of 79 . 83 / 83 . 83 . however , the improvements over the previous state of the art model are not statistically significant ( p < 0 . 01 ) .
6 shows the f1 score on conll - 2003 english ner task . the lstm models outperform all the other models in terms of parameter number . lrn shows the best performance with a f1 of 90 . 56 . however , it does not outperform the gru model in the ner tasks .
results are shown in table 7 . snli model achieves the best performance with base + ln setting and test perplexity on ptb task with base setting .
results are shown in table 4 . word embeddings ( mtr ) w / system retrieval and word2vec features are used in all systems except for human . sent attention is primarily focused on word2vec feature - rich systems , with a marginal effect on sentence quality . in general terms , all systems trained on the human dataset are considered to be error - free , with the exception of the one in the human dataset , where all systems are evaluated using the word2 vec feature . all systems trained only on human datasets are pretrained , with only one exception : the system in which the system is trained on is the one that is not part of human dataset .
4 shows the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 8 . the best performing system is seq2seq , with a total of 25 . 2 % overall improvement over human evaluation .
3 shows the performance of all the models trained on the corpus dataset . our model outperforms all the other models except for those trained on docsub . we observe that our model performs on par with the best performing docsub and docsub datasets , but on the df dataset , it performs slightly worse than the other two models .
3 shows the performance of all the models trained on the corpus dataset compared to those trained on docsub . the results are summarized in table 3 . for corpus , we use the best performing model , europarl , while the other models perform slightly worse . for docsub , our model outperforms all the other baselines except for the two that we trained on corpus . we observe that the performance on corpus is consistent across all three models , with the exception of the one that was developed on the df dataset .
3 shows the performance of all the models trained on the corpus dataset compared to those trained on docsub . the results are summarized in table 3 . our model outperforms all the other models except for the two that we trained on corpus : docsub and eurparl ( p < 0 . 005 ) . for corpus , we see that the performance achieved by our model is comparable to the performance obtained by the original docsub model .
3 shows the performance of our model compared to the previous best state - of - the - art on all metrics . our model outperforms all the baselines except for the two that embed maxdepth : eurparl has the worst performance on both metrics , while it achieves the best performance on the metric .
3 shows the performance of our model compared to the previous best state - of - the - art on all metrics . our model outperforms all the baselines except for the two that embed maxdepth : eurparl has the worst performance on both metrics , while it achieves the best performance on the other metrics .
performance ( ndcg % ) comparison for the experiments of applying our system on the validation set of visdial v1 . 0 . lf is the enhanced version of the original visdial model , while s [ italic ] r3 denotes hidden dictionary learning . with respect to r2 and r3 , we use r0 and r2 weights to denote regressive loss , weighted softmax loss , and binary sigmoid loss .
2 shows the performance ( ndcg % ) of different ablative studies on different models on the visdial v1 . 0 validation set . the best performing model is lrva , which relies on hidden dictionary learning .
5 compares the performance of these models on hard and soft alignments . the results are summarized in table 5 . the hmd - recall model outperforms all the other models except for fi - en .
3 presents the results of our approach with respect to direct assessment and eureor . our approach obtains the best performance on all metrics with a minimum of 0 . 7 % difference in performance compared to the baseline bertscore - f1 score . the results are summarized in table 3 . we observe that our approach significantly outperforms the baselines using ruse ( * ) and sent - mover features .
performance on bagel and sfhotel sets is reported in table 3 . we observe that the baselines trained on bleu - 1 and bertscore - f1 are significantly better than those trained on the other baselines . the results are summarized in table 1 . the bleu - 1 metrics are significantly worse than the baseline , but the difference between the two sets is less pronounced for both sets . sent - mover and smd sets significantly outperform the baseline on both sets ,
performance on all metrics is reported in table 3 . the summaries obtained by baselines are summarized in bold . leic and bertscore - recall have the highest performance on both metrics , with the exception of m2 . meteor and spice have the worst performance , with a gap of 0 . 7 % on the m2 metric . retrieving summaries from these summaries further improves the performance of the summaries .
results are shown in table 3 . we observe that for all models except for those using simuli - based embeddings , our model performs slightly better than the previous state - of - the - art model on all metrics except for the shen - 1 metric .
results are presented in table 4 . we present the results of our model on the transfer quality and semantic preservation tasks . the results are summarized in tables 4 and 5 . in all but one case , the semantic preservation task is significantly better than the semantic one . semantic preservation tasks are further improved with the addition of semantic features . with the exception of semantic preservation , all the syntactic features are better for semantic preservation . syntactic preservation tasks have a significant drop in performance compared to semantic preservation due to the nature of the semantic part of the task . using semantic preservation features , we can further improve the semantic performance by adding semantic features such as semantic features and semantic features to the semantic ones . finally , we compare the performance of yelp , m7 and m7 with other popular semantic preservation methods .
5 shows the human evaluation results on yelp and lit . the results are summarized in table 5 . we show the results of human evaluation using a weighted average number of tokens per sentence compared to sim and human ratings of semantic preservation . in both cases , the performance of the human evaluations is significantly better than the sim evaluation results .
results are shown in table 3 . we observe that for all models except for those using m0 + para + lang , the performance is comparable with that of m1 . in particular , for m2 , we observe that when using shen - 1 as part of a sentence representation , it improves performance by 2 . 8 % over the previous state of the art .
results on yelp sentiment transfer are shown in table 6 . the best models achieve higher bleu than the best models using simple - transfer and generative embeddings . we also observe that the classifiers in use in this transfer are worse than those in the previous work , indicating that there is a need to refine the model to handle multiple layers of sentiment . tweets with different classifiers are not included in the results because of the differences in the classification of sentiment . sentiment transfer is a relatively simple process , with the exception of the removal of human references from tweets that are not part of the sentiment conversation .
2 shows the number of instances that were correctly predicted as disfluencies . reparandum length is reported in table 2 . the number of repetition tokens that are correctly predicted to be disfluent is in the range of 1 - 2 .
3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and the repair ( content - function ) . in table 3 , the fraction of tokens predicted to contain a word in each category is shown in table 3 . in the case of the reparandum ( the content word is contained in the repair ) , the fraction containing a word is less than the fraction predicted as containing a disfluency in either category .
results are shown in table 4 . in general terms , we see that the text + innovations model outperforms the single - sentiment model in terms of dev and innovations , in particular , it achieves the best performance with an absolute improvement of 0 . 2 on the f1 metric compared to the previous best state - of - the - art .
2 shows the performance of our model on the fnc - 1 test dataset compared to the state - of - art algorithms on the widely used test set of word2vec . our model achieves the best performance with an absolute accuracy of 28 . 53 % and 69 . 43 % on the micro f1 test set , respectively .
2 shows the performance of different methods on the apw and nyt datasets for the document dating problem . the best performing method is burstysimdater .
3 shows the performance of our method in terms of word attention and graph attention . it is clear from table 3 that neuraldater performs better than the other two methods in both cases .
3 shows the performance of our proposed method on the validation set of jvmee vs . dmcnn . our proposed method outperforms the previous state - of - the - art on all validation set except for the one in which it relies on pre - trained word embeddings .
1 and table 2 summarize the results of our method in terms of event identification . cross - event identification ( f1 ) and event classification ( roca ) are the most common methods for event identification ( table 2 ) . in all but one case , the method is able to distinguish between two groups of events without a significant difference in performance . in both cases , the identification and the event classification performed on the event belong to the same class of event . all the methods trained on this data are completely different from the one used on the previous data set .
3 shows the performance of all models trained on the same training set . all except for english - only - lm are better than all the other models except for those using fine - tuned word embeddings . note that all the models trained only on the spanish - only network are worse than those using the original word2vec features .
4 shows the results on the dev set and on the test set using discriminative training with only subsets of the code - switched data . we observe that fine - tuning achieves the best results with only 50 % train dev and 75 % train test .
5 shows the performance on the dev and test set compared to the monolingual model . we observe that the difference in performance between the two models is less pronounced for the dev set , but still significant for the test set . fine - tuned - disc achieves a comparable performance to the fine - tuned - disc model .
results in table 7 show that type - aggregated gaze features significantly improve recall and f1 scores for the three eye - tracking datasets trained on the conll - 2003 dataset ( p < 0 . 01 ) .
5 shows the precision and f1 scores for using type - aggregated gaze features on the conll - 2003 dataset . the results are shown in table 5 . type - aggregation features significantly improve recall ( p < 0 . 01 ) for the two sets of features , respectively .
results on belinkov2014exploring ’ s ppa test set are shown in table 1 . syntactic - sg embeddings are used in wordnet 3 . 1 , and glove - retro is used on wordnet 4 . 1 . the hpcd ( full ) is derived from the original paper ( faruqui et al . , 2015 ) and is based on syntactic skipgram embedding . it can be seen from table 1 that the syntactic embedding derived by syntacticskipgram has a significant impact on the performance of wordnet , as shown in the second section .
results from rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . table 2 shows the performance of the system with different features . it can be seen that the dependency parser has a large impact on predicting pp attachments .
3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . note that the attention removal reduces the ppa acc . by 0 . 3 points .
2 shows the results of combining subtitle data and domain tuning for image caption translation . subsfull embeddings outperform the domain - tuned model in terms of bleu % scores . in addition , the multi30k model outperforms all the other models except for the one in en - de , where the model performs best .
3 shows the performance of subs1m on en - de and in - de . sub - domain - tuned models outperform all the other models on both datasets except for mscoco17 , which shows the diminishing returns from domain - tuning . in both cases , the performance drops significantly when usingdomain tuned models , the improvements are only statistically significant when using only domain tuned models .
4 shows the bleu scores of models using automatic captions . as the table shows , the models using multi30k embeddings outperform the model using only one or all 5 captions , however , using only the best five captions shows a slight improvement over the performance of the original model . with the exception of the two models using the single captions feature , we see that the automatic image captions perform better than the original ones .
5 shows the performance of different approaches for integrating visual information . we use multi30k + ms - coco + subs3mlm with enc - gate as the dec - gate layer . the results are summarized in table 5 . with the exception of the en - de layer , we observe that the enc - gated layer has the highest bleu % score on both datasets . finally , we see that decoding the visual information improves the performance by 2 % .
3 shows the performance of subs3m in terms of multi - lingual features compared to subs6m on en - de and on flickr16 . in both cases , the performance is comparable to that of subs2m ( which relies on word embeddings ) . in the latter case , the combination of semantic features boosts performance over the single - language model . sub - categories such as the " intensemble - of - 3 " feature set , " intelligent features " and " signal features " improve performance .
3 shows the performance of our system compared to the previous best state - of - the - art en - fr - phrases on mtld and en - rnn - ff . the results are summarized in table 3 . we observe that the best performing system is the enfr - ht - ff model , which significantly outperforms the alternatives .
shown in table 1 , the number of parallel sentences in the train , test and development splits for the language pairs we used .
2 shows the performance of our system with respect to the english , french and spanish vocabularies . our system outperforms the previous stateof - the - art systems in both languages .
5 shows the bleu and ter scores for the rev systems . automatic evaluation scores ( bleu ) show that the system performs better than the previous stateof - the - art systems on ter and en - notes .
2 shows the performance of our visually supervised model compared to the previous stateof - the - art model from chrupala2017representations .
results on synthetically spoken coco are shown in table 1 . we observe that the hierarchical supervised model outperforms the adversarial adversarial model by a significant margin . segmatch significantly outperforms both adversarial and hierarchical embeddings
1 shows the results of different classifiers compared to the original on sst - 2 . we report further examples in table 1 . for example , dan turns in a < u > screenplay that shows the edges at the edges ; it ’ s so clever you want to hate it . for cnn , the same thing is true for rnn . the difference between the original and the original is that the edges of the screenplay are curved and the edges are in the shapes of the words . this shows that when a word is in a sentence , it is in the shape of the word " hate it ” and not in the edges .
2 shows the effect of fine - tuning on the number of occurrences in sst - 2 compared to the original sentence . note that most of the words in this table belong to the " nouns " category , not the " grammatical " category . however , there are a few exceptions . for example , there is a small but noticeable drop in the percentage of words in the nouns category compared to " cnn " .
3 shows the sentiment change in sst - 2 compared to the original sentence . it can be seen that negative labels are sometimes flipped to positive , but this does not always result in a significant drop in sentiment .
results are presented in table 3 . we report the performance of our method on the test set of pubmed and sst - 2 . the results are summarized in table 4 . it is clear that our method has a high impact on the quality of research . in addition , we note that our approach has the advantage of confirming whether the results are positive or negative ( p < 0 . 005 ) . we note that the use of the word " sift " has a generally positive effect on quality . however , it does not improve the interpretability of the results . our approach is able to distinguish between positive and negative aspects of the sentence . with this in mind , we report the results of our joint study with pubmed .
