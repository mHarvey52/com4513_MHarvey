2 : throughput for processing the treelstm model on our recursive framework and tensorflow ' s iterative approach , with the large movie review dataset as our training example . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the iteration approach shows better performance on training data .
shown in table 1 , the balanced dataset exhibits the highest throughput thanks to the high degree of parallelization . however , it does not improve as well as the linear dataset , because there is only a small room of performance improvement left to train the model .
performance for each model with different number of parameters is shown in table 2 . the max pooling strategy consistently outperforms all the base models with different representation . moreover , the boost function with the maximum number of hyper parameters performs better in all model variations .
results of using the shortest dependency path on each relation type are shown in table 1 . with sdp as dependency path , our model achieves the best f1 ( in 5 - fold ) with sdp and the best diff . scores ( in f1 > 50 ) . with the same dependency path as in the previous experiment , the model performs better than the macro - averaged model . however , the difference between the performance of the two relation types is less pronounced .
results are presented in table 3 . y - 3 : y , y , and f1 achieve 50 % and 100 % f1 50 % improvement respectively over the previous state of the art models .
performance of mst - parser on the paragraph level is reported in table 1 . the results of the best performing model are reported in tables 1 and 2 .
4 : c - f1 ( 100 % ) in % for the two indicated systems ; essay vs . paragraph level . note that the mean performances are lower than the majority performances over the runs given in table 2 .
results of experiment 1 are shown in table 1 . the original and original methods are presented in bold . table 1 shows the results for each system . the results for original and original are summarized in tables 1 and 2 . table 1 summarizes results for both systems .
results for the original e2e data and our cleaned version are shown in table 1 . the number of distinct mrs , total number of textual references , and the number of slot matching script instances as measured by our data statistics , see section 3 . as shown in fig . 1 , the original and the cleaned versions have a significant margin over the original , but it is harder to clean than the cleaned version .
performance of original and original models on the test set is presented in table 1 . original models outperform all the other methods except for the one that tgen + tgen + nist performs on .
results of manual error analysis of tgen on a sample of 100 instances from the original test set ( table 4 ) . in addition , we found that the original errors were caused by slight disfluencies in the training data ( e . g . , missing correct values , wrong values ) .
model performance on the external and internal datasets is reported in table 1 . our dcgcn model outperforms all the models except for tree2str in terms of bias metric .
results on amr17 are shown in table 2 . our model achieves 24 . 5 bleu points and achieves 27 . 5 epm . as table 2 shows , the model size and the number of parameters denote single model models , respectively .
models trained on bow + gcn ( bastings et al . , 2017 ) and published in english - czech , the results of these models are reported in table 1 . the results of the models in english and german are summarized in tables 1 and 2 .
5 shows the effect of the number of layers inside dc on the performance of the network expansion . the first group shows that when only one layer is added to a stack , the effect is less pronounced .
results are shown in table 6 . rc + rc denotes gcn with residual connections . with residual connections , gcns with gcn * rc + la ( 2 ) and dcgcn * ( 6 ) show lower performance than those without . however , when gcn + la is used as a reference , the performance remains the same .
model 3 presents the results of our model on the bias metric . our model outperforms all the models except dcgcn by a significant margin .
8 shows the ablation study results for amr15 in terms of density of the connections in the dev set . the results show that removing the dense blocks helps the model to improve its performance . further , the reduction in the number of dense blocks leads to a lower performance .
show the ablation study results for the graph encoder and the lstm decoder . the results of " - linear combination " and " - coverage mechanism " are shown in table 9 . however , the differences in coverage between the two models are less pronounced for the two encoder models .
investigate the effects of different initialization strategies on probing tasks . our paper ( 2017 ) shows that our method outperforms all the base - level initialization strategies except for the one that boosts precision . the results are summarized in table 7 .
presented in table 1 , we observe that our approach obtains the best performance when subtracted from the number of points in the subjnum .
1 and table 2 summarize our results on the subj and mpqa datasets . our model outperforms all the methods except cmp . cbow / 784 and sst5 except for sst2 , which is superior in terms of mrpc and trec .
results on unsupervised downstream tasks attained by our models are shown in table 3 . hybrid models outperform both cbow and cmp . cbow on all downstream tasks , except for those in the low - supervised settings .
results for initialization strategies on supervised downstream tasks are shown in table 8 . our model outperforms all the competition with respect to the mrpc and mpqa tasks . however , it is still superior than sick - e and sst5 .
performance for different training objectives on the unsupervised downstream tasks is shown in table 6 . for example , cbow - c improves performance on the sts13 and sts14 tasks by 3 . 7 points . however , it does not achieve the best performance on sts15 .
can be seen in table 1 , the cbow - c method outperforms all the other methods except for the one that requires a higher level of precision .
subj and sick - r models outperform all the other methods except for cbow - c , which improves performance on the mrpc and mpqa test sets .
system performance in table 1 , all org and per scores are reported in table 1 . our system outperforms all the other systems except for the one that does not use misc . the results of using the best performing system are summarized in tables 1 and 2 . we observe that the combination of domain - aware learning with the best org scores ( mil - nd and mil - nd ) significantly improves the results for all systems except those that do not use the best feature set .
results on the test set under two settings are shown in table 2 . our system outperforms all the models in terms of e + p and f1 scores . name matching and supervised learning achieve the best results with 95 % confidence intervals of f1 score . supervised learning also achieves the best e − p and 94 % f1 performance with the same level of accuracy . further , the performance improvement over the previous state of the art model is modest but significant .
6 : entailment ( ent ) and ref ( g2s - gat ) results are presented in table 6 . the model outperforms all the other models except for those that do not use ref .
results of experiment 1 are shown in table 1 . the models performing best on the ldc2017t10 test set are presented in tables 1 and 2 . table 1 summarizes the results for each of the models .
results on ldc2015e86 test set when models are trained with additional gigaword data . the results on this test set are shown in table 3 .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . bilstm significantly outperforms the other models in terms of model size and performance .
results are shown in table 1 . we observe that the g2s model significantly outperforms other models in terms of terms of sentence length and sentence length . sentence length is reported in tables 1 - 7 and table 1 shows the results for all models except those that do not use word embeddings . graph diameter and average sentence length are reported in table 2 . note that the lower average number of frames per sentence , indicating that the model is more suitable for the task at hand .
shown in table 8 , the fraction of elements missing in the input graph that are present in the generated sentence ( g2s - gin ) is significantly less than those in the output graph ( miss ) . this indicates that the g2s model is able to distinguish between the reference sentences and the output ones .
4 shows the performance of our approach with different target languages . we use the 4th nmt encoding layer , trained with 200k sentences as inputs and tested on a smaller parallel corpus ( 200k sentences ) .
2 : pos and sem tagging accuracy with baselines and an upper bound . mft : most frequent tag ; unsupemb : most frequently tag ; word2tag : upper bound encoder - decoder .
results are presented in table 1 . table 1 summarize our results for each category . our results show that our method significantly outperforms the competition in terms of accuracy and precision .
5 shows the accuracy with different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages .
performance on different datasets is shown in table 8 . the difference between the attacker score and the corresponding adversary ’ s accuracy is reported in δ , a difference of 2 . 2 points .
training directly towards a single task , the performance gap between dial and sentiment decreases significantly .
shown in table 2 , the balanced and unbalanced task averages are significantly worse than those of balanced and balanced data , respectively .
performance on different datasets with an adversarial training is shown in table 3 . the difference between the attacker score and the corresponding adversary ’ s accuracy is significant . in pan16 , the performance is significantly lower than that of the trained classifier . sentiment and gender bias are also important factors in predicting performance .
6 shows the concuracies of the protected attribute with different encoders . for example , rnn is more likely to embed leaky information than it is not .
results of experiment 1 are shown in table 1 . our model outperforms all the other models with finetune features . we observe that our model achieves the best results with a 3 . 5x improvement on the ptb base and a 2 . 4x boost on the wt2 + finetune feature set . however , the performance gap between our model and the original model is still significant . our model achieves a 4 . 3x performance improvement over the previous state - of - the - art model .
performance of our model compared to previous models is presented in table 5 . our model outperforms all the models with a minimum of 2 . 87m iterations . we observe that the increase in performance is due to the shorter training time and the higher number of iterations .
results of experiment 1 are shown in table 1 . table 1 shows the performance of our model compared to previous work on multiple datasets . our model outperforms all the other models in terms of both the average time and the number of iterations in the ama dataset .
3 shows the bleu score on wmt14 english - german translation task . our model outperforms all the previous models with a gap of 1 . 15k training steps . however , it is still better than the state - of - the - art gnmt model .
4 shows the performance of our model with respect to the parameter number of base . the results published by wang et al . ( 2017 ) show that our model can significantly improve match / f1 score over the baselines with a boost of 2 . 4 points .
6 shows the f1 score of our model on conll - 2003 english ner task . it can be seen that our approach significantly improves the performance by increasing the parameter number . the lstm model outperforms all the other models with a large margin .
results on snli and ptb task with base + ln setting and test perplexity on the snli task with base setting .
performance of all systems is reported in table 1 . word - based systems retrieval and logreg feature - rich systems ( mtr ) significantly outperform human on all three tasks . table 1 shows the results for all systems when word - based system is used , with a slight improvement in performance for logreg . logreg features are presented in tables 1 and 2
4 : human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 ( best ) . our system outperforms all the systems except seq2seq ,
results are presented in table 3 . our proposed model outperforms all the other models except for the one that is used in corpus ( p < 0 . 001 ) . the results of our model outperform the performance of other models , such as df , docsub and docmax .
shown in table 1 , all the models trained on the corpus dataset are slightly outperform their counterparts in terms of performance . however , for all but one of the models , the performance gap between english - based docsub and german - based europarl is significantly larger than that of english - language docsub .
shown in table 1 , all the models trained on the corpus dataset are comparable in terms of performance . our model outperforms all the other models except for those using docsub . the difference between the performance of docsub and europarl is minimal , however it does slightly outperform the df model .
3 shows the performance of our model compared to the previous best state - of - the - art models . our model achieves the best results on all metrics with a gap of 1 . 78 points from the official score of eurparl , while europarl achieves the worst performance .
3 shows the performance of our model compared to the previous best state - of - the - art models . our model achieves the best results with a maxdepth of 79 . 7 % on the dsim test set , and a boost of 3 . 6 % overall on the hclust test set .
performance ( ndcg % ) comparison for the experiments of applying our nlst principles on the validation set of visdial v1 . 0 . we note that the enhanced version of the nlst model performs better than the original one .
performance ( ndcg % ) of different ablative studies on different models on the visdial v1 . 0 validation set is shown in table 2 . the best performing model is the one using p2 as the history shortcut .
5 compares the performance of these models on hard and soft alignments . the results are summarized in table 5 . for example , hmd - recall + bert significantly improves the performance on hard alignments over the soft ones .
results of our approach are presented in table 1 . the results of the direct assessment and bertscore - f1 scores are summarized in tables 1 and 2 . table 1 shows that our approach significantly improves the performance of our model compared to previous approaches .
performance of our models on the bagel and sfhotel test sets is presented in table 1 . the results of these models are summarized in bold . they show that our bleu - 1 model outperforms all the baseline models by a significant margin .
performance of the models according to these baselines is reported in table 3 . the results are presented in tables 1 and 2 . the summaries obtained by word - mover are summarized in table 1 . the leic scores ( p < 0 . 001 ) significantly outperform the f1 scores by a margin of 0 . 005 and 0 . 749 respectively .
results are shown in table 3 . table 3 shows that for all models with at least one exception , the shen - 1 model outperforms the m0 model on all metrics except for those with less than 5 % overlap .
performance of our model on the transfer quality and semantic preservation tests is presented in table 4 . the results are presented in tables 4 and 5 . semantic preservation features are strongly concentrated in one domain , with the exception of the case of semantic preservation , where the focus on semantic preservation is entirely on the semantic preservation dataset . syntactic preservation features , such as m6 , m7 and m7 , have a generally positive effect on transfer quality .
human ratings of semantic preservation are shown in table 5 . the results of human evaluation are summarized in terms of acc and pp scores . table 5 shows that the performance obtained by human evaluation is comparable to that of machine and human evaluations .
results are presented in table 3 . table 3 shows that for all models with at least one type of representation , the representation representation of the word " shen - 1 " is the most important part of the system .
results on yelp sentiment transfer are shown in table 6 . our best model achieves the highest bleu and the highest acc ∗ scores , respectively , compared to prior work using only one classifier , such as word embeddings and sentence prediction . however , the performance of our model is slightly worse than those of previous work using multiple classifiers , indicating that there is a need to refine the model ' s vocabulary .
statistics for nested disfluencies are shown in table 2 . reparandum length and number of tokens that are correct for each disfluency are reported in tables 2 and 3 . the average number of repetition tokens is 3 . 5 .
3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and the repair ( table 3 ) . as table 3 shows , the number of tokens predicted to contain a word in each category is considerably less than the number predicted as containing a part of the content word . table 3 also shows the percentage of tokens correctly predicted to belong to each category .
results of experiment 1 are shown in table 1 . we observe that text + innovations model outperform all the other models in terms of dev and test results . in addition , the model with the most innovations achieves the best results with an absolute improvement of 2 . 36 points .
performance comparison with the state - of - art algorithms on the fnc - 1 test dataset . our model achieves the best performance with a reasonable level of accuracy , even when the topic is discussed and the conversation is not .
performance of different methods on the apw and nyt datasets for the document dating problem is shown in table 2 . the unified model significantly outperforms all previous models except burstysimdater .
3 : accuracy ( % ) of our component models with and without attention . this results show the effectiveness of both word attention and graph attention for this task . as shown in table 3 , the performance of ac - gcn is comparable with that of neuraldater .
model performance on all stages is reported in table 1 . embedding + t model outperforms all the other models except for the one that performs in the distractor and fullwiki setting .
system ' s performance on the event is presented in table 4 . all the methods used for this analysis belong to the " multi - event " category . table 4 presents the results of the method in terms of both event identification and classification . in all but one of the cases , the method has achieved a significant performance improvement on both events .
can be seen in table 4 , all the models trained on the original word2vec layout outperform all the other models except for the one that relies on word2vec embeddings . note that the difference in performance between english - only and spanish - only is less pronounced for the two languages .
results on the dev set and on the test set are shown in table 4 . we trained fine - tuned with only subsets of the code - switched data , and only train test data .
performance on the dev set and the test set is shown in table 5 . as expected , fine - tuned models perform better than monolingual models on both dev and test sets .
results in table 7 show that type - aggregated gaze features significantly improve recall and f1 - score for the three eye - tracking datasets , respectively .
5 : precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features on the conll - 2003 dataset . note the significant improvement in precision ( p ≤ 0 . 01 ) over the previous state of the art model , as shown in table 5 .
results on the original wordnet test set are shown in table 1 . syntactic - sg embeddings are derived from the original paper ( faruqui et al . , 2015 ) and are used in wordnet 3 . 1 . wordnet has the best performance with syntactic sg as the embedding layer , and it uses glove - retro as the reference layer . however , it performs slightly worse than the syntactic - based wordnet we base it on . wordnet , in particular , suffers from semantic loss . the syntactic loss of syntactic functions is caused by the loss of the semantic functions of skipgram .
results from the rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . table 2 shows the results from various models using various aspects of pp attachment prediction . however , the best performing model is the oracle pp model .
3 : effect of removing sense priors and context sensitivity ( attention ) from the model . the effect of removing context sensitivity is shown in table 3 .
2 : adding subtitle data and domain tuning for image caption translation ( marian amun et al . , 2018 ) . as expected , the multi30k model outperforms all the other models except for those using domain - tuned embeddings . however , the domain tuning results are still slightly better than the model without domain tuning .
results are shown in table 1 . sub - domain - tuned models outperform the models in all but one of the comparisons . the results are presented in tables 1 and 2 , with the exception of the one in fig . 3 .
4 shows bleu scores in terms of automatic captions with marian amun as our model . the results are shown in table 4 . as expected , the multi30k model outperforms all the other models except for the one with the best image captions .
5 shows the performance of our approach with respect to visual information . transformer and multi30k + ms - coco + subs3mlm converge on the same visual information ( bleu % scores ) . with the exception of capt - gate , we observe that the enc - gate and dec - gate features are comparable in terms of bleu % .
performance of subs3m and subs4m on the en - de dataset is presented in table 4 . the results are presented in tables 4 and 5 . table 4 shows the performance of the models with different visual features . sub - text - only models perform better than the models without the visual features ,
3 shows the results of our approach on mtld compared to the original embeddings . the results are presented in table 3 . table 3 shows that our approach outperforms both the original approach and the original one .
shown in table 1 , the number of parallel sentences in the train , test and development splits for the language pairs we used .
2 : training vocabularies for the english , french and spanish data used for our models . the results are shown in table 2 .
system reference bleu and ter scores for the rev systems are shown in table 5 . automatic evaluation scores ( bleu ) consistently show lower performance than ter and en - fr - rev .
results on flickr8k are shown in table 2 . the visually supervised model outperforms segmatch and rsaimage with a 3 . 8 % recall boost .
results on synthetically spoken coco are shown in table 1 . the visually supervised model outperforms the similarly supervised audio2vec - u model by a margin of 3 . 9 points .
shown in table 1 , orig ( the original on sst - 2 ) turns in a < u > screenplay that shows the edges at the edges ; it ’ s so clever you want to hate it . also , rnn ( which shows the different classifiers ) shows that it can be used to edit a screenplay without losing its edges .
2 : part - of - speech ( pos ) changes in sst - 2 : , , and indicate that the number of occurrences have increased , decreased or stayed the same through fine - tuning respectively . the symbols are purely analytic without any notion of goodness . however , they indicate that there is a considerable amount of overlap with the original sentence . these results indicate that fine tuning has not resulted in a significant drop in the quality of words .
3 shows the change in sentiment from positive to negative in sst - 2 . negative labels are flipped to positive and vice versa . however , this does not account for the significant drop in sentiment .
results are presented in table 3 . table 3 summarizes the results of our method in terms of ppmi and reward learning . our approach outperforms all the methods except sift except for the one that appears in pubmed . the results are summarized in table 1 .
