2 : throughput for processing the treelstm model on our recursive framework and tensorflow ’ s iterative approach , with the large movie review dataset as the training example . the system achieves the best performance on inference with a large number of training instances .
shown in table 1 , the balanced dataset exhibits the highest throughput thanks to the high degree of parallelization . however , it does not improve as well as the linear dataset , because there is only a small room of performance improvement left to train the model in a more sophisticated manner .
2 shows the performance of different parameter optimization strategies for each model with different number of parameters . the best performing model is softplus . the max pooling strategy consistently performs better in all model variations . the hgnll08 model achieves the best performance with the maximum number of hyper parameters . moreover , the boost function generated by sigmoid performs similarly to softplus in all the other variations .
1 shows the effect of using the shortest dependency path on each relation type . it can be observed that macro - averaged model achieves the best f1 ( in 5 - fold ) with sdp as well as the best diff . the relation type is qualitatively similar to human - generated model , but it has the advantage of having more relation types . in particular , it helps the model to improve the f1 by a significant margin .
results are presented in table 3 . in general terms , the performance of y - 3 compared to y - 2 is significantly better , with the difference being less pronounced for r - f1 and f1 50 % .
3 shows the results for paragraph and f1 . the results are presented in table 3 . our proposed method outperforms the best stateof - the - art method on both categories . on the paragraph level , our model achieves a f1 score of 3 . 59 / 4 . 59 on average compared to the previous state of the art method . the accuracy of our method is significantly higher than that of the original mst - parser model .
4 shows the c - f1 scores for the two indicated systems at the essay vs . paragraph level . note that the mean performances are lower than the majority performances over the runs given in table 2 . also , the difference between paragraph and utterance is larger than the difference in overall performance .
results are shown in table 1 . original and original results are presented in tables 1 and 2 . table 1 shows the performance of each system when trained on a single dataset . the error reduction on original is much smaller than on the other two systems . however , the improvement on the bleu test set is larger than on any other training set .
results for the original and the cleaned versions are shown in table 1 . the number of distinct mrs , total number of textual references , and the number of slot matching scripts as measured by our matching script , see section 3 . in the original e2e dataset , we managed to get 1 , 358 mrs ( 1 , 358 ) cleaned ( 0 . 00 ) compared to the original . the cleaned version , on the other hand , contains 1 , 362 mrs and 1 , 368 ser , which shows the diminishing returns from mixing multiple scripts .
results are shown in table 1 . original and original results are presented in tables 1 and 2 . all the other models perform similarly on the test set , with the exception of sasaki et al . ( 2018 ) . the difference is minimal , however , due to the large variation in bleu score between original and tgen models , leading to a drop of more than 2 points in performance on test set .
results of manual error analysis on a sample of 100 instances from the original test set are shown in table 4 . the biggest percentage of errors we found was in the addition of wrong values to the training data . further , we found that the slight disfluencies in the original training data were mostly caused by incorrect values .
model performance on the external and internal datasets is reported in table 1 . the dcgcn ensemble performs significantly better than the state - of - the - art models on all three datasets , with the exception of seq2seqk ( konstas et al . , 2016 ) .
2 shows the results on amr17 . our model achieves 24 . 5 bleu points on average compared to the previous best state - of - the - art model , seq2seqb ( beck et al . , 2018 ) . as expected , the model size and the number of parameters denote single - model models .
results in english - language are shown in table 1 . the best performing model is bow + gcn ( bastings et al . , 2017 ) . the results are presented in tables 1 and 2 . as expected , the results are significantly better in english than in german , although the difference is less pronounced in english .
5 shows the effect of the number of layers inside the dc stack on the performance of the model when we add the layers of layers of the stack together . we observe that for example , if a layer has multiple layers , it leads to a better performance than if only one layer had multiple layers .
results are shown in table 6 . rc + la also improves gcn performance over residual connections . with residual connections , gcn performs slightly better than dcgcn2 . however , when gcn has residual connections with multiple gcns , the improvement is only 2 . 5 % overall . using rc - based gcn models shows that residual connections are beneficial for gcn to improve performance .
model 3 shows the performance of dcgcn models when trained on state - of - the - art data . the results are presented in table 3 .
8 shows the ablation study results on the dev set of amr15 . it can be seen that dense blocks reduce the number of connections with the i - th block . this shows that removing the dense blocks helps the model to improve its performance .
9 shows the ablation study results for the graph encoder and the lstm decoder . the best performing model is the global encoder , with a gap of 22 . 9 % in performance between the two . encoder modules used in table 9 show that multi - factor co - ordination is crucial for the design to achieve outstanding results .
investigate the effects of different initialization strategies on probing tasks . we show in table 7 the results for each initialization strategy that we base it on . our model obtains the best performance with a gap of 10 . 5 % in the precision score .
presented in table 1 show that our method outperforms the best state - of - the - art cbow / 400 model on every metric . however , it is still inferior in the sub - categories of " depth " and " tense " which shows the diminishing returns from mixing features .
cbow / 784 shows significant performance improvement over other baselines when combined with sst2 and sst5 . sub - categories include mrpc , mpqa , and trec . cbow performs slightly better than sick - e , but is superior than sst - b , indicating that the superior mrpc performance comes from a better understanding of the underlying mechanism . selective cbow models perform similarly to the best on both mrpc and mpqas . the best performing cbow model is cmp .
results on unsupervised downstream tasks attained by our models are shown in table 3 . hybrid models perform better than cmp . however , it is comparable with cmp in some downstream tasks , such as those on sts15 and sts16 .
8 shows the performance for different initialization strategies on supervised downstream tasks . our paper shows that the best performing model is sst2 , which improves upon the performance of mpqa .
6 shows the performance for different training objectives on the unsupervised downstream tasks . our approach outperforms cbow - c on all three tasks .
1 shows the numerical results for each sub - category . our model outperforms all the other methods except cbow - c except for the one that it obtains in the deep sub - categories . the difference is most prevalent in the case of coordinv and subjnum , which show the performance of the method when combined with the number of tokens in the subtense categories . this shows the performance obtained when combining the features of the two categories . the results are presented in table 1 .
subj and sst5 perform comparably to other methods except for cbow - c . subj models outperform all the other models except for sst2 , which is more related to mpqa . sick - e performs slightly better than sst3 , but is comparable with sst6 in terms of mrpc performance . the difference is mostly due to the smaller size of the training set , which underscores the competitiveness of subj model in the low - supervision settings .
system e + org and per are shown in table 1 . all org methods outperform all stateof - the - art methods except for mil - nd , which obtains the best e + per score . supervised learning ( mil - nd ) achieves an e + org score of 43 . 57 / 71 . 03 on the test set of hotpotqa , in which all the org scores computed by the system are obtained using the best performing feature set . table 1 shows the performance of the best supervised learning models when combined with all the other features of the training set . the results are presented in table 2 .
results on the test set under two settings are shown in table 2 . the best performing model is mil - nd ( model 1 ) which results in better e + p scores than the model 1 . supervised learning also results in higher e + f1 scores , further improvements are expected in the final set of results . name matching , in particular , improves the general performance of the model in terms of recall scores . finally , automatic learning improves the e − p scores by 2 . 59 points over the model baseline .
6 shows the performance of the models when ref and ref are added to their ref scores . ref significantly outperforms gen and g2s - gat by a margin of 2 . 36 points , which shows the advantage of ref over ref when combined with ref .
3 provides detailed results on the ldc2017t10 test set . the results are presented in table 3 . the first set of results show that g2s models outperform the best stateof - the - art models on all metrics , with the exception of the bleu metric .
results on ldc2015e86 test set are shown in table 3 . the best performing model is g2s - ggnn , which learns to program with gigaword data .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . the results show that the use of bilstm improves the generalization ability of the model , leading to a larger performance improvement .
results are shown in table 4 . the average number of frames per sentence is 7 . 5 , which shows the competitive advantage of g2s - gin over other models . sentence length is shorter than sentence length , indicating that the model is more suitable for the task at hand . finally , the average sentence diameter is longer than the average length of sentences , indicating the model performs well on the training set . note that the recall length is small but the average sentence length is larger than those on the other models showing lower recall .
shown in table 8 , the fraction of elements that are not present in the input graph that are missing in the generated sentence ( g2s - gin ) . however , this fraction is much smaller than in the output graph , which indicates that the model is better at selecting the relevant tokens from the reference sentences .
4 shows the performance of our system with different target languages . we use the 4th nmt encoding layer , trained with 200k sentences .
2 shows the pos and sem tags accuracy with baselines and an upper bound . mft : most frequent tag ; word2tag : the most frequent encoder - decoder . note the lower bound on the upper bound for both tags .
results reported in table 1 show that the accuracy obtained by our method can be further improved with the help of additional training data . the results are presented in tables 1 and 2 .
5 shows the accuracy with different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages .
performance on different datasets is shown in table 8 . the average age of the attacker is 9 . 7 and the corresponding adversary is 14 . 3 . the difference between the correct response and the incorrect response is 10 . 2 .
1 shows the performance of the models trained directly towards the single task . accuracy is significantly higher when training directly towards a single task ( e . g . , mention ) . the difference is less pronounced for pan16 , however , for other models trained on pan16 .
2 shows the effect of the additional cost term on the balanced and unbalanced data splits . the classifier named pan16 shows severe over - fitting since the imbalance in the generated data is caused by gender bias . sentiment , however , does not account for the balanced data splits , i . e . , there is no significant difference in the performance of the classifier labeled with the protected attribute label .
performance on different datasets with an adversarial training is shown in table 3 . sentiment is the difference between the attacker score and the corresponding adversary ’ s accuracy . the difference between accuracy and leakage is computed using the weighted average number of tokens in the training set . in addition , gender bias also affects the performance , as shown in fig . 3 .
6 shows the performance of the embeddings for different encoders . embedding leaky is easier for rnn to do than it is for leaky .
3 shows the performance of our model compared to previous work on the topic . our model achieves the best performance on both datasets with finetune and domain - aware features . the results are presented in table 3 . our model outperforms the previous stateof - the - art models on both the ptb and the wt2 dataset with a gap of 2 . 36 % in performance on the training set . we show the results of phase 1 and phase 2 on the test set of hotpotqa , a joint research study by yang et al . ( 2018 ) and rln ( 2018 ) .
performance of our model compared to previous stateof - the - art models is presented in table 4 . the results are presented in tables 1 and 2 . table 1 shows that our lstm model is significantly faster than the previous state of the art model on both datasets . as expected , the difference in training time from the original model to the current model is much smaller .
3 shows the performance of our model compared to previous stateof - the - art models . the results reported in table 3 show that our model outperforms both the original and yelppolar time in terms of both err and f1 metrics . on the other hand , our model performs slightly worse than the original on both datasets . we observe that the difference between the average f1 metric and the average ranking of the two models is less pronounced on average compared to the original model . finally , we observe that google translate ( which takes the form of a word embeddings ) significantly boosts performance for both datasets when compared with the original one .
3 shows the bleu score on the test set of wmt14 english - german translation task . it is clear from table 3 that the value of tokens is important to the translation performance of the model when trained with only one sentence decoding . further improvements are expected from additional training batches of 500m ,
4 shows the performance of our model with respect to match / f1 score on squad dataset . as shown in the table , our model obtains the best performance with a parameter number of 2 . 44m . however , this model performs slightly worse than the other models with a higher f1 score . the only exception is the lstm model , which obtains a better match rate with a larger parameter number . we observe that the sru model performs similarly to the other model with a similar number of parameters .
6 shows the f1 score on conll - 2003 english ner task . it is clear from table 6 that the lstm model significantly improves upon the strong lemma baseline by 1 . 59 points in ner tasks .
performance of elrn with base + ln setting is shown in table 7 . the results show that the model can distinguish between accuracy on snli task with base - level setting and perplexity on ptb task with base setting . however , the difference is not statistically significant , i . e . , glrn has an absolute improvement of 2 . 36 points over the previous state of the art .
system evaluation results are presented in table 4 . word embeddings ( emtd ) and word2vec ( mtr ) outperform human on all metrics except system retrieval . all metrics shown in table 1 are statistically significant ( hochreiter et al . , 2017 ) with respect to system retrieeval . the attention span is minimal , with a marginal drop of 0 . 05 % compared to previous stateof - the - art methods . table 1 compares the performance of human and machine learning on the 3rd and 4th stages of the evaluation data . in general terms , the attention span of the two stages is the most significant , with an absolute drop of 2 % compared with previous state of the art systems . on the other hand , for the 5th stage , we see a slight improvement ( 2 . 05 % ) compared to the previous state - of - art on all the other stages .
4 shows the human evaluation results on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 8 . the best performing system is seq2seqaug . the system is ranked in the top 1 or 2 for overall quality .
results reported in table 1 show that our approach outperforms the competition on three of the four datasets . our joint model , europarl , achieves the best performance on all three datasets , outperforming both df and docsub by a large margin . on the other hand , our joint model performs slightly worse than the best on the three datasets .
results reported in table 1 show that our proposed approach outperforms the competition on three of the four datasets , namely , df , docsub , and hclust . the results are summarized in table 2 .
results reported in table 1 show that our approach outperforms the competition on three of the four datasets . our joint model , europarl , achieves the best performance on all three datasets , outperforming both df and docsub by a large margin . on the other hand , our joint model performs slightly worse than the best on the three datasets .
embeddings for each metric are shown in table 1 . our system achieves the best performance with a maxdepth of 1 . 78 on average compared to the previous best state - of - the - art model , europarl . on the other hand , our system suffers from a significant drop in performance compared to maxdepth due to the high overlap between maxdepth and maxdepth . this is reflected in the averagedepth of our metric , which shows the diminishing returns from mixing source and target vectors .
embeddings for each metric are shown in table 1 . our system achieves the best performance with a minimum of 79 % truedepth on the metric compared to 1 . 5 % on the comparison metric . europarl also outperforms the maxdepth baseline on both metric with a gap of 1 . 7 % on both metrics .
performance of our model on the validation set of visdial v1 . 0 . 0 is shown in table 1 . the enhanced version of our system , lf , performs better than the original embeddings . we notice that the r0 , r2 , r3 scores denote regressive loss , weighted softmax loss , and generalized ranking loss .
2 shows the performance ( ndcg % ) of different ablative studies on different models on the visdial v1 . 0 validation set . the best performing model is lrva . note that only applying p2 with the history shortcut achieves the most effective results .
5 compares the performance of our hmd - recall and wmd - bigram models on hard and soft alignments .
3 presents the results of our approach . the results are summarized in table 3 . the first set of results show that our approach significantly outperforms the baselines on three of the four metrics . the difference is most prevalent on lemmatization , with the exception of ruse ( p ≤ 0 . 005 ) . the second set shows the performance on the test set of bertscore - f1 , with a gap of 1 . 0 points from the last set . our approach significantly improves the interpretability by improving the recall scores of the lemma targets for both sets .
results are shown in table 1 . the bleu - 1 scores are significantly better than the baseline scores on all baseline metrics . the difference is most prevalent in relation to bertscore - f1 , which measures performance on a single metric , with a gap of 0 . 7 points from baseline to 0 . 9 points on the bert score .
performance of the models according to these baselines is reported in table 4 . the results are summarized in bold . the summaries displayed in bold indicate that the performance obtained by the models is superior on m2 than on m3 . however , the leic score is slightly worse than the f1 score indicating that the model performs better on both sets .
3 shows the performance of the models trained on the shen - 1 dataset compared to the previous state of the art models . we observe that for m0 , the model performs slightly better than the state - of - the - art model on both datasets , with the exception of simuli .
results are shown in table 4 . semantic preservation and transfer quality are the most important features for both datasets . the results are summarized in tables 4 and 5 . syntactic preservation and semantic preservation are the top features for semantic preservation . for semantic preservation , the best results are obtained by mixing the features from two sets of documents , namely , the embeddings of the documents for transfer and the semantic preservation scores of the two sets . the semantic preservation performance is improved with the addition of additional features as the cost decreases with the increase in performance .
5 shows the human evaluation results . the results are shown in table 5 . it is clear from the table 5 that the accuracy obtained by parsers is relatively high , which indicates that the performance obtained by human judgement is relatively consistent .
3 shows the performance of the models trained on the shen - 1 dataset compared to the previous state of the art models . the results are summarized in table 3 . we observe that for m0 , the model performs slightly better than the state - of - the - art model on both datasets , with the exception of sasaki et al . ( 2018 ) . further , for m2 , the performance drops significantly as the model learns to rely less on data augmentation .
6 shows the results on yelp sentiment transfer . our best model achieves higher bleu than any prior work at similar levels of acc ∗ , but the highest acc score is obtained using the best classifier , yang2018unsupervised . the results on simple - transfer are not included as they are worse than the best state - of - the - art model . sentiment transfer is restricted to 1000 sentences , with the exception of the english embeddings . it is possible to improve the acc score by adding classifiers and user references , but this is not possible to do .
2 shows the number of instances that were correctly predicted as disfluencies . the average number of repetition tokens is 1 , 2 , 3 , 6 .
shown in table 3 , the frequency of rephrases correctly predicted as disfluent is the highest in the three categories ( table 3 ) . the fraction of tokens predicted to contain a content word is less than the fraction predicted as containing a non - content word , in both cases the reparandum is longer .
results are shown in table 4 . we observe that the best performing model is text + innovations , which in turn leads to better test results . the results are presented in tables 1 and 2 . text + innovations model outperforms the single model in terms of dev and test mean . in particular , the text + raw model achieves the best results with an absolute improvement of 2 . 36 points over the best state of the art model .
2 compares our model with the state - of - art algorithms on the fnc - 1 test dataset . the results are shown in table 2 . our model achieves the best performance with a reasonable level of recall . however , the biggest performance drop is seen in the accuracy of self - attention embedding , which shows the performance gain from further iteration .
2 shows the performance of different approaches on the apw and nyt datasets . the best performing approach is maxent - joint , which significantly outperforms all previous models .
3 shows the performance of our method with and without attention . the accuracy is shown in table 3 . it can be observed that both word attention and graph attention give a significant performance boost .
3 shows the performance of the models trained on state - of - the - art data augmentation schemes . our model outperforms all the models except for dmcnn , which shows the competitiveness of object embedding . moreover , we observe that for all models , the performance improvement is modest but consistent , with the exception of jnn .
experimental results of all methods are shown in table 1 . all methods cause a significant drop in performance when trained on a single domain . in all but one case , the error reduction is statistically significant ( p < 0 . 001 ) . cross - event learning methods use the word " dataset " for identification and classification . the method used for the task has the advantage of pre - training of the model on the single domain , with the exception of the case of tn . table 1 shows the performance of the method for each domain .
can be seen in table 4 the results for english and spanish are presented in table 5 . all the models trained on the concatenated word embeddings are shown in bold . all except english - only - lm shows a reduction in performance compared to the previous state - of - the - art models . the difference is most prevalent in english , where the dev perp and test acc are the only ones that perform better than the original spanish - only model .
results on the dev set and on the test set are shown in table 4 . fine - tuning with only subsets of the code - switched data achieves the best results .
5 shows the performance on the dev and test set compared to the monolingual model . fine - tuned - disc achieves a better performance than fine - tuned - disc on the test set . however , it is still inferior to the clarity - switched model in the dev set .
results in table 7 show that type - aggregated gaze features significantly improve recall for the three eye - tracking datasets compared to the baseline model .
5 shows the precision , recall and f1 scores for using type - aggregated gaze features on the conll - 2003 dataset . note the significant drop in precision from the baseline to the now - normal 94 . 5 % f1 score due to the higher recall . the performance drop from the previous state of the art model is modest but significant ( p < 0 . 001 ) .
results on belinkov2014exploring ’ s ppa test set . the glove - retro embeddings give a performance gain of 3 . 1 points over traditional wordnet . however , this gain is not significant over traditional skipgram , which results in a drop of 4 . 3 points .
results from rbg dependency parser are shown in table 2 . oracle pp pre - trained rbg with various features coming from various pp attachment predictors and oracle attachments . the results from table 2 show that once the dependency parser is trained on oracle pp , it becomes more accurate and predictive .
3 : effect of removing sense priors and context sensitivity ( attention ) from the model . the effect of removing context sensitivity is shown in table 3 . when the attention layer is removed , the model shows a drop in performance .
2 shows the results for english captions with marian amun as the model . adding subtitle data and domain tuning for image caption translation ( bleu % scores ) improves the overall results for both models .
results are shown in table 4 . the best performing subs1m model is the en - de model , which means that the domain - tuned models perform better on the largerickr16 and mscoco17 datasets . table 4 shows the performance of the models trained on the single - domain learner dataset . the improvements over the previous state - of - the - art model are mostly due to the high quality of the domain tuning data .
4 shows the bleu scores of models using automatic captions . we show the results in table 4 . the best model is multi30k , which shows the advantage of concatenation of multiple captions in a single sentence . adding multi - domain captions improves bleus scores in the low - supervision settings .
5 compares our approach with prior approaches on en - de and mscoco17 . the results are presented in table 5 . we use multi30k + ms - coco + subs3mlm , and detectron mask surface . as expected , the performance drop significantly when we add enc - gate and dec - gate , as shown in fig . 5 , using transformer improves the performance by 10 % .
performance of subs3m compared to subs6m is presented in table 4 . the text - only model outperforms the ensemble - of - 3 model in terms of performance on both datasets , moreover , the improvements on the flickr16 dataset are larger than those on the mscoco17 dataset . in addition , the multi - lingual model ( which relies on word embeddings ) achieves an overall improvement of 2 . 36 points over the baselines on the en - de dataset .
3 shows the performance of the best models compared to the original embeddings . we observe that the best performing model is the en - fr - rnn - ff model , which performs slightly better than the original model on mtld . the results are shown in table 3 .
shown in table 1 show the number of parallel sentences in the train , test and development splits for the language pairs we used .
2 : training vocabularies for the english , french and spanish data used for our models .
system reference bleu and ter scores for the rev systems are shown in table 5 . automatic evaluation scores ( bleu ) consistently show lower performance than ter , en - fr - rev and en - es - rev .
2 shows the performance of our visually supervised model compared to the baseline model on flickr8k . the row labeled vgs is the average rank of the models trained on the embeddings from chrupala2017representations . the difference is less pronounced for segmatch ,
results on synthetically spoken coco are shown in table 1 . the model trained on the embeddings from chrupala2017representations outperforms segmatch and image2vec - u . the difference is less pronounced for rsaimage .
1 shows the results of different classifiers compared to the original on sst - 2 . for example , dan turns in a < u > screenplay that shows the edges at the edges ; it ’ s so clever you want to hate it . from table 1 we can see that for rnn , the edges edges of a screenplay are edges edges , and for cnn , they show the shape of the dialogues . this indicates that if you want hate hate hate , you can use it as a verb . on the other hand , for cnn we see that it is easier to use the word " hate it " .
2 shows the overall effect of fine - tuning on the ranking performance . the results are shown in table 2 . the largest percentage increase is seen in rnp , which means that the number of occurrences have increased , decreased or stayed the same through fine tuning . these results indicate that the effectiveness of the lexical features has not been significantly impacted by the increased number of words in the corpus .
3 shows the sentiment change in sst - 2 from positive to negative . it is clear from table 3 that negative sentiment is the most prevalent sentiment in the two datasets .
results are shown in table 1 . the competitive advantage of using word embeddings is evident from the large difference in ppmi between positive and negative states . on the other hand , the competitive advantage is less pronounced on sst - 2 , indicating that the use of cross - domain learning ( sift ) can improve interpretability . table 1 shows the performance of our method for both review and evaluation . our joint model outperforms the competition on both test sets with 98 % of the ppmi ( 98 % ) .
