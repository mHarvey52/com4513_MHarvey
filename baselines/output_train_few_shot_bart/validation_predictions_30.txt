2 : throughput for processing the treelstm model on our recursive framework and tensorflow ' s iterative approach , with the large movie review dataset as our training example . the recursive approach performs the best on inference with efficient parallel execution of the tree nodes , while the iteration approach shows better performance on training .
shown in table 1 , the balanced dataset exhibits the highest throughput thanks to the high degree of parallelization . however , it does not improve as well as the linear dataset , because there is only a small room of performance improvement left to train the model .
2 shows the performance of the different parameter optimization strategies for each model with different number of parameters . the max pooling strategy consistently performs better in all model variations . feature maps are the most representative of the hyper parameters , and softplus is the only one that gives better performance . the hyper parameters activation func . and volkova coefficient are the only ones that perform better in the multi - model setup . finally , the learning rate and the number of iterations of the hgnst feature maps obtain the best performance .
1 shows the effect of using the shortest dependency path on each relation type . it empirically shows that macro - averaged model can achieve the best f1 ( in 5 - fold ) with sdp as well as the best diff . however , it is harder to solve the problems of generation of relation types without sdp . the comparison of our model with the strongest dependency path is difficult .
performance of the y - 3 models compared to the previous stateof - the - art models is presented in table 3 . in general terms , the performance of y - 2 models is significantly better than those of y - 3 ,
results are shown in table 4 . the first set of results show that mst - parser outperforms all other methods except for the word " paragraph " . the second set shows the performance of the model when combined with the number of instances in the essay dataset . the results are presented in tables 4 and 5 .
4 shows the c - f1 scores for the two indicated systems ; essay vs . paragraph level . note that the mean performances for the lstm - parser system are lower than the majority performances over the runs given in table 2 .
3 shows the performance of our system on the original and false test sets . our system outperforms all the other methods except for the original one . the results are presented in table 3 .
results for the original and the cleaned versions are shown in table 1 . we show the performance of our model with the highest number of distinct mrs , total number of textual references , and the number of slot matching script instances as measured by our data statistics , see section 3 . the number of instances in our original e2e dataset is significantly higher than the number in our cleaned version .
performance of original and original models on the test set is presented in table vii . original models generally perform better than the original model , however , their performance is slightly worse on test set .
manual error analysis of tgen on a sample of 100 instances from the original test set is shown in table 4 . we found a significant drop in the absolute number of errors caused by incorrect values in the training set .
model performance on the external and internal datasets is reported in table 4 . the dcgcn ensemble performs substantially better than the other models in terms of performance on both datasets , with the exception of peyrard et al . ( 2018 ) .
results on amr17 are shown in table 2 . our model achieves 24 . 5 bleu points and achieves 27 . 5 epmu points .
results in english - german and english - czech are shown in table 4 . the best performing model is the ggnn2seqb ( beck et al . , 2018 ) . the difference between the english - language and single - language models is minimal , but significant . the difference is due to the large number of examples in the dataset in which the model is used .
5 shows the effect of the number of layers inside the dc stack on the performance of the model in table 5 . for example , we observe that for every layer with two layers , there are three more layers that contribute to the overall performance .
results are shown in table 6 . rc + la also denotes gcns with residual connections . with residual connections , dcgcn2 ( 27 ) shows significant performance improvement . however , when gcn is combined with multiple connections , gcn performance drops significantly .
model 3 shows the performance of the dcgcn models compared to other stateof - the - art models . dcgcnn achieves the best performance with a minimum of 300 model b and 500 model d model e model f1 .
8 shows the ablation study results on the dev set of amr15 . the dense blocks in the i - th block signify removing the dense connections . they also reduce the number of dense connections in the Î² - dense blocks .
shown in table 9 , the global encoder and the lstm decoder use the best performing model . the ablation study results show that the global network and the multi - factor encoder have comparable performance on the graph encoder . however , the differences in performance between the two models are narrower .
7 shows the performance for different initialization strategies on probing tasks . our paper shows that our method achieves the best performance with a gap of 2 . 5 % in performance compared to other initialization strategies .
1 shows the performance of our method compared to other methods . we observe that our approach obtains the best performance . it achieves the best results with a gap of 2 . 8 % in precision and a boost of 3 . 6 % in accuracy .
subj and sick - r perform comparably to other methods . cbow outperforms both the hybrid and hybrid models in terms of mrpc score . however , it does not match the performance of mpqa . rather , it performs slightly better than both the original and the hybrid models . categories sst2 and sst5 are superior on mrpc , while ssts - b performs slightly worse on the hybrid model .
results on unsupervised downstream tasks attained by our models are shown in table 3 . hybrid outperforms cbow on almost all downstream tasks , but on the sts15 and sts16 datasets .
results for subj and sick - r are shown in table 8 . our paper shows that the initialization strategies perform better than the mrpc and mpqa tasks , respectively .
6 shows the performance for different training objectives on the unsupervised downstream tasks . our cmow - r model outperforms cbow - c on all the supervised tasks except for the sts14 .
observe that cbow - r outperforms all the other methods in terms of depth and event extraction . however , it does not have the best performance on the level of coordinv and topconst .
subj and sick - r perform comparably to other methods of classification . cbow - r outperforms all the other methods except for sst2 and sst5 . however , it has the advantage of training on the larger mrpc corpus .
3 presents the results of all methods for e + and per . our system outperforms all stateof - the - art methods in terms of all e + metrics and in all per metrics . the results reconfirm that the domain name matching ( mil - nd ) does indeed improve the e + metric , however , the performance drop is still significant for all systems that do not use the feature - rich org metric . supervised learning and cross - class learning achieve the best results with a minimum of performance drop .
2 : the results on the test set under two settings are shown in table 2 . our system outperforms all the previous models in terms of e + p and f1 scores . supervised learning results show that the system performs well in all settings . in fact , it even outperforms the previous stateof - the - art models in all metrics . name matching and recall are the only ones that do not perform well in the standard settings . name matching
6 shows the performance of the models trained on ref and ref compared to the original embeddings . ref outperforms gen , but does not exceed ref ' s performance .
results are shown in table 3 . the best performances are obtained by our model ( g2s - ggnn ) on the ldc2017t10 test set . note that our model outperforms all the other models in terms of bleu score on the test set , as measured by the number of frames in the table .
results on ldc2015e86 test set are shown in table 3 . our model outperforms all the other models using gigaword data .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . the results show that the use of bilstm improves the generalization ability of the model .
results are shown in table 4 . we observe that the g2s - gin model significantly outperforms the other models in terms of sentence length and sentence length . sentence length is reported in tables 1 and 2 and the average sentence length is computed in tables 3 and 4 .
shown in table 8 , the fraction of elements in the output that are not present in the input graph that are missing in the generated sentence ( g2s - gin ) . these tokens are used in the comparison of the reference sentences .
4 shows the performance of our method with different target languages . it achieves the best performance with 96 % accuracy using the 4th nmt encoding layer .
2 shows the pos and sem tagging accuracy with baselines and an upper bound . mft : most frequent tag ; unsupemb : second most frequently tag ; word2tag : upper bound encoder - decoder .
results are presented in table 4 . our proposed method outperforms all the other methods with a gap of 3 . 5 points from the last published results ( f1 ) .
5 shows the accuracy with different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages .
performance on different datasets is shown in table 8 . the difference between the performance of the attacker and the corresponding adversary is reported in p - value < 0 . 01 .
results in table 1 show that the training directly towards a single task can significantly improve performance .
2 shows the effect of the additional cost term on the balanced and unbalanced task averages . the classifier named pan16 in the second report shows that the classifier is able to detect instances of imbalance in the balanced task averages , but cannot detect them .
3 shows the performance on different datasets with an adversarial training set . the difference between the performance of the attacker and the corresponding adversary is shown in table 3 . according to the table , the training set gives a significant ( p < 0 . 001 ) boost in performance when the context is trained with a trained classifier .
6 shows the performance of the different embeddings for different encoders .
3 shows the performance of our model compared to previous work on the topic of finetune . our model outperforms all the other models with a large improvement in performance . the results reconfirm that our model performs well on both datasets when trained and tested on a single dataset . we observe that the lrn model achieves the best performance with a minimum of training time and a maximum of 80 % accuracy when using all the features available on the training dataset .
3 shows the performance of our model compared to previous stateof - the - art models . the results are shown in table 3 . we observe that our model performs better than previous models in terms of both the training time and thebert time . however , the difference is less pronounced for this model due to the longer training time .
3 shows the performance of our model compared to other models trained on the same topic . our model outperforms all the other models in terms of both err and f1 metrics . the results are summarized in table 3 . we observe that our model performs better than both the original and the yelppolar time model .
3 shows the bleu score on the test set of wmt14 english - german translation task . it can be seen from table 3 that the sru model considerably outperforms the gru model in terms of decoding one sentence . however , the difference is less pronounced for the gru model .
4 shows the performance of our model with respect to match / f1 score on squad dataset . it can be observed that our model performs better than other models with the parameter number of 1 . 67m .
6 shows the f1 score of our model on conll - 2003 english ner task . it can be seen that the lstm model considerably outperforms the other models in terms of parameter number .
performance on ptb task with base + ln setting and test perplexity on snli task with base - based setting .
3 shows the performance of the word - based systems for each domain . word - based system retrieval ( mtr ) is the most sophisticated on the system , and is used by more than 50 % of the time on all datasets . the hierarchical task learning method ( hvm ) is particularly effective for the task requiring a significant amount of data . all the other methods used by the system are multi - task , with a minimum of 80 % attention required .
4 shows the human evaluation results on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 8 . the best performance among all systems is highlighted in bold .
3 shows the performance of our model compared to the previous best state - of - the - art models . our model outperforms all the other models except for the one that we use in df . the difference is most pronounced on docsub , with the exception of eurparl . in df , our model performs slightly better than the other two models .
3 shows the performance of our model compared to the previous best state - of - the - art systems . our model outperforms all the other models except for the one that we use on docsub . the results are summarized in table 3 . we observe that our model performs slightly better than the other two baselines on three out of the four scenarios .
3 shows the performance of our model compared to the previous best state - of - the - art models . our model outperforms all the other models except for the one that we use in df . the difference is most pronounced on docsub , with the exception of eurparl . in df , our model performs slightly better than the other two baselines .
3 shows the performance of our model compared to the previous best state - of - the - art systems . our model achieves the best performance with a depth - cohesion score of 1 . 78 on each metric , outperforming all the other baselines except for the one that had the greatest impact on our model . we observe that our model has the worst performance .
3 shows the performance of our model compared to the previous best state - of - the - art systems . our model achieves the best performance with a maxdepth score of 9 . 43 on the dsim test set , while europarl achieves the highest score of 8 . 29 .
apply our system on the validation set of visdial v1 . 0 . the results are shown in table 1 . we observe that the enhanced version of our system , lf , performs better than the original version .
2 shows the performance ( ndcg % ) of different ablative studies on different models on the visdial v1 . 0 validation set . we apply p2 as the most effective one , while using coatt as the history shortcut .
5 compares the performance of these models on hard and soft alignments . the results are summarized in table 5 . the hmd - recall model outperforms both the soft and hard alignments with a large margin .
3 presents the results of our approach with respect to three key metrics : direct assessment ( ruse ) , eur - en and bertscore - f1 . the results are summarized in table 3 . the first set of results show that our approach significantly outperforms the previous stateof - the - art models on both metric sets .
results are shown in table 2 . the bleu - 1 and bertscore - f1 scores show high performance on both sets , with the exception of the case of sfhotel . the results are summarized in table 1 .
performance of the models according to these baselines is reported in table vi . the results are summarized in table vii . the summaries obtained by the models generally exceed the leic score by a significant margin . however , the results are still significantly worse than those obtained using spice .
3 shows the performance of all models trained on the shen - 1 corpus compared to the original embeddings . we observe that for all models , the model performs better than the original model on all metrics except for the tabula rasa .
results are shown in table 4 . semantic preservation and transfer quality scores are the most important features for all models trained on the Î´pp dataset . the results are presented in tables 4 and 5 .
5 shows the human evaluation results . the results are shown in table 5 . we apply the best performing method , Ï b / w negative pp , to match the quality of the sentences with the human ratings of fluency . we show that our system achieves the highest performance with 96 % of the machine and human judgments that match .
3 shows the performance of the models trained on the shen - 1 corpus compared to the previous stateof - the - art models . we observe that for all models , the model performs better than the original model on both sim and pp .
results on yelp sentiment transfer are shown in table 6 . our best model achieves higher bleu than any prior work at similar levels of acc â compared to simple - transfer . however , the difference is less pronounced for the unlabeled tweets than for the ones in the flipped - out corpus . multi - decoder models are not included as they are worse than the original model . sentiment embeddings have a generally positive effect ( i . e . name a vs . name b ) on sentiment transfer performance , but it is less significant for the untransferred ones .
2 shows the number of instances that were correctly predicted as disfluencies . the number of repetition tokens predicted to be in the disfluency range from 1 - 8 . reparandum length is reported in table 2 .
errors predicted as disfluent are shown in table 3 . the fraction of tokens predicted to contain a content word in the reparandum is less than the fraction predicted as contained in the repair word . table 3 shows the distribution of the tokens predicted for each category . for the three - step reparandum program , we have used the function - function as the source .
results are shown in table 4 . we observe that the text + innovations model outperforms the single model in terms of dev and test results . the results show that the use of text and innovations improves the model ' s performance in the production setting .
2 compares our model with state - of - the - art algorithms on the fnc - 1 test dataset . our model achieves the best performance with the help of the word2vec embeddings and the self - attention sentence embedding .
2 shows the performance of different methods on the apw and nyt datasets for the document dating problem . compared to previous methods , burstysimdater performs significantly better .
3 shows the performance of our method with and without attention . it shows the effectiveness of word attention and graph attention for this task . compared to ac - gcn , our model performs better .
3 shows the performance of models trained on stateof - the - art data augmentation systems . our model outperforms all the other models except for the one that performs on the gold - standard cnn network .
1 and table 2 show the performance of all methods for the event classification . our method outperforms all the other methods in terms of both event identification and classification . all methods cause a significant drop in performance when interacting with multiple entities .
3 shows the results for english and spanish . all the fine - tuned models appear to have better performance on the test set compared to the original spanish - only model .
4 shows the results on the dev and test set with only subsets of the code - switched data . fine - tuning achieves the best results with a 25 % train dev and a 75 % test set .
5 shows the performance of our system in the dev set compared to monolingual mode . we observe that our system performs better than the language - switched model in the test set , since the gold sentence in the cd is more stable .
results in table 7 show that the type - aggregated gaze features trained on the conll - 2003 dataset significantly improve recall ( p < 0 . 01 ) and f1 - score ( p ( cid : 0 . 01 ) .
5 shows the precision , recall and f1 scores for using the type - aggregated gaze features on the conll - 2003 dataset . the results are shown in table 5 . since the presence of these features in the pre - trained gaze , the performance is relatively consistent ,
results on belinkov2014exploring â s ppa test set . we use the syntactic - sg embeddings obtained by applying autoextend rothe and schÃ¼tze ( 2015 ) on wordnet 3 . 1 . we also use glove - retro as an initialization layer . the results on this set are shown in table 1 . we use syntactic sg as the initialization layer for wordnet , and it uses syntactic skipgram as the reference layer . our model improves upon the original wordnet by 9 . 5 % in performance .
performance of our system using various pp attachment predictors and oracle attachments . table 2 shows the results from the models using the best performing feature set , ontolstm - pp , and ppa acc . these models outperform the previous stateof - the - art models in terms of uas performance .
3 : effect of removing sense priors and context sensitivity ( attention ) from the model . the effect of removing context sensitivity is shown in table 3 .
2 : adding subtitle data and domain tuning for image caption translation ( marian amun et al . , 2018 ) with the help of subsfull embeddings . as expected , the multi30k dataset outperforms the other en - de models in terms of bleu % scores .
results are shown in table 4 . sub - domain - tuned models perform better on the largerickr16 and mscoco17 datasets than on the smallerickr17 datasets . table 4 shows the results for all subs1m models trained on the largeickr16 dataset .
4 shows bleu scores in terms of automatic captions with marian amun as our model . we also include results for en - de and mscoco17 . the results are shown in table 4 . compared to the original embeddings , multi30k is still better than all the other models .
5 compares our approach with other approaches for integrating visual information . we use transformer , multi30k + subs3mlm and detectron mask surface , enc - gate and dec - gate achieve similar results ( bleu % scores ) . however , we notice a drop in performance for enc - gate compared to the other approaches .
performance of subs3m compared to subs6m is presented in table 4 . the text - only model outperforms the ensemble - of - 3 model in terms of performance on both datasets , moreover , the improvements on the flickr16 dataset are larger than those on the mscoco17 dataset .
3 shows the performance of our model compared to the original embeddings . we observe that our model outperforms all the alternatives except for the one that is trained on the word " fr " .
shown in table 1 show the number of parallel sentences in the train , test and development splits for the language pairs we used .
2 : training vocabularies for the english , french and spanish data used for our models .
5 shows automatic evaluation scores ( bleu and ter ) for the rev systems . it can be seen that ter and en - fr - rev have higher performance than bleu , and ter have lower performance .
2 shows the performance of our visually supervised model compared to the mean mfcc score from flickr8k . the row labeled vgs is the result of re - scoring our model from the table 2 .
results on synthetically spoken coco are shown in table 1 . the visually supervised model outperforms the similarly supervised audio2vec - u model in terms of recall @ 10 and mean mfcc score .
1 shows the results of different classifiers compared to the original on sst - 2 . for example , dan shows that it turns in a < u > screenplay that shows the edges at the edges ; it â s so clever you want to hate it . similarly , rnn shows similar results as the original does .
2 shows the part - of - speech ( pos ) changes in sst - 2 . we see that the number of occurrences have increased , decreased or stayed the same through fine - tuning respectively . these results indicate that the value of the word " nouns " has not been significantly increased , but remain the same as in the original sentence .
3 shows the sentiment change in sst - 2 from positive to negative . it can be seen that the flipped sentiment labels have a significant effect on sentiment .
results are presented in table 2 . table 2 summarizes the performance of our method on the test set of pubmed andpubmed . our agent ( sift ) outperforms all the other methods with a large margin . the results are clear from table 2 : our agent ( 98 % ) has good performance on both test set and on the evaluation set .
