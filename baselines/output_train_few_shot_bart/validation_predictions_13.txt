2 shows the performance of the treelstm model on the large movie review dataset compared to the iterative approach , which performs better on inference with efficient parallel execution of tree nodes .
shown in table 1 , the balanced dataset exhibits the highest throughput thanks to the high degree of parallelization . however , it does not improve as well as the linear dataset when the batch size increases to 25 .
2 shows the performance of the max pooling strategies for each model with different hyper parameters . the maximum pooling strategy consistently performs better in all models with different representation . as shown in table 2 , the hgn outperforms sigmoid and softplus in all model variations .
1 shows the effect of using the shortest dependency path on each relation type . our model achieves the best f1 ( in 5 - fold ) with sdp compared to the macro - averaged model . the results are shown in table 1 . our model outperforms all the other models in terms of f1 by a significant margin .
results in table 3 show that the y - 3 model outperforms all the other models in terms of f1 and f1 scores .
3 presents the results of our study on the paragraph level . the results are presented in table 3 . our model outperforms all the other methods in terms of terms of f1 score . the best results are achieved on the essay level , with 50 % of the entries classified as invalid . we also outperform all the methods except for the mst - parser .
4 shows the c - f1 scores for the two indicated systems ; for the lstm - parser , it is 60 . 62 ± 3 . 54 and 57 . 24 ± 2 . 87 respectively , compared to the majority performances for the other two systems .
3 shows the results of the original and the second iteration . the results are shown in table 3 . the original and second iteration outperform the original in all but one of the three cases . in both cases , the errors are caused by incorrect labels . in the original case , the error is caused by the incorrect label on the tgen dataset , and in the second case , by incorrect label removal .
results in table 1 show that the original and the cleaned versions of the e2e data are comparable in quality ( number of distinct mrs , total number of textual references , ser ) as measured by our slot matching script , and the number of concatenated mrs in the slot matching scripts , respectively .
shown in table 1 , the original and the original embeddings perform better than the original ones . the results are shown in bold . the original embedding system outperforms the original in all but one of the three cases .
4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set . we found that adding incorrect values significantly increased the chance of errors , and caused slight disfluencies in the training set .
3 shows the performance of our dcgcn model on the external and external datasets compared to the previous best state - of - the - art models . all models outperform all the other models except for tree2str ( konstas et al . , 2017 ) in terms of both performance and recall . for the single dataset , all models perform better than all the others .
results on amr17 are shown in table 2 . our model achieves 24 . 5 bleu points in terms of parameters , compared to the previous best state - of - the - art model , seq2seqb ( beck et al . , 2018 ) achieves 28 . 3m and 27 . 5m bleus , respectively .
3 shows the results for english - german and english - czech . the results are presented in table 3 . the best performing models are published in english , german , french , turkish , russian , russian and turkish . we observe that the best performing model outperforms all the other models in both languages .
5 shows the effect of the number of layers inside dc on the quality of the layers . we observe that for all layers , there is a significant difference in the performance . for example , for example , when we add layers of layers , we see a drop of 2 . 5 points in performance compared to the previous state of the art .
results in table 6 show that the gcn with residual connections outperforms the other models with respect to residual connections . the results show that when gcn has residual connections , it can improve performance .
shown in table 1 , the dcgcn model outperforms all the other models in terms of both performance and recall . the results are shown in bold .
8 shows the results of ablation study on the dev set of amr15 . the results are shown in table 8 . - { i , 4 } dense blocks denotes removing the dense connections in the i - th block , whereas - { 3 , 4 } ) dense blocks denote removing the layers of the dense blocks .
9 shows the ablation study for the graph encoder and the lstm decoder . the results are shown in table 9 . the best performing model is the dcgcn4 , with a gap of 3 . 5 points in coverage between the two .
shown in table 7 , our method outperforms all the other approaches except for subjnum and coordinv . the results are shown in bold .
results are shown in table 1 . the best performing models are h - cbow and h - cmow , both of which outperform their counterparts in terms of precision . however , the best performing ones are the ones with the best performance .
3 presents the results of our method on the subj and mrpc datasets . our model outperforms both the sst2 and mpqa datasets in terms of mrpc performance . it is clear that the cbow / 784 model has superior performance on both datasets . however , the difference in performance between the two models is less pronounced on the mrpc dataset , indicating that the superior performance of both models is due to better recall .
results on unsupervised downstream tasks attained by our models are shown in table 3 . we observe that the cbow model outperforms the hybrid model in all but one of the three cases .
8 shows the results for all initialization strategies on supervised downstream tasks . our model outperforms all the other approaches except for subj and sst2 , which are comparable in performance .
6 shows the performance for different training objectives on the unsupervised downstream tasks . cmow - c outperforms cbow - r on all three tasks except for sts14 .
results are shown in table 1 . we observe that cbow - r outperforms all the other methods in terms of depth and precision . however , it does not achieve the best results in both categories .
results are shown in table 3 . we observe that the best performing cbow - r model outperforms both sst2 and sst5 in terms of mrpc performance . however , it does not outperform sst - b on all metrics .
results in table 3 show that the best performing system outperforms all the best systems in terms of e + org and per . in all but one case , the system performs better than all the other systems except for the one that does not have the best org performance .
results on the test set under two settings are shown in table 2 . our system outperforms all the previous models in e + p and f1 scores . in all but one case , it achieves the best e + f1 score . in both cases , the system performs better than the previous model .
6 : entailment ( ent ) and ref ) in the model outperform ref ( g2s - gat ) . note that ref outperforms ref in all but one of the cases .
results are shown in table 1 . the best performing models outperform the best performing ones in all but one of the three categories .
3 shows the results on ldc2015e86 test set when models are trained with additional gigaword data . the results are shown in table 3 . our model outperforms the previous state - of - the - art models in both external and external settings .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . the results show that bilstm improves the performance of the model by 3 . 5 % over the previous state of the art model .
results are shown in table 1 . we observe that the s2s model significantly outperforms the g2s - gin model in terms of sentence length and sentence length . sentence length is reported in bold as the average sentence length of the two models compared to the previous state - of - the - art models .
8 shows the fraction of elements in the output that are missing in the input ( g2s - gin ) , and the fraction of elements that are not present in the generated sentence ( miss ) . the results are shown in table 8 . the g2s model outperforms all the other models in terms of fraction and miss .
4 shows the performance of our system with different target languages trained with different features extracted from the 4th nmt encoding layer . the results are shown in table 4 .
2 shows the accuracy with baselines and an upper bound . the results are shown in table 2 . we use word embeddings as the classifier , and word2tag as the upper bound encoder - decoder . our model outperforms all the other methods .
results reported in table 1 show that the accuracy obtained by our method outperforms the performance obtained by the other methods .
5 shows the accuracy with different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages , and in all but one case , when using only one layer of the four layers .
performance on different datasets is shown in table 8 . the difference between the attacker score and the corresponding adversary ’ s accuracy is reported in δ . in pan16 , the attacker scored 14 . 3 % higher than the trained adversary .
1 shows the accuracies when training directly towards a single task . for pan16 , we trained with the word " sentiment " . the results are shown in table 1 . sentiment and gender are the most important factors in predicting the performance of a task .
shown in table 2 , the balanced and unbalanced data splits are significantly worse than those of balanced and balanced data splits .
3 shows the performance on different datasets with an adversarial training . the difference between the attacker score and the corresponding adversary ’ s accuracy is shown in table 3 . in both datasets , the gender - neutral responses are significantly worse than those of the classifier .
6 shows the performance of the embeddings with different encoders . the results are shown in table 6 . embedding rnn is better than embedded ,
results are shown in table 1 . the best performing models outperform all the models in terms of finetuning . we observe that the best performing model outperforms both the original and the finetune models in both cases . in particular , we observe that this model performs better than the lstm model by a significant margin .
results in table 5 show that our model outperforms all the other models in terms of training time . we observe that the lstm outperforms both the previous models in both the training time and the time to analyze the data . our model also outperforms the previous best models in all three domains .
3 shows the performance of our model compared to previous work on the amapolar time dataset . our model outperforms all the other models in terms of both err and f1 metrics .
3 shows the bleu score on wmt14 english - german translation task . our model outperforms all the other models in terms of decoding one sentence in the training batch . we also outperform the previous state - of - the - art models in both decoding and decoding .
4 shows the performance of our model on squad dataset . our model outperforms all the models in terms of match / f1 score . we also observe that our model performs better than the sru model in all but one case .
6 shows the f1 score on conll - 2003 english ner task . lstm * denotes the parameter number in the reported result , and lrn denotes the performance of the model . we also observe that the lrn model outperforms all the other models in terms of f1 scores .
performance on snli task with base + ln setting and test perplexity on ptb task with base setting . the results are shown in table 7 .
results are shown in table 1 . all systems trained on the word " italic " outperform system retrieval in terms of performance . the word " sent " significantly improves the results for both systems , sentrieval significantly outperforms the system in both systems . word " sent " significantly boosts the performance of the system ,
4 shows the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 ( k 1000 ) . the best results among all systems are highlighted in bold , with statistical significance marked with ∗ .
results are shown in table 1 . we observe that all the models trained on the corpus dataset outperform their counterparts in terms of performance . however , we observe that the best performing models are europarl and docsub , while the best are docsub .
results are shown in table 1 . we observe that all the models trained on the corpus dataset outperform their counterparts in terms of performance . the best performing models are europarl , df and docsub , while the best performing ones are docsub and tf . our model outperforms all the other models except for docsub .
results are shown in table 1 . we observe that the best performing english - based systems outperform the best - performing ones in terms of both df and tf . the best performing systems are docsub and europarl , respectively .
3 shows the performance of our system compared to the previous best state - of - the - art systems . our system achieves the best performance on all metrics , with a gap of 1 . 78 points between the best and worst scores . our approach outperforms the best in both metric and semantic embeddings .
3 shows the performance of our system compared to the previous best state - of - the - art systems . our system outperforms both the best and worst baselines in terms of depthcohesion . we observe that our system achieves the best performance on all metrics , with the exception of docsub achieving the best results .
performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . lf is the enhanced version of the original visdial model , and r3 is the weighted softmax loss , respectively .
2 shows the performance ( ndcg % ) of different baselines on different models on the visdial v1 . 0 validation set . the best performing model is lf , while the worst performing one is coatt .
5 shows the performance on hard and soft alignments . the hmd - f1 model outperforms all the pretrained models in terms of bert scores . however , it does not outperform both the soft and hard targets .
results are shown in table 1 . the results are presented in bold . our approach outperforms all the baselines except meteor + + and bertscore - f1 by a significant margin . the results of our approach are summarized in table 2 . the best performing approach is ruse + f1 .
results in table 1 show that the bertscore - f1 model outperforms all the baselines in terms of performance on all metrics except for bagel and sfhotel . the results are reported in table 2 . the bert scores are presented in bold , with the exception of bleu - 2 .
results are shown in table 1 . the baselines are presented in bold . leic and bertscore - recall achieve the best results , respectively , on the m2 and m2 datasets . sent - mover achieves the best performance on both datasets , with an overall improvement of 0 . 7 / 0 . 9 on the leic metric .
results in table 3 show that the m0 model outperforms the previous state - of - the - art models in all but one of the three cases .
results are shown in table 3 . semantic preservation and semantic preservation are the most important aspects of semantic preservation . the results are summarized in terms of the transfer quality and the semantic preservation scores . for semantic preservation , we use semantic preservation as the key metric for semantic preservation while semantic preservation is the core metric for syntactic preservation . we also use the semantic preservation feature of yelp , which allows semantic preservation without sacrificing semantic relations . syntactic preservation improves the semantic relations across all domains .
5 shows the results of human sentence - level validation . the results are shown in table 5 . the results of the human evaluation are comparable to those of the sim and human , but the difference is less pronounced for sim . moreover , the results are comparable for human evaluation .
results in table 3 show that the m0 + para + lang model outperforms the m2 model in all but one of the four scenarios . the results are shown in table 4 . in the case of the shen - 1 model , we see that it is better to have para - based models than to have a single - dimensional model .
results on yelp sentiment transfer are shown in table 6 . our best model achieves the highest bleu than any prior work at similar levels of acc ∗ . the best model outperforms all the other models except for the one that used the best embeddings . multi - decoder outperforms the best model by a significant margin .
2 shows the accuracy of the nested disfluencies for each repetition . reparandum length is reported in table 2 . the number of repetition tokens that are correctly predicted to be disfluent is shown in bold . for all instances , the repetition tokens are reported in the same sentence .
3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and the repair ( content - content ) . the fraction of tokens correctly predicted to contain the content word is shown in table 3 . in both cases , the number of tokens incorrectly predicted as containing a word is less than the number predicted for the disfluency . as shown in fig . 3 , the accuracy of the prediction for each category is lower than the accuracy for both .
results are shown in table 1 . text + innovations outperform text + text in all but one of the scenarios , in addition , text + innovations improves the model ' s performance by 0 . 2 points over the best state - of - the - art model . in particular , it improves the performance of the single model when using innovations .
2 shows the performance of our model on the fnc - 1 test dataset compared to the state - of - art algorithms on the test dataset . our model outperforms all the other models in terms of both accuracy and recall .
2 shows the performance of different methods on the apw and nyt datasets for the document dating problem . the best performing method is burstysimdater , which significantly outperforms all previous methods .
3 shows the performance of both the word attention and graph attention for this task . the results show that neuraldater performs better than the other two models in both instances .
results are shown in table 1 . the best performing models are cnn , dmcnn , jmee and trigger . we observe that the best performing model is the jnn model , which outperforms all the other models .
3 presents the results of our method in the event of a single event . our method outperforms all the other methods in terms of both event identification and event classification . in all but one case , the method has the best overall performance . in both cases , the identification and the event classification results are significantly better than the one in the other scenario .
results are shown in table 1 . all models trained on the original english - only model outperform all the other models except for those trained on spanish - only . we observe that the best performing model outperforms all the others in terms of dev metrics .
4 shows the results on the dev set and on the test set using discriminative training with only subsets of the code - switched data . the results are shown in table 4 . for the full train test dataset , we trained only on the train dev and only the train test data .
5 shows the performance of our system on the dev set compared to the monolingual set . our system achieves the best performance on the test set .
results in table 7 show that the type - aggregated gaze features trained on the conll - 2003 dataset significantly improve recall and f1 - score for the combined model .
5 shows the precision and f1 scores for using type - aggregated gaze features on the conll - 2003 dataset . precision ( p ≤ 0 . 001 ) and recall ( f1 - score ) are statistically significant improvements over the previous state - of - the - art model .
results on the original wordnet dataset are shown in table 1 . syntactic - sg embeddings are used in wordnet 3 . 1 and wordnet 4 . 1 , respectively . they are used for the syntactic embedding of wordnet and verbnet , respectively , and for the semantic embedding . they also use syntactic skipgram . syntactic skip - gram is used for wordnet , and it is used on verbnet . wordnet has the best performance on the ppa test set .
results from the rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . the results are shown in table 2 .
3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . we report the ppa acc . ( normalized by the number of frames ) for each context sensitivity evaluation . it is clear that the loss of context sensitivity significantly affects the model performance .
2 shows the results of combining subtitle data with domain tuning for image caption translation ( bleu % scores ) and multi30k decoding ( marian amun et al . , 2018 ) . adding subtitle data and domain tuning results in the same direction as the original embeddings , but with a significant drop in performance .
3 shows the performance of subs1m in en - de and out - of - the - fr contexts . the results are presented in table 3 . subdomain - tuned models outperform all the other models in terms of performance , with the exception of mscoco17 , which shows a slight improvement in performance . subdomain tuning improves the results for all models .
4 shows the bleu scores of the models using automatic captions . the results are shown in table 4 . as expected , the model with the best results with only the best five captions is better than the model without the best ones .
5 shows the performance of our approach with respect to dec - gate . the results are summarized in table 5 . we observe that our approach outperforms all the other approaches in terms of bleu % scores .
3 shows the performance of subs3m and subs6m in terms of text - only and multi - lingual features . the results are shown in table 3 . sub - saharan - based models outperform all the other models except for the one with the word embeddings . in the case of the single - language model , the best performance is obtained by combining text and semantic features with the semantic features .
results are shown in table 1 . the best performing en - fr - ff model outperforms both the best - performing and worst - performing models . we observe that the best performing model is en - rnn - ff , which improves the translation performance by 3 . 5 points .
shown in table 1 , the number of parallel sentences in the train , test and development splits for the language pairs we used . the number of pairs in each language pair is shown in bold .
2 shows the results for the english , french and spanish data used for our models .
5 shows the bleu and ter scores for the rev systems , respectively , compared to ter scores . the automatic evaluation scores ( bleu , ter ) show that the re - rev systems perform better than the original ones .
results on flickr8k are shown in table 2 . the vgs model outperforms segmatch and rsaimage in terms of recall @ 10 , respectively .
results on synthetically spoken coco are shown in table 1 . our model outperforms the previous state - of - the - art models in all but one of the three cases .
1 shows the results of the different classifiers compared to the original on sst - 2 . we report further examples in table 1 . for example , cnn turns in a < u > screenplay that shows the edges at the edges ; it ’ s so clever you want to hate it . for dan , we show that the edges of the screenplay are more interesting than the edges in the original . for cnn , the edges are interesting and the curves are clever . for rnn , we see that it is more interesting to turn in the edges than it is to turn on the edges . this shows that the use of these classifiers can improve the performance for hate speech .
2 shows the results of fine - tuning on sst - 2 . the results are shown in table 2 . we observe that the number of occurrences in the original sentence has increased , decreased or stayed the same , indicating that the quality of the sentence has not changed .
3 shows the change in sentiment with respect to the original sentence in sst - 2 . the results are shown in table 3 . positive labels are more frequently used than negative ones , indicating that the effect of negative labels is less pronounced on sentiment .
results are presented in table 1 . the results are summarized in bold . as expected , the results are slightly better than expected . however , the performance improvement is less than expected , indicating that our approach is more effective . in addition , our results are more consistent across all three categories . we report the results of our joint study with the aim of improving interpretability and interpretability .
