2 : throughput for processing the treelstm model on our recursive framework and tensorflow ' s iterative approach , with the large movie review dataset as our training example . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the iteration approach shows better performance on training .
shown in table 1 , the balanced dataset exhibits the highest throughput thanks to the high degree of parallelization . however , it does not improve as well as the linear dataset , because there is only a small room of performance improvement left to train the model .
2 shows the performance of the max pooling strategy for each model with different number of parameters . our system achieves the best performance with different iterations . the number of iterations with different hyper parameters is shown in table 2 . softplus performs better than sigmoid in all model variations . it also outperforms softplus in the accuracy of applying the filtering rate to the hyper parameters generated by the model .
1 shows the effect of using the shortest dependency path on each relation type . our model achieves the best f1 ( in 5 - fold ) with sdp and the best diff . the results are shown in table 1 . it can be observed that our model improves the f1 by 2 . 36 points without sacrificing too many relation types .
results are shown in table 3 . y - 3 shows significant performance improvement over the baseline on both f1 and r - f1 with the exception of the 50 % boost on f1 .
3 presents the results of our method in relation to the word embeddings . we present the results in table 3 . our method achieves the best results with 100 % accuracy on the test set . our model outperforms all the other methods with a minimum of 50 % accuracy . we observe that the accuracy obtained by our method is significantly better than those obtained by the original method .
4 shows the c - f1 scores for the two indicated systems ; for the lstm - parser , it is 60 . 62 ± 3 . 54 and 62 . 24 ± 2 . 87 respectively compared to the majority performances of the other systems .
results are shown in table 4 . original and original models are completely different from the original ones . we observe that the bleu score ( which shows the effectiveness of predictive accuracy ) is significantly better than those of tgen + on other systems . this is reflected in the fact that the accuracy obtained by the original model is much lower than those by tgen − which shows the effect of pre - training .
shown in table 1 , the original and the cleaned versions have the highest number of distinct mrs and the average number of instances of ser as measured by our slot matching script , see section 3 . for the original e2e data and our cleaned version , we have the best overall performance ( 71 . 42 % ) compared to the original ( 37 . 69 % ) . we also have the worst overall performance on the cleaned version ( 31 . 69 % ) compared with the original .
results are shown in table 2 . original and original results are presented in bold . original results show that tgen + has the best performing bleu score on both test sets . however , the accuracy drops significantly when adding incorrect information to the original results . these results result in a significant drop in performance for all models that rely on the original algorithm . this is reflected in the results of re - scoring the original scores for each test set .
results of manual error analysis of tgen on a sample of 100 instances from the original test set . the absolute numbers of errors we found are shown in table 4 . in general , the errors are caused by slight disfluencies in the training data . in particular , we found that adding incorrect values to training data caused a significant drop in performance .
3 shows the performance of our dcgcn model on the external and bias metric compared to the previous stateof - the - art models . for the external metric , we report all the performance on the single metric , with the exception of snrg ( which relies on syntactic or semantic word embeddings ) . the two - dimensional approach achieves the best performance on both metric with a gap of 2 . 7 % in performance between the original and the proposed seq2seqk model .
2 shows the results on amr17 . our model achieves 24 . 5 bleu points on the model size in terms of parameters , compared to the previous best state - of - the - art model , seq2seqb ( beck et al . , 2018 ) . we observe that the ensemble model performs better than the single - model model .
3 shows the results for english - german and english - czech . our model outperforms the previous best models in both languages . the results are shown in table 3 . we observe that the single model performs better in english - language than the other models that rely on the cnn + gcn embeddings model . our model achieves the best results in all languages when the number of models is used in the combination of the two .
5 shows the effect of the number of layers inside the network on the performance of the model in table 5 . the first group shows that when layers are added to the network , the effect is less pronounced for the other layers .
6 shows the performance of residual connections with gcns with residual connections . rc + la ( 2 ) and dcgcn4 ( 27 ) show that residual connections have a significant effect on gcn performance . however , when gcn has residual connections , the performance remains the same .
model 3 presents the results of our model on the bias metric . our model outperforms all the models except dcgcn in that the number of iterations is small but the improvement is significant . for example , dcgcnn ( 1 ) achieves a 17 . 2 % overall improvement on the previous state - of - the - art model over the previous model .
8 shows the ablation study results on the dev set of amr15 . we observe that the dense blocks in the i - th block have the greatest effect on the model performance . they reduce the number of dense connections by 2 . 5 % .
9 presents the ablation study for the graph encoder and the lstm decoder . the results of " - linear combination " and " - coverage mechanism " show that the global network has the greatest effect on the quality of the output .
7 shows the performance for different initialization strategies on probing tasks . our paper shows that our method obtains the best performance with a gap of 2 . 7 % in the precision score for each parameter .
can be seen in table 1 , the method with the highest precision is cbow / 400 . it obtains the best performance with a gap of 2 . 7 % in precision from the previous state of the art . it also achieves the best results with a 2 . 4 % boost in precision . however , this gap is narrower than those with the smallest gap .
3 presents the results of our method with respect to subj and mrpc . our model outperforms all the other methods except for cmp . cbow shows significant performance improvement on both mrpc and subj metric . it also outperforms both sst2 and sst5 on mrpc metric , it is clear from the results that the cbow / 784 model is more suitable for hybridization .
results on unsupervised downstream tasks attained by our models are shown in table 3 . hybrid models outperform both cbow and cmp . cbow on the sts13 and sts15 tasks , respectively . however , the relative change between the two models is less striking than in hybrid , which indicates that the cbow model performs better in supervised downstream tasks .
8 shows the performance of our initialization strategies on supervised downstream tasks . our model outperforms all the stateof - the - art models with respect to mrpc and mpqa scores . it is clear that the sick - e model performs better than the sst2 baseline , but it has the advantage of having access to more training data .
6 shows the performance for different training objectives on the unsupervised downstream tasks . our cmow - c model outperforms the cbow - r model on all three tasks except for the one that requires supervised supervision . the results for the sts13 and sts15 datasets are shown in table 6 .
results are shown in table 1 . we observe that cbow - c obtains the best performance with a gap of 2 . 5 % in precision from the previous state of the art model . however , it does not exceed the upper boundary of our model , which shows the diminishing returns from mixing the subtense and subtense functions . this underscores the extent to which our model can be improved with a reasonable selection of features . these features complement the strong performance of the previous model .
3 presents the results of our model on subj and mrpc . our model outperforms all the other methods except for cbow - c , which is more closely related to the sst2 model . the results of sick - e are summarized in table 3 . we observe that cbow improves the mrpc scores by 3 . 8pp over the previous state of the art models .
results in table 3 show that the system performs well in all aspects of the evaluation , with the exception of the e + function . the results of all org and per measures are shown in table 1 . all org measures are statistically significant , name matching and multi - task learning ( mil - nd ) achieve the best results with a minimum of 50 % org . supervised learning achieves the best e + and per scores with an absolute improvement of 2 . 36 % over the previous state - of - the - art model .
results on the test set under two settings are shown in table 2 . our system achieves the best results with 95 % confidence intervals of f1 score . we observe that the automatic learning model ( mil - nd ) outperforms the supervised learning model in all but one of the four scenarios . the accuracy of the model is shown in the table 2 . name matching improves the f1 scores by 2 . 5 % in the standard task formulation .
6 presents the results of ref and ref experiments on the g2s - gat dataset . the results are shown in table 6 . ref significantly outperforms gen , but it is less effective than ref because ref is more effective . in addition , ref also improves the generalization ability of the model .
3 provides detailed results on the ldc2017t10 and ldc2015e86 datasets . our model outperforms all the other models except for those using g2s - gat . the results are reported in table 3 .
3 shows the results on ldc2015e86 test set when models are trained with additional gigaword data . our model outperforms the previous stateof - the - art models in both external and internal settings . note that the g2s - ggnn model is more stable and therefore requires fewer training instances .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . it can be seen that the bilstm performs better than the traditional ldcstm model on the development set .
results are shown in table 1 . we observe that g2s - gin has the best overall performance on both δ and from the table 1 , we observe that the average number of frames per sentence is longer than the average length of the sentences , indicating that the model is more suitable for the task at hand . the results of applying the weighted average distance of the sentence length and the average sentence length are less significant than those of the other models showing the same performance .
8 shows the fraction of the elements in the output that are not present in the input ( g2s - gin ) , and the fraction of those that are missing in the generated sentence ( miss ) . it can be seen that the g2s models are better than the reference sentences in that their fraction of elements is missing . table 8 also highlights the effect of token lemmas on the output .
4 shows the performance of our system with respect to pos tags . we use the 4th nmt layer , trained with different target languages on a smaller parallel corpus ( 200k sentences ) . our system achieves the best performance with 96 . 7 % accuracy using the four - step nmt decoding layer .
2 shows the pos and sem accuracy with baselines and an upper bound . mft : most frequent tag ; unsupemb : most frequently tag ; word2tag : upper bound encoder - decoder .
results in table 1 show that our method significantly outperforms other methods in terms of accuracy . our results show that the accuracy obtained by our method is superior than those obtained by other methods .
5 shows the accuracy with different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . we find that for bi , pos has 87 . 9 % accuracy and res has 90 . 9 % . on the other hand , for bi - specific nmt , it has 88 . 5 % accuracy .
8 shows the performance of the trained attacker on different datasets . the difference between the attacker score and the corresponding adversary ’ s accuracy is shown in table 8 . for pan16 , the attacker scored 14 . 3 % on a training set 10 % held - out . similarly , the age of the target is 9 . 7 % .
1 shows the accuracies when training directly towards a single task . our system obtains the best performance with a minimum of training time . we observe that the training time with the word " task " is significantly longer than the one with the other two features .
2 shows the effect of the additional cost term on the balanced and unbalanced task averages . the classifier pan16 ( which pretrained upon the word " task " has a negative effect on the task performance ) and the classifier has a positive effect . dial and sentiment both have negative effects on task performance , however , the effect on classifier performance is less pronounced .
3 shows the performance on different datasets with an adversarial training . the difference between the attacker score and the corresponding adversary ’ s accuracy is shown in table 3 . in particular , we notice significant difference in the performance of the trained classifier , pan16 , compared to the trained pan16 model . sentiment and gender are the most important factors in predicting whether an attacker will miss a task or not .
6 shows the concuracies of the protected attribute with different encoders . embedding rnn is easier for the model to do than it is for the original embeddings .
3 presents the performance of our model with respect to finetuning . our model outperforms the previous stateof - the - art models in both modeling and finetune . the results are shown in table 3 . we observe that our lstm model achieves the best performance with a minimum of 0 . 05 bias and 0 . 01 bias . our model further improves upon the strong lemma baseline on the wt2 dataset with a maximum of 22m on the training set . finally , we observe that the performance improvement on the final set of models is modest but significant , improving upon the performance by 2 . 36 bias metric by 3 . 36 points .
5 shows the performance of our model compared to previous models . the results are shown in table 5 . we use the best performing model with a minimum of 2 . 87m iterations and a maximum of 3 . 45m iterations . table 5 shows that our model has the best performance on both the base time andbert datasets . it also outperforms the previous models in both the base time and the average time of training data .
3 shows the performance of our model in real - time compared to the previous stateof - the - art models . the results are shown in table 3 . we observe that our model significantly outperforms other models in terms of both the average time and the number of iterations in the yearly average time . however , our model performs slightly worse than our model on both the yahoo time and the amafull time datasets . finally , we observe that the improvement over our model is due to the increased accuracy of the model in the amapolar time dataset . table 3 compares our model with other models that use the same time span .
3 shows the bleu score on the wmt14 english - german translation task . our model improves upon the state - of - the - art gru by 0 . 5k in seconds on the training batch and by 1 . 15k in the final task . it also outperforms the original gnmt model by a noticeable margin .
4 shows the performance of our model with respect to match / f1 score on squad dataset . it can be seen that our model improves upon the strong lemma baseline by adding the parameter number of β - params to the model .
6 shows the f1 score on conll - 2003 english ner task . the lstm model is significantly better than the other models that use the same number of parameters . it can be seen that the lrn model performs better in the low - supervision setting than the gru model .
7 shows the performance of our model with base + ln setting and test perplexity on ptb task with base setting . the results are shown in table 7 . it can be observed that our model significantly improves upon the previous state of the art on snli task with the base setting setting .
3 shows the performance of our system with respect to oracle retrieval . we use word embeddings ( mtr ) and word2vec to train our system . the word - based approach is more effective than the system - based one we use for system evaluation . word - based approaches are used to train the system with a minimum of 80 % chance of error on the validation set . in general terms , the system is better than the human - based system , although it is more suitable for system evaluations .
4 presents the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 8 . the best performance among all the automatic systems is shown in bold . our system is ranked in the top 1 or 2 for each of the four aspects .
2 presents the results of our model on the test set of corpus . our model outperforms all the other models except for those using docsub . the results are shown in table 2 . we observe that our model performs better than both the original df and docsub models . however , the difference between the performance of docsub and our model is larger than those of our original model .
results are shown in table 3 . we observe that the best performing model is europarl , followed by the more realistic df model . our joint model outperforms both the df and docsub models in terms of performance . however , our joint model performs slightly worse than the df model , which suggests that our approach is more suitable for the task .
2 presents the results of our model on the test set of corpus . our model outperforms all the other models except for those using docsub . the results are shown in table 2 . we observe that our model performs better than both the original df and docsub models . however , the difference between the performance of docsub and our model is larger than those of our original model .
are shown in table 3 . our system achieves the best performance with a maxdepth of 1 . 78 on the dsim metric compared to the previous best state - of - the - art model , europarl . however , our system suffers from a significant drop in performance compared to our maxdepth metric . this is mostly due to the high overlap in the number of iterations between maxdepth and maxdepth . this underscores the extent to which our system can be improved with the help of additional cost - effective features .
are shown in table 1 . our system achieves the best performance with a maxdepth of 79 on the dsim metric compared to 1 . 5 . europarl also achieves the highest score with a truedepth of 1 . 1 .
performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . we note that lf is the enhanced version of the original visdial model , and r3 is the weighted softmax loss . it further reduces the effect of hidden dictionary learning .
2 shows the performance ( ndcg % ) of different ablative studies on different models on the visdial v1 . 0 validation set . we apply p2 as the most effective one ( i . e . , hidden dictionary learning ) . note that only applying p2 with the history shortcut achieves the best performance .
5 compares the performance of these models on hard and soft alignments . the results are shown in table 5 . the hmd - f1 model outperforms both the soft and hard alignments with bert scores .
3 presents the results of our approach with respect to direct assessment . the results are presented in table 3 . we observe that our approach significantly improves the performance of our model compared to the baseline bertscore - f1 baseline on three of the four test sets . our approach significantly outperforms the baseline on all three sets except for the one in which it is tested .
3 presents the bagel and sfhotel scores . the results of these models are summarized in table 3 . they use bleu - 1 and bertscore - f1 measures , respectively , to derive the scores for each model . the results are presented in tables 3 and 4 . they show that both the b - agel scores and the corresponding scores are significantly better than those of other baselines . the b - gel scores are markedly better than the other baseline scores indicating that the model has good predictive performance .
performance of the models according to these baselines is reported in table 3 . the results are summarized in bold . the summaries obtained by the models generally outperform the baselines on all metrics except for those using bertscore - recall . these results show that the combination of elmo and p < 0 . 005 gives a significant performance boost to the m2 score .
results are shown in table 3 . we observe that for all models that use the shen - 1 embeddings , the model performs better than the previous state - of - the - art models .
results are shown in table 4 . we present the results of our model with a focus on semantic preservation . the results are presented in tables 4 and 5 . the results of the best performing model are summarized in the final results . we observe that the semantic preservation approach significantly improves the results for both semantic and semantic preservation , respectively . semantic preservation is further improved with the addition of semantic features to the model . yelp , in turn , achieves the best results with a transfer quality score of 69 . 5 / 71 . 7 / 72 . 4 . 3 .
5 presents the human evaluation results . we show the results of human evaluation using the [ italic ] ρ b / w negative pp and human ratings of fluency . these results show that the accuracy obtained by human evaluation is relatively high , which indicates that the quality obtained by machine evaluation is high .
results are shown in table 3 . we observe that for all models that use the word " shen - 1 " , the performance improvement is minimal . however , for those using m0 + para + lang , the performance drop is significant ( p < 0 . 001 ) . we notice that for the m2 model , the gap between the performance of the word ' shen ' and " matrix ' is much smaller than for m3 .
results on yelp sentiment transfer are shown in table 6 . our best model achieves higher bleu than any prior work at similar levels of acc ∗ . however , the improvement is less than that of simple - transfer , because the classifiers in use are worse than those in the previous work . we also observe that the use of semantic embeddings can further improve the model ' s interpretability . tweets containing multiple classifiers can improve the interpretability of the sentence prediction .
statistics for nested disfluencies are shown in table 2 . the number of repetition tokens that are correctly predicted to be disfluent is in the low - supervision range . reparandum tokens are generally longer than repetition tokens .
3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and the repair ( table 3 ) . the fraction of tokens predicted to contain a word in each category is in parentheses , indicating that the disfluency in the reparandum is caused by the function of the word in the repair .
results are shown in table 4 . we observe that the use of text and innovations improves the model ' s performance in the low - supervision settings . in addition , the improvement in the best - performing model is less pronounced for the single model . the text + innovations model outperforms the other models in terms of both dev and test mean .
2 compares our model with the state - of - art algorithms on the fnc - 1 test dataset . our model achieves the best performance with the ability to agree and disagree without significantly increasing the f1 score . however , the accuracy is still significantly lower than the average of word2vec embedding .
2 shows the performance of different methods on the apw and nyt datasets for the document dating problem . compared to previous methods , burstysimdater significantly outperforms all previous models .
3 shows the accuracy ( % ) of our method with and without attention . this results show the effectiveness of both word attention and graph attention for this task . please see section 6 . 2 for more details . the performance of ac - gcn is also shown in table 3 . the accuracy of our model is significantly higher than that of the other two components .
3 shows the performance of the models trained on the proposed dmcnn framework . our model outperforms all the models except for those trained on trigger . we observe that the model performs better on all stages , with the exception of the one in which it is tested on . the results are presented in table 3 .
3 shows the performance of our method in the event of a single event . our method outperforms all the other methods in terms of both event identification and event classification . all the methods used in this analysis fail to distinguish between the two scenarios . in particular , the method that relies on word embeddings for identification and classification has the worst performance on both events . the method is used in the context of cross - event learning . the method has the advantage of significantly improving the f1 score .
can be seen in table 4 , all the models trained on the proposed concatenated word embeddings are shown in bold . all the fine - tuned models seem to be better than the original spanish - only model in terms of test accuracy . however , the difference between the performance of the original english - only and the original french - only models is much smaller . this is mostly due to the high overlap between the features of the two languages .
4 shows the results on the dev set and on the test set using discriminative training with only subsets of the code - switched data . we observe that fine - tuning achieves the best results with only 25 % train dev and 75 % train test .
5 shows the performance on the dev and test set compared to the monolingual model in the standard set . fine - tuned - lm shows a slight improvement over the performance of fine - tuned - disc in the dev set and on the test set , though still performing worse than fine - uned - disc .
results in table 7 show that type - aggregated gaze features significantly improve recall and f1 - score for the three eye - tracking datasets .
5 shows the precision and f1 scores for using the type - aggregated gaze features on the conll - 2003 dataset . the results are shown in table 5 . type - aggregation features significantly improve recall ( p < 0 . 01 ) and precision ( f1 - score ) for using type - based gaze features .
results on belinkov2014exploring ’ s ppa test set . we use hpcd ( full ) and glove - retro ( part - of - speech embeddings ) . the results on this set are shown in table 1 . syntactic - sg embedding gives the best performance on wordnet 3 . 1 . however , it is less effective than syntactic - based wordnet , which relies on skipgram for embedding . we also use syntactic embedding as the base layer of our wordnet semantic network . the results show that semantic embedding can further improve the performance of wordnet .
2 shows the results of our system with features coming from various pp attachment predictors and oracle attachments . the results from table 2 show that the combination of oracle pp and lstm - pp pre - trained models significantly improves the ppa acc . score for all models except rbg .
3 : effect of removing sense priors and context sensitivity ( attention ) from the model . the effect of removing context sensitivity is shown in table 3 .
2 shows the results with domain - tuned and multi30k captions . the results with subssfull are shown in table 2 . adding subtitle data and domain tuning for image caption translation ( bleu % scores ) improves the overall results with marian amun ' s model . subsfull captions have a generally positive effect , but when combined with domain tuning , the effect is less pronounced .
results are shown in table 4 . we observe that subs1m is better than the other models in the en - de setup on both datasets , with the exception of the one that has domain - tuned h + ms - coco in the lab . the results are presented in tables 4 and 5 . table 4 summarizes the results of the combined models . we show that the combined model achieves an overall improvement of 3 . 6pp over the previous state of the art on all datasets .
4 shows the bleu scores of models using automatic captions . we show the results in table 4 . the model using the multi30k model outperforms all the models using the decoder . the results with the exception of the model using mscoco17 are shown in bold . we notice that combining the two features results in the same sentence , but with the best one or all 5 captions is beneficial .
5 compares the performance of our approach with prior approaches on en - de and flickr16 . we observe that our approach obtains better results than those using transformer , multi30k + ms - coco + subs3mlm , and detectron mask surface .
results are shown in table 4 . we observe that subs3m has superior performance on the en - de dataset compared to subs6m on the large - scale datasets of flickr16 and mscoco17 . moreover , the multi - lingual model without the visual features improves the performance for both datasets . for example , the combination of the word embeddings and the semantic features boosts performance for the larger - scale dataset . finally , combining the features of the two improves the overall performance .
3 shows the performance of our system compared to the original embeddings . we use the word " forward " instead of " phrased " . the results are shown in table 3 . we observe that our system significantly outperforms the alternatives in terms of ttr and ame .
shown in table 1 , the number of parallel sentences in the train , test and development splits for the language pairs we used .
2 : training vocabularies for the english , french and spanish data used for our models . we use the src and trg embeddings . the results are shown in table 2 . our model outperforms both the english and french baseline .
5 shows the bleu and ter scores for the rev systems . automatic evaluation scores ( bleu ) and ter measures show that the model can significantly improve the performance of its rev system .
2 shows the performance of our visually supervised model compared to the standard rsaimage model from flickr8k . the row labeled vgs is the average rank of the models trained on the schematics of chrupala2017representations . the difference in performance is minimal , however it is statistically significant .
results on synthetically spoken coco are shown in table 1 . the model trained on the embedded embeddings of chrupala2017representations is significantly better than the similarly supervised audio2vec - u model . the difference is less pronounced for rsaimage , but still significant for vgs .
1 shows the results of different classifiers compared to the original on sst - 2 . we report further examples in table 1 . for example , cnn turns in a < u > screenplay that shows the edges at the edges ; it ’ s so clever you want to hate it . in the same way , dan shows the effect of the word " hate " on the screenplay . in the other hand , rnn shows the effects of a word “ hate ” on the edges of a screenplay . it is so clever to show the effect that the word has on the evaluation results . this shows that when a word is shown in a screenplay , the attention is directed towards the words “ hate ” and “ n * gga ” .
2 shows that fine - tuning has indeed increased the number of occurrences in sst - 2 . this is evident from the large difference in pos scores between the original sentence and those of punct . ( i . e . , those that belong to the dan sub - category ) . note that the pos scores of some words have increased , while others have stayed the same . these numbers indicate that the amount of occurrences have not increased , but remain the same , indicating that the quality of the sentence has not changed .
3 shows the change in sentiment from positive to negative in sst - 2 . it can be seen that the flipped sentiment labels have a significant effect on sentiment , as shown in table 3 .
results are shown in table 2 . we observe that the competitive advantage of using word embeddings is relatively high ( p < 0 . 005 ) compared to using plain text summarisation ( sst - 2 ) . the results are presented in tables 2 and 3 . our joint model ( sift - 2 ) outperforms the competition on three of the four aspects . the most striking thing about the results is that it is able to distinguish between positive and negative aspects of the model . this is reflected in the performance of our joint model in the pubmed setting .
