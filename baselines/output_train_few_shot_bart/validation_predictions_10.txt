2 shows the performance of our recursive approach on the large movie review dataset compared to the iterative approach . throughput performs best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training .
shown in table 1 , the balanced dataset exhibits the highest performance compared to the linear dataset when the batch size increases from 1 to 25 .
2 shows the performance of the max pooling strategies for each model with different representation size . the maximum pooling strategy consistently performs better in all model variations . as shown in table 2 , the model with the smallest number of hyper parameters has the highest performance on all models .
1 shows the effect of using the shortest dependency path on each relation type . the best f1 ( in 5 - fold ) without sdp is obtained by using the best dependency path with sdp . we also show the f1 obtained using sdp as the shortest path . our model outperforms the macro - averaged model in terms of f1 and diff .
3 shows the performance of the y - 3 model compared to the previous state - of - the - art models in terms of f1 and f1 scores .
3 shows the performance of our proposed model on the essay level . our proposed model outperforms all the previous models in terms of performance on the paragraph level . the results are summarized in table 3 .
4 shows the c - f1 scores for the two indicated systems ; for the lstm - parser , it is 60 . 62 ± 3 . 54 and 57 . 24 ± 2 . 87 , respectively , compared to the majority performances for the other two systems .
3 shows the performance of our proposed system on the test set . the results are presented in table 3 . we show the results of the original and the correct ones . the best performing system is the sc - lstm model , which has the best performance on both test sets . the best performance is on the correct test set , while the best performing ones are on the wrong set .
shown in table 1 , we compare the original and the cleaned versions of our e2e data with the original ones . the number of distinct mrs , total number of textual references , and the number of slot matching scripts as measured by our slot matching script , see section 3 . we also compare the performance of our original and our cleaned versions with that of the original .
3 presents the results of the original and the second set of experiments . the results are presented in table 3 . the original and second set are shown in bold . the second set shows the performance of the two sets of test sets on the bleu and nist datasets .
4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set . we found that adding incorrect values to the training set can cause disfluencies , which can be seen in table 4 . adding incorrect values can also cause a disfluency to occur .
3 shows the performance of our dcgcn model on the external and external datasets compared to the previous best state - of - the - art models . we observe that our proposed model outperforms all the other models in terms of performance on both datasets .
2 presents the results on amr17 . our model achieves 24 . 5 bleu points on the model size in terms of parameters , respectively . the results are shown in table 2 . we observe that our dcgcn model outperforms all the ensemble models ,
3 presents the results for english - german and english - czech . the results are presented in table 3 . we show the results of our model in english - language and german - language . our model outperforms all the other models in terms of performance in both languages . the best performing model is bow + gcn ( bastings et al . , 2018 ) . we also show the performance of the single and the multi - language models in english .
5 shows the effect of the number of layers inside dc on the overall performance of the layers . table 5 shows that for each layer , there are three layers of layers that contribute to the performance . we observe that for all but one of these layers , there is a significant drop in performance for the other layers .
6 shows the performance of the baselines with residual connections . we observe that the gcn has the highest performance on the b - test compared to the previous state - of - the - art gcn . rc + la ( 2 ) and gcn + rc ( 4 ) show that the residual connections have a significant effect on gcn performance .
3 shows the performance of our dcgcn model on the b - test set . we observe that the dcgcnn model outperforms all the other models in terms of performance on both test sets .
8 shows the ablation study on the dev set of amr15 . table 8 shows that removing the dense connections in the i - th block improves the performance of the model . the reduction in the number of dense connections indicates that the model is more suitable for the task at hand . we also observe that the removal of the dense blocks improves the quality of our model .
9 presents the ablation study for the graph encoder and the lstm decoder . the results are shown in table 9 . we observe that the global network encoder has the best coverage , with a gap of 2 . 5 points between the gap between the two .
7 shows the performance of our initialization strategies on probing tasks . we show that our method outperforms all the other approaches except for subjnum and coordinv .
3 presents the results of our proposed method on the table . table 3 shows the performance of our method in terms of depth and subtraction . the results are presented in table 3 . it is clear that our method obtains the best performance on the subtraction test set .
3 presents the results of our model on the mrpc and mpqa datasets . our model outperforms all the other models in terms of performance on both datasets . we observe that the cbow / 784 model performs better than both the sst2 and sst5 models on both metrics .
3 shows the performance of our models on unsupervised downstream tasks attained by our models . we show the relative change with respect to hybrid compared to cbow . our model outperforms both the hybrid and hybrid models in terms of performance .
8 shows the performance of our initialization strategies on supervised downstream tasks . our model outperforms all the other models in terms of mrpc and mpqa scores . it is clear that the sst2 model is better at initialization than sst3 .
6 shows the performance for different training objectives on the unsupervised downstream tasks . the cmow - c model outperforms the cbow - r model on all three tasks .
results are shown in table 1 . we observe that cbow - c has the best performance on all three metrics , with the exception of subjnum .
3 presents the results of our model on the subj and mrpc datasets . our model outperforms all the other models in terms of both mrpc and mpqa performance . we observe that our model performs better than the other two models in both respects .
3 presents the e + and per scores for each system in table 3 . the e + org scores are shown in bold . in [ italic ] e + per scores , we see that the system performs better than all the other systems in terms of e + or gisc scores . on the other hand , it performs worse than the other two systems in all but one of the table 3 shows that the best performing system is the one that performs best in all of the three scenarios .
2 shows the performance of our system on the test set under two settings . our model outperforms all the other models in terms of e + p , e + r and f1 scores . the results are shown in table 2 . our model achieves the best performance in all three settings .
6 : entailment ( ent ) and ref ( ref ) on the model of g2s - gat is shown in table 6 . the model outperforms all the other models in terms of ref , ref and gen scores . in particular , the model performs better on the ref scores than on the gen scores , indicating that the model is more suitable for the task .
3 presents the results of our model on the ldc2017t10 and ldc2015e86 datasets . the results are presented in table 3 . we observe that the model outperforms all the other models in terms of performance on both datasets .
3 shows the results on ldc2015e86 test set when models are trained with additional gigaword data . the results are shown in table 3 . our model outperforms the previous state - of - the - art models in both external and external settings .
4 presents the results of the ablation study on the ldc2017t10 development set . the results are summarized in table 4 . the bilstm model outperforms all the other models in terms of size and performance .
results are shown in table 1 . we observe that g2s - gin has the best overall performance on the graph diameter and graph diameter metrics . in particular , it has the highest average f1 score of all the models in terms of f1 scores . on the other hand , we observe that the model has the worst overall performance of all models . as a result , it is more likely to outperform the model on both the graph and semantic metrics .
8 shows the fraction of elements in the input graph that are missing in the generated sentence ( g2s - gin ) , and the fraction of elements that are not present in the output graph ( miss ) , for the test set of ldc2017t10 . it can be seen in table 8 that the model outperforms the reference sentences in terms of fraction .
4 shows the performance of our model on a smaller parallel corpus ( 200k sentences ) with different target languages . our model outperforms all the other models in terms of accuracy .
2 shows the pos and sem accuracy with baselines and an upper bound . mft : most frequent tag ; unsupemb : most frequently tag ; word2tag : upper bound encoder - decoder embeddings ; and sem : highest precision .
3 shows the performance of the pos tagging accuracy and the accuracy of the f1 scores for each of the four categories . the results are shown in table 3 . table 3 summarizes the performance for all four categories , except for the one for f1 .
5 shows the accuracy with different layers of the uni / bidirectional / residual nmt encoders , averaged over all non - english targets .
8 shows the performance of the attacker on different datasets . the difference between the attacker score and the corresponding adversary ’ s accuracy is shown in table 8 . for pan16 , the attacker scored 14 . 3 % on a training set 10 % held - out .
1 shows the accuracies for each task trained directly towards a single task . as table 1 shows , the training directly towards the task has the greatest effect on the performance .
2 shows the performance of our model on the balanced task and unbalanced task splits . we observe that our model has the best performance on both tasks , with the exception of the one in which it has the worst performance .
3 shows the performance on different datasets with an adversarial training . the difference between the attacker score and the corresponding adversary ’ s accuracy is shown in table 3 . in particular , the difference between gender and age is significant , in pan16 , gender is the most important factor in predicting the performance of the target .
6 : accuracies of the protected attribute with different encoders . we show the performance of rnn and rnn when embeddings are used .
3 presents the results of our work on the model with respect to the finetune model . the results are summarized in table 3 . our model outperforms all the other models in terms of model performance . we also observe that our model performs better on the " finetune " model than the " lstm " model , we observe that the performance of the lstm model is comparable to that of the previous models .
3 shows the performance of our model on the base acc and bert datasets . we observe that our model has the best performance on both datasets , with the exception of lstm , which has the worst performance on bbert dataset .
3 shows the performance of our model on the four different datasets . we show the performance on the three different datasets in table 3 . our model outperforms all the other models in terms of err performance . the best performing model is the yelppolar time model , which is comparable in performance to the lstm model .
3 presents the bleu score on the wmt14 english - german translation task . we show the performance of our model in terms of time in seconds per training batch measured from 0 . 2k training steps on the newstest2014 dataset . it is clear that our model is able to decode one sentence in a single training batch compared to the previous state - of - the - art system .
4 shows the performance of our model on squad dataset . we observe that the parameter number of base is significantly larger than that of lrn and sru , indicating that the model is more suitable for match / f1 - score prediction . further , we observe that our model performs better than the other models in terms of parameter number .
6 shows the f1 score on conll - 2003 english ner task . lstm * denotes the parameter number in the reported result . it also denotes the performance of the model . we observe that the model performs better than all the other models in the system . the performance of lrn is comparable to that of gru .
7 shows the performance of our model on snli task with the base + ln setting and test perplexity on ptb task with base setting . table 7 shows that our model performs well on both tasks .
3 shows the performance of our system in terms of word evaluation . we use word evaluation as a baseline for our system , word evaluation is the most effective . the word evaluation results are presented in table 3 . word evaluations are performed in the context of system retrieval , the average word evaluation is weighted in the order of sentence length . in the case of system evaluation , word evaluation and word evaluation are the best tools for the task . using word evaluation , we use word - based evaluation as the baseline .
4 presents the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 . the best performance among all systems is shown in table 4 .
3 shows the performance of our models on the df and df datasets . our model outperforms all the other models in terms of performance on both datasets . we observe that our model performs best on df , df , tf , tf and tf datasets .
3 presents the results of our experiments . we show the performance of our models on the df and df datasets . the results are summarized in table 3 . our model outperforms all the other models in terms of performance . we observe that our model performs better on df than on df .
3 shows the performance of our models on the df and df datasets . our model outperforms all the other models in terms of performance on both datasets . we observe that our model performs better on df than on df , but still performs worse on df .
3 shows the performance of our model in terms of the depthcohesion metric . our model outperforms all the other models on the metric , except for the one in which it has the highest performance .
3 shows the performance of our model in terms of the maxdepth and maxdepth metrics . our model outperforms all the other models on both metric metrics , except for the one in which it has the best performance .
1 shows the performance ( ndcg % ) on the validation set of visdial v1 . 0 . lf is the enhanced version of the original visdial model , and r1 , r2 , r3 denote regressive loss , weighted softmax loss and generalized ranking loss , respectively .
2 shows the performance ( ndcg % ) of different ablative studies on different models on the visdial v1 . 0 validation set . the best performing model is the one using the history shortcut .
5 compares the performance on hard and soft alignments . we observe that the hmd - f1 model outperforms all the other models in terms of bert scores .
3 presents the performance of our model on the direct assessment test set . our model outperforms all the other models on the test set by a significant margin . the results are summarized in table 3 . it is clear that our model has the best performance on both test sets .
3 presents the baselines on the bagel and sfhotel datasets . the baselines are presented in table 3 . we observe that the bleu - 1 baseline outperforms all the other baselines except for bertscore - f1 , which shows that it is possible to improve the performance of both baselines without sacrificing performance .
3 presents the metric and baseline scores for each setting . the metric scores are presented in table 3 . the baselines are summarized in terms of leic scores , bertscore - recall scores , and spice scores . we observe that the leic score is slightly higher than the spice score , indicating that the model is more suitable for the task . finally , the baselines show that the models are more suitable to the task of training the learner .
results are shown in table 3 . we observe that the m0 model outperforms the previous state - of - the - art models in all but one of the three cases .
3 presents the results of our model on the semantic preservation and semantic preservation datasets . the results are summarized in table 3 . we show the performance of the model on all three datasets , with the exception of semantic preservation , which is the case for semantic preservation . our model outperforms all the other models in terms of transfer quality . as expected , the performance on semantic preservation is significantly better than semantic preservation on all other datasets .
5 presents the results of human sentence - level validation . we show the performance of our model on the test set in table 5 . the human ratings of semantic preservation are significantly higher than those of the machine and human judgments that match . our model outperforms both the human and the machine in terms of performance .
results are shown in table 3 . we observe that the m1 model outperforms the m2 model in terms of performance . in particular , we observe that m1 models are more likely to perform better than m2 models . the difference between the performance of m1 and m2 is due to the fact that the shen - 1 model performs better than the other models .
6 shows the bleu scores of our models on yelp sentiment transfer . our best model achieves the highest acc ∗ score , but the worst performing model is the one that relies on semantic embedding . we also observe that using semantic embeddings improves the performance of our model by a significant margin . in addition , we observe that the use of syntactic embedding improves the quality of the model .
2 shows the percentage of reparandum tokens that are correctly predicted to be disfluent . for nested disfluencies , the repetition tokens are the most frequently predicted to have a disfluency . the number of repetition tokens is slightly larger than the number of disfluences .
3 : relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and the repair ( content - content ) . table 3 shows the percentage of tokens correctly predicted to contain a word in each of the three categories . the fraction of tokens that contain the content word is shown in table 3 . it is clear that the disfluency in the reparandum is caused by the function - function misfluent .
model outperforms all the other models in terms of dev and innovations performance . table 1 shows the performance of the model on the single and multi - step test set . we observe that the model performs best on both the single - step and multistep test sets , with the exception of the one in which the text + innovations feature is used .
2 shows the performance of our model on the fnc - 1 test dataset . our model achieves the best performance on the micro f1 ( % ) and self - attention embedding ( c ) test datasets .
2 shows the performance of different approaches on the apw and nyt datasets for the document dating problem . the best performing method is burstysimdater .
3 presents the performance of our model with and without attention . we show that our model obtains the best performance on both word attention and graph attention tasks .
3 presents the performance of our proposed model on the test set . our proposed model outperforms the previous state - of - the - art models in all but one of the three stages . the results are summarized in table 3 . we observe that our proposed method outperforms all the other models in terms of performance .
3 presents the results of our method on the event identification and event classification task . our method outperforms all the other methods in terms of f1 score . in the event of a single event , the method is used to identify and classify the events in the event .
3 shows the performance of all models in english - only mode . all models are comparable in terms of dev perp , test acc and test wer , respectively . in english , all models perform better than all the other models except for those that do not use the word perp .
4 shows the results on the dev set and on the test set using discriminative training with only subsets of the code - switched data .
5 shows the performance of our model on the dev set and the test set . we observe that our model performs slightly better than the monolingual model on both sets , but still performs worse than the original model .
shown in table 7 , the performance of type - aggregated gaze features trained on all three eye - tracking datasets is comparable to those trained on the conll - 2003 dataset . the precision ( p ) , recall ( r ) and f1 - score ( f ) are statistically significant improvements over the baseline .
5 shows the performance of type - aggregated gaze features on the conll - 2003 dataset . precision ( p ) , recall ( r ) and f1 - score ( f ) are statistically significant improvements over the previous state - of - the - art model ( table 5 ) . type - aggregation features have a significant impact on recall ( p < 0 . 001 ) .
1 shows the hpcd results for wordnet and verbnet 3 . 1 . syntactic - sg embeddings are derived from the original wordnet paper , and are used in wordnet to embed the semantic skipgram encoder . the syntactic embedding of glove - retro is used in verbnet , and it uses syntactic skipgrams encoder for embedding the embedding . we note that the semantic embedding used in the original paper is syntactic .
2 presents the results of the rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . the results are presented in table 2 .
3 : effect of removing sense priors and context sensitivity ( attention ) from the model . table 3 shows the effect of removing context sensitivity and the ppa acc . from table 3 , we observe that the loss of context sensitivity is accompanied by increased attention .
2 shows the performance of our model with domain tuning for image caption translation ( bleu % scores ) and domain tuning ( marian amun et al . , 2018 ) . subdomain tuning improves the performance by 3 . 5 points over multi30k .
3 shows the performance of subs1m in en - de and in - de . the results are presented in table 3 . subdomain - tuned models outperform the other models in terms of performance . we observe that the performance is comparable across all domains , with the exception of the subdomain tuning . in the case of the spanish sub - domain , we see that the model performs better than the english subs - coco on all domains .
4 shows the bleu scores of the models using automatic captions . our model outperforms all the other models except for those using the multi30k model . the results are shown in table 4 . as expected , the model with the best image captions outperforms the model using only the best ones .
5 compares the performance of our enc - gate and dec - gate strategies for integrating visual information . the results are summarized in table 5 . we observe that our approach achieves the best performance on the en - de dataset , we also observe that enc - gates have the most positive effect on the bleu % scores . finally , we observe that decoding the visual information improves the performance by a significant margin .
3 shows the performance of subs3m and subs6m in terms of text - only and semantic features . sub - text - only models outperform all the other models on the en - de test set , in particular , we see that the semantic features are the most important part of the overall performance . in terms of semantic features , the performance is comparable to that of the ensemble - of - 3 models . the performance of the multi - lingual models is also comparable to those of subs4m ,
3 shows the performance of our system on the mtld test set compared to the previous state - of - the - art models . the results are summarized in table 3 . we observe that the en - fr - rnn - ff model outperforms all the other models in terms of performance on the test set .
shown in table 1 , the number of parallel sentences in the train , test and development splits for the language pairs we used . the number of lines in each language pair is shown in bold .
2 presents the training vocabularies for the english , french and spanish data used for our models .
5 presents the bleu and ter scores for the rev systems . as table 5 shows , the automatic evaluation scores ( bleu , ter ) are significantly worse than those of the manual evaluation scores .
2 shows the performance of our visually supervised model on flickr8k . the vgs model is shown in table 2 . we observe that it performs better than segmatch and segmatch in terms of recall @ 10 .
1 : results on synthetically spoken coco are presented in table 1 . we show the performance of the hierarchical supervised model with the highest f1 score . it is clear from table 1 that the model performs better than other hierarchical supervised models in terms of recall .
we report further examples in table 1 . for example , cnn turns in a < u > screenplay that u > at the edges ; it ’ s so clever you want to hate it . table 1 shows the performance of the different classifiers compared to the original on sst - 2 . it is clear that cnn turns on a on the edges of the screenplay that is so clever that it is difficult to hate .
2 presents the results of fine - tuning on sst - 2 . the results are shown in table 2 . we observe that the number of words in the original sentence has increased , decreased or stayed the same . the number of occurrences in the second sentence has also increased , indicating that the quality of the sentence has not changed .
3 shows the change in sentiment in sst - 2 compared to the original sentence . it can be seen in table 3 that negative labels are flipped to positive , and vice versa . in the case of cnn , the positive label is flipped to negative .
results are presented in table 1 . the results are summarized in bold . it is clear that the performance of sift is strongly linked to the quality of the literature , and that it is important to investigate whether the results are positive or negative . table 1 summarizes the results of our proposed method in terms of ppmi score . our proposed method is similar in that it has the best performance .
