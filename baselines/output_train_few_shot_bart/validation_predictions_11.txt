2 shows the performance of the treelstm model on inference and training datasets , with the large movie review dataset as the training dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the iterative approach shows better performance on training .
shown in table 1 , the balanced dataset exhibits the highest throughput thanks to the high degree of parallelization , but at the same time does not improve as well as the linear dataset .
2 shows the performance of the max pooling strategies for each model with different representation size . the maximum pooling strategy consistently performs better in all model variations . as shown in table 2 , the average number of parameters in each model is significantly larger than the number of features in the other models . the performance of sigmoid and ud v1 . 3 is comparable to that of softplus and softplus in all but one of the three cases .
1 shows the effect of using the shortest dependency path on each relation type . the best f1 ( in 5 - fold ) is obtained with sdp and the worst f1 is obtained using sdp . the relation type with the most dependency path has the highest f1 and the best diff . diff . is higher than the relation type without sdp , indicating that the dependency path is more suitable for the task at hand .
3 shows the performance of the y - 3 model compared to the previous state - of - the - art models in terms of f1 and f1 scores . the results are shown in table 3 . in the case of the r - f1 model , the difference between the average f1 score and the average score of the other models is much smaller .
results are presented in table 1 . the results of our method are shown in bold . the best performing method is mst - parser , which achieves the best results on paragraph level f1 and on essay level r - f1 . our method outperforms all the other methods in terms of word embedding and word embeddings . our approach achieves the highest performance on both categories .
4 shows the c - f1 scores for the two indicated systems ; for the lstm - parser , it is 60 . 62 ± 3 . 54 and 69 . 24 ± 2 . 87 , respectively , compared to the majority performances of the other systems .
shown in table 1 , we show the results of the original and the second set of test sets . the results are presented in tables 1 and 2 . table 1 shows the results for each set . the best performing set is the sc - lstm , which has the best performance . the worst performance is on the bleu test set , where the performance is significantly worse than on the original .
shown in table 1 , the original and the cleaned e2e data are comparable in terms of mrs , mrs and ser as measured by our slot matching script , see section 3 .
results are shown in table 1 . the original system performs better than the original on all but one of the four test sets . the best performing system is sc - lstm , which has the best performance on all test sets except for the one in which it has the worst performance . the worst performance is on the test set , where the performance is significantly worse than on the original .
4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set . we found a total of 22 errors in the original training set , and a slight number of them in the disfluency setting .
table 1 shows the performance of our dcgcn model on the external and external datasets compared to the previous state - of - the - art models . the results are presented in tables 1 and 2 . table 1 . all models outperform all the other models in terms of performance . for example , seq2seqk has the best performance in the external dataset , with a drop of 0 . 2m over the baseline .
2 shows the results on amr17 . our model achieves 24 . 5 bleu points in terms of parameters , compared to the previous state - of - the - art seq2seqb model , which achieves 27 . 5 points . the results of our model are shown in table 2 .
3 presents the results of the english - german and english - czech models on the test set . the results are presented in table 3 . we show the results in english - language and german - language , respectively . our model outperforms all the other models except for the german model , which is more suitable for english - speaking contexts . the results show that our model performs well in both languages , with the exception of german , where the results are slightly worse than english .
5 shows the effect of the number of layers inside dc on the quality of the layers in the output . table 5 shows that for all layers , there is a significant drop in the performance of the layer size compared to the previous state - of - the - art .
6 shows the performance of gcn with residual connections compared to the baselines . rc denotes gcn that has residual connections with the residual connections . the results are shown in table 6 . with residual connections , gcn has a better performance than other baselines in terms of overall performance .
shown in table 1 , the dcgcn model outperforms all the other models in terms of performance . the results are summarized in table 2 . the best performance is achieved in the second set of models , where the average number of participants is closer to 10 . 5m .
8 shows the ablation study on the dev set of amr15 with the dense blocks in the i - th block . the results are shown in table 8 . the dense blocks have the highest density of the three dense blocks , and the number of dense connections has the highest . table 8 shows that removing the dense connections in the - th blocks has a significant effect on the performance .
9 shows the ablation study for the graph encoder and the lstm decoder . the results are shown in table 9 . the best performing encoder is the dcgcn4 , which has the highest coverage of all the nodes . however , the best performing decoder has the worst performance .
7 shows the performance of our initialization strategies on probing tasks . our model outperforms the state - of - the - art in all but one of the three ways . it is clear from table 7 that our approach obtains better results than the state of the art .
results are shown in table 1 . table 1 shows the performance of our method in terms of depth and subtraction . we observe that our h - cmow model outperforms all the other methods except for h - cbow / 400 , which shows a slight improvement in performance .
3 presents the results of our model on the subj test set . our model outperforms all the other models except for the one that has the best performance . the best performance is on the mrpc test set , which is comparable to sst2 and sst5 . we observe that the cbow / 784 model is better than the other two models in terms of performance . it also outperforms the other three by a significant margin .
3 shows the performance of our model on unsupervised downstream tasks attained by our models . the results are shown in table 3 . our model outperforms both the hybrid and hybrid models in terms of performance . cbow shows the relative change in performance with respect to cmp . hybrid shows a slight improvement in performance .
8 shows the performance of our initialization strategies on supervised downstream tasks . our model outperforms all the state - of - the - art initialization strategies except for subj , which is more comparable to sst2 and sts - b .
6 shows the performance for different training objectives on the unsupervised downstream tasks . the cmow - c model outperforms the cbow - r model on all three tasks except the sts13 and sts14 tasks . however , the difference between the two approaches is less pronounced .
results are shown in table 1 . the best performing method is cbow - c , which outperforms all the other methods in terms of depth and subtraction . it achieves the best performance in all but one of the three categories . it obtains the best results in all three categories , with the exception of subjnum , where it obtains a better performance .
3 presents the results of our approach on the subj and mrpc datasets . our approach outperforms both the sst2 and sst5 baseline by a significant margin . our model outperforms all the other methods except for sst1 , which is more comparable to the mrpc baseline . we observe that our approach obtains the best performance on all three datasets . the best performance is obtained on both datasets .
3 shows the e + and per scores for each system . all org scores are reported in table 3 . the best performing system is mil - nd , which has the best e + org score and the best per score . in table 3 , we show the results for all systems with the exception of the one that has the worst e + or per scores . we also show the performance of the best system with the best org and best per scores , the results are shown in table 4 . our system outperforms all the other systems except for the one with the worst performance .
results on the test set under two settings are shown in table 2 . the best performing system is mil - nd , which outperforms all the other models in e + p and f1 scores . it also improves the e + f1 score by 2 . 5 points compared to the previous state - of - the - art model , which improves the performance of the system by 2 points .
6 : entailment ( ent ) on the g2s - gin model is presented in table 6 . we observe that the model outperforms all the other models in terms of ref and ref scores , with the exception of s2s , where ref outperforms ref .
3 presents the results of our model on the ldc2015e86 and ldc2017t10 datasets . the results are presented in table 3 . our model outperforms all the other models in terms of bleu score , with the exception of g2s - gin .
3 shows the results on ldc2015e86 test set when models are trained with additional gigaword data . our model outperforms the previous state - of - the - art models in both external and external settings . the results are shown in table 3 .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . bilstm significantly outperforms the other two models in terms of size and performance .
results are shown in table 1 . we observe that the average number of sentences per sentence is significantly larger than the average length of the sentence , indicating that the model is more suitable for the task at hand . in particular , we observe that g2s - gin has the best overall performance on average compared to other models in terms of average sentence length .
8 shows the fraction of elements in the output that are missing in the input ( added ) and the fraction of elements that are not present in the generated sentence ( miss ) . gold refers to the output of the reference sentences , while g2s - gin is used in the comparison . the fraction of missing elements in an input graph is lower than the fraction that are present in a generated sentence .
4 shows the accuracy of the two features extracted from the 4th nmt encoding layer , trained with different target languages on a smaller parallel corpus ( 200k sentences ) .
2 shows the pos and sem accuracy with baselines and an upper bound . mft : most frequent tag ; unsupemb : most frequently tag ; word2tag : upper bound encoder - decoder ; and sem : more frequent tag .
results are shown in table 1 . pos tagging accuracy is significantly better than the average accuracy on both datasets , and is comparable to the best performance on the two datasets . the results are presented in table 2 .
5 shows the accuracy with features from different layers of the 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . the results are shown in table 5 .
8 shows the performance of the trained attacker on different datasets . the difference between the attacker score and the corresponding adversary ’ s accuracy is shown in table 8 . for pan16 , the average score is 14 . 3 , while the average accuracy is 15 . 3 .
1 shows the accuracies when training directly towards a single task . for pan16 , we trained directly towards the task , and the results are shown in table 1 . as expected , the training results are significantly worse than those for pan16 .
2 shows the state of the art for classifier leakage . we show the results for both languages , with the exception of pan16 , which shows a slight drop in performance compared to pan16 . the results are shown in table 2 . in both cases , the average number of tweets per conversation is slightly higher than those in the other two languages .
3 shows the performance on different datasets with an adversarial training . the difference between the attacker score and the corresponding adversary ’ s accuracy is shown in table 3 . in pan16 , the average error is significantly higher than that of the trained target , indicating that the training target is more likely to target the target .
6 shows the concatenation of the protected attribute with different encoders . for example , rnn embeddings the word " leaky " and " rnn " . for rnn , the encoded word " laky " is encoded as a word with a different encoder .
results are shown in table 2 . the best performing model is the lstm model , which outperforms all the other models in terms of finetune performance . our model improves upon the state - of - the - art model by a significant margin . we show the results of our model in table 1 . it outperforms both the best performing models by a large margin .
results are presented in table 5 . we show the performance of our model on the base acc and the bert datasets . our model outperforms all the other models in terms of training time , with the exception of the lstm model , which has the best performance on the bbert dataset . the average time taken to train is about 2 . 5 seconds , and the average number of training sessions is about 3 . 5 hours .
results are presented in table 4 . we show the performance of our model on the four different datasets . our model outperforms all the other models in terms of err . the best performing model is yelppolar time , which is comparable to the best state - of - the - art model on both datasets . table 4 shows the average performance of the models on all four datasets .
3 shows the bleu score on wmt14 english - german translation task . our model outperforms all the other models in terms of time in the training batch measured from 0 . 2k training steps on the newstest2014 dataset . it also outperforms the best state - of - the - art system on the german translation task as well .
4 shows the performance of our model on the squad dataset compared to the previous state - of - the - art model . we observe that our model performs better than the baselines in terms of match / f1 score , indicating that the model is more suitable for the task at hand . in addition , our model outperforms all the other baselines except for the lstm model .
6 shows the f1 score on conll - 2003 english ner task . lstm * denotes the parameter number in the reported result . sru is comparable to gru in terms of number of parameters , but is slightly better than gru .
performance on snli task with base + ln setting and test perplexity on ptb task with the base setting setting . table 7 shows the performance of the snli model with the base setting setting as well as the evaluation results for ptb .
results are shown in table 2 . word embeddings significantly improve the performance of the human model compared to the previous state - of - the - art system . in general , word embedding improves the performance by 2 . 5 points compared to human - based system retrieval in particular , the performance improvement is more pronounced for human - aligned systems than for system - based systems . as a result , the improvement is less pronounced for both human and system - specific systems .
4 shows the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 . the best results are highlighted in bold , with statistical significance marked with ∗ ( table 4 ) . the best result is candela , which is ranked in the top 1 or 2 for overall quality . the worst result is h & w hua and wang ( 2018 ) ( p < 0 . 001 ) .
results are shown in table 3 . the best performing english language language is europarl , followed by the best performing german language language , both in terms of performance and quality . our joint model outperforms both the df and docsub models , however it is slightly worse than our joint model . for the docsub model , the best performance is achieved on the low - supervision test set ( p < 0 . 001 ) .
results are shown in table 1 . the best performing english language language is europarl , which outperforms both the best and worst performing german language languages . however , it is significantly worse than both the german language and the french language . for example , the english language is slightly better than the german - language language , but still slightly worse than the english - language .
results are shown in table 3 . the best performing english language language is europarl , followed by the best performing german language language , both in terms of performance and quality . our joint model outperforms both the df and docsub models . the performance of our joint model is comparable to that of the other two , but is slightly better than the best - performing german language .
3 shows the performance of our model compared to the previous best state - of - the - art models . our model outperforms all the other baselines except for the best ones , such as df , docsub and hclust . the results are shown in table 3 . we observe that our model achieves the best performance on all the baselines , with a drop of 1 . 78 points in performance compared to our baseline .
3 shows the performance of our model compared to the previous best state - of - the - art models . our model outperforms all the other baselines except for the best ones , such as df , docsub and hclust . we observe that our model performs better than the best state of the art on both metric and metric metrics . for example , our model achieves a better performance on metric metrics than those of the other best baselines .
1 shows the performance ( ndcg % ) of our model on the validation set of visdial v1 . 0 . lf is the enhanced version of the original visdial model , and r2 is the improved version . the difference in performance between qt and lf is less pronounced , but the difference is larger .
2 shows the performance ( ndcg % ) of different ablative studies on different models on the visdial v1 . 0 validation set . the most effective method is p2 , which is implemented by the implementations in section 5 .
5 shows the performance on hard and soft alignments . the hmd - f1 model outperforms both the soft and hard alignments in terms of bert scores .
3 shows the performance of our model on the direct assessment test set . our model outperforms all the other baselines except for the two that we tested in table 1 . the results are summarized in table 2 . we observe that our model significantly outperforms the baselines in both direct assessment and the bertscore - f1 score by a significant margin .
performance of bleu - 1 and bertscore - f1 on the baselines is shown in table 1 . the bagel baseline outperforms all the other baselines in terms of performance , with the exception of the sfhotel baseline , which is slightly better than the bleu - 2 baseline . we observe that the bert score has a significant drop in performance compared to the baseline , with a drop of 0 . 5 points in performance over the baseline .
3 presents the metric and baseline scores for each setting . the metric scores are presented in table 3 . the leic scores are shown in bold . the meteor scores are derived from the leic score , which shows that the lecithin scores are comparable to those of other baselines . bertscore - recall scores are obtained using a weighted average of bert score - mover and wmd - 1 scores . we observe that the weighted average bert scores are significantly higher than those of the baselines , indicating that the training data are more suitable for the task at hand .
results are shown in table 3 . we observe that the m0 model outperforms all the other models except for the one that has the best performance on the shen - 1 test set .
results are presented in table 2 . we present the results of our model on the transfer quality and semantic preservation datasets . the results are summarized in table 1 . semantic preservation outperforms the semantic preservation baseline in all but one of the three cases . for semantic preservation , we compare the performance of the two datasets in terms of transfer quality . as expected , the results are significantly better than those of semantic preservation . our model outperforms all the other baselines except for the one in which it is more difficult to achieve the best performance .
5 shows the results of human sentence - level validation on the human and machine sentences . the results are shown in table 5 . the human ratings of semantic preservation are significantly higher than the human ratings , indicating that the human judgments are more accurate than the machine ones . we also observe that the accuracy of the semantic preservation ratings is much higher than that of the machine and human ratings .
results are shown in table 1 . we observe that the m0 model outperforms the m1 model in terms of performance on all metrics except for grammatical accuracy . in particular , we observe that m0 models outperform the m2 model in both grammatical and semantic terms .
results on yelp sentiment transfer are shown in table 6 . the best model achieves the highest bleu than the best model , yang2018unsupervised , which is comparable to the best state - of - the - art model . multi - decoder outperforms all the other classifiers except for the word embeddings . in addition , the best classifier is simple - transfer , and the worst performer is the classifier that is used in the most recent work , delete / retrieve . table 6 shows the performance of our best model in the sentiment transfer task .
2 shows the number of tokens that are correctly predicted as disfluencies . reparandum tokens have the highest average number of repetition tokens , but are more frequently predicted to be disfluent . for example , the repetition tokens have a higher average length than the disfluency tokens .
3 shows the percentage of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and the repair ( content - function ) . the percentage of tokens correctly predicted to contain a word is shown in table 3 . for disfluency that contains a part of a word , the fraction of tokens that contain the word is significantly higher than the fraction predicted as containing the word . for example , for the reparandum , the number of tokens is slightly lower than the number predicted as a function word .
model outperforms all the other models except for the single model , which shows the best performance on the test compared to the best model . we show the best results for each model in table 1 . the best results are shown in bold . text + innovations outperform text + text in all but one of the four cases . in addition , text + innovations improves the performance of the model on the single and the one with the best improvement .
2 shows the performance of word2vec embedding on the fnc - 1 test dataset compared to the state - of - art algorithms on the test dataset . our model significantly outperforms all the other embeddings in terms of both accuracy and performance .
2 shows the performance of different methods on the apw and nyt datasets for the document dating problem . the best performing method is burstysimdater , which significantly outperforms all previous methods in terms of accuracy .
3 shows the performance of our method for word attention and graph attention compared to the previous state - of - the - art neuraldater . the results are shown in table 3 . for word attention , the performance is comparable to that of ac - gcn . for graph attention , the accuracy is comparable .
3 shows the performance of our model on each stage . the best performing model is the jnn model , which outperforms all the other models except for the one with the worst performance . we observe that jnn outperforms jnn in all but one of the three stages , while the other two show lower performance .
3 presents the results of our method on the event identification task . our method outperforms all the other methods in terms of both identification and classification . in all but one case , our method obtains a significant advantage over all the others . in both cases , the identification task is more difficult to solve than the other ones . cross - event identification tasks are harder to solve because of the large number of participants .
results are shown in table 1 . all but the english - only - lm model outperforms all the other models in terms of dev perp , test acc and test wer . in addition , all but the spanish - only model outperform all the others in the test set by a significant margin .
4 shows the results on the dev set and on the test set using discriminative training with only subsets of the code - switched data . the results are shown in table 4 . for the full train test dataset , we use fine - tuned train dev and fine - tuned train test . the difference between the results is small , but significant .
5 shows the performance of our system on the dev set and the test set , compared to the state of the art for monolingual and code - switched languages . our system performs better than the state - of - the - art on both sets .
shown in table 7 , type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset . precision ( p ) , recall ( r ) and f1 - score ( f ) are statistically significant improvements over the baseline ( p ≤ 0 . 05 ) on the type combined and type combined datasets .
5 shows the precision and f1 scores for using type - aggregated gaze features on the conll - 2003 dataset . precision ( p ) and recall ( f1 ) are statistically significant improvements over the baseline , while f1 - score is significantly lower than the baseline ( p < 0 . 05 ) .
results on the original wordnet test set are shown in table 1 . syntactic - sg embeddings are used in wordnet 3 . 1 , and glove - retro is used in verbnet 4 . 1 . the syntactic embedding feature is used on wordnet , and it uses syntactic skipgram for embedding .
2 shows the results of the rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . the results are presented in table 2 . rbg has the best uas performance and the best ppa accomplishment .
3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . we show the ppa acc . ( normalized by the number of frames ) and ppa ppa ( perceived as high ) on the performance of the model when removing context sensitivity . it is clear from table 3 that the removal of context sensitivity has a significant effect on ppa .
2 shows the results of adding subtitle data and domain tuning for image caption translation ( bleu % scores ) . the results are shown in table 2 . adding subtitle data significantly improves the performance of multi30k compared to the previous state - of - the - art embeddings .
shown in table 1 , subs1m outperforms all the other models except for the one with domain - tuned h + ms - coco on the en - de dataset . the results are presented in tables 1 and 2 . subsequently , the results are shown in tables 3 and 4 . subdomain tuning improves the performance of the subs2m model by 2 . 5 points over the previous state - of - the - art model .
4 shows bleu scores in terms of the best captions and the best ones in % . the results with marian amun are shown in table 4 . as expected , the automatic captions outperform the multi30k ones . in the en - de dataset , the best one or all 5 captions are displayed in the best part of the image captions . however , the improvement is less than the improvement with the concatenated ones .
5 shows the performance of our approach on en - de with respect to dec - gate . the results are summarized in table 5 . we use transformer , multi30k + ms - coco + subs3mlm , and detectron mask surface to integrate visual information ( bleu % scores ) . we observe that the enc - gate approach improves the performance by 2 . 5 points over the previous state - of - the - art approach .
3 shows the performance of subs3m on the en - de dataset compared to subs6m . the results are presented in table 3 . we observe that the subs2m model outperforms all the other models in terms of multi - lingual features , with the exception of mscoco17 , which shows a slight improvement in performance compared to the previous state of the art model .
results are shown in table 1 . we observe that the en - fr - ht model outperforms all the other models in terms of translation performance . table 1 shows the performance of the two models in relation to each other . in particular , we observe that en - rnn - ff has the best performance on mtld compared to en - es - ht . the difference in performance between the two is due to the large number of errors in the translation algorithm .
shown in table 1 , the number of parallel sentences in the train , test and development splits for the language pairs we used . the average number of pairs is 1 , 467 , 489 and the average number is 5 , 734 .
2 shows the training vocabularies for the english , french and spanish data used for our models . the results are shown in table 2 . our model outperforms all the other languages except english .
5 shows the bleu and ter scores for the rev systems compared to the ter baseline . the automatic evaluation scores are shown in table 5 . we observe that the en - fr - rev and en - es - trans - rev systems outperform all the other systems except for the one in which ter is used . in the case of en - rnn - rev , we observe that ter is better than en - s - rev .
2 shows the performance of our visually supervised model on flickr8k . the results are shown in table 2 . we observe that the vgs model outperforms all the other models in terms of recall performance .
results on synthetically spoken coco are shown in table 1 . the vgs model is the visually supervised model from chrupala2017representations . it has a significantly higher recall rate than the other two models . the difference in recall rate between the two is statistically significant .
1 shows the results of the different classifiers compared to the original on sst - 2 . we report further examples in table 1 . for example , cnn turns in a < u > screenplay that shows the edges at the edges ; it ’ s so clever you want to hate it . dan shows that the edges of the screenplay are better than the edges . cnn shows that it is better to have the edges in the screenplay than to have them in the dialogues . for cnn , the edges are better and the curves are better . for rnn , we show the results as a result of the clever use of the word " hate " in the paragraph . for other networks , we see that using the word “ hate ” is a better way to express hate .
2 presents the results of fine - tuning on sst - 2 . the results are shown in table 2 . we observe that the number of words in the original sentence has increased , decreased or stayed the same . table 2 shows the percentage of words that have increased or decreased in the average number of occurrences . the most striking thing is the change in pos scores .
3 shows the change in sentiment in sst - 2 compared to the previous state of the art . negative labels are flipped to positive and vice versa . as shown in table 3 , negative labels have a larger effect on sentiment than positive ones . in particular , they have a significant effect on the sentiment score of cnn .
results are presented in table 2 . the results are summarized in terms of ppmi and the percentage of positive and negative responses . in general , the results are comparable to those obtained by pubmed , although the difference is smaller . table 2 shows the results of our joint study on the sst - 2 and pubmed datasets . our joint study shows that the joint study significantly improves the performance of the two datasets .
