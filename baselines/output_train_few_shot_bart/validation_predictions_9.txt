2 shows the performance of the treelstm model on the large movie review dataset compared to the recursive approach . throughput performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows the best performance on training .
shown in table 1 , the balanced dataset exhibits the highest throughput compared to the linear dataset . further , it exhibits the worst performance compared to a balanced dataset when the batch size increases to 25 .
2 shows the performance of the max pooling strategies for each model with different representation size . the maximum pooling strategy consistently performs better in all model variations than softplus and sigmoid , respectively . as shown in table 2 , the max pooling strategy outperforms all the other approaches except softplus .
1 shows the effect of using the shortest dependency path on each relation type . with sdp , we obtain the best f1 ( in 5 - fold ) without sdp and the best diff . we also compare the performance of our model with the best relation type with sdp . our model outperforms the macro - averaged model in terms of f1 , diff . and f1 by a significant margin .
results are shown in table 3 . y - 3 outperforms y - 2 in f1 100 % and f1 50 % with the exception of r - f1 50 % . the results are reported in table 4 . we observe that y - 4 : y ( y - 3 ) achieves the best f1 score with a 50 % chance of achieving f1 and 50 % of f1 .
3 presents the results of our test set . the results are presented in table 3 . the results of the test set are shown in bold . our model outperforms all the other methods except mst - parser in terms of f1 and paragraph level f1 . we also show that our model achieves the best performance on all the test sets . as expected , the average f1 score is significantly higher than the average of the other models .
4 shows the c - f1 scores for the two indicated systems ; for the lstm - parser , the average score is 60 . 62 ± 3 . 54 and the average performance is 57 . 24 ± 2 . 87 , respectively . for stagblcc , the mean performance is lower than the majority performance .
3 shows the performance of our proposed system on the test set . the results are presented in table 3 . we show the results of the original and the correct training set . the results show that our system outperforms all the other test sets in terms of performance . we also show that the original is more accurate than the correct set .
shown in table 1 , the original e2e data and our cleaned version are comparable in number of distinct mrs , total number of textual references , and the number of slot matching scripts as measured by our slot matching script , see section 3 .
3 presents the results of the original test set on the bleu and nist test sets . the results are presented in table 3 . the original set outperforms all the other test sets in terms of accuracy . the best performing set is sc - lstm , which outperforms both the original and the nist set .
4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set . the errors we found were slight disfluencies , but not significant ones . we also found that adding incorrect values to the training set caused a significant drop in accuracy . further , we found that removing incorrect values caused a slight drop in performance .
3 shows the performance of our dcgcn model on the external and external datasets compared to the previous state - of - the - art models . all models outperform all the other models except for snrg ( song et al . , 2017 ) and tree2str ( flanigan et al . 2017 ) . the difference between the two models is due to the fact that all the models have the same number of participants ( e . g . , all but seq2seqk ) .
2 shows the results on amr17 . our model achieves 24 . 5 bleu points . the model size in terms of parameters , respectively , is comparable to that of seq2seqb ( beck et al . , 2018 ) . we note that our model size is larger than the ensemble model , but smaller than the single ensemble model .
3 presents the results for english - german and english - czech . the results are presented in table 3 . our model outperforms all the other models in terms of english - language performance . the results show that the single - language model is superior to the multi - language models in both languages . we also show that our model performs better than the other baselines in english - korean , german , french , russian , czech and czech .
5 shows the effect of the number of layers inside dc on the performance of the model . table 5 shows that when we add layers of layers , the model outperforms the previous state - of - the - art model in terms of performance .
6 shows the performance of gcns with residual connections . we note that the gcn + rc ( 2 ) and + rc + la ( 4 ) outperform all other baselines except for dcgcn2 ( 27 ) . we also observe that the residual connections between gcn and residual connections have a significant effect on gcn performance .
3 shows the performance of the dcgcn model compared to the previous state - of - the - art models in terms of number of participants . the results are summarized in table 3 . the results show that the model outperforms all the other models on both metrics except for the one in which the model had the largest number of users .
8 shows the ablation study on the dev set of amr15 . - { i } dense blocks denotes removing the dense connections in the i - th block . further , - { 3 , 3 , 4 } dens blocks denote removing the layers of dense connections . table 8 also shows that - { 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 27 , 28 , 29 , 28 .
9 shows the ablation study for the graph encoder and the lstm decoder . the results are shown in table 9 . multi - decoder design outperforms the previous state - of - the - art models in terms of coverage . in addition , the multidecoder model outperforms all the other models except for the original dcgcn4 model . as shown in fig . 9 , - global node & linear combination < c > 22 . 9 53 . 2 as seen in table 8 , - linearcombination > 23 . 9 shows a significant improvement over the original model .
7 shows the performance of our initialization strategies on probing tasks . our model outperforms all the other approaches except for subjnum and coordinv . we also outperform all the others by a significant margin .
3 presents the results of our method on the subtraction test set . the results are presented in table 3 . table 3 shows the performance of our approach on subtraction tests . the results show that our approach outperforms all the other methods in terms of depth and subtraction .
3 presents the results of our model on the subj and mpqa datasets . our model outperforms all the other models in terms of performance . we observe that subj outperforms both the mrpc and mrpc baseline by a margin of 0 . 2 % and 0 . 4 % respectively on the sst2 and sst3 datasets , respectively . we also observe that the cbow / 784 baseline outperforms the other two baselines by 0 . 3 % and - 0 . 4 % , respectively .
3 shows the relative change with respect to hybrid compared to cbow on unsupervised downstream tasks attained by our models . our model outperforms both the hybrid and hybrid models in terms of performance . cbow outperforms the hybrid model by a significant margin . the difference in performance between hybrid and cbow shows that cbow is more effective at improving the performance of the model compared to hybrid .
8 shows the performance of our initialization strategies on supervised downstream tasks . our model outperforms all the state - of - the - art approaches except for subj and mpqa . we also outperform all the other approaches except sst2 and sst5 by a margin of 3 . 5 points .
6 shows the performance for different training objectives on the unsupervised downstream tasks . cmow - c outperforms cbow - r on all three tasks except sts13 , sts14 and sts15 . the difference between the performance between the two training objectives is significant .
3 presents the results of our proposed method . the results are summarized in table 3 . we show that our method outperforms all the other approaches except for cbow - c . we observe that our approach obtains the best performance on all three metrics .
3 presents the results of our model on the subj and mpqa datasets . our model outperforms all the other approaches except subj , which is comparable in terms of performance . however , it outperforms both the sst2 and sst5 datasets by a significant margin . we also observe that our approach outperforms the best of the best on both datasets .
3 presents the results of our system on the e + and per metrics . our system outperforms all the other systems in e + org , e + per and e + e + misc by a significant margin . the results are shown in table 3 . our system obtains the best performance in all three metrics . all org scores are in the best state of the art . we also observe that our model obtains better performance than all other systems . in all three metrics , we observe that the best performing system is mil - nd .
results on the test set under two settings are shown in table 2 . our system outperforms all the other models in terms of e + p , e + r , f1 and f1 scores . our model outperforms the previous state - of - the - art model in all but one of the three settings . we also outperform all the models in all three settings except name matching and supervised learning . the difference between the two settings is evident in the difference in f1 score between the model and the model .
6 : entailment ( ent ) and ref ( ref ) in the model outperforms all the other models except for g2s - gat , which outperforms both models .
3 presents the results of our model on the ldc2017t10 test set . the results are presented in table 3 . our model outperforms all the other models in terms of performance on the test set , with the exception of g2s - ggnn .
3 shows the results on ldc2015e86 test set when models are trained with additional gigaword data . our model outperforms the previous state - of - the - art models in both external and external settings .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . the results show that bilstm significantly improves the performance of the model compared to the baseline model .
shown in table 3 , the model outperforms the g2s - ggnn model in terms of sentence length and sentence length . we observe that the model significantly improves sentence length over the previous state - of - the - art model . the model significantly outperforms both the previous model and the baseline model by a significant margin . in particular , we observe a significant drop in sentence length from 0 - 20 to 5 . 8 % compared to the baseline .
8 shows the fraction of elements in the output that are not present in the input ( added ) and the fraction of elements that are missing in the generated sentence ( miss ) . we note that the gold - based model outperforms all the reference sentences in ldc2017t10 .
4 shows the performance of the four nmt decoding layers on a smaller parallel corpus ( 200k sentences ) . our model outperforms the previous state - of - the - art models in terms of accuracy .
2 shows the accuracy with baselines and an upper bound . the results are shown in table 2 . the best performing classifier is word2tag , which is the most frequently used one .
3 shows the performance of the pos tagging accuracy and fagging accuracy on the test set . the results are shown in table 3 . the performance of both sets is comparable to those of the other two baselines , however , the performance is slightly worse than the other baselines .
5 shows the accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . the results are shown in table 5 .
8 shows the performance of the attacker on different datasets . the difference between the attacker score and the corresponding adversary ’ s accuracy is shown in table 8 .
shown in table 1 , training directly towards a single task improves the performance for pan16 and pan16 , respectively .
2 shows the state of the art in the multi - task conversation . we show the performance of pan16 and pan16 in the balanced and unbalanced data splits . in the unbalanced and balanced data splits , we see a slight drop in performance compared to pan16 . however , this drop is not statistically significant .
3 shows the performance of pan16 and pan16 on different datasets with an adversarial training . δ is the difference between the attacker score and the corresponding adversary ’ s accuracy . in pan16 , the performance is significantly worse than that of the trained adversary .
6 shows the concatenation of the protected attribute with different encoders . embedding guarded and embedded embeddings significantly improve the performance of rnn .
3 presents the results of our work on the two models . our model outperforms all the other models in terms of base and finetune performance . the results show that our approach outperforms both the original and the original models . our model also outperforms the original model by a significant margin . we also observe that our model improves the performance of the final model by significantly improving the finetuning performance .
3 presents the results of our model on the base acc and bert datasets . our model outperforms all the other models by a significant margin . the results are shown in table 3 . we show that the lstm model has the best performance on both datasets . our model also outperforms the previous models in terms of time and distance .
3 shows the performance of our model compared to other models . our model outperforms all the other models in terms of err . we observe that our model performs better than other models when using the same time - based model . the difference between our model and other models is due to the fact that the model is more accurate at the time of the task . our model also outperforms other models by a significant margin .
3 shows the bleu score on wmt14 english - german translation task . the model outperforms all the other models except sru and lrn in terms of decoding one sentence . in addition , the training batch outperforms the previous state - of - the - art models in bleus by 0 . 99 and 2 . 67 points , respectively .
4 shows the performance of our model on squad dataset . the model outperforms all the other models in terms of match / f1 - score . we observe that the model performs better than all the models except lrn and sru . however , the model does not outperform the model in match / f1 score . further , we observe that sru outperforms the model by a significant margin .
6 shows the f1 score on conll - 2003 english ner task . lstm * denotes the parameter number in the reported result ( see table 6 ) . sru also denotes the reported results ( see lample et al . ( 2016 ) . gru also shows f1 scores ( see sru et al . , 2016 ) . lrn also shows a higher f1 than sru , indicating that the model is better able to interpret the ner tasks .
7 shows the performance of our model on snli task with base + ln setting and test perplexity on ptb task with base setting . our model outperforms the previous state - of - the - art on both tasks .
3 shows the performance of human vs . human in terms of system retrieval . the average performance is 2 . 55 / 3 . 05 / 2 . 05 vs . 2 . 05 on system retrieeval word and mtr significantly outperform human in both systems , however , the performance is lower for both systems . word vs . mtr also outperforms human the performance of both systems is lower than human , but it is still comparable to human . in both cases , the word vs . the mtr performance is comparable to that of human , although the difference is less pronounced .
4 presents the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 . the best performance among automatic systems is shown in bold . we note that the best performance is achieved on the k 1000 and k 1000 datasets , respectively . our system outperforms all the other systems except seq2seqaug and candela in terms of overall quality .
3 presents the results of our experiments . we show that our approach outperforms the previous approaches in terms of performance . our model outperforms all the other approaches except for df , df , docsub and docsub . we observe that df outperforms df by a significant margin , while docsub outperforms docsub by a considerable margin .
3 presents the results of our experiments . we show that our approach outperforms all the other approaches except for the one we use in corpus . the results are shown in table 3 . we observe that our model outperforms both the df and df models in terms of performance . however , the performance of our approach is slightly worse than those of the other three approaches .
3 presents the results of our experiments . our model outperforms all the other models in terms of performance . we observe that our approach outperforms both the df and df models by a significant margin . we also observe that the df models outperform the df model by a large margin . however , we observe that df outperforms df by a considerable margin .
3 shows the performance of our model on the metric . our model outperforms all the other models in terms of depthcohesion . we observe that our model achieves the best performance on both metric and metric metrics . however , our model performs worse on metric metrics than our model . we also observe that we do not have a significant performance drop in metric metrics compared to our baseline .
3 shows the performance of our model on the metric . our model outperforms all the other models in terms of depthcohesion . we observe that our model achieves the best performance on both metric and metric metrics . however , our model performs worse on metric metrics than our model . we also observe that we do not have a significant effect on metric performance .
1 shows the performance ( ndcg % ) of our model on the validation set of visdial v1 . 0 . lf is the enhanced version as we mentioned in table 1 . qt , r1 , r2 , r3 denote regressive loss , weighted softmax loss , and generalized ranking loss , respectively . the r1 and r2 denote linear ranking loss . the r2 and r3 are the enhanced versions of our system .
2 shows the performance ( ndcg % ) of different ablative studies on different models on the visdial v1 . 0 validation set . p2 shows the most effective one ( i . e . , hidden dictionary learning ) compared to coatt and coatt .
5 shows the performance of hmd - f1 + bert on hard alignments and soft alignments . the results are summarized in table 5 . we see that the hmd model outperforms all the other models except for ruse , which has the best performance .
3 presents the performance of our model on the direct assessment test set . our model outperforms all the other models on the test set by a significant margin . the results are summarized in table 3 . we observe that our model significantly outperforms the baseline by a large margin . our model significantly improves the performance by 0 . 5 points compared to the baseline . the bertscore - f1 test set outperforms both the baseline and the baseline in terms of direct assessment .
3 shows the performance of bleu - 1 and bertscore - f1 on the baselines . we note that both sets have strong baselines , with the exception of sfhotel , which has strong performance on both sets . the baselines also show strong performance across all three sets .
3 presents the metric and baselines of bertscore - recall . the baselines are summarized in table 3 . the metric scores are presented in terms of leic and p < 0 . 939 , 0 . 769 and 0 . 869 respectively . the baselines are described in table 1 . the leic scores are shown in bold . the p scores are reported in table 2 . the spice scores are derived from the leic score , which shows that the spice score is significantly superior to the eor score . the eor scores are obtained in the same manner as eor .
results are shown in table 3 . we observe that the m0 model outperforms the previous state - of - the - art models by a significant margin . in particular , we observe that m0 models outperform the m1 model by significantly improving the performance .
3 presents the results of our model on the transfer quality and transfer quality metrics . we show that our model outperforms all the other models in terms of transfer quality . the results are summarized in table 3 . as expected , the results are significantly better than the previous state - of - the - art models . however , the difference between the two models is still significant . in the case of the semantic preservation metric δsim , we observe that the difference in transfer quality between semantic preservation and semantic preservation is significant .
5 presents the results of human sentence - level validation . the results are shown in table 5 . the results show that the human ratings of semantic preservation are superior to the machine ratings of fluency . however , the results are slightly worse than those of the machine and human judgments that match . table 5 shows that the performance of the human evaluations is comparable to the human evaluation .
results are shown in table 3 . we observe that the m0 model outperforms the m2 model by a significant margin . the difference between the two models is due to the fact that the shen - 1 model is significantly more verb - based than the m1 model . however , the difference between m0 and m2 shows that the model is more suitable for the task .
6 shows the bleu scores on yelp sentiment transfer compared to prior work . our best model achieves the highest bleu than any prior work at similar levels of acc ∗ . multi - decoder outperforms all the other models in the classifier category . however , the best model outperforms the best ones by a significant margin . in addition , we also outperform the best models in terms of classifier embeddings . the best models outperform all the others by a margin of 3 . 5 points .
2 shows the percentage of reparandum tokens that are correctly predicted to be disfluent . reparandum length [ bold ] 1 - 2 , 3 - 5 , 6 - 8 , 8 + and 9 + respectively . the number of repetition tokens is slightly higher than the number of disfluencies , but still slightly lower than the rate of repetition .
3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and the repair ( content - function ) . the fraction of tokens correctly predicted to contain a word in each category is shown in table 3 . in both cases , the number of tokens correct to contain the content word is significantly higher than the number predicted in the original reparandum .
model outperforms all the other models in terms of dev and innovations . the best models outperform the best models by a significant margin . table 3 shows the performance of the model on the single and multi - task test . in addition , the best model achieves the best performance on the multi task test , all the models in table 3 show the best state - of - the - art features for the single task . we observe that the best features are in the text and innovations category .
2 shows the performance of our model on the fnc - 1 test dataset compared to the state - of - art word2vec embeddings on the test dataset . our model outperforms all the other models in terms of f1 and micro f1 ( % ) by a significant margin . however , our model performs worse than the state of the art on both test datasets .
2 shows the performance of different methods on the apw and nyt datasets for the document dating problem . the best performing method is burstysimdater , which significantly outperforms all previous methods .
3 shows the accuracy ( % ) of our model with and without attention . we show that our model achieves the best performance with both word attention and graph attention for this task . as expected , our model performs better than the state - of - the - art model .
3 presents the performance of our model on the test set . our model outperforms all the state - of - the - art models in terms of performance . the best performing model is the cnn model , which outperforms both the dmcnn model and the jmee model by a significant margin .
3 presents the results of cross - event training . our method outperforms all the other methods in terms of f1 score . we show that cross - event training outperforms the traditional method by a significant margin . our approach outperforms both the traditional and cross - event approaches . we observe that the two approaches converge on the same threshold , which leads to a significant improvement in performance .
3 shows the performance of all models on the test set . all models outperform all the other models in terms of dev perp , test acc , test wer and wer , respectively . all but one model outperforms all the others in both metrics . all except english - only - lm are better than all but one .
4 shows the results on the dev set and on the test set using discriminative training with only subsets of the code - switched data . the results are shown in table 4 . for the full train test set , the trained train dev outperforms the fine - tuned train dev .
5 shows the performance on the dev set compared to the test set . our model outperforms all the other models except for fine - tuned - lm , which is comparable to monolingual .
7 shows the performance of type - aggregated gaze features trained on the conll - 2003 dataset and tested on all three eye - tracking datasets . precision ( p ) , recall ( r ) and f1 - score ( f ) show statistically significant improvement over type combined gaze features .
5 shows the performance of type - aggregated gaze features on the conll - 2003 dataset . precision ( p ) , recall ( r ) and f1 - score ( f ) are significantly better than type combined gaze features ( table 5 ) .
results on belinkov2014exploring ’ s ppa test set are shown in table 1 . syntactic - sg embeddings are used in wordnet , verbnet , and glove - extended , respectively . the syntactic embedding of wordnet and verbnet is used in the original paper , and it uses syntactic skipgram embedding . in addition , the syntactic transferdings obtained by the lstm - pp ( 2015 ) are used on wordnet 3 . 1 .
2 presents the results of the rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . the results are presented in table 2 . we observe that the hpcd ( full ) model outperforms all the other models in terms of uas and ppa accuracy .
3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . in table 3 , we observe that removing the context sensitivity from the models results in a significant reduction in ppa ( perception ) and ppa acc . ( perceived ppa ) .
2 shows the performance of domain tuning for image caption translation ( bleu % scores ) and multi30k ( 37 . 7 % ) compared to the previous state - of - the - art model . adding subtitle data and domain tuning improves the performance by 3 . 7 % compared to sub - tuned models .
3 shows the performance of subs1m on en - de and in - de . subdomain - tuned models outperform all the other models in terms of performance . in - de models perform better than all the models except for mscoco17 , which shows a slight improvement in performance compared to the previous state - of - the - art .
4 shows bleu scores in terms of automatic captions . the results with marian amun are shown in table 4 . the best ones are those with the best image captions , while the worst ones are ones with only the best one or all 5 .
5 shows the bleu % scores for different strategies for integrating visual information . multi30k + ms - coco + subs3mlm outperforms dec - gate and enc - gate on both datasets . in the en - de setting , we see a slight improvement over the previous state - of - the - art approach , but still outperforms the baseline .
3 shows the performance of subs3m and subs6m in terms of text - only and multi - lingual features . the performance is comparable to that of en - de , however , the performance is slightly worse . sub - ensemble - of - 3 features outperforms all the other models except for the one that features the word embeddings . we observe that the performance improvement is due to the superior quality of the semantic features .
results are shown in table 3 . we note that our approach outperforms the best state - of - the - art mtld and en - fr - ff in terms of translation performance . however , we do not see significant improvement in the performance of our model compared to previous work .
shown in table 1 , the number of parallel sentences in the train , test and development splits for the language pairs we used .
2 presents the training vocabularies for the english , french and spanish data used for our models . our model outperforms both the english and french models .
5 shows the bleu and ter scores for the rev systems . the bleu scores are shown in table 5 . the ter scores are significantly worse than en - fr - rev and en - es - trans - rev , respectively .
2 shows the performance of our visually supervised model on flickr8k . the vgs model outperforms all the other models in terms of recall @ 10 and f1 scores .
1 shows the performance of our visually supervised model on synthetically spoken coco . the results are shown in table 1 . we observe that our model outperforms the previous state - of - the - art models in terms of recall @ 10 and f1 scores .
shown in table 1 , we report further examples of the different classifiers used in the original . for example , cnn turns in a < u > screenplay that u > at the edges ; it ’ s so clever you want to hate it . dan < c > shows the difference between the original and the original on sst - 2 . for dan , we show the difference in the number of turns . for cnn , we see that the edges of the screenplay are much better than the edges . for rnn , the edges are much more compact and the curve is much more pronounced .
2 presents the results of fine - tuning on sst - 2 . the results are shown in table 2 . we observe that the number of words in the original sentence has increased , decreased or stayed the same . the number of occurrences has increased by 2 . 5 % compared to the previous state - of - the - art system .
3 shows the change in sentiment in sst - 2 compared to the original sentence . we note that the negative labels are flipped to positive and vice versa . the positive label is flipped to negative and the negative label to positive . as shown in table 3 , the positive label has a significant effect on sentiment .
results are presented in table 3 . the results are summarized in terms of ppmi and f1 scores . in general , the results are similar to those of the previous work . however , the difference between positive and negative results is less pronounced . table 3 shows the performance of our proposed approach . our proposed approach outperforms all the other approaches except for the one we tested in table 1 . we show the results of our approach in table 2 .
