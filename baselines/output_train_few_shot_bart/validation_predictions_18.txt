2 shows the performance of our recursive framework on the large movie review dataset in terms of inference and training , compared to using the iterative approach of recur and iterative .
shown in table 1 , the balanced dataset exhibits the highest throughput thanks to the high degree of parallelization . however , it exhibits the smallest performance improvement compared to the linear dataset .
2 shows the performance of the max pooling strategies for each model with different number of parameters . the results are shown in table 2 . the maximum pooling strategy performs better in all models with different representation size . as shown in fig . 2 , the hgn model has different performance on multiple parameters . hgn models have different learning rate and the number of iterations for each representation is the same for all models .
1 shows the effect of using the shortest dependency path on each relation type . with sdp as dependency path , our model achieves the best f1 ( in 5 - fold ) with sdp and the best diff . with ndp . the macro - averaged model outperforms all the other models using only sdp dependency paths .
results are shown in table 3 . y - 3 shows significant performance improvement over the previous state - of - the - art models in terms of f1 and f1 score .
results are shown in table 1 . the results are presented in tables 1 and 2 . the results of the best performing method are summarized in terms of paragraph level . our method outperforms all the methods except for mst - parser .
4 shows the c - f1 scores for the two indicated systems ; for the lstm parser , it is 60 . 62 ± 3 . 54 and 58 . 24 ± 2 . 87 respectively compared to the majority performances of the other systems .
results are shown in table 4 . the original and the original results are presented in table 5 . the results are summarized in terms of bleu scores and the number of errors for each system tested . these results are reported in tables 1 and 2 . table 5 summarizes the results of the experiments . our system performs better than the previous stateof - the - art systems on all but one of the test sets . we observe that the cleaned tgen models outperform all the other methods except for those that do not need to be tested . this is mostly due to the small size of the training set ( e . g . sc - lstm ) and the large number of false positives reported in table 1 .
shown in table 1 , the original e2e data and our cleaned version are comparable ( number of distinct mrs , total number of textual references , ser ) as measured by our slot matching script , and the number of slot matching scripts as measured in tables 1 and 2 .
results are shown in table 2 . original and original results are presented in table 1 . the results are summarized in bold . original results show that the tgen model outperforms all the other methods except for those using bleu . however , the results are slightly worse than original results . for example , sc - lstm is slightly better than tgen + on some of the test sets , but still performs worse than the original .
4 shows the results of manual error analysis on a sample of 100 instances from the original test set of tgen ( see table 4 ) . in all but one case , there was a significant drop in the absolute number of errors ( from 0 to 14 ) in the output of training data . these errors are caused by slight disfluencies in the training data , which can be seen in table 4 .
model is presented in table 1 . the best performing models are the dcgcn ( single ) and tree2str ( the other two are reported in table 2 ) . all models are significantly better than the state - of - the - art models in all but one of the two cases . for example , when the two models are paired , their performance drops significantly .
2 shows the results on amr17 . our model achieves 24 . 5 bleu points on the model size in terms of parameters , compared to the previous best state - of - the - art models by seq2seqb .
3 shows the results for english - german and english - czech . the results are presented in table 3 . the best performing models are published in english , german , french , russian , turkish , russian and turkish . we also include the best performing model in english - language , both in terms of the number of participants and the type of model used in the model .
5 shows the effect of the number of layers inside dc on the performance of the model in table 5 . the first group shows that when only one layer is added , the model performs better than the others .
6 shows the performance of baselines with residual connections . rc denotes gcns with residual connection . as shown in table 6 , when rc is applied to multiple gcns , the performance drops significantly .
model 3 shows that dcgcn outperforms all the other models in terms of performance on both datasets when combined with the training data .
8 shows the ablation study results for amr15 in terms of density of the connections in the dev set . the results are shown in table 8 . it is clear that removing the dense blocks severely affects the model ' s performance , as shown in the table .
9 shows the ablation study for the graph encoder and the lstm decoder . the results are shown in table 9 . the best performing model is the global encoder , with a gap of 2 . 5 points in performance between the two models .
7 shows the performance of our initialization strategies on probing tasks . our model outperforms the previous stateof - the - art methods on all three tasks .
can be seen in table 4 , the best performing models are h - cbow and h - cmow . the results are summarized in table 5 . the best performing model is cbow / 400 , with a gap of 3 . 5 points in performance between the two methods .
results are shown in table 3 . our model outperforms all the other methods except for cmp . cbow shows significant performance improvement on both mrpc and mpqa datasets . on the other hand , it outperforms both the sst2 and sst5 datasets in terms of mrpc score . it is clear that cbow performs better on both datasets when trained with the same type of training set .
3 shows the performance of our models on unsupervised downstream tasks attained by our models . the best performances are on the sts12 and sts14 datasets , where cmp . cmow shows the relative change with respect to hybrid . the results are shown in table 3 . when cmp is used as a parameter , the model performs better than both the hybrid and hybrid models .
8 shows the performance of initialization strategies on supervised downstream tasks . our model outperforms all the stateof - the - art models except for the ones that do not use the word " sentiment " . the results are summarized in tables 8 and 9 .
6 shows the performance for different training objectives on unsupervised downstream tasks . the best performances are obtained on the sts12 and sts14 datasets , respectively . the worst performance is obtained on sts15 , where cbow - r outperforms cmow - c .
results are shown in table 3 . the best performing models are cbow and cbow - r , both of which show strong performance on subtense and subtense contexts . however , for subtense , the performance is lower than those on the other two models .
results are shown in table 3 . the best performing models are cbow , cbow - r and sst5 . these models outperform all the other methods except for sst2 , which shows the diminishing returns from using subj as the base .
results are shown in table 3 . the best performing system is named named matching ( mil ) . it obtains the best e + and per scores with a minimum of 0 . 05 and 0 . 01 points , respectively , compared to the best state - of - the - art systems . the results are summarized in tables 1 and 2 . these results show that the combination of domain name matching and name matching improves the system ' s performance over all the other methods .
2 shows the results on the test set under two settings . name matching and supervised learning achieve the best results with 95 % confidence intervals of f1 score . these results are shown in table 2 . the results are summarized in tables 2 and 3 . name matching improves the general performance of the system by 2 . 5 % in both e + p and f1 scores . it further boosts the performance by 3 . 4 % in all cases .
6 : entailment ( ent ) and ref ( g2s - gat ) are shown in table 6 . the model outperforms all the other models in terms of both epm and f1 scores , ref and refs are more than 50 % better than ref ,
results are shown in table 3 . the best performing models are g2s and gat , both of which outperform the best stateof - the - art models in terms of performance .
3 shows the results on ldc2015e86 test set when models are trained with additional gigaword data . the results are summarized in table 3 . our model outperforms the previous stateof - the - art models in both external and external gigawords .
4 shows the ablation study results on the ldc2017t10 development set . results are summarized in table 4 . bilstm significantly outperforms both the original model and the base model by a large margin .
results are shown in table 3 . we observe that the g2s models outperform all the other models in terms of sentence length , sentence length and sentence length . the results are summarized in table 4 .
shown in table 8 , the fraction of elements missing in the input graph that are present in the generated sentence ( g2s - gin ) is lower than those in the output graph , indicating that the model relies on token lemmas to generate sentences .
4 shows the performance of the four models trained with different target languages . our model outperforms the previous stateof - the - art models in all but one of the cases .
2 : pos and sem tagging accuracy with baselines and an upper bound . mft : most frequent tag ; unsupemb : most frequently tag ; word2tag : upper bound encoder - decoder .
results are shown in table 4 . table 4 summarize our results on the word - parity test set for both domains . our results are summarized in terms of accuracy , completeness , and precision . the results are presented in tables 1 and 2 , respectively . our proposed method outperforms all the methods except for the one described in table 1 .
5 shows the accuracy with different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . our model outperforms all the four layers with a gap of 3 . 5 points in accuracy .
performance on different datasets is shown in table 8 . the difference between the attacker score and the corresponding adversary ’ s accuracy is reported in δ ( p < 0 . 001 ) .
1 shows the accuracies when training directly towards a single task . for pan16 , we trained directly towards the single task of pan16 .
2 shows the status of theected attribute leakage in the context of balanced and unbalanced data splits . it can be observed that the presence of gender - neutral tags in the generated data leads to a significant drop in the performance of the model compared to those in the unbalanced dataset .
3 shows the performance on different datasets with an adversarial training . the difference between the attacker score and the corresponding adversary ’ s accuracy is shown in table 3 . sentiment and gender are the most important factors in predicting whether an target will receive a reward or not . in pan16 , the average age of the target is 62 . 5 and the average gender is 58 . 5 .
6 shows the performance of different encoders when embeddings are trained on the same network .
results are shown in table 3 . the results are summarized in tables 1 and 2 . our model outperforms the previous stateof - the - art models in both modeling and finetuning . in particular , it achieves the best performance with a minimum of 2 . 5x performance on the model when finetuned and de - tuned .
results are shown in table 4 . the results are summarized in table 5 . it can be observed that our model has the best performance on both datasets when trained on the same training set . when trained on a single dataset , the time taken to train on the training set is the same as when using the original dataset . we can also see that the model is better than the original lstm model in terms of training time .
results of experiment 1 are shown in table 1 . the results are presented in tables 1 and 2 . table 1 shows that our model outperforms all the other models in terms of both time and recall . we observe that ama is better than both the original lstm and the amafull time model , we also observe that the difference in recall between ama and full time is less pronounced for both datasets . the performance of our model is reported in table 2 .
3 shows the bleu score on wmt14 english - german translation task . our model improves upon the state of the art gnmt model by 3 . 5 points in bleu score .
4 shows the performance of our model with respect to match / f1 score on squad dataset . it can be seen that our model obtains the best performance with a parameter number of 2 . 4m . with this parameter number , it can be observed that the model has the advantage of matching f1 scores better than other models with the same parameter number .
6 shows the f1 score on conll - 2003 english ner task . lstm * denotes the model with the highest parameter number . it can be seen that all the models use the same number of parameters as the reference ones .
results are shown in table 7 . snli model achieves the best performance with base + ln setting and test perplexity with base setting .
results are shown in table 3 . word embeddings are used for all systems except for system retrieval , where the word embedding is used only for system evaluation . sent attention is used for both systems , with the exception of system evaluation ( mtr ) . word attention is primarily used for system evaluations , with word attention being used for sentence selection .
4 : human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 . the best performance among all systems is reported in table 4 .
results are shown in table 3 . the results are presented in tables 1 and 2 . we observe that all the models trained on the corpus dataset are slightly better than those trained on other datasets , such as docsub , docsub and docsub . however , the results are slightly worse than those on the other datasets for both groups .
3 shows the performance of all the models trained on the same dataset . our model outperforms all the other models except for those trained on docsub . the results are summarized in table 3 . for example , our model performs better than both the df and docsub datasets on both datasets , with the exception of the one on the df dataset where it performs worse .
3 shows the performance of all models trained on the same dataset . our model outperforms all the other models except for those trained on docsub . the results are summarized in table 3 . for example , our model performs better than both the df and docsub datasets on both datasets , with the exception of the one on the df dataset where it performs worse than the other two .
results are shown in table 3 . the results are summarized in tables 1 and 2 . our system achieves the best performance with a minimum of 3 . 5 % truedepth compared to the maxdepth of our model . our model is better than both the df and docsub models .
3 shows the performance of our models on the three datasets . our model achieves the best performance on all three datasets with a minimum of 3 . 5 % truedepth and a maxdepth of 1 . 5 % . our model outperforms all the baselines except for the two that we included in table 3 .
performance of our model on the validation set of visdial v1 . 0 . 0 is shown in table 1 . the enhanced version of our system ( lf ) performs better than the enhanced version .
2 shows the performance ( ndcg % ) of different ablative studies on different models on the visdial v1 . 0 validation set . the best performing model is lrv , which relies on hidden dictionary learning .
5 compares the performance of these models on hard and soft alignments . the results are summarized in table 5 . for the hard alignments , hmd - recall + bert improves performance by 0 . 8pp on both metrics .
results are shown in table 4 . the results are summarized in table 5 . the summaries presented in table 1 show that our approach significantly outperforms the baselines on three of the four metrics by a significant margin . for example , the summaries obtained by bertscore - f1 are significantly better than those by ruse ( * ) on all three metrics except for those by lemmatized entities .
results are shown in table 3 . the bleu - 1 model outperforms all the baselines except for those using bertscore - f1 . the results are summarized in table 1 . the results of baselines that rely on word embeddings are reported in tables 1 and 2 . table 1 summarizes the performance of these models in terms of bert scores and their performance on the validation set .
results are shown in table 3 . the summaries are summarized in terms of lemma metrics . they are broken down into three categories : " m1 " , " m2 " and " w2v " . for both categories , we base our summaries on the leic score ( p < 0 . 001 ) and bertscore - recall scores ( p > 0 . 005 ) . for the m2 category , we use the elmo and p scores of the models trained on the spice dataset ( p ≤ 0 . 01 ) and " word - mover " . these models outperform all the baselines except for those trained on w2v . for the wmd - 1 dataset , we rely on word - mover and sentence - based embeddings .
results are shown in table 3 . we observe that for all models except for the one that relies on word embeddings , our model performs better than the previous state - of - the - art models .
results are shown in table 4 . the results are summarized in terms of transfer quality and semantic preservation . semantic preservation is the most important part of the semantic preservation task , and it is strongly related to semantic preservation δsim . syntactic preservation is further improved with the addition of semantic preservation features . for semantic preservation , the best results are obtained with a minimum of performance drop of 2 . 5 points . when using semantic preservation as the baseline , the results are reported in tables 4 and 5 .
5 shows the human evaluation results for each dataset . the results are summarized in table 5 . we show the results of human evaluation on three of the four datasets . it is clear from table 5 that the accuracy obtained by human evaluation is high , indicating that the quality of the sentence is high .
results are shown in table 3 . we observe that for all models , our model performs better than the previous state - of - the - art models on all three datasets except for the one in which it relies on word embeddings .
results on yelp sentiment transfer are shown in table 6 . our best model achieves the highest bleu ( 31 . 4 % ) and the highest acc ∗ ( 29 . 4 % ) . however , the results are slightly worse than those obtained by simple - transfer . multi - decoder models ( yuan et al . , 2018 ) do not have the best performance on the three types of sentiment transfer datasets . sentiment embedding alone does not improve the model ' s performance , but it is comparable to previous work on the topic . the best model is yang2018unsupervised , which means it is better to transfer 1000 words than the original ones . it is clear from table 6 that this is a result of different classifiers in use , and that these classifiers are not used to train models .
2 shows the number of tokens that were correctly predicted as disfluencies in the nested disfluency task . for all but one of the cases , repetition is the most difficult part of the task to predict . reparandum length is reported in table 2 , but it is difficult to predict exactly how many repetition tokens are correct for each domain .
3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and the repair ( content - function ) . the percentage of tokens correctly predicted to contain a word is shown in table 3 . the fraction of tokens that contain the word is small but significant , indicating that the disfluency of the word contains a significant amount of words .
results are shown in table 3 . text + innovations model outperforms all the other methods in terms of dev and test performance . in particular , text + innovations models have the best performance on the single test set , in addition , when text is raw , the model achieves the best results on the multi - sample test set .
2 compares our model with state - of - art algorithms on the fnc - 1 test dataset . the results are shown in table 2 . our model achieves the best performance with both agree and disagree features . however , it is unable to distinguish between these two features .
2 shows the performance of different methods on the apw and nyt datasets for the document dating problem . the best performing model is maxent - joint , which significantly outperforms all previous methods .
3 shows the performance of our model with and without attention . it can be seen that both word attention and graph attention are effective for this task . however , the performance is lower than that of ac - gcn .
3 shows the performance of models trained on stateof - the - art data augmentation systems trained on the jvmee dataset . our model outperforms all the models except for the one that relies on pre - trained word embeddings .
table 1 , we present the results of our method with respect to event identification . the results are presented in tables 1 and 2 . in all but one case , the event identification method outperforms the other methods in terms of both event identification and event classification . cross - event identification ( f1 ) is the most popular method for event identification , with a large margin for error in both cases . for both scenarios , the identification method ( p < 0 . 001 ) obtains the best performance on both occasions .
can be seen in table 4 , all models trained on the original word2vec embeddings are shown in bold . all except for the one that is pre - trained and is available in english ( except for those that are only available in spanish ) . note that all the fine - tuned models shown in table 1 show performance below the threshold for pre - training on only one of the three languages , namely , english .
results on the dev set and test set are shown in table 4 . we use fine - tuned training with only subsets of the code - switched data in it , and only train test data in the test set .
5 shows the performance on the dev and test set compared to monolingual and code - switched systems . the results are summarized in table 5 . for the dev set , we use fine - tuned word embeddings as the base sentence for the gold sentence , while for the test set we use text - switching .
results are shown in table 7 . for type - aggregated gaze features trained on all three eye - tracking datasets , precision ( p < 0 . 01 ) and f1 - score ( f1 ) are significantly improved over the baseline ( p > 0 . 05 ) .
5 shows the performance of type - aggregated gaze features for the conll - 2003 dataset . precision ( p ≤ 0 . 01 ) and f1 - score ( f1 ) are statistically significant improvements over the baseline ( p > 0 . 05 ) for type - aligned gaze features .
results on the original wordnet 3 . 1 test set are shown in table 1 . syntactic embeddings are the most important components of wordnet , and they are used in the translation task as well as in wordnet 2 . 1 . they are derived from the original work of fauqui et al . ( 2015 ) and schütze ( 2015 ) , and are based on the syntactic skipgram embedding of the original paper . they can be seen in the output of fig . 1 , where we use glove - retro as the initialization vector . it can also be seen as an alternative to syntactic - sg embedding .
2 shows the performance of our system when combined with various pp attachment predictors and oracle attachments . the results are presented in table 2 .
3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . the results are shown in table 3 . it can be observed that the removal of context sensitivity significantly decreases the ppa acc . by 3 points .
2 : adding subtitle data and domain tuning for image caption translation ( see table 2 ) . the results are summarized in table 2 . subdomain - tuned multi30k embeddings have a significant impact ( bleu % scores ) and multilingual translation has a significant effect ( marian amun et al . , 2018 ) . domain tuning improves the model ' s interpretability by 3 . 5 points ( p < 0 . 001 ) and the model performs better than domain tuning .
results are shown in table 4 . subdomain - tuned models outperform the models in all but one of the comparisons . in particular , the results are striking in the en - de setting , where the subs1m model outperforms the model in terms of domain - tuning .
4 shows the bleu scores of models using automatic captions . our model outperforms all the models using only the best ones or all 5 captions , showing the importance of multi - factor features .
5 shows the performance of different approaches for integrating visual information . multi - de embeddings outperform enc - gate and dec - gate on both datasets , as table 5 shows , using transformer and multi30k + ms - coco + subs3mlm improves the results for both datasets ( bleu % scores ) . with the exception of capt - gate , which improves the performance for the en - de model , we observe that enc - gating improves the picture quality .
3 shows the performance of subs3m in terms of text - only and multi - lingual features compared to subs6m on en - de ( cf . table 1 ) . the results are summarized in table 1 . sub - text - only models perform better than subs6 and other models that rely on word embeddings for semantic features .
results are shown in table 3 . the first two results are summarized in tables 1 and 2 . the results are presented in tables 3 and 4 . table 3 summarizes the results of our system on the word - phrases for english and german . we observe that the best performing models are the en - fr - ff and en - rnn - ff , respectively .
results are shown in table 1 . the number of parallel sentences in the train , test and development splits for the language pairs we used is shown in bold .
2 : training vocabularies for the english , french and spanish data used for our models .
5 shows the bleu and ter scores for the rev systems . automatic evaluation scores ( bleu , ter ) show that the models are better than the previous stateof - the - art systems in terms of recall .
2 shows the performance of our visually supervised model on flickr8k . the results are summarized in tables 2 and 3 .
results on synthetically spoken coco ( see table 1 ) are shown in bold . the models trained on the embeddings of chrupala2017representations are presented in tables 1 and 2 . the results are summarized in table 1 . our model outperforms both rsaimage and segmatch with a significant margin .
1 shows the results of different classifiers compared to the original on sst - 2 . for example , dan turns in a < u > screenplay that shows the edges at the edges ; it ’ s so clever you want to hate it . for cnn , we report further examples in table 1 . the difference between the original and the original is small but noticeable . for rnn , we show that the edges of the screenplay are small but the shapes are large enough to show the effect of hate speech . for other news networks , we see that there is a noticeable difference in the results . for tv , we observe that when the word " hate " is used in the screenplay , the shape of the words " hate hate " is very small and the edges are not large enough for a large audience to notice .
2 shows that fine - tuning has indeed increased the number of words in sst - 2 and the percentage of occurrences in the original sentence that have stayed the same . these results are shown in table 2 .
3 shows the sentiment changes in sst - 2 compared to the original sentence . positive and negative sentiment are the most prevalent words in the sentence , followed by negative and positive sentiment . the results are shown in table 3 . the change in sentiment is small but significant .
results are presented in table 3 . the results are summarized in table 1 . we report the results of our joint study on the word " sift " . our results are consistent across all three groups : positive , negative , and positive . our joint study method outperforms all the methods except sift in terms of p2p score .
