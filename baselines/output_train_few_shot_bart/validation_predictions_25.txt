2 shows the performance of our recursive framework on the large movie review dataset . our approach achieves the best performance on inference with efficient parallel execution of tree nodes , while tensorflow performs the best on training .
shown in table 1 , the balanced dataset exhibits the highest throughput thanks to the high degree of parallelization . however , it does not improve as well as the linear dataset , because there is only a small room of performance improvement left to train with a small amount of redundancy .
2 shows the performance of our approach with different hyper parameters in each model with different representation . our model achieves the best performance with different number of parameters . the number of hyper parameters in the validation set is small but the number of feature maps is large . the max pooling strategy consistently performs better in all model variations . ud v1 . 3 performs better than softplus and sigmoid in all three scenarios . finally , the performance gain from adding the different hyper parameters to the model is slim but significant .
1 shows the effect of using the shortest dependency path on each relation type . with sdp as dependency path , our model achieves the best f1 ( in 5 - fold ) with only sdp . however , this model does not outperform the macro - averaged model in terms of f1 with both sdp and diff . the results are shown in table 1 . syntactic models like topic and topic have the worst performance .
3 shows the performance of the y - 3 model compared to the previous state - of - the - art models . for brevity , we only report f1 100 % , r - f1 50 % , and f1 50 % . however , when we combine these features , the performance drops significantly .
3 presents the results of our method in terms of test set . our approach achieves the best results with a minimum of 50 % of the entries . our model achieves the highest score with a f1 score of 31 . 23 on the paragraph level .
4 shows the c - f1 scores for the two indicated systems ( the lstm - parser and the paragraph system ) compared to the majority performances of the other systems .
3 shows the performance of our system on the original and false test sets . our system outperforms all the other methods except for the original , which shows the diminishing returns from mixing original and wrong data .
shown in table 1 , the original and the cleaned versions have the highest number of distinct mrs and the average number of errors as measured by our slot matching script , see section 3 . the difference between original and cleaned versions is less pronounced , but still significant .
performance of original and original models on the test set is shown in table 1 . original models outperform all the other methods except for the one that adapts to the task at hand . the difference between original and original is less pronounced , however it is larger than the difference between the two .
results of manual error analysis on a sample of 100 instances from the original test set of tgen ( see table 4 ) . the results are shown in table 4 . the errors caused by adding incorrect values to the training set are mostly caused by slight disfluencies . in addition , the original errors are caused by misfortunes .
model performance is reported in table 1 . our dcgcn model outperforms all the other models in terms of both external and bias metric with a large margin .
2 shows the results on amr17 . our model achieves 24 . 5 bleu points on the model size in terms of parameters , compared to the previous best state - of - the - art model , seq2seqb ( beck et al . , 2018 ) .
results in english - language are shown in table 1 . the best performing model is bow + gcn ( bastings et al . , 2017 ) which performs best in english , german , french , russian , turkish , russian and turkish . it also outperforms the previous stateof - the - art models in both languages .
5 shows the effect of the number of layers inside the network on the performance of the different layers . for example , we observe that for the " italic " layer , there is a significant drop in performance compared to the " b " layer due to larger variation in performance .
6 shows the performance of different gcns with residual connections . rc + la ( 2 ) and dcgcn4 ( 6 ) show significant performance improvement . with residual connections , gcn has comparable performance with other baselines .
model 3 shows the performance of the dcgcn models when combined with the number of training examples . the results are summarized in table 3 .
8 shows the ablation study results on the dev set of amr15 . the results are shown in table 8 . the dense blocks dominate the i - th block , which shows that removing the dense connections leads to a better performance .
show the ablation study results for the different types of graph encoder and the lstm decoder . encoder modules used in table 9 show that the hierarchical nature of the graph attention layer leads to better coherence .
investigate the effects of different initialization strategies on probing tasks . our paper presents the results in table 7 . our model obtains the best performance with a gap of 3 . 5 points in performance between intuition and threshold based initialization strategies .
can be seen in table 1 , the models trained on our proprietary cbow / 400 model have the best performance . however , our model has the worst performance on both subtense and subtense metrics , which shows the diminishing returns from using such specialized tools .
3 shows the performance of our model compared to other methods . our model outperforms all the other methods except for sst2 , which shows the competitiveness of mpqa . cbow also outperforms sst3 and sst5 in terms of mrpc score . however , it has the advantage of training on a larger corpus . this suggests that our approach has superior recall ability on the mrpc dataset .
results on unsupervised downstream tasks attained by our models are shown in table 3 . hybrid models outperform both cbow and cmp . on the sts13 and sts15 datasets , cbow shows the relative change with respect to hybrid models . however , when using cbow instead of cmp , the performance remains the same .
8 shows the performance of different initialization strategies on supervised downstream tasks . our model outperforms all the base models except for sst2 , whose mrpc score is closer to sick - e than mpqa .
6 shows the performance for different training objectives on the unsupervised downstream tasks . our cmow - c model outperforms the cbow - r model on all three tasks . however , on the sts15 dataset , the performance drops significantly .
can be seen in table 1 , the topconst method outperforms both cbow and cbow - r in terms of accuracy . however , it has the worse performance on the subtense and subtense tests .
subj models outperform all the other methods except for sst2 , which shows the competitiveness of subj model compared to mpqa . cbow - r also outperforms sst - b , sst5 and sst10 in terms of mrpc score , respectively .
3 presents the results of our system in evaluation mode . our system obtains the best e + and per scores ( out - of - the - box ) and performs well in all evaluation modes . all org scores are reported in table 3 . supervised learning ( mil - nd ) outperforms all the other methods with a large margin . we observe that the combination of features that give the best results ( e . g . name matching , multi - task learning ) and domain learning ( supervised learning ) have a significant impact on the performance of the system .
results on the test set under two settings are shown in table 2 . the best performing model is mil - nd ( model 1 ) which achieves the best e + p score and the best f1 score . supervised learning also improves the general performance of the model , both the model with the best performance and the model without the training data is shown in bold . name matching and supervised learning achieve the best results with 95 % confidence intervals of f1 scores .
table 6 , we can see that our model outperforms the previous stateof - the - art models in terms of both ref and ref scores , ref significantly outperforms g2s - gat in both cases , as table 6 shows , when ref is used as ref , then ref becomes ref . however , when using ref alone , ref performs slightly better than ref or ref even though it has more training data .
results are shown in table 1 . the best performing models are the ldc2017t10 and ldc2015e86 , respectively , and the g2s - gat model , which outperforms both the previous models .
3 shows the results on ldc2015e86 test set when trained with additional gigaword data . our model outperforms all the other models except for the one trained with g2s - ggnn embeddings .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . the results show that the use of bilstm improves the performance of the model by a noticeable margin .
results are shown in table 1 . we observe that the g2s model has the best overall performance when compared to other models in terms of sentence length . from the same set of test sets , we also observe that g3s - gin has the worse overall performance . note that the average sentence length is shorter than the average number of frames , finally , the averagesentence length is longer than g2s - gat , indicating that the model has better recall .
shown in table 8 , the fraction of elements that are missing in the input graph that are present in the generated sentence ( g2s - gin ) . however , this fraction is much smaller than those in the output graph , indicating the importance of token lemmas in design .
4 shows the performance of our approach with respect to target languages . we use the 4th nmt encoding layer , trained with 200k sentences . our model achieves the best performance with 96 % accuracy using the four features extracted from the 4 - layer nmt decoding layer .
2 shows the pos and sem tagging accuracy with baselines and an upper bound . mft : most frequent tag ; unsupemb : second most frequently tag ; word2tag : upper bound encoder - decoder .
results are shown in table 1 . our pos tagging accuracy model outperforms all the other methods except for the one that it performs on the larger test set .
5 shows the accuracy with different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . our uni model shows a slight improvement over res model , but still outperforms res .
performance on different datasets is shown in table 8 . the average age of the attacker is 9 . 7 and the average gender is 8 . 3 . the difference between the performance of the trained adversary and the corresponding adversary is 10 . 2 .
performance on a single task is shown in table 1 . the training agent trained directly towards the single task has the highest performance .
2 shows the status of theected attribute leakage in the balanced and unbalanced datasets . the results are shown in table 2 . dial and text tags have the most significant effect , however , the imbalance is much worse in unbalanced dataset . sentiment also has a significant impact , however it is less pronounced in balanced dataset .
performance on different datasets with an adversarial training is shown in table 3 . the difference between the performance of the trained classifier and the corresponding adversary is measured in δ . sentiment and gender are the most important factors in predicting whether an attacker will reach the task goal . in pan16 , the advantage is less pronounced , but still significant .
6 shows the performance of different encoders when embeddings are trained on the same protected attribute .
3 shows the performance of our model on the training set of hotpotqa with respect to finetune . our model outperforms all the other models with a large margin . the results are shown in table 3 . we additionally compare our model with the strong lemma baseline on the wt2 dataset , we observe that our model performs better on both the training and the test set with the same number of parameters . finally , we observe that the performance gap between our model and the original model is small but significant . our model achieves the best performance on both datasets with a minimum of training time and a maximum number of training days .
performance of our model on the training data is reported in table 4 . the results are reported in tables 1 and 2 . table 1 shows that our approach has the best performance on both datasets when trained on the same training data . when trained on a single dataset , the time taken to train with the same model is the only difference . with respect to model time , we also observe that our model has the worst performance on the two datasets . this is evident from the large difference in training time between training data and the actual time to train .
3 shows the performance of our model compared to previous work on three different datasets . our model outperforms both the original and the yelppolar time dataset in terms of both err and f1 metric . the results are summarized in table 3 . we observe that our model performs better on both datasets , with the exception of the amafull time dataset .
3 shows the bleu score on the test set of wmt14 english - german translation task . our model outperforms the previous stateof - the - art models in both languages with a gap of 1 . 15 training steps .
4 shows the performance of our model with respect to match / f1 score on squad dataset . it can be seen that our model significantly outperforms other models with the parameter number of 1 . 59m and 2 . 44m , respectively .
6 shows the f1 score on conll - 2003 english ner task . it can be seen that the lstm model significantly improves upon the strong lemma baseline by 3 points .
results in table 7 show that our model can distinguish between the accuracy of lrn with base + ln setting and test perplexity on ptb task with base setting .
3 shows the evaluation results of the word - based systems . word - based system retrieval outperforms all the other methods except for the one that is used in the human dataset ( hochreiter and schmidhuber , 1997 ) . word based system evaluation results are summarized in table 3 . all the word based system evaluations ( mtr ) are significantly better than the previous state - of - the - art systems on all three datasets . the word based systems use a lot of data , so the performance gap between human and system evaluation is less than that of the other two .
4 shows the human evaluation results on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 8 . the best performing system is seq2seqaug , which is comparable to the best human evaluation .
can be seen in table 1 , the results of the best performing models are summarized in p < 0 . 005 . our joint model outperforms the competition on three of the four datasets , with the exception of docsub . our model performs slightly better than both the df and docsub datasets .
can be seen in table 1 , the results of the best performing models are summarized in p < 0 . 005 . our joint model outperforms all the other models except for the one that we use , namely , docsub . we observe that our joint model performs better on both datasets , with the exception of df .
can be seen in table 1 , the performance of our model on the test set of corpus compared to the previous best performing model , europarl . our model performs slightly better than both the df and docsub models , but is slightly worse than the other two models .
3 shows the performance of our model compared to the previous best state - of - the - art models . our model achieves the best performance with a gap of 1 . 78 points between the official score of eurparl and our maxdepth metric , which is slightly better than our baseline . however , our model has the advantage of greater accuracy on the metric due to the high overlap of feature sets .
3 shows the performance of our model compared to the previous best state - of - the - art systems . our model achieves the best performance with a maxdepth score of 79 . 43 on the dsim test set , while europarl achieves the worst result with a gap of 1 . 5 points .
performance of our model on the validation set of visdial v1 . 0 . 0 is shown in table 1 . we use the enhanced version of our system , lf , as the model performs better than the original embeddings . surprisingly , the r0 , r2 and r3 scores are considerably worse than those of plain rl .
2 shows the performance ( ndcg % ) of different ablative studies on different models on the visdial v1 . 0 validation set . the best performing model is lrv , which can be seen in table 2 .
5 compares the performance of these models on hard and soft alignments . the results are summarized in table 5 . we see that hmd - f1 significantly outperforms the strong lemma baseline on hard alignments and fi - en is slightly better .
3 presents the results of our approach on the test set of ruse . the results are summarized in table 3 . the first group shows that our approach significantly improves the performance of the baselines on both test sets . our approach outperforms the strong lemma baseline by a noticeable margin . the second group shows the impact of different metrics on the performance . for example , our approach improves the recall scores of both lemma and test set by significantly improving the accuracy of the proposed ruse model .
3 presents the results of our approach with respect to bleu - 1 and bertscore - f1 . the results are summarized in table 3 . our approach obtains the best performance on both sets , with the exception of sfhotel , which is slightly better than the baseline .
performance of the models according to these baselines is reported in table 3 . the results are summarized in bold . leic scores significantly outperform the meteor scores of both m1 and w2v , however , the difference between the scores of the two sets is more pronounced for word - mover , which results in significantly higher performance on the m2 setting .
3 shows the performance of the models trained on the shen - 1 dataset compared to the previous state - of - the - art models . as shown in fig . 3 , m0 models outperform both m1 and m2 models in terms of performance on both datasets , with the exception of the case of m3 where the model with more than 2d2d has the better performance .
results are shown in table 3 . semantic preservation and transfer quality are the most important aspects of semantic preservation . our approach outperforms all the other methods with a large margin . the results are summarized in tables 3 and 4 .
5 shows the human evaluation results on three different datasets . the first is the acc dataset , where we show the performance of our model on the test set of yelp . the second is the evaluation results of our predicate schemas , which shows the quality of the semantic preservation evaluations . the results are shown in table 5 . the machine and human ratings of accuracy are significantly higher than the human ratings .
3 shows the performance of the models trained on the shen - 1 dataset compared to the previous state - of - the - art models . we observe that the m0 model performs better than the m2 model on both datasets , with the exception of the case of simuli - 1 where the model has more data .
results on yelp sentiment transfer are shown in table 6 . our best model achieves higher bleu than the best model using simple - transfer or n - word embeddings . however , the improvement is less pronounced for unlabeled sentences than for flipped ones . we also notice that our multi - decoder model is slightly worse than the previous best model , yang2018unsupervised , and the difference between n - words is much smaller . finally , we see that the use of classifiers in the training set can further improve the acc ∗ function .
statistics for nested disfluencies are shown in table 2 . the number of tokens that are correctly predicted to be disfluent is small but consistent . reparandum tokens are notably longer than repetition tokens , indicating that the syntactic patterns used to encode the tokens are more stable .
3 shows the relative frequency of rephrases correctly predicted as disfluent for the three categories . the number of tokens predicted to contain a content word is in parentheses , indicating that the function contains both the content word and the repair word . table 3 shows that the fraction of tokens that contain the word is less than the fraction predicted as containing the word .
results are shown in table 1 . the best performing model is the text model with the best performance on the test set . in addition , the best model with text + innovations is the one with the worst performance . table 1 shows the results for the single model with different features . text + innovations significantly improve the model ' s performance on both test sets . we empirically found that the use of text and innovations significantly boosts the model performance .
performance comparison with the state - of - art algorithms on the fnc - 1 test dataset . the results are shown in table 2 . our model achieves the best performance with the help of the sophisticated self - attention embeddings .
2 shows the performance of different methods on the apw and nyt datasets for the document dating problem . the best performing model is burstysimdater .
3 shows the performance of our method with and without attention . it achieves the best performance with 61 . 8 % and 63 . 9 % respectively compared to the performance with only graph attention .
1 / 1 and 2 / n models show the performance of the models trained on the pre - trained models . our model outperforms all the models except for the one that performs on the test set . we observe that for all models , our model performs better than the other models .
3 shows the test set ' s performance on each event . our method outperforms all the traditional methods in terms of both event and domain identification . all the methods use the word " f1 " . in fact , the method has the best performance on both events . the fact that the method relies on word " disambiguation " instead of " identification " sets the event apart from the original one . table 3 presents the results of the method with respect to event identification . the method used in table 3 shows that it is able to distinguish between multiple stages of the event .
can be seen in table 1 , all the models trained on the same hidden embeddings are shown in bold . all except for fine - tuned - lm are better than all the other models except for the one that relies on syntactic or semantic information . the difference between the two is less pronounced for the spanish - only model than for the english - only one .
4 shows the results on the dev set and on the test set using discriminative training . we use only subsets of the code - switched data to train our model , and only train test data can be trained on this data .
5 shows the performance of our system in the dev set compared to the monolingual set . the difference is less pronounced in the test set , however it is larger in the production set . fine - tuned - disc shows much better performance than fine - tuned - disc .
results in table 7 show that type - aggregated gaze features significantly improve recall for the three eye - tracking datasets tested on the same benchmark .
5 shows the precision ( p , r ) and f1 scores for using the type - aggregated gaze features on the conll - 2003 dataset . the results are shown in table 5 . when using the pre - trained gaze features , precision is considerably better than f1 score .
experimental results on wordnet 3 . 1 are shown in table 1 . the basic embeddings are derived from the original work ( lei et al . , 2014 ) and use syntactic skipgram . however , for syntactic - sg - based wordnet , we use glove - retro instead . the results on the ppa test set are reported in tables 1 and 2 . we use the syntactic coreference of wordnet 2 . 1 , and the semantic coreference obtained by applying autoextend rothe and schütze ( 2015 ) on the same set . the syntactic embedding gives a performance gain of 2 . 8 % over the original wordnet .
performance of our rbg dependency parser with features coming from various pp attachment predictors and oracle attachments is shown in table 2 . using oracle pp as the dependency parser , rbg improves upon the strong lemma baseline by 3 points .
3 : effect of removing sense priors and context sensitivity ( attention ) from the model . the effect of removing context sensitivity is shown in table 3 . with the attention removed , the model exhibits the best performance .
2 shows the results of domain tuning with respect to image caption translation . our model outperforms both the en - de and mscoco17 datasets in terms of bleu % scores .
results are shown in table 1 . the first group shows the results of domain - tuned subs1m on en - de while the second group shows results on flickr16 . in both cases , the results are slightly better than those of the other groups , but still superior than the others . sub - domain - tuning improves the results for both groups .
4 shows the bleu scores of models using automatic captions . our model outperforms all the other models except for the one that has the best number of captions in the set .
5 shows the performance of our approach with respect to captions . we use transformer , multi30k + ms - coco + subs3mlm and detectron mask surface . the results are summarized in table 5 . with the exception of the en - de embeddings , our approach obtains the best performance with an absolute improvement .
performance of subs3m is shown in table 1 . the best performances are obtained using the en - de embeddings ( cf . table 1 ) , while the best performance is obtained using mscoco17 . moreover , the multi - lingual approach relies less on visual features , leading to a better performance on the larger datasets .
3 shows the results of our approach compared to the previous best state - of - the - art en - fr - ht model on the test set of hotpotqa . the results are summarized in table 3 . as expected , the results are slightly worse than those of the original embeddings .
shown in table 1 , the number of parallel sentences in the train , test and development splits for the language pairs we used .
2 : training vocabularies for the english , french and spanish data used for our models .
5 shows the automatic evaluation scores ( bleu ) for the rev systems as well as ter evaluation scores . the results are shown in table 5 . the system ' s bleu scores are slightly worse than ter , but still comparable to ter .
2 shows the performance of our visually supervised model compared to the strong lemma baseline on flickr8k . the row labeled vgs is the visual supervised model from chrupala2017representations . the difference in performance is minimal , however it does indicate that our approach has a high impact on model performance .
experimental results on synthetically spoken coco are shown in table 1 . the model trained on the hidden embeddings of chrupala2017representations is comparable to the strong lemma baseline of rsaimage . however , our model has significantly worse performance than our model .
1 shows the results of different classifiers compared to the original on sst - 2 . for example , dan shows that it turns in a < u > screenplay that shows the edges at the edges ; it ’ s so clever you want to hate it . rnn shows similar results . similarly , cnn shows the same results .
2 shows the overall effect of fine - tuning on the ranking performance . the first row shows the percentage of words that have changed since the original sentence was written . the second row indicates that the number of words has increased , decreased or stayed the same through fine tuning . these results indicate that the effectiveness of linguistic modeling has not been significantly impacted by the increased number of occurrences .
3 shows the sentiment change in sst - 2 from positive to negative . it is clear from table 3 that the flipped sentiment labels have a significant effect on sentiment .
results are summarized in table 1 . the most striking thing about our results is that our approach is more interpretable than positive . this suggests that our method is more effective for target task , not only encouraging but also helps to improve interpretability . table 1 summarizes the results of our approach with respect to topic identification . our approach establishes a strong baseline on both types of documents , with the exception of sst - 2 , which shows that it can be improved with a larger corpus .
