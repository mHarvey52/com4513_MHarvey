2 : throughput for processing the treelstm model on our recursive framework and tensorflow â€™ s iterative approach , with the large movie review dataset as our training example . the recursive approach performs the best on inference with efficient parallel execution of the tree nodes , while the iteration approach shows better performance on training .
shown in table 1 , the balanced dataset exhibits the highest throughput thanks to the high degree of parallelization . however , it does not improve as well as the linear dataset , because there is only a small room of performance improvement left to design a more balanced dataset .
2 shows the performance for each model with different hyper parameters . we use conll08 as the model representation and softplus as the learning rate . the number of hyper parameters in each model is the average number of parameters for each representation , and the number of iterations in the validation set is the max pooling strategy we use . we also use the volkova et al . ( 2018 ) hyper parameters and sigmoid as the feature - rich layer . finally , we use the boost function to boost the model ' s f1 by 1 . 66 points in the f1 score . this helps improve the prediction accuracy .
1 shows the effect of using the shortest dependency path on each relation type . it can be observed that macro - averaged models achieve the best f1 ( in 5 - fold ) with sdp as well as the best diff . relation types are qualitatively similar to human - generated models , with different dependency paths having different f1 scores . we empirically found that the two approaches have the same effect on the f1 score , with the exception of the subtraction path .
3 shows the performance of the y - 3 model compared to the previous state - of - the - art models . in general terms , the results are slightly better than those of y - 2 : y - 3 shows that the r - f1 score is more than 50 % better on average and that the f1 score increases as well .
3 presents the results on the paragraph level . our model achieves state - of - the - art results on both essay and paragraph levels . the results are presented in tables 1 and 2 .
4 shows the c - f1 scores for the two indicated systems at the essay vs . paragraph level . note that the mean performances are lower than the majority performances over the runs given in table 2 . paragraph also exhibits lower performance , however , compared to the majority performance .
3 shows the results for each error generation system . our system performs on par with the original , with the exception of sasaki et al . ( 2018 ) . the results are shown in table 3 . original and original results are slightly worse than those on the other two systems . however , the improvement is slim , with only one exception : the improvement of bleu score over the original .
results for the original and cleaned versions are shown in table 1 . the number of distinct mrs , total number of textual references , and the number of slot matching script instances as measured by our data statistics , see section 3 . we also evaluated the concatenated scores of our original and the cleaned versions , to see whether the differences in ser are statistically significant ( table 1 ) .
performance of original and original models on the test set is presented in table 1 . original models dominate all test set with a large margin . the difference between accuracy is minimal , but significant for rouge - l and cider . also , the number of errors in the original model is relatively small , so we do not need to assume that all errors are caused by incorrect programming .
manual error analysis we performed on a sample of 100 instances from the original test set of tgen . the results are shown in table 4 . all the errors we found were caused by incorrect values in the training data , which we found to be significant ( i . e . , a significant percentage of incorrect values ) .
model performance on the external and internal datasets is reported in table 1 . our dcgcn model outperforms the state - of - the - art models on both metric metrics , with the exception of the bias metric .
2 presents the results on amr17 . our model achieves 24 . 5 bleu points on the model size in terms of parameters , compared to the previous best state - of - the - art model , seq2seqb ( beck et al . , 2018 ) .
3 shows the results for english - german and english - czech . our model outperforms the previous stateof - the - art models in both languages . the results are shown in table 3 . we notice that the english - language model performs better in the single - language setting than the other models we base it on . our model achieves the best results in both english - and german - language .
5 shows the effect of the number of layers inside the network on the performance of the model in the low - supervision setting . we observe that for all layers , there are only marginal differences in performance between the baseline and the expected output .
6 shows that rc + la also improves performance for gcns with residual connections . rc - based gcns outperform other baselines in terms of bias metric , with the exception of dcgcn2 .
model 3 shows that dcgcn outperforms all stateof - the - art models in terms of model b and model c on both metric metrics .
8 shows the ablation study results for amr15 in the low - supervision setting . the dense blocks dominate the i - th block , which means that the dense blocks tend to have less dense connections .
9 shows the ablation study results for the graph encoder and the lstm decoder . the results are shown in table 9 . the global encoder has the best performance , with a gap of 2 . 5 % in coverage .
investigate the effects of different initialization strategies on probing tasks . we show in table 7 the results for each initialization strategy that we base it on . the results are summarized in tables 7 . our model obtains the best performance with a gap of 2 . 5 points in performance .
can be seen in table 1 , the models trained on our proprietary cbow / 400 layout have the best performance . they also have the worst performance on the subtense and subtense subcategories , respectively .
cbow / 784 outperforms all the other models except for the one that cmp performs best on the mrpc test set . cbow also outperforms both the sst2 and sst5 baseline models in terms of mrpc score . sick - e is qualitatively better than sst3 , but it has the advantage of training on a larger corpus . it also improves the recall scores of sick and sick scores by 3 . 8 % in the standard setup .
performance on unsupervised downstream tasks attained by our models is shown in table 3 . hybrid models outperform cbow and cmp on almost all downstream tasks , except for those on the sts13 and sts15 datasets . in these cases , cmp performs better than cmp . cbow shows the relative change with respect to hybrid .
8 shows the performance for all initialization strategies that are tested on supervised downstream tasks . our model outperforms all the base lines with a gap of 3 . 5 points in performance from our baseline model .
6 shows the performance for different training objectives on the unsupervised downstream tasks . our cmow - c model outperforms cbow - r on both benchmarks . on the sts13 dataset , it achieves the best performance . however , it is still competitive with cbowr on the benchmark tasks .
1 shows the numerical results for each sub - step . our model outperforms all the other methods except for cbow - r . it is observed that the subtraction patterns that give the best results are the ones that have the worst performance . these results are reported in table 1 .
3 presents the results of all models tested on the subj test set . our model outperforms all the base models except for those using cbow - r and sst5 . we observe that all models except sst2 perform similarly on the mrpc test set , with the exception of sick - e .
3 presents the results of our system in italic and e + per . our system obtains the best e + org score and achieves the best per score . the results are shown in table 3 . all org scores are computed using the best performing feature set , mil - nd , in combination with other supervised learning systems . we also include the average org score of all systems in the table as well as the average per score of the systems using all misc scores . name matching and multi - task learning ( mil - nd ) achieve the best results with a minimum of 0 . 05 error on average . supervised learning , on the other hand , achieves the worst result with a 17 . 45 error on all org metrics . in fact , it even outperforms the best state - of - the - art systems in all metrics .
2 : the results on the test set under two settings are shown in table 2 . our system achieves the best results with 95 % confidence intervals in both settings . we observe that the automatic learning model mil - nd outperforms all the models in terms of e + p and f1 scores . supervised learning also improves the generalization ability of the model in both setups . name matching and recall accuracy are the most important factors in improving the results for both scenarios .
6 : entailment ( ent ) with ref = ref + ref ) is presented in table 6 . all models except g2s - gat outperform ref in terms of ref score . ref and ref scores are computed using ref as the model ' s ref metric and gen as the ref test set .
3 provides detailed results on the ldc datasets . our model outperforms the previous stateof - the - art models on three of the four datasets . the results are summarized in table 3 .
3 shows the results on ldc2015e86 test set when trained with additional gigaword data . our model outperforms all the other models with a large margin .
results of the ablation study are shown in table 4 . bilstm significantly improves the results for the ldc2017t10 development set , it also boosts the performance ofmeteor by 3 . 5 % .
results are shown in table 1 . we observe that the baseline g2s models have the best overall performance when compared to other models in terms of sentence length and sentence length . the models also have the worst average sentence length on average , which shows the diminishing returns from using word embeddings during training . finally , the average sentence diameter is slightly increased with the growth of g2s - gin ,
shown in table 8 , the fraction of elements in the output that are not present in the input graph that are missing in the generated sentence is much lower than in the standard nldc2017t10 model . this indicates that the use of token lemmas in the model can further improve interpretability for reference sentences .
4 shows the performance of our approach using the 4th nmt layer . it achieves the best performance with 96 . 7 % accuracy on a single corpus , outperforming all the other approaches except for pos .
2 : pos and sem tagging accuracy with baselines and an upper bound . the results are shown in table 2 . word2tag has the most frequent tag and is more likely to classify using unsupervised word embeddings .
results are presented in table 4 . our proposed method outperforms both published and unpublished methods . it achieves the best performance on both metric with a 92 . 9 % boost in accuracy . we observe that the accuracy obtained by our method exceeds the performance of both published methods .
5 shows the accuracy with different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . we find that for bi , pos is 87 . 9 % better than res and res , respectively , with a boost of 3 . 6 % over res .
performance on different datasets is shown in table 8 . the average age of the attacker is 9 . 7 and the average race is 14 . 3 . the difference between the attacker score and the corresponding adversary â€™ s accuracy is significant .
1 shows the accuracies for both languages when training directly towards a single task . for pan16 , the training agent trained directly towards the single task beats the baseline by 3 . 8 points .
2 shows the effect of the additional cost term on the balanced and unbalanced data splits . the classifier named pan16 leads the way in both cases . it is clear from the table 2 that the classifier is responsible for the imbalance in the balanced dataset , and that it has the ability to detect instances of gender bias in the conversation .
performance on different datasets with an adversarial training is shown in table 3 . the difference between the attacker score and the corresponding adversary â€™ s accuracy is reported in Î´ . in particular , for pan16 , the performance is significantly worse than that of the trained classifier . sentiment and gender bias are also significant factors in the performance , with a drop of 2 . 5 points in performance from the baseline .
6 shows the accuracies of the protected attribute with different encoders . the rnn embeddings outperform the embedding variants in both cases .
results are shown in table 1 . the first set of results show that our approach achieves the best results on both datasets with a minimum of training time and a finetune score . we also observe that our model outperforms the competition on both metric with a large margin on the ptb and the wt2 metric , the second set shows the performance gain on the model when trained with the finetuning score of 10m . finally , we observe that combining the features of the max and max features boosts the performance of both models .
results are shown in table 5 . we report the average time taken to train our model and the number of iterations used to train it . the results are presented in tables 5 and 6 . table 5 shows that our model has the best performance on both datasets when trained with the minimum of training time and when using the minimum training time .
3 shows the performance of our model compared to previous models . we report both the official score ( yuan et al . , 2015 ) and the result of re - scoring our model in the new window . the results are summarized in table 3 . yelppolar time significantly outperforms our model on both metric with an absolute improvement of 4 . 57 % on average compared to our model . finally , we also observe that our model has the best overall performance on both datasets when using the amapolar time and yelpfull time .
3 shows the bleu score on the wmt14 english - german translation task . it measures the time in seconds that the model takes to decode one sentence , measured from 0 . 2k training steps on the newstest2014 dataset . we also note that the sru model has the best performance in terms of decoding one sentence in a single training batch .
4 shows the performance of our model with respect to match / f1 score on squad dataset . the model obtains the best performance with a parameter number of 2 . 44m and achieves the highest f1 score with a score of 79 . 83 / 83 . 86 . 03 on the benchmark dataset , which shows the diminishing returns from mixing parameter number with model number .
6 shows the f1 score on the conll - 2003 english ner task . it can be seen that all the models use the same parameter number in the model , which indicates that the model performs well in the low - supervision settings .
results are shown in table 7 . lrn significantly improves upon the performance of elrn with base + ln setting and test perplexity on ptb task with base setting . however , it still performs slightly worse than glrn on snli task with base setting .
3 shows the evaluation results for human and system retrieval . all metrics are evaluated using word2vec feature set . the results are summarized in tables 1 and 2 . in general terms , all metrics are significantly better than the previous state - of - the - art systems . doc2vec features are particularly useful for human as the task is multi - task , with a minimum of 80 % improvement on average . word3vec feature sets are used for human evaluation .
4 presents the human evaluation results on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 8 . our system ranked in the top 1 or 2 for each of the four aspects .
3 provides detailed results on the en - de news commentary ( en ) dataset for english . our model outperforms all the other models except for the two that we included in this analysis . the results are summarized in table 3 . for english , we see that our model performs slightly better than the others on three of the four scenarios .
3 shows the performance of all models trained on the new corpus dataset . our model outperforms all the baseline models except for the one that we trained on it , eurparl , in all but one of the cases . we observe that our model performs slightly better than the baseline on all three datasets , with the exception of corpus .
3 provides detailed results on the en - de news commentary ( en ) dataset for english . our model outperforms all the other models except for the two that we included in this analysis . we observe that our model performs on par with the best on both datasets , with the exception of the one on corpus . the results are presented in table 3 .
performance of our model on the metric is reported in table 3 . our model achieves the best performance with a minimum of 3 . 5roots on each metric compared to the previous best state - of - the - art model , europarl . additionally , we achieve the best results with a maxdepth score of 1 . 78 on every metric , with a marginal drop of 0 . 05roots .
performance of our model on the metric is reported in table 3 . our model achieves the best performance on both metric metrics with a minimum of 1 . 5 % truedepth score on each metric compared to the previous state - of - the - art . europarl also outperforms the maxdepth and averagedepth scores of all metrics except for the metric of depthcohesion . the results are summarized in table 1 .
performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . we note that the enhanced version of our system , lf , obtains significantly better performance than the original version .
performance ( ndcg % ) of different ablative studies on different models on the visdial v1 . 0 validation set is shown in table 2 . using only p2 exposes the model to hidden dictionary learning . this shows that the effectiveness of p2 can be further improved with the addition of a history shortcut .
5 compares the performance of these models on hard and soft alignments . the results are summarized in table 5 . the hmd - recall model outperforms both the soft and hard alignments with a large margin .
3 presents the results on the direct assessment and bertscore - f1 scores . the results are summarized in table 3 . our approach obtains the best performance on both sets . our model outperforms both the baseline and the ruse baseline on both metrics with a significant improvement on average .
3 presents the results on the bagel and sfhotel scores . the results are summarized in table 3 . our baseline model obtains the best performance on both sets with a minimum of 0 . 005 bleu - 1 score and a maximum score of 1 . 005 . these results are reported in tables 3 and 4 .
performance of the models according to these baselines is reported in table 2 . the results are summarized in table 1 . the summaries are presented in bold . they are statistically significant even when using only plain averaged word embeddings , meaning that the performance obtained by the models is relatively consistent across all three settings .
3 shows the performance of the models trained on the shen - 1 dataset compared to the original embeddings . we observe that for all models , the model performs better than the original model on both datasets , with the exception of m1 .
results are shown in table 4 . semantic preservation and transfer quality scores are the most important features for semantic preservation . the results are summarized in tables 4 and 5 . we observe that both semantic and semantic preservation features are strongly concentrated in one domain , with the semantic preservation having the most significant impact . complicate features are concentrated in the transfer quality aspects of the dataset , leading to a drop in performance from the baseline ( m0 to m7 ) . the semantic preservation feature of the Î´pp dataset is strongly concentrated within a single domain , which results in a significant improvement over the baseline .
human ratings of accuracy are shown in table 5 . it can be observed that both the quality of acc and pp scores are high , indicating that the model performs well in the task . the results are also statistically significant , with the difference being less pronounced for sim and pp . table 5 shows the results of human evaluation .
results are shown in table 6 . all the models trained on the shen - 1 dataset outperform the baseline on all metrics except the percentage of para in the acc setting . we observe that for all models except m1 , the model with the leastpara + lang improves performance .
6 shows the results on yelp sentiment transfer . our best model achieves higher bleu than any prior work at similar levels of acc âˆ— with the exception of fu - 1 . the results are shown in tables 6 and 7 . again , the difference between using the best model and using the worst classifier is minimal , but we do not include it in table 6 .
statistics for nested disfluencies are shown in table 2 . the number of repetition tokens that were correctly predicted as disfluency is small but significant . reparandum length is relatively high , meaning that the number of tokens predicted to be disfluentiable is relatively small .
3 shows the relative frequency of rephrases correctly predicted as disfluent for each category . the fraction of tokens that contain a content word is in parentheses , indicating that the model has correctly predicted the word to belong in both the reparandum and the repair . table 3 also shows the distribution of tokens in each category as well as the number of tokens predicted to belong to both .
results are shown in table 4 . we observe that the text model with the most innovations achieves the best results when the model is tested on the single dataset . the results are presented in tables 1 and 2 . in particular , the results are statistically significant with respect to the number of iterations in the model .
performance comparison with state - of - art algorithms on the fnc - 1 test dataset is shown in table 2 . our model achieves the best performance with a reasonable level of accuracy , and is slightly better than the other models we compare with .
2 shows the performance of different methods on the apw and nyt datasets for the document dating problem . the best performing model is burstysimdater .
3 shows the performance of our method with and without attention . it achieves the best performance with 61 . 8 % and 63 . 9 % respectively on average , compared to ac - gcn .
model performance on each stage is reported in table 1 . our model outperforms all the models except trigger in terms of performance . we observe that for all stages , our model performs better than the other models except for the one that is pre - trained . this suggests that the model is more suitable for production use .
3 shows the test set for each event . our method outperforms the other methods in terms of both event identification and classification . all the methods cause a significant drop in performance when trained on a single event . in all but one case , the error reduction is statistically significant ( p < 0 . 001 ) . cross - event features are beneficial for both systems , with the exception of the case of empty . the method used in this study establishes a new state - of - the - art on all events .
can be seen in table 1 , all the models trained on our model are slightly better than the original spanish - only model in terms of test acc and test wer . all the fine - tuned models seem to have better performance on the dev acc task than in english , as shown in the table .
results on the dev set and on the test set are shown in table 4 . we trained fine - tuned with only subsets of the code - switched data in the training set . we also trained with only the train dev and full train test .
5 shows the performance on the dev set compared to the test set . we find that fine - tuning achieves the best performance with the gold sentence in the set , while the monolingual model performs slightly worse .
results in table 7 show that type - aggregated gaze features significantly improve recall for the three eye - tracking datasets tested on the conll - 2003 dataset .
5 shows the precision and f1 scores for using the type - aggregated gaze features on the conll - 2003 dataset . the results are shown in table 5 . type - aggregation features significantly improve recall ( p < 0 . 001 ) and f - score ( p ( cid : 28 ) by 0 . 01 ) . with the additional features , the performance drops significantly .
results on belinkov2014exploring â€™ s ppa test set . we use hpcd as the base for wordnet 3 . 1 . it uses syntactic - sg embeddings obtained by applying autoextend rothe and schÃ¼tze ( 2015 ) on glove . the results on this test set are shown in table 1 . we use the full set of features from the original paper , and we use them as the initialization vector . we also use the syntactic features of skipgram as the reference vector . our model improves upon the strong lemma baseline by 10 . 8 % in the final set , but still performs substantially worse than the original wordnet . further improving performance by applying the semantic features of syntactic sg allows further improvements .
results from the rbg dependency parser are shown in table 2 . all the features coming from various pp attachment predictors and oracle attachments have a significant impact on uas performance . however , rbg also outperforms oracle pp and lstm - pp by a significant margin .
3 : effect of removing sense priors and context sensitivity ( attention ) from the model . the effect of removing context sensitivity is shown in table 3 .
2 : adding subtitle data and domain tuning for image caption translation ( marian amun et al . , 2018 ) with subsfull embeddings improves the results for both en - de and flickr17 . the model achieves the best ensemble - of - 3 score .
results are shown in table 1 . the results are presented in tables 1 and 2 . our model outperforms all the en - de models except for the one that mscoco17 uses . table 1 shows that the domain - tuned subs1m outperforms the other models in terms of performance .
4 shows the bleu scores in terms of automatic captions added via marian amun ( marian amun et al . , 2018 ) . the results are shown in table 4 . the models using the multi30k model outperform the models using only one or all 5 captions . as expected , the model with the best five captions is better than the model without .
5 compares the approaches for integrating visual information with our captions . we use multi30k + ms - coco + subs3mlm and detectron mask surface . results are summarized in table 5 . we empirically found that enc - gate and dec - gate achieve the best results with a 62 . 57 bleu % improvement on average compared to the previous state of the art .
performance of subs3m is presented in table 4 . we observe that the text - only model outperforms the ensemble - of - 3 model in all but one of the comparisons . moreover , the multi - lingual model ( which relies on word embeddings ) achieves an overall improvement of 3 . 36 points over the baseline on the en - de dataset ( mscoco17 ) .
3 shows the results of our approach compared to the original embeddings . the results are summarized in table 3 . we observe that our approach outperforms the best previous approaches on both metric , with the exception of mtld .
shown in table 1 , the number of parallel sentences in the train , test and development splits for the language pairs we used .
2 : training vocabularies for the english , french and spanish data used for our models .
5 shows the automatic evaluation scores for rev systems . the bleu scores and ter scores both show significant performance drop as a result of the increased variation in rev performance .
2 shows the performance of our visually supervised model compared to the baseline model from flickr8k . the row labeled vgs is the weighted average of the 10 frames in our model , followed by the mean mfcc rank . the results are reported in tables 2 and 3 .
results on synthetically spoken coco are shown in table 1 . the model trained on the embeddings of chrupala2017representations is significantly better than the baseline model , confirming the importance of the clustering quality .
1 shows the results for each classifier compared to the original on sst - 2 . for example , dan turns in a < u > screenplay that shows the edges edges at the edges ; it â€™ s so clever you want to hate it . similarly , for cnn , the edges of the screenplay are slightly curved . this shows that when a model turns on a feature - rich screenplay , the model can easily distinguish between the features of the original and the ones of a new classifier . we find this interesting since the model tends to be more appealing for the target audience .
2 : part - of - speech ( pos ) changes in sst - 2 : , , and indicate that the number of occurrences have increased , decreased or stayed the same through fine - tuning respectively . these results are shown in table 2 . we observe that the pos scores for most words have remained the same , indicating that the language adaptation has not resulted in a significant drop in performance .
3 shows the sentiment changes in sst - 2 as the number of negative labels is flipped from positive to negative . these results show that the effect of negative sentiment is not only localized on the original sentence but also on the sentiment score as well .
results are presented in table 2 . the results are summarized in table 1 . we observe that the competitive nature of our method is relatively consistent across all three clusters , with the exception of the case of sst - 2 . table 1 shows that our approach has the advantage of significantly improving interpretability .
