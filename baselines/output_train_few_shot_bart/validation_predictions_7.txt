2 shows the performance of the treelstm model on the recursive framework , with the large movie review dataset , and tensorflow ’ s iterative approach on the recur dataset .
table 1 shows the performance improvement for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . throughput ( instances / s ) balanced vs . linear shows the best performance compared to the linear dataset when the batch size increases to 25 .
2 shows the performance of the max pooling strategy for each model with different representation . the maximum pooling approach outperforms all other approaches except softplus , which performs better in all model variations .
1 shows the effect of using the shortest dependency path on each relation type . the best f1 ( in 5 - fold ) with sdp is achieved in the model - averaged setting , while the worst f1 is obtained in the macro - averaging setting . we observe that using sdp improves the f1 by 2 . 5 points compared to using the best dependency path .
results are shown in table 1 . y - 3 : y ( y - 3 ) achieves 50 % f1 and 50 % r - f1 50 % the results are comparable to those of y - 2 : y , with the exception of f1 50 % , and the difference between y - 4 : y and y - 5 : y . the difference between the results is significant .
results are presented in table 1 . the results of the test set are summarized in terms of paragraph level and f1 . we observe that mst - parser outperforms all other methods except for the mst parser , which achieves the best performance on both test sets .
4 shows the c - f1 scores for the two indicated systems , lstm - parser and stagblcc . note that the mean performances are lower than the majority performances over the runs given in table 2 .
results are presented in table 1 . the results are summarized in table 2 . the original model outperforms all the other models in terms of bleu and accuracy . the best performing model is the sc - lstm , which outperforms both the original and the original models in both cases .
1 compares the original e2e data with our cleaned version . the results are summarized in table 1 . the original and the cleaned versions are comparable in terms of mrs , mrs and refs , and ser as measured by our slot matching script .
results are presented in table 1 . original and tgen models outperform the original in all but one of the three cases . the results are summarized in table 2 . original model outperforms both the original and the original models in terms of performance . the results of the original model outperform all the other models except for sc - lstm .
4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set . the results are summarized in table 4 . adding and removing incorrect values significantly reduces the number of errors in the training set , but it does not improve the performance .
results are presented in table 1 . we observe that the dcgcn model outperforms all the other models in terms of performance , with the exception of seq2seqk .
2 shows the results on amr17 . our model achieves 24 . 5 bleu points in terms of parameters , compared to the previous best state - of - the - art model , ggnn2seqb ( beck et al . , 2018 ) and dcgcn ( ours ) achieves 27 . 5bleu point . we observe that our model size is comparable to the best state of the art seq2seqb model , which is comparable in size to seq3seq .
results are presented in table 1 . the results are summarized in bold . our model outperforms all the other models in terms of english - german b and english - czech c . as expected , the results are similar across all languages except german , with the exception of czech . we also observe that our model performs better than the best in both english and german .
5 shows the effect of the number of layers inside dc on the performance of the model . table 5 shows that when we add layers , the model outperforms the previous state - of - the - art models in terms of performance . we observe that when adding layers , we can see that the model performs better than the other models .
6 : comparisons with baselines with residual connections . the results are shown in table 6 . gcn + rc ( 2 ) and dcgcn + la ( 9 ) show that residual connections are beneficial for gcn performance . however , gcn + rc ( 6 ) shows a significant drop in performance compared to other baselines .
results are presented in table 1 . we observe that dcgcn ( 1 ) outperforms all models in terms of performance by a significant margin . the results are summarized in table 2 . the results show that the model outperforms both the best and worst performing models .
8 shows the results of ablation study on amr15 . the results are shown in table 8 . table 8 shows that removing the dense connections in the i - th block improves the performance of the dev set by 2 . 5 points compared to the original amr14 model . adding the dense blocks improves performance by 2 points compared with the original model .
9 shows the ablation study for the graph encoder and the lstm decoder . the results are summarized in table 9 . the model used in this study outperforms all the other models in terms of coverage and performance .
7 shows the performance of our initialization strategies on probing tasks . our model outperforms all the other approaches except for subjnum and coordinv . the results are summarized in table 7 .
results are presented in table 1 . table 1 shows the performance of our method in terms of depth and subtraction . the best performing method is h - cmow / 400 , which outperforms all the other methods except for h - cbow , which performs better .
results are presented in table 2 . the results are summarized in table 1 . our model outperforms all the other methods except for subj , which outperforms both subj and sst2 in terms of performance .
3 shows the performance on unsupervised downstream tasks attained by our models compared to hybrid models . the results are summarized in table 3 . hybrid models outperform both cbow and cmp . cbow shows the relative change in performance with respect to hybrid model .
8 shows the performance of our initialization strategies on supervised downstream tasks . our model outperforms all the other approaches except for subj , which outperforms both subj and mpqa in terms of performance .
6 shows the results for different training objectives on the unsupervised downstream tasks . cmow - c performs better than cbow - r on both training objectives .
results are presented in table 1 . the best performing method is cbow - c , which outperforms all the other methods in terms of depth and precision . it also outperforms the best performing ones , such as somo .
results are presented in table 1 . we observe that cbow - r outperforms both sst2 and sst5 in both mrpc and mpqa by a significant margin . in both cases , we observe that the performance of both models is comparable to those of the other two methods .
results are presented in table 1 . our system outperforms all previous systems in e + org and e + per . the results are summarized in table 2 . we observe that our system performs better than all other systems except for the one in which it performs best . in table 1 , we observe that the system performs worse than all the other systems in all but one of the three cases . as expected , the performance of our system is significantly worse than those of the previous systems .
results on the test set under two settings are shown in table 2 . our system outperforms all previous models in e + p , e + r and f1 scores . our model achieves the best performance in all three settings , with 95 % confidence intervals of f1 score . our model outperforms the previous model in all but one of the three settings except name matching .
6 : entailment ( ent ) and model ( g2s - gat ) in table 6 shows that the model outperforms both the ref and gen models in terms of performance . in particular , the models outperform the g2s models by a significant margin .
results are presented in table 1 . we observe that g2s - ggnn outperforms all other models in terms of performance , with the exception of the model that performs better on the ldc2017t10 test set .
3 shows the results on ldc2015e86 test set when models are trained with additional gigaword data . the results are shown in table 3 . our model outperforms all the other models in terms of performance . we observe that the g2s - ggnn model performs significantly better than the previous model ,
results of the ablation study on the ldc2017t10 development set are shown in table 4 . bilstm significantly outperforms both models in terms of size and size . in particular , bilstms significantly outperform both models .
results are presented in table 1 . we observe that g2s - ggnn outperforms all models in terms of sentence length and sentence length . the results are summarized in table 2 . in particular , we observe that the average sentence length is significantly longer than the average word length .
shown in table 8 , the fraction of elements in the input graph that are missing in the generated sentence ( g2s - ggnn ) is significantly larger than the fraction in the output graph ( miss ) . this shows that using token lemmas improves the performance of the model in the test set .
4 shows the performance of the 4th nmt encoding layer on a smaller parallel corpus ( 200k sentences ) .
2 shows the accuracy of the word embeddings with baselines and an upper bound . the results are shown in table 2 . the best performing word embedding is word2tag .
results are presented in table 1 . pos tagging accuracy outperforms all other methods except for those that rely on the word embeddings . table 1 shows the performance of all the models except for the ones with the most accuracy . the results are summarized in table 2 . the results of all models are shown in the table 1 .
5 shows the accuracy with different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english targets .
8 shows the performance of the trained attacker on different datasets . the difference between the attacker score and the corresponding adversary ’ s accuracy is significant .
1 shows the performance of pan16 when training directly towards a single task . the results are shown in table 1 .
2 shows the performance of our model on the balanced task and unbalanced task splits . the results are shown in table 2 . our model outperforms pan16 in balanced task splits , while pan16 performs worse in unbalanced tasks splits .
3 compares the performance of pan16 and pan16 on different datasets with an adversarial training . the difference between the attacker score and the corresponding adversary ’ s accuracy is significant .
6 : accuracies of the protected attribute with different encoders . the results are shown in table 6 .
results are presented in table 1 . this model outperforms all models except lrn , which outperforms both lrn and lstm in terms of model performance . the results are summarized in table 2 . we observe that lrn outperforms lrn in both model performance and model performance by a significant margin . in particular , we observe that the performance of lrn is comparable to that of sru in both models .
results are presented in table 5 . our model outperforms all previous models in terms of time and accuracy . the results are summarized in table 6 . we observe that our model has the best performance on both datasets .
results are presented in table 1 . our model outperforms all previous models in terms of err performance . the results are summarized in table 2 . we observe that our model performs better than all other models except for yelppolar time , which is comparable in performance to both ama and amafull time . table 1 shows the performance of our model compared to other models .
3 shows the bleu score on wmt14 english - german translation task . the model outperforms all the other models in terms of training steps , with the exception of gru , which outperforms both sru and lrn . in addition , the performance of sru is comparable to that of lrn , with a difference of 0 . 99 and 1 . 15 , respectively .
4 shows the performance of our model on squad dataset . our model outperforms all models in terms of match / f1 score , except lrn , which outperforms lrn and atr by a significant margin . in addition , our model performs better than all the models except sru , which performs worse than lrn . we observe that our model achieves the best performance with respect to match and f1 score .
6 shows the f1 score on conll - 2003 english ner task . the lstm model outperforms all the models except sru and lrn in terms of parameter number . in both cases , the parameter number is significantly higher than lrn and sru , indicating that the model is able to perform well in both cases . however , the performance of the lrn model is significantly worse than the sru model . table 6 shows that the performance improvement is due to the high precision of the model .
7 shows the performance on snli task with base + ln setting and test perplexity on ptb task with base setting . the results are shown in table 7 .
results are shown in table 1 . word and oracle retrieval outperform human and human in terms of performance . the results are summarized in table 2 . using word and system retrieval , the results are significantly better than human , but still slightly worse than human . as expected , word and machine learning outperform both human and system - based approaches . in particular , word / machine learning outperforms human on both systems . when using word - based methods , the performance is comparable to human , with the exception of human , which is more likely to be due to the large number of errors .
4 shows the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 . the best performance among automatic systems is shown in bold , with statistical significance marked with ∗ ( approximation randomization test , p < 0 . 0005 ) . our system outperforms all systems except seq2seqaug .
results are presented in table 1 . the results are summarized in table 2 . our model outperforms all the other models except for df , which is comparable in performance to df . we observe that our model performs better than both df and df in terms of performance . however , the performance of our model is comparable to that of df .
results are shown in table 1 . the results of our experiments are summarized in table 2 . we observe that our approach outperforms both the df and df models in terms of performance . our model outperforms the df model by a significant margin . in df , we observe that the performance of our model is comparable to that of our df model .
results are shown in table 1 . the results of our experiments are summarized in table 2 . we observe that our approach outperforms both the df and df models in terms of performance . our model outperforms the df model by a significant margin .
results are presented in table 2 . the results are summarized in table 1 . our model outperforms all the other models in terms of depthcohesion , with the exception of europarl , which achieves the best performance on both metrics .
results are presented in table 2 . the results are summarized in table 1 . our model achieves the best performance on both metric and metric metrics . our model outperforms the best in both metric metrics , with the exception of maxdepth , which is slightly worse than our model .
1 shows the performance ( ndcg % ) on the validation set of visdial v1 . 0 . lf outperforms the original visdial model in terms of r0 , r2 , r3 , and weighted softmax loss , respectively .
2 shows the performance ( ndcg % ) of different ablative studies on different models on visdial v1 . 0 validation set . our model outperforms all the other models in terms of accuracy ( i . e . , p2 and coatt ) .
5 compares hmd - f1 + bert on hard and soft alignments . the results are summarized in table 5 . the hmd model outperforms all the other models in terms of bert performance .
results are presented in table 1 . the results are summarized in terms of direct assessment ( ruse ) and bertscore - f1 ( bertscore ) scores . the results of ruse ( * ) are comparable to those of meteor ( md + w2v ) , but significantly worse than those of md + w2v . as expected , the results are comparable with those of both mdm + w1 and md + . however , the performance of both models is slightly worse than that of mdm - w1 .
results are presented in table 1 . the bleu - 1 model outperforms all the other baselines except for the bertscore - f1 model , which achieves the best performance on both sets . the results are summarized in table 2 .
results are presented in table 1 . the baselines are summarized in terms of bertscore - recall scores . our model outperforms all the baselines except for wmd - 1 and w2v . we observe that our model achieves the best performance on both metrics , with the exception of wmd - 1 , where it achieves the worst performance .
results are presented in table 1 . we observe that the m0 model outperforms the previous state - of - the - art models in terms of performance . as expected , the performance of the m1 model is comparable to those of the previous models . in fact , it outperforms all the other models except for the m2 model , which performs worse than the previous model .
results are presented in table 1 . the results are summarized in terms of transfer quality and semantic preservation . we observe that the results are comparable across all models , with the exception of yelp , where the transfer quality results are significantly worse . as expected , the results for both models are similar across all but one of them . in both cases , transfer quality performance is significantly worse than that of the other models , indicating that the performance of both models is comparable .
5 shows the results of human sentence - level validation . the results are shown in table 5 . our model outperforms both human and machine models in both acc and pp . we observe that the human ratings of semantic preservation are higher than the machine ratings of fluency , indicating that the system is more accurate than machine and human judgments .
results are presented in table 1 . we observe that the m0 model outperforms the m2 model in terms of performance , with the exception of the shen - 1 model , where the model performs worse than the m1 model .
results on yelp sentiment transfer are shown in table 6 . our best model achieves the highest bleu than any prior work , but it is significantly worse than our best model , yang2018unsupervised , which is restricted to 1000 transferred sentences and human references .
2 shows the percentage of reparandum tokens that are correctly predicted to be disfluent . the percentage of repetition tokens that were correctly predicted as disfluencies is slightly higher than the percentage predicted as repetition tokens . in nested disfluency , the repetition tokens are much more likely to be misfluent than the repetition ones .
3 shows the percentage of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and the repair ( content - function ) . the percentage of tokens correctly predicted to contain the content word is shown in table 3 . in both cases , the fraction of tokens that contain the word is significantly higher than those that contain only the word .
results are presented in table 1 . the model outperforms all the other models in terms of dev mean and innovations . in particular , the models outperform all the models except for the single model , which shows the best performance . we observe that the model performs best on both the single and multi - step test sets , with the exception of the one in which text + innovations are used .
2 shows the performance of word2vec embedding on the fnc - 1 test dataset . our model outperforms all the state - of - art models on the test dataset by a significant margin .
2 shows the performance of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the best performing method is burstysimdater .
3 shows the performance of both word attention and graph attention for this task . in table 3 , the accuracy of word attention is comparable to that of graph attention . as expected , the performance is comparable with that of ac - gcn .
results are shown in table 1 . the best performing models are cnn , dmcnn , jmee and jrnn . our model outperforms all the other models in terms of performance , with the exception of cnn .
results are presented in table 1 . in the case of cross - event , our method outperforms all the other methods in terms of identification and classification . the results of cross - event training are summarized in table 2 . all the methods used in cross - event training outperform all the methods except for the one used in the original study .
results are shown in table 1 . all models outperform all models except for the ones with the best performance . the results are summarized in table 2 .
4 shows the results on the dev set and on the test set using discriminative training with only subsets of the code - switched data . the results are shown in table 4 .
5 shows the performance on the dev set compared to the test set . we observe that the gold sentence in the standard set outperforms the monolingual sentence in both the dev and test sets .
results are shown in table 7 . precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset ( table 7 ) .
5 shows the precision ( p ) , recall ( f1 ) and f1 scores for using type - aggregated gaze features on conll - 2003 dataset .
results on belinkov2014exploring ’ s ppa test set are shown in table 1 . syntactic - sg embeddings are used in wordnet , verbnet and wordnet 3 . 1 , and glove - retro is used on wordnet 4 . 1 . the hpcd ( full ) results show that using syntactic sg embedding improves the performance of wordnet and verbnet .
2 shows the results of the rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . the results are summarized in table 2 . in particular , the results show that the system performs better than the hpcd ( full ) and oracle pp ( partial ) models .
3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . the results are shown in table 3 .
2 shows the results of combining subtitle data and domain tuning for image caption translation ( bleu % scores ) . the results are summarized in table 2 . subsfull and multi30k embeddings outperform the domain - tuned models in terms of bleu % .
results are shown in table 1 . subdomain - tuned subs1m outperforms all models except those using domain - tuning . the results are summarized in table 2 . in the en - de setting , the models outperform all models in terms of performance , with the exception of those using subdomain tuning .
4 shows bleu scores in terms of multi30k captions . the results with marian amun are summarized in table 4 . the best results with the best captions ( dual attn . ) are shown in the table 4 . adding automatic image captions shows that the system performs well .
5 compares the performance of different strategies for integrating visual information ( bleu % scores ) . multi30k + ms - coco + subs3mlm outperforms dec - gate and enc - gate in terms of bleu % score . the results are summarized in table 5 . in the en - de setting , we use transformer and multiterramlm to integrate visual information . we observe that the enc - gating performance is comparable to the dec - gated performance on en - fr , but is slightly better than dec - gate .
3 shows the performance of subs3m and subs6m in terms of text - only and multi - lingual features . the results are summarized in table 1 . sub - text - only models outperform all models except for subs2m , which outperforms all models . in addition , subs3ms outperforms both models when using multiple layers of the same model . as expected , the performance is comparable across all models , with the exception of the models that use multiple layers .
results are presented in table 1 . we observe that en - fr - ht outperforms both mtld and en - rnn - ff in terms of translation performance . the results are summarized in table 2 . in addition , we observe that the performance of both models is comparable to those of the original models . our model outperforms the original model in both cases .
results are shown in table 1 . the number of parallel sentences in the train , test and development splits for the language pairs we used is small .
2 shows the results of training vocabularies for the english , french and spanish data .
5 shows the bleu and ter scores for the rev systems compared to the ter and en - fr systems , respectively .
2 shows the performance of our visually supervised model on flickr8k . the results are shown in table 2 . we observe that the vgs model outperforms segmatch and segmatch in terms of recall performance .
results are shown in table 1 . the model outperforms all previous models except audio2vec - u , which achieves the best performance .
1 shows the results of the different classifiers compared to the original on sst - 2 . we report further examples in table 1 . for example , cnn turns in a < u > screenplay that has edges edges edges and curves , and for rnn , the edges edges edge edges are curved .
2 shows the results of fine - tuning on sst - 2 . the results are shown in table 2 . we observe that the number of words in the original sentence has increased , decreased or stayed the same for the second sentence . it is clear that fine tuning has not improved the quality of the sentence .
3 shows the change in sentiment in sst - 2 compared to the original sentence . the results are shown in table 3 . negative labels are flipped to positive and negative labels to indicate that the sentiment is more positive . positive labels are also flipped to negative and vice versa . we observe that negative labels are more likely to cause negative sentiment .
results are presented in table 1 . the results are summarized in terms of ppmi ( p < 0 . 001 ) and positive ( p > 0 . 005 ) . in addition , the results are reported in table 2 . in general , the performance of the sift model is comparable to that of pubmed ( p ≤ 0 . 01 ) . the performance of sift is comparable with that of other approaches in the literature . however , the difference in performance between the two approaches is significant .
