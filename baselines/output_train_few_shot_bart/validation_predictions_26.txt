2 : throughput for processing the treelstm model on our recursive framework and tensorflow ’ s iterative approach , with the large movie review dataset as our training example . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the iteration approach shows better performance on training .
1 shows the performance of the balanced and balanced rnn datasets compared to the linear rnn dataset , which exhibits the highest degree of performance improvement . further , the rnn model performs slightly worse than the linear dataset when the number of iterations is increased from 1 to 25 .
2 shows the performance for each model with different hyper parameters . the max pooling strategy consistently performs better in all model variations . the number of hyper parameters in the validation set is small but consistent , and the number of feature maps are high . adding the hyper parameters boosts f1 by 1 . 66e - 03 . adding the feature maps reduces the f1 score by 0 . 57 . using sigmoid embeddings reduces the h2 score by 2 . 59 points .
1 shows the effect of using the shortest dependency path on each relation type . it can be observed that macro - averaged models achieve the best f1 ( in 5 - fold ) with sdp , but do not outperform these models in terms of diff . diff . the dependency path alone does not improve the f1 by much .
3 shows the performance of the y - 3 model compared to the previous state - of - the - art models . in general terms , it achieves 50 % and 50 % advantage over the f1 model , respectively .
3 presents the results of our method in terms of paragraph level . our approach achieves the best results with 50 % of the time on average compared to 50 % on average .
4 shows the c - f1 scores for the two indicated systems at the essay vs . paragraph level . note that the mean performances are lower than the majority performances over the runs given in table 2 .
3 shows the performance of our system on the original and wrong test sets . our system performs on par with the best previous stateof - the - art systems , but on the wrong test set , it performs slightly worse than the original . this suggests that future work may need to consider further improvements .
1 compares our original and cleaned versions of the e2e data with the original ones . we find that the number of distinct mrs , total number of textual references , and number of slot matching script instances are small but significant , compared to the original . we also find that our cleaned version contains more than 50 % more textual references ( 60 % ) .
performance of original and original models compared to original ones is presented in table 4 . original and original models perform better than original models , but their performance is slightly worse than original ones .
4 shows the results of manual error analysis on a sample of 100 instances from the original test set . for each error we found that there was a minor miss or a slight mispelling in the training data , which caused the errors to be added .
model performance on the external and internal datasets is reported in table 1 . the best performing dcgcn models are tree2str and dcgcnl ( konstas et al . , 2016 ) and pbmt ( pourdamghani and cohen , 2018 ) . the difference between the external and external models is minimal but significant , with the exception of seq2seqk ( which relies on syntactic or semantic information ) .
2 presents the results on amr17 . our model achieves 24 . 5 bleu points , which exceeds previous stateof - the - art models by 3 . 5 points . as expected , our model size exceeds the ensemble model size of seq2seqb .
3 shows the results for english - language and german , compared to english - czech . our model outperforms the previous best - performing models in both languages . the results are presented in table 3 . we observe that the single model performs better than the two - step model in english , while the improvement in german is more striking in english . peyrard et al . ( 2017 ) and bach et al . , 2018 ) show that the combination of single and multi - step models improves the performance for both languages ,
5 shows the effect of the number of layers inside the dc stack on the performance of the model when we add the layers of layers of the stack . for example , we can see that there is a significant imbalance in performance between layers that contribute to the same output , i . e . that there are fewer layers than expected by previous work .
6 shows that rc + la improves gcn performance over residual connections . with residual connections , gcn improves performance over other baselines .
model 3 shows that dcgcn has the best performance on both types of datasets . as shown in table 3 , dcgcnn has the worst performance on all three of the models .
8 shows the ablation study results for amr15 . it can be seen that dense blocks reduce the number of connections in the dev set . this shows that removing the dense blocks helps the model to improve its performance .
shown in table 9 , the global encoder and the lstm decoder use the best performing model . however , the differences in coverage between the two models are minimal , i . e . - linear combination < 22 . 9 % and - coverage mechanism > 23 . 2 % respectively compared to the previous best state - of - the - art model .
investigate the effects of different initialization strategies on probing tasks . table 7 presents the results for each initialization strategy . our model outperforms the previous stateof - the - art models in every respect . the most striking thing about our model is that it learns to rely less on superficial cues .
1 shows the performance of our method compared to other methods . we observe that our approach obtains the best performance when subtraction is only considered when the gap between threshold and threshold is less than 50 % of the time . also , our method obtains a better performance than our previous model .
1 shows the performance of our method compared to other methods . our model outperforms all the other methods except subj except for cmp . cbow / 784 shows lower performance on mrpc compared to sick - e , sst2 and sst5 . however , our model performs better than both subj and mpqa . this suggests that our approach has superior generalization ability on both aspects .
performance on unsupervised downstream tasks attained by our models is shown in table 3 . hybrid models perform better than cmp . however , it is comparable to cmp in some downstream tasks , such as those on sts15 and sts16 .
8 presents the performance of our model on supervised downstream tasks . our model outperforms all the base models except sst2 and sst5 by a margin of 3 . 5 points .
6 shows the performance for different training objectives on unsupervised downstream tasks . cmow - c shows lower performance than cbow - r on some of the supervised tasks , but higher performance on sts15 .
2 presents the results of our method . we observe that cbow - c has the best performance in terms of depth and subtraction . however , it is inferior in both generative and syntactic ways to the other two methods .
subj and sick - r models outperform all subj methods except cmow - c . cbow - r improves performance on both mr and sst2 by 3 . 8 points . however , it has the advantage of outperforming both sick and subj models in terms of recall .
system e + org and per are presented in table 3 . all org methods outperform the best state - of - the - art systems in terms of e + per and misc scores . supervised learning models perform better than all methods except for mil - nd , which shows the diminishing returns from mixing source and target labeled training data . domain name matching ( mil - nd ) shows a slight improvement over the strong performance of previous models on all org metrics . name matching and multi - task learning ( map ) achieve the best results . however , the biggest performance drop is on the org metric , which underscores the limitations of supervised learning . we notice a slight drop in performance compared to previous approaches .
2 presents the results on the test set under two settings . our system outperforms all the models except mil - nd in terms of e + p and f1 score . the results are shown in table 2 . name matching and domain learning improve the results , but do not improve the performance of the model in all three settings .
table 6 , we can see that the entailment model outperforms the original models in terms of ref , ref and ref scores , but it is inferior than g2s - gat due to the larger size of the model .
3 provides detailed results on the ldc datasets . our proposed model outperforms the previous stateof - the - art models in terms of bleu score and precision .
3 shows the results on ldc2015e86 test set when trained with additional gigaword data . our model outperforms all the models except for those using g2s - ggnn embeddings .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . the results show that the use of bilstm improves the performance of the model by a noticeable margin .
results are presented in table 4 . the first set of results show that the g2s - gin model has the best overall performance when compared to other models in terms of sentence length and sentence length . after applying the graph diameter and sentence length measures , the model performs slightly better than other models except for those usingempty embeddings . table 4 shows the results of applying the weighted average number of frames across all δ tables . note that the smaller size of the graph diameter indicates that the model has better recall ability .
8 shows the fraction of the elements in the output that are not present in the input graph that are missing in the generated sentence ( g2s - gin ) . it is clear from table 8 that the use of token lemmas helps the model to extract more elements from the output .
4 presents the performance of our method . we use word embeddings extracted from the 4th nmt encoding layer , trained with different target languages on a smaller parallel corpus ( 200k sentences ) . our approach improves the semantic performance by 10 % over the previous stateof - the - art model .
2 : pos and sem tagging accuracy with baselines and an upper bound . mft : most frequent tag ; unsupemb : most frequently tag ; word2tag : upper bound encoder - decoder .
3 shows the performance of our method compared to previous methods . our method outperforms all the other methods except for the one that we use in table 4 .
5 shows the accuracy with different layers of 4 - layer nmt encoders , averaged over all non - english target languages . we find that combining all the features boosts the accuracy , but does not improve the general performance .
performance on different datasets is shown in table 8 . the difference between the attacker score and the corresponding adversary ’ s accuracy is reported in δ . in pan16 , the average age of the target is 9 . 7 and the rate at which the attacker is mentioned is 15 . 3 .
1 shows the performance of our method when training directly towards a single task . accuracy is significantly higher than that of pan16 , indicating that the training target is already well trained .
2 shows the effect of theected attribute leakage on the balanced and unbalanced task averages . dial and sentiment both individually cause significant data splits ( see table 2 ) . sentiment also contributes significantly to the balanced task performance , but it is harder to detect instances of gender - neutral attribute leakage .
performance on different datasets with an adversarial training is shown in table 3 . the difference between the attacker score and the corresponding adversary ’ s accuracy is reported in δ . sentiment and gender are the most important factors in predicting whether an object will reach the task . detection is based on age , gender , and the ability to detect the object . detection by gender is important , but it is less important to detect whether the object is moving or not .
6 shows the performance of different encoders when embeddings are trained on the same protected attribute .
1 shows the performance of our model compared to previous work on the topic of finetuning . our model outperforms both existing and new models in terms of both performance and finetune . the results are presented in table 2 . we observe that our lstm model performs better than the previous stateof - the - art models on both the ptb and the wt2 dataset . it is clear from table 2 that the performance gain comes from a better model design and not a drop in performance from the previous model .
performance of our model compared to previous models is presented in table 4 . the results are presented in tables 4 and 5 . we observe that our model performs better than previous models in terms of both the number of iterations and the average error rate . further , our model is more accurate in both the base time and thebert time metrics .
3 shows the performance of our model compared to previous work on multiple datasets . our model outperforms all the other models in terms of err and f1 metrics . we observe that yelppolar time is comparable with our model , but is slightly better than our model . finally , we observe that our model is better than both the original lstm and our model on both datasets .
3 shows the bleu score on wmt14 english - german translation task . our model improves over previous stateof - the - art models like sru , lrn , and atr by a noticeable margin .
4 shows the performance of our model on squad dataset . it can be seen that our model performs better than other models using only one parameter : the parameter number of rnet . however , it is still inferior to other models that use more sophisticated neural networks . we notice that our lstm model is more stable and therefore requires fewer parameter drops .
6 shows the f1 score on conll - 2003 english ner task . lstm * denotes the model with the highest parameter number . it also shows the performance of the sru model when trained with the same number of parameters .
results in table 7 show that our model can easily distinguish between the performance of base + ln and test perplexity on snli task with base setting .
system evaluation results are presented in table 4 . word embeddings ( mtr ) w / oracle retrieval and word2vec topics perform best compared to human on all three systems . word - based learning methods ( r - 2 ) and mtr - 2 perform better than human on both systems , word - based learned methods are comparable in terms of performance on all systems , but do not outperform human when using the same type of learning method .
4 presents the human evaluation results on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 8 . our system outperforms all the systems except seq2seq .
3 presents the results of the models trained on the new embeddings . our model outperforms all the other models except for the one that we chose for this analysis : eurparl , ted talks , and europarl . the difference between these models is minimal but significant , with the exception of docsub .
3 presents the results of the best performing models . our model outperforms all the other models except for the one that we chose , namely , europarl . the results are summarized in table 3 .
3 presents the results of the models trained on the best performing datasets . our model outperforms all the other models except for the one that we chose , namely , europarl . the results are summarized in table 3 . we observe that our model performs slightly better than the competition on all three datasets .
3 shows the performance of our model compared to the previous best state - of - the - art models . our model achieves the best performance with a maxdepth score of 1 . 78 on the dsim metric , compared to 1 . 67 on the europarl metric . however , our model has the worst performance with maxdepth scores of 43 . 43 and 43 . 46 respectively compared to 8 . 46 and 9 . 46 on the maxdepth metric .
3 shows the performance of our model compared to the previous best state - of - the - art models . our model achieves the best performance with maxdepth and maxdepth , respectively , compared to a gap of 1 . 5 points on the dsim metric . however , our model performs slightly worse than our maxdepth metric . this is mostly due to the high number of iterations in our model , as measured by the europarl metric .
performance of our model on the validation set of visdial v1 . 0 . 0 is shown in table 1 . we use the enhanced version of our l2 model , lf + p1 , to learn question type and hidden dictionary learning . qt , r1 , r2 and r3 denote regressive loss , weighted softmax loss , and generalized ranking loss , respectively .
performance ( ndcg % ) of different ablative studies on different models is shown in table 2 . using p2 exposes the model to hidden dictionary learning and boosts rva performance . note that using p2 also improves coatt performance as well .
5 compares our hmd - f1 and wmd - prec models with soft and hard alignments . results are summarized in table 5 .
3 presents the results of our approach . the first set of metrics shows that our approach significantly improves the results for both sets . the results are comparable across all metrics , with exception of the one for burkhard - keller , whose results are slightly worse than those of ruse . the second set shows the performance improvement on both sets when trained with ruse - based metrics .
performance of these models on the training data is presented in table 2 . the bleu - 1 scores are significantly better than the baseline scores , indicating that the reliance on word embeddings is a reasonable baseline for baseline performance . however , the difference is not statistically significant , as the results are still significantly worse than those obtained using pre - trained models .
performance of the models according to these baselines is presented in table 4 . the results are summarized in bold . the most striking thing about the summaries is that they are significantly worse than the leic scores of " m2 " and " w2v " . word - mover also significantly outperforms the other baselines in terms of elmo and p scores , however , the difference between the scores of those using the original m2 and w2v dictionaries is not statistically significant , epm scores are significantly lower than those using spice , indicating that the reliance on pre - trained word embeddings leads to incorrect interpretability .
3 shows the performance of our models compared to previous stateof - the - art models . we observe that the m0 model performs better than the m1 model on all three scenarios except for the one where shen - 1 has the worst performance .
3 presents the results of our final model on the transfer quality and semantic preservation tasks . our model outperforms all the other models with a large margin on both datasets . semantic preservation tasks are qualitatively better than semantic preservation , confirming the importance of semantic preservation . yelp models perform particularly well in the semantic preservation task , with a drop of 0 . 9 % in performance compared to the previous stateof - the - art model . syntactic preservation tasks rely on word embeddings and semantic tags to maintain a high level of semantic recall . the semantic preservation approach relies on syntactic or semantic tags , hence leading to a drop in performance .
5 presents the human evaluation results . we show that our method outperforms previous approaches in terms of accuracy , completeness , and recall .
3 shows the performance of our models compared to previous stateof - the - art models . we observe that m0 + para + lang bridges the gap between performance of m1 and m2 , while also improving performance for m6 .
6 presents the results on yelp sentiment transfer . our best model achieves higher bleu than any prior work at similar levels of acc ∗ , but only slightly outperforms the best model using simple - transfer . we also observe that the use of multi - decoder reduces the performance of sentiment embeddings , but does not help the model with translation .
2 shows the number of tokens that were correctly predicted to be disfluencies . reparandum length and number of repetition tokens are the most important factors in predicting the length of utterances .
3 shows the relative frequency of rephrases correctly predicted as disfluent for each category . the number of tokens predicted to contain a content word is in parentheses , but the fraction predicted as containing a non - content word is less than the number predicted in the original embeddings .
3 presents the results of our model in terms of best and worst case . our model outperforms all the other models with a large margin . we observe that when text is raw , the model is better than the single model with text + innovations as the most important part of the model .
2 compares our model with the state - of - art algorithms on the fnc - 1 test dataset . the results are summarized in table 2 . our model achieves the best performance with an absolute accuracy of 28 . 53 % , which is slightly better than the previous best performance .
2 shows the performance of different methods on the apw and nyt datasets . the best performing method is burstysimdater , which significantly outperforms all previous methods .
3 shows the performance of our method compared to previous approaches . it is clear from table 3 that neuraldater relies on word attention and graph attention to train its model .
model performance on all stages is reported in table 1 . the best performing model is dmcnn , which performs better than trigger , embedding + t and word2vec .
3 shows the test set ' s performance on each event . our method outperforms all the other methods in terms of both event and domain identification . in all but one case , the error reduction is minimal compared to the original method . cross - event domain classification ( emty ) and event identification methods show significant performance improvement on both events .
can be seen in table 4 , all the models trained on our system are comparable in terms of performance . all except for the one that relies on word embeddings and pre - trained models is better than the other ones . moreover , the difference in performance between english - only and spanish - only is less pronounced , but still comparable with the performance of the other two models .
4 shows the results on the training set and on the test set . we use fine - tuned training with only subsets of the code - switched data . this results show that using discriminative training can improve the results for both training sets .
5 shows the performance of our system in the dev set compared to monolingual mode . it is clear from table 5 that fine - tuning has a significant impact on performance , but it does not have a significant effect on performance in the test set .
results in table 7 show that type - aggregated gaze features significantly improve recall , f1 - score and recall scores for the three eye - tracking datasets compared to the previous methods .
5 presents the performance of the combined and type - aggregated gaze features for the conll - 2003 dataset . precision ( p ) , recall ( r ) and f1 - score ( f ) are all statistically significant improvements over the baseline model ( p ≤ 0 . 05 ) .
results on wordnet 3 . 1 are shown in table 1 . type embeddings are derived from the original work ( faruqui et al . , 2015 ) . glove - retro is based on syntactic skipgram , and it learns to adapt to syntactic - sg embedding . wordnet has the best performance on the three - step ppa test set . it learns to use syntactic and semantic skipgrams , and adapts to the syntactic embedding patterns of wordnet 2 . 1 .
2 shows the performance of our system when combined with various pp attachment predictors and oracle attachments . using oracle pp as dependency parser , rbg achieves 94 . 60 % ppa acc . and 98 . 97 % pda acc . these results show that using oracle pp helps the system to interpret pp attachments better .
3 : effect of removing sense priors and context sensitivity ( attention ) from the model . the effect of removing context sensitivity is clear in table 3 .
2 : adding subtitle data and domain tuning for image caption translation ( marian amun et al . , 2018 ) improves the results for all models except for those using mscoco17 embeddings . adding domain - tuned data boosts the performance for multi30k , but to a smaller extent than incorporating subssfull data . domain - tuning improves the image caption decoding performance .
1 shows the performance of the subs1m models trained on the en - de dataset . we see that the domain - tuned models perform better than the original models on both datasets , with the exception of mscoco17 , which shows the diminishing returns from mixing multiple data types .
4 shows the bleu scores of models using automatic captions . we use the best model , mscoco17 , and en - de . adding multi30k captions improves the results for all models except for those using dual attn .
5 compares our approach against various approaches . we use transformer , multi30k + subs3mlm and detectron mask surface . results are summarized in table 5 . transformer significantly outperforms our approach , enc - gate and dec - gate have a significant advantage over enc - gate , but their performance is lower than our approach . we also observe that our approach obtains better interpretability across multiple layers .
performance of subs3m compared to subs6m is presented in table 4 . the best performances are obtained using the en - de embeddings layer , while the best performance is obtained using mscoco17 . moreover , the multi - lingual approach relies less on visual features , resulting in a better interpretability performance .
3 shows the performance of the best models using the word - forwardforwardforwardphrases . we observe that the best performing model is the en - fr - rnn - ff model , which performs slightly better than the other two models .
shown in table 1 , the number of parallel sentences in the train , test and development splits for the language pairs we used is significantly smaller than the number in the test set .
2 : training vocabularies for the english , french and spanish data used for our models . the results are summarized in table 2 .
5 shows automatic evaluation scores ( bleu and ter ) for the rev systems . it is clear from table 5 that the automatic evaluation results are superior than the bleu - based ones .
2 shows the performance of our visually supervised model compared to the naive rsa image on flickr8k . the row labeled vgs is the average rank of the models trained on a single mirrored mirrored image .
experimental results on synthetically spoken coco are shown in table 1 . our model outperforms the previous stateof - the - art models in terms of recall scores .
1 shows the results of different classifiers compared to the original on sst - 2 . for example , cnn turns in a < u > screenplay that shows the edges at the edges ; it ’ s so clever you want to hate it . table 1 shows that dan also uses the edges of the screenplay as well as the shapes in the original .
2 shows that fine - tuning has indeed increased the number of words in sst - 2 from 15 to 69 . 5 .
3 shows the sentiment change in sst - 2 from positive to negative . it is clear from table 3 that the flipped sentiment patterns are very similar across the three systems .
table 2 , we report the results of our joint study on word - phrases forpubmed and smartmed . results are presented in tables 1 and 2 . the results are summarized in table 2 . our joint study method significantly outperforms the competitive approach of sift , confirming the effectiveness of our submission . note that our approach is more than cross - validating , as the results are more consistent across all evaluation methods .
